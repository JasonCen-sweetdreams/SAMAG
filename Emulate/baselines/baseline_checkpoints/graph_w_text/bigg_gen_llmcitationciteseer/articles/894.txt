Multimodal sentiment analysis has gained significant attention in recent years, as it enables more accurate sentiment detection from diverse data sources such as text, images, and videos. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that effectively integrates multimodal features and captures complex relationships between them. Our approach leverages a modality-specific attention mechanism to selectively focus on relevant features and a hierarchical fusion strategy to combine the outputs from different modalities. Experiments on two benchmark datasets demonstrate the superior performance of our HAN model over state-of-the-art multimodal sentiment analysis approaches.