Neural architecture search (NAS) has emerged as a promising technique for automating the design of deep neural networks. However, existing methods often suffer from high computational costs and limited exploration capabilities. This paper proposes a novel graph-based reinforcement learning framework, 'GraphNAS', which utilizes graph neural networks to represent the search space and reinforcement learning to guide the search process. We demonstrate that GraphNAS achieves state-of-the-art performance on several benchmark datasets while reducing search time by up to 3x compared to existing methods.