Emotion recognition in human-computer interaction (HCI) is a challenging task due to the complexity of human emotions and the variability of expression across different modalities. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates facial expression, speech, and physiological signals to recognize emotions in HCI. Our HAN model uses multi-modal fusion and hierarchical attention to selectively focus on relevant features and modalities, outperforming state-of-the-art methods in emotion recognition accuracy and robustness. We evaluate our approach on a large-scale multimodal emotion recognition dataset, demonstrating its effectiveness in real-world HCI applications.