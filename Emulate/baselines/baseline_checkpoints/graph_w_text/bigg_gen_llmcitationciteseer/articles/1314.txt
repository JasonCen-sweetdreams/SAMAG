Autonomous navigation in partially observable environments is a challenging problem in robotics and AI research. This paper presents a novel deep reinforcement learning framework, 'DeepPOMDP', which leverages hierarchical abstraction and attention mechanisms to learn effective navigation policies in complex, dynamic environments. We demonstrate the efficacy of DeepPOMDP in a series of simulated and real-world experiments, including navigation in warehouse and office environments with varying levels of observability. Our results show significant improvements over state-of-the-art methods in terms of navigation efficiency and adaptability.