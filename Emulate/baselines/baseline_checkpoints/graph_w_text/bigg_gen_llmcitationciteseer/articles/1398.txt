Virtual assistants (VAs) are increasingly integrated into our daily lives, but their lack of emotional intelligence (EI) hinders effective human-computer interaction. This paper presents a multimodal framework for designing VAs with EI, which combines natural language processing, computer vision, and machine learning to recognize and respond to users' emotions. We conducted a user study with 50 participants, demonstrating that our EI-infused VA reduces user frustration and improves overall interaction experience. The proposed framework has implications for developing more empathetic and human-centered VAs in various applications, such as healthcare and education.