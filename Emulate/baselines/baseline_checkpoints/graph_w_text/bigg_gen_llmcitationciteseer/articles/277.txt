Coordinating multiple agents in complex, dynamic environments is a challenging problem in decentralized control. We propose a hierarchical reinforcement learning framework, 'HRL-MA', that enables agents to learn coordinated policies in a decentralized manner. HRL-MA uses a two-level hierarchy, where high-level policies focus on coordination and low-level policies focus on local control. We evaluate HRL-MA in a simulated multi-agent pursuit-evasion scenario and demonstrate improved coordination and adaptability compared to flat, single-agent reinforcement learning approaches.