Pre-trained language models have achieved state-of-the-art results in various natural language processing tasks. However, their large size and complexity hinder their deployment on resource-constrained devices. This paper presents a novel knowledge distillation approach that leverages contrastive representation learning to transfer knowledge from pre-trained language models to smaller models. Our method, called 'ContraDistill', learns to align the representations of the teacher and student models using a contrastive loss function. Experimental results on several benchmark datasets demonstrate that ContraDistill outperforms existing knowledge distillation methods, achieving up to 3% improvement in accuracy while reducing the model size by 75%.