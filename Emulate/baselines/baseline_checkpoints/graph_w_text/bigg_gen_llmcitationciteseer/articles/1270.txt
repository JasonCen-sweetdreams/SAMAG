This paper presents a novel approach to intent recognition in human-computer interaction using gaze-based interfaces. We propose a multimodal fusion framework that integrates eye-tracking data with machine learning models to accurately predict user intentions. Our approach leverages a deep neural network to learn gaze patterns and contextual cues, enabling the system to differentiate between intentional and unintentional gaze behaviors. Experimental results demonstrate significant improvements in intent recognition accuracy and user experience compared to traditional keyboard-and-mouse interfaces.