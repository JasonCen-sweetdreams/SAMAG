Deep reinforcement learning (DRL) has achieved impressive results in various applications, but hyperparameter tuning remains a major bottleneck. This paper presents a novel Bayesian optimization framework, 'DRLOPT', which leverages probabilistic modeling to efficiently search the vast hyperparameter space of DRL algorithms. By incorporating prior knowledge from domain experts and exploiting the structure of DRL problems, DRLOPT significantly reduces the number of required evaluations and improves the quality of the obtained hyperparameters. Experimental results on several Atari games demonstrate the effectiveness of DRLOPT in finding better hyperparameters than traditional methods, leading to improved policy performance and faster convergence.