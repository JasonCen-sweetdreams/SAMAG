Visual question answering (VQA) models often struggle to provide interpretable explanations for their predictions. We propose a novel hierarchical attention-based approach, called HARE, which generates explainable reasoning graphs for VQA tasks. HARE leverages a multi-level attention mechanism to selectively focus on relevant regions of the image and relevant words in the question, while simultaneously modeling the relationships between them. Our experiments on the VQA-X dataset demonstrate that HARE outperforms state-of-the-art VQA models in terms of accuracy and provides more interpretable explanations.