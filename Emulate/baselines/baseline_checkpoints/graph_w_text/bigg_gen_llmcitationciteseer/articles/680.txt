Deep neural networks have achieved state-of-the-art performance in various machine learning tasks, but their performance is heavily dependent on the choice of hyperparameters. Bayesian optimization has emerged as a powerful technique for hyperparameter tuning, but it can be computationally expensive and requires significant expertise. This paper proposes a novel approach that leverages transfer learning to adapt Bayesian optimization for efficient hyperparameter tuning. Our method, called 'BO-TL', uses pre-trained neural networks as surrogate models to reduce the number of evaluations required for Bayesian optimization. We demonstrate the effectiveness of BO-TL on several benchmark datasets, achieving significant improvements in tuning efficiency and model performance compared to existing methods.