Multi-modal dialogue systems have gained popularity in recent years, but they often rely on shallow, symbolic representations of knowledge. This paper proposes a novel knowledge graph embedding framework, 'KG-MMDE', which jointly learns vector representations of entities, relationships, and modalities. We leverage a graph attention mechanism to selectively incorporate relevant knowledge graph triples into the embedding space, allowing for more informed and context-aware dialogue responses. Experimental results on a large-scale multi-modal dialogue dataset demonstrate significant improvements in response accuracy and fluency.