Emotion recognition in human-robot interaction (HRI) is crucial for developing empathetic and socially intelligent robots. This paper proposes a hierarchical attention network (HAN) that integrates visual, acoustic, and linguistic modalities to recognize emotions in HRI scenarios. Our HAN architecture consists of modality-specific attention modules and a hierarchical fusion layer that adaptively weights modalities based on their relevance to the emotional context. We evaluate our approach on the EMOTIC dataset and demonstrate significant improvements in emotion recognition accuracy compared to state-of-the-art multimodal fusion methods.