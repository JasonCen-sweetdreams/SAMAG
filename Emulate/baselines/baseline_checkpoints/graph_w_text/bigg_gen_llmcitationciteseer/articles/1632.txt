Neural architecture search (NAS) has emerged as a promising approach to automate the design of efficient neural networks. However, existing methods often rely on expensive computations and neglect the inference cost of the discovered architectures. This paper proposes a novel NAS framework, 'EffiNAS', which learns to optimize neural network architectures for efficient inference on resource-constrained devices. We introduce a differentiable search space and a novel reward function that incorporates both accuracy and inference cost. Experimental results on several benchmarks demonstrate that EffiNAS discovers architectures with improved accuracy and reduced inference cost compared to state-of-the-art NAS methods.