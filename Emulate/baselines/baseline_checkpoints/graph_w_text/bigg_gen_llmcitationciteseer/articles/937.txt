Traditional document retrieval methods rely on bag-of-words representations, which neglect document structure and semantic relationships. This paper presents a novel document retrieval framework that leverages graph attention mechanisms to model latent semantic relationships between document entities. Our approach, dubbed 'GraphLDA', represents documents as graphs and applies attention weights to capture contextual dependencies between entities. Experimental results on the TREC-8 dataset demonstrate that GraphLDA outperforms state-of-the-art methods in terms of mean average precision and recall, particularly for long-tail queries.