This paper presents a novel gesture-based interface designed to assist people with motor impairments. Our approach leverages deep learning techniques to recognize and classify hand gestures from computer vision data. We propose a two-stage framework that first detects hand regions using a YOLO-based detector and then employs a convolutional neural network (CNN) to classify gestures. The proposed system is evaluated on a dataset of 100 participants with varying levels of motor impairment, achieving an average accuracy of 92.5%. The results demonstrate the potential of our approach to enable individuals with motor impairments to interact more effectively with digital systems.