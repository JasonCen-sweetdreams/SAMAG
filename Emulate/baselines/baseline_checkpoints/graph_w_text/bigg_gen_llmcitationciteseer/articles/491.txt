Emotion recognition from multimodal inputs remains a challenging task due to the complexity of human emotions and the variability of modalities. We propose a novel hierarchical contrastive learning framework, HiCMER, which leverages both intra-modal and inter-modal relationships to learn robust and discriminative representations. Our approach consists of a hierarchical structure that progressively refines the feature representations, and a contrastive loss function that encourages the model to capture both local and global patterns. Experiments on the CMU-MOSEI dataset demonstrate the effectiveness of HiCMER in achieving state-of-the-art performance on multimodal emotion recognition tasks.