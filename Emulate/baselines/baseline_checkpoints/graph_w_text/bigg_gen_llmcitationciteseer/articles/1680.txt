Emotion recognition is crucial for developing empathetic human-robot interaction systems. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates facial expression, speech, and physiological signals to recognize emotions in real-time. Our approach leverages self-attention mechanisms to selectively focus on relevant modalities and temporal segments, improving recognition accuracy and robustness. Experimental results on a large-scale emotion dataset demonstrate the effectiveness of our HAN model, achieving a 12.5% improvement over state-of-the-art multi-modal fusion methods.