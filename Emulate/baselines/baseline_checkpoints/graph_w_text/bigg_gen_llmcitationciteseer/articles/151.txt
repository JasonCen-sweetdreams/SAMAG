Multimodal retrieval systems struggle to effectively integrate textual and visual cues, leading to suboptimal search results. This paper proposes a novel adaptive query expansion approach, 'GraphSEM', which leverages graph-based semantic embeddings to capture the intricate relationships between query terms and multimodal features. Our method dynamically updates the query representation by incorporating contextual information from the graph, allowing for more accurate retrieval of relevant documents. Experimental results on a large-scale multimodal dataset demonstrate significant improvements in retrieval performance and robustness to query ambiguity.