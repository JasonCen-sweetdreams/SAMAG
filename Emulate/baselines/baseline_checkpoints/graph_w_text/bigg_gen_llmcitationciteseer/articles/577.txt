Cloud computing has become a ubiquitous platform for deploying large-scale applications, but efficient resource allocation remains a significant challenge. This paper presents a novel deep reinforcement learning (DRL) framework, 'CloudOptimizer', that learns to allocate resources in real-time based on workload patterns and system dynamics. Our approach combines a deep neural network with a Q-network to optimize resource utilization and minimize latency. Experimental results on a real-world cloud dataset demonstrate that CloudOptimizer outperforms state-of-the-art heuristic-based methods by up to 30% in terms of resource efficiency and 25% in terms of latency reduction.