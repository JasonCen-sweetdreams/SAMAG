Multi-task learning (MTL) has become increasingly popular in deep learning, but interpreting the relationships between tasks remains challenging. We propose Hierarchical Attention Networks (HANs), a novel MTL framework that learns to selectively share knowledge across tasks while providing interpretable attention weights. HANs consist of a hierarchical encoder and a task-specific attention mechanism, enabling the model to capture complex task relationships. We evaluate HANs on several benchmark datasets and demonstrate improved performance and explainability compared to state-of-the-art MTL methods. Our approach has significant implications for real-world applications, such as healthcare and finance, where model interpretability is crucial.