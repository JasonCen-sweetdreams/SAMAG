As virtual reality (VR) technology becomes increasingly prevalent, there is a growing need to develop more intuitive and human-centered interfaces. In this paper, we propose EyeGazeAnalytics, a novel framework that leverages machine learning and computer vision techniques to infer user intent from gaze patterns in VR. Our approach combines convolutional neural networks (CNNs) and long short-term memory (LSTM) networks to model the relationships between gaze behavior, user goals, and task context. We evaluate EyeGazeAnalytics on a large-scale dataset of VR user interactions and demonstrate significant improvements in intent recognition accuracy and latency compared to existing methods.