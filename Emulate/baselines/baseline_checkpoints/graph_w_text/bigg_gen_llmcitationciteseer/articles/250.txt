This paper presents a novel approach to real-time human emotion recognition using multimodal fusion of speech and facial features. We propose a deep neural network architecture that integrates acoustic features extracted from speech signals with facial action units detected from video frames. Our model leverages attention mechanisms to selectively weigh the importance of each modality based on the emotional intensity of the input. Experimental results on a large-scale multimodal emotion dataset demonstrate that our approach outperforms state-of-the-art unimodal and multimodal methods, achieving an accuracy of 92.5% in recognizing six basic emotions.