Social media platforms exhibit a plethora of modalities, including text, images, and videos. Effective sentiment analysis in such multi-modal environments is crucial for businesses and policymakers. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages both intra-modal and inter-modal attention mechanisms to capture complex contextual relationships across modalities. Experimental results on a large-scale, multi-modal social media dataset demonstrate that our approach outperforms state-of-the-art methods in sentiment classification, achieving an F1-score improvement of 8.2% over the best baseline.