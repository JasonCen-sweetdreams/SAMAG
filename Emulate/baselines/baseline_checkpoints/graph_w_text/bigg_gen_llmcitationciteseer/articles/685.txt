Virtual reality (VR) systems have the potential to provide immersive experiences for people with disabilities. However, existing VR interfaces often rely on manual input, which can be challenging for individuals with motor impairments. This paper presents a novel gaze-based interaction system for accessible VR, which leverages deep reinforcement learning to predict user intentions from eye movements. Our approach, 'GazeVR', uses a deep Q-network to learn a mapping between gaze patterns and desired actions in VR, eliminating the need for manual input. Experimental results demonstrate that GazeVR achieves high accuracy and user satisfaction in a range of VR tasks, including object manipulation and navigation.