This paper presents a novel hierarchical reinforcement learning (HRL) framework for efficient robot navigation in dynamic environments. The proposed approach combines a high-level task planner with a low-level motion controller, leveraging the strengths of both to adapt to changing environmental conditions. We introduce a novel curriculum learning strategy that gradually increases the complexity of the environment, allowing the agent to learn more efficiently. Experimental results in a simulated warehouse scenario demonstrate significant improvements in navigation efficiency and adaptability compared to traditional flat reinforcement learning methods.