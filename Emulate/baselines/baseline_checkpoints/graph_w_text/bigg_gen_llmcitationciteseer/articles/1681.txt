Deep neural networks (DNNs) have become ubiquitous in modern AI applications, but their computational requirements pose significant challenges for real-world deployments. This paper presents a novel batch processing framework, 'HierGPU', which leverages hierarchical GPU clustering to accelerate DNN inference. By dynamically allocating batch sizes and distributing computations across a cluster of GPUs, HierGPU minimizes memory overhead and maximizes throughput. Experimental results on popular DNN architectures demonstrate that HierGPU achieves up to 4.5x speedup over state-of-the-art batch processing techniques while maintaining accuracy.