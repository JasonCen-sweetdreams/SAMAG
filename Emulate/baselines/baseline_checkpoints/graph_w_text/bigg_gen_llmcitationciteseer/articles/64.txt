Multimodal emotion recognition (MER) has become increasingly important for human-computer interaction. However, existing approaches struggle to effectively fuse features from diverse modalities, leading to suboptimal performance. This paper introduces a novel hierarchical attention mechanism, 'HydraNet', which adaptively weighs the importance of different modalities and features at multiple levels of abstraction. Our approach enables more efficient and accurate MER by selectively focusing on the most informative inputs. Experimental results on the IEMOCAP dataset demonstrate the superiority of HydraNet over state-of-the-art models, achieving a 12.5% improvement in F1-score.