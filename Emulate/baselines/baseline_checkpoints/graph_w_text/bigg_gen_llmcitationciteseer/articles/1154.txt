Neural retrieval models have achieved state-of-the-art performance in information retrieval tasks, but often struggle with lexical mismatch between queries and documents. This paper proposes a novel query expansion approach that leverages contrastive learning to learn semantic relationships between words. Our method, CLQE, uses a contrastive objective to train a dense vector space, where semantically similar words are pulled closer together, and dissimilar words are pushed apart. We demonstrate that CLQE improves the retrieval performance of neural models, especially for queries with low term frequency, and outperforms traditional query expansion methods. Our experiments on several benchmark datasets show significant improvements in terms of nDCG and ERR metrics.