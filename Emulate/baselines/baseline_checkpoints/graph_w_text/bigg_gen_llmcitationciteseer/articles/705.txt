Explainable recommendation systems (ERS) have gained significant attention in recent years, but existing methods often rely on shallow models that fail to capture complex relationships between users and items. This paper proposes a novel knowledge graph embedding framework, 'ExplainKG', which integrates entity disentanglement and attention-based graph convolutional networks to learn interpretable representations of user-item interactions. We demonstrate the efficacy of ExplainKG on several benchmark datasets, showcasing improved recommendation accuracy and enhanced model interpretability through visualized explanations.