Understanding human emotions is crucial for developing empathetic human-computer interfaces. This paper presents EmoTract, a multimodal approach that combines electroencephalography (EEG) and computer vision (CV) to model affective states in real-time. We propose a novel fusion framework that leverages the strengths of both modalities to recognize emotional states, achieving a 15% improvement in accuracy compared to uni-modal approaches. EmoTract is evaluated on a dataset of 50 participants, demonstrating its potential for enhancing user experience in various HCI applications, such as affective gaming and virtual reality.