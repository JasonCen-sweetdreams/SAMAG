Emotion recognition from multimodal data, such as speech, text, and facial expressions, is a challenging task due to the complexity of human emotions and the varying characteristics of different modalities. This paper proposes a novel Hierarchical Attention Network (HAN) that integrates multiple attention mechanisms to selectively focus on relevant features from each modality. Our HAN architecture consists of three levels of attention: modality-level, feature-level, and decision-level. Experimental results on the IEMOCAP dataset show that our approach outperforms state-of-the-art multimodal fusion methods, achieving an average F1-score of 0.842.