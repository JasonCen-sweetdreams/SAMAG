Deep reinforcement learning (DRL) has shown promise in autonomous vehicle control, but uncertainty estimation remains a crucial challenge. This paper presents a novel DRL framework, 'UncerLane', which integrates uncertainty-awareness into the decision-making process for lane changing. We propose a Bayesian neural network architecture that outputs both the expected action and its uncertainty, allowing the agent to adjust its exploration-exploitation trade-off accordingly. Experimental results on a simulated highway environment demonstrate that UncerLane outperforms state-of-the-art DRL methods in terms of safety, efficiency, and adaptability to changing scenarios.