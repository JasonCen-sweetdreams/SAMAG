Autonomous driving requires agents to adapt to diverse scenarios and make decisions in real-time. This paper proposes a novel hierarchical reinforcement learning framework, 'HRL-AD', which leverages a deep hierarchical policy to balance exploration-exploitation trade-offs in complex environments. We introduce a curiosity-driven intrinsic reward function that encourages the agent to explore novel situations and a policy abstraction mechanism that enables efficient knowledge transfer across tasks. Experimental results on a realistic autonomous driving simulator demonstrate that HRL-AD outperforms state-of-the-art methods in terms of task completion rate and overall driving efficiency.