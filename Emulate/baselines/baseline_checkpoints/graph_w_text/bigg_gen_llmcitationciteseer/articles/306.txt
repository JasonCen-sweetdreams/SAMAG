Relevance feedback is a crucial component of interactive information retrieval systems, as it enables users to refine their queries based on the retrieved documents. However, traditional relevance feedback methods rely heavily on heuristics and often fail to capture complex user behaviors. This paper proposes a novel approach, 'DeepRF', which leverages deep reinforcement learning to optimize relevance feedback. We model the user's feedback as a Markov decision process and train a deep Q-network to learn the optimal policy for selecting the most informative documents. Experimental results on the TREC datasets demonstrate that DeepRF significantly outperforms state-of-the-art methods in terms of retrieval effectiveness and user satisfaction.