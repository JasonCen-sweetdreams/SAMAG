Sentiment analysis from multi-modal data, such as text, images, and videos, is a challenging task due to the complex interactions between modalities. This paper proposes a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and regions to accurately predict sentiment. Our HAN consists of modality-specific attention modules that adaptively weight features from each modality, followed by a hierarchical fusion mechanism that integrates information across modalities. Experimental results on the CMU-MOSI dataset demonstrate that our approach outperforms state-of-the-art methods by 3.5% in terms of sentiment prediction accuracy.