Explainability is a critical aspect of AI-driven decision support systems, as it enables trust and accountability in high-stakes applications. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that incorporates attention mechanisms at multiple levels to provide transparent and interpretable decision-making processes. We evaluate our approach on a real-world credit risk assessment dataset and demonstrate improved performance and explanation quality compared to state-of-the-art models. Our results show that HAN can effectively identify relevant features and relationships, leading to more informed decision-making and enhanced trust in AI systems.