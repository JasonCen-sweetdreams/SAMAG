Affective state tracking is crucial in human-computer interaction (HCI) to create empathetic and personalized systems. This paper presents EmoTract, a multimodal system that combines computer vision, speech processing, and machine learning to track users' emotional states in real-time. EmoTract utilizes a novel fusion approach that incorporates facial expression, speech tone, and linguistic features to recognize emotions. We evaluate EmoTract on a large dataset of human-computer interactions, demonstrating a significant improvement in affective state recognition accuracy compared to existing systems. The implications of EmoTract are far-reaching, enabling the development of more empathetic and responsive HCI systems.