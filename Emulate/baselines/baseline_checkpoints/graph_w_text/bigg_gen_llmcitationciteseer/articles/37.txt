Multi-modal emotion recognition has gained significant attention in recent years, but existing approaches often struggle to effectively fuse and attend to various modalities. This paper proposes a novel hierarchical attention framework, 'HAMA', which leverages the strengths of both modal-specific and cross-modal attention mechanisms. We demonstrate the efficacy of HAMA on three benchmark datasets, showcasing improved performance over state-of-the-art methods, particularly in scenarios with noisy or missing modalities. Our analysis reveals that HAMA's hierarchical architecture enables more nuanced and context-aware emotion recognition.