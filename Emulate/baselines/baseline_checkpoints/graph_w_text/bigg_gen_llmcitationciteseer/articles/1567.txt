Zero-shot learning (ZSL) has gained significant attention in recent years, but existing methods often struggle with scalability and computational efficiency. This paper proposes a novel hierarchical attention network (HAN) that addresses these limitations by selectively focusing on relevant semantic features and relationships. Our approach leverages a graph-based attention mechanism to model complex interactions between seen and unseen classes, enabling efficient ZSL with minimal additional training data. Experimental results on benchmark datasets demonstrate the superiority of HAN over state-of-the-art methods in terms of accuracy and computational efficiency.