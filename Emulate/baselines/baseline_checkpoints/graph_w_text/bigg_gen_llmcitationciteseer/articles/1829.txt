Emotion recognition is a crucial aspect of human-robot interaction, as it enables robots to respond empathetically and build trust with users. This paper presents a novel hierarchical attention network (HAN) for multi-modal emotion recognition, which integrates speech, facial expression, and body language cues. Our HAN model learns to focus on relevant modalities and time segments, improving emotion recognition accuracy in real-world scenarios. We evaluate our approach on a large-scale human-robot interaction dataset and demonstrate significant performance gains over state-of-the-art uni-modal and multi-modal baselines.