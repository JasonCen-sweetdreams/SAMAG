Multimodal sentiment analysis (MSA) aims to predict sentiment from heterogeneous data sources, such as text, images, and audio. However, existing approaches often rely on modality-specific models, limiting their generalizability. This paper proposes a hierarchical transfer learning framework, 'HTL-MSA', which leverages pre-trained language and vision models to learn shared representations across modalities. We introduce a novel attention-based fusion mechanism that adaptively weights modality-specific features, enabling effective sentiment prediction in MSA tasks. Experimental results on benchmark datasets demonstrate the superiority of HTL-MSA over state-of-the-art methods, with significant improvements in sentiment accuracy and robustness.