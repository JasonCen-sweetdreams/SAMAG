Autonomous vehicles rely on complex decision-making algorithms to navigate real-world scenarios. This paper presents a hierarchical reinforcement learning framework, 'HRL-AV', which integrates high-level goal-directed planning with low-level motion control. Our approach leverages a novel hierarchical abstraction of the environment, allowing the agent to focus on relevant features and ignore distractions. We demonstrate improved decision-making performance in simulated urban driving scenarios, achieving a 25% reduction in accidents and a 15% increase in route efficiency compared to state-of-the-art methods.