Multi-modal emotion recognition (MMER) is a challenging task due to the complexity of integrating and processing disparate modalities such as speech, text, and vision. This paper proposes a novel hierarchical attention network (HAN) architecture that leverages attention mechanisms to selectively focus on relevant modalities and features. Our approach adaptively weighs the contributions of each modality to improve emotion recognition accuracy. Experimental results on the IEMOCAP dataset demonstrate that our HAN model outperforms state-of-the-art MMER methods while reducing computational overhead.