Multimodal information retrieval (MIR) has become increasingly important with the growth of multimedia data. While query expansion techniques have shown promise in improving MIR effectiveness, they often neglect the contextual information of the query. In this paper, we propose a context-aware query expansion framework that incorporates both visual and textual features of the query. Our approach leverages a graph-based neural network to model the relationships between query keywords and their corresponding visual concepts. Experimental results on a benchmark dataset demonstrate that our approach significantly outperforms state-of-the-art MIR methods, especially in scenarios with ambiguous queries.