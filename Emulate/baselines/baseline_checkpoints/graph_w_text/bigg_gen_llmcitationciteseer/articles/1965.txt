This paper proposes a novel hierarchical graph attention network (HGAT) for multi-modal question answering, which integrates visual and textual features to better understand complex questions. Our HGAT model learns to selectively focus on relevant regions of the image and corresponding text snippets, thereby improving question answering accuracy. We evaluate our approach on a large-scale dataset and demonstrate state-of-the-art performance on several benchmarks, outperforming existing methods by up to 4.5%.