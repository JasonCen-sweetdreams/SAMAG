Augmented reality (AR) systems require intuitive interaction modalities to enhance user experience. This paper presents a novel gaze-based interaction approach, leveraging deep neural networks to estimate user intent. Our method, 'Gaze2Action', utilizes a convolutional neural network (CNN) to analyze eye movement patterns and predict the desired action. We also introduce a transfer learning framework that adapts the model to individual users, improving accuracy and reducing calibration time. Experimental results demonstrate that Gaze2Action outperforms existing gaze-based interaction methods in AR environments, achieving an average accuracy of 92.5%.