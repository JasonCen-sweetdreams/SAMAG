Distributed database systems have become increasingly prevalent in modern data centers, but optimizing query performance remains a significant challenge. This paper proposes a novel approach that leverages machine learning to predict optimal query execution plans. We develop a graph neural network model that learns to represent database schema and query patterns, and demonstrate its effectiveness in reducing query latency by up to 30% compared to traditional query optimization techniques. Experimental results on a real-world dataset show that our approach can adapt to varying workloads and database configurations, making it a promising solution for large-scale distributed databases.