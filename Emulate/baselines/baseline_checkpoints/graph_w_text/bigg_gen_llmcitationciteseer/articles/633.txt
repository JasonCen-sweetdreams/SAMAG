Virtual Reality (VR) has the potential to revolutionize various aspects of our lives, but its adoption is hindered by the lack of inclusive designs. This paper presents EmoTract, a novel affective state tracking system that enables VR systems to recognize and respond to users' emotional states in real-time. EmoTract combines computer vision, machine learning, and cognitive modeling to detect emotional cues from facial expressions, body language, and physiological signals. Our user study with 30 participants demonstrates that EmoTract-enhanced VR experiences lead to increased user engagement, comfort, and emotional satisfaction, particularly for users with disabilities.