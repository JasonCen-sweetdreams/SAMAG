Multimodal dialogue systems have gained popularity in various applications, including virtual assistants and human-robot interaction. However, effectively integrating and processing multimodal inputs (e.g., speech, text, vision) remains a significant challenge. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant input modalities and contextual elements during dialogue processing. Our experiments on the Multimodal Dialogue Dataset (MMD) demonstrate that HAN outperforms state-of-the-art models in dialogue act recognition, intent detection, and response generation tasks, while providing interpretable attention patterns.