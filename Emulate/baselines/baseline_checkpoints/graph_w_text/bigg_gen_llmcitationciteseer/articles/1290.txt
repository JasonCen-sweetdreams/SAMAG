Individuals with severe motor impairments rely on assistive communication interfaces, such as eye-tracking systems, to express themselves. However, these systems can be prone to errors, leading to frustration and decreased user satisfaction. This paper presents 'GazeCheck', a novel error detection framework that leverages machine learning and computer vision techniques to identify and correct gaze-based errors in real-time. GazeCheck utilizes a combination of gaze features, including fixation duration, blink rate, and pupil size, to detect anomalies and alert the user. Our evaluation with 20 participants shows that GazeCheck reduces error rates by 35% compared to existing systems, while maintaining a high degree of user satisfaction.