Context-dependent word embeddings have revolutionized natural language processing (NLP) tasks. However, most existing approaches require expensive offline computations, making them prohibitive for real-time applications. This paper presents an online learning framework, 'ConTEX', that efficiently updates context-dependent embeddings as new data arrives. ConTEX leverages a novel, streaming-friendly algorithm for incremental singular value decomposition and a hierarchical caching mechanism to minimize computational overhead. Our experiments on several benchmark NLP datasets demonstrate that ConTEX achieves competitive performance to state-of-the-art offline methods while significantly reducing computational costs and memory usage.