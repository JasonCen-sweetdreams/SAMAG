Deep neural networks often suffer from a lack of transparency, making it challenging to understand the reasoning behind their predictions. This paper introduces a novel hierarchical clustering approach, 'HCLUST-EX', to identify and visualize meaningful feature representations in deep learning models. By recursively grouping neurons with similar activation patterns, HCLUST-EX provides a hierarchical abstraction of the feature space, enabling the identification of explainable clusters and improving model interpretability. Experimental results on several benchmark datasets demonstrate the effectiveness of HCLUST-EX in revealing insightful relationships between input features and model predictions.