Individuals with dysarthria often rely on voice-controlled interfaces, but these systems can be error-prone, leading to frustration and decreased adoption. We propose a novel approach that leverages gaze-based input to detect errors in voice-controlled interfaces. Our system, 'GazeCorrect', uses machine learning models to analyze the user's gaze patterns and identify instances of incorrect voice recognition. We evaluate GazeCorrect with a group of participants with dysarthria and demonstrate a significant reduction in error rates compared to traditional voice-only interfaces. Our findings have implications for the design of more accessible and usable voice-controlled systems.