Multimodal sentiment analysis has gained significance in recent years, but existing methods often struggle with the complexity of integrating multimodal features. This paper proposes a novel hierarchical attention network (HAN) architecture that effectively captures both intra-modal and inter-modal relationships. Our HAN model employs a multimodal fusion module that adaptively weighs the importance of different modalities, followed by a sentiment analysis module that leverages hierarchical attention to focus on sentiment-bearing regions. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods in terms of accuracy and computational efficiency.