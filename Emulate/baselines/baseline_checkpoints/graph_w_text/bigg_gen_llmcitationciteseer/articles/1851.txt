Accurate affective state recognition is crucial for creating empathetic human-computer interfaces. This paper proposes EmoTract, a novel multimodal framework that fuses facial expression, speech, and physiological signals to recognize users' emotional states. We introduce a hierarchical attention mechanism that adaptively weights modalities based on their relevance to the emotional context. Our evaluations on a large, annotated dataset demonstrate EmoTract's superior performance in recognizing nuanced emotions, such as anxiety and boredom, compared to unimodal and early-fusion approaches. EmoTract has implications for developing more empathetic and personalized HCI systems.