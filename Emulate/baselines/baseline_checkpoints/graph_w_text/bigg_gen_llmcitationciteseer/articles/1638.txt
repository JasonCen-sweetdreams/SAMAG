Automatic video summarization has garnered significant attention in recent years, but most existing methods lack interpretability. This paper proposes a novel hierarchical attention network (HAN) for explainable video summarization. Our HAN architecture consists of two stages: a frame-level attention module that identifies relevant regions, and a shot-level attention module that selects informative shots. We introduce a novel attention regularization term that encourages the model to focus on diverse aspects of the video, thereby improving summary diversity and coherence. Experimental results on two benchmark datasets demonstrate the effectiveness of our approach in generating concise, informative summaries while providing insights into the summarization process.