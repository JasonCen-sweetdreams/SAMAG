As AI systems become increasingly prevalent, they are vulnerable to sophisticated adversarial attacks that can manipulate their decision-making processes. This paper proposes a novel graph neural network (GNN) based approach for detecting adversarial attacks on AI systems. Our method, 'GNN-Defense', leverages graph-based representations of AI models and their inputs to identify anomalous patterns indicative of attacks. Experimental results on a range of AI models and datasets demonstrate that GNN-Defense outperforms state-of-the-art detection methods, achieving an average detection accuracy of 95.2% across various attack scenarios.