This paper proposes a novel hierarchical attention network (HAN) for multi-modal emotion recognition in human-robot interaction. Our approach leverages facial expression, speech, and physiological signals to recognize emotions in real-time. The HAN architecture incorporates a modality attention mechanism to dynamically weigh the importance of each modality and a hierarchical attention layer to capture both local and global dependencies between modalities. Experimental results on the SEMAINE dataset demonstrate improved emotion recognition accuracy and robustness compared to state-of-the-art multi-modal fusion methods.