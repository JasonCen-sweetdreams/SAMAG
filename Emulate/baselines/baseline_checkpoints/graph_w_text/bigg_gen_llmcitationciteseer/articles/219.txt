Ad-hoc search query expansion is a crucial step to improve retrieval effectiveness in information retrieval systems. Existing methods rely on handcrafted rules or supervised learning, which can be suboptimal and require extensive labeled data. This paper proposes a novel reinforcement learning framework, 'RL-QE', which formulates query expansion as a sequential decision-making problem. By leveraging a reward function that balances precision and recall, RL-QE adaptively generates effective expansion terms and outperforms state-of-the-art methods on multiple benchmark datasets. We also investigate the impact of different reward functions and exploration strategies on the query expansion process.