Unsupervised representation learning for time series data remains a challenging task. Existing methods often rely on simple aggregation operations or hand-crafted features, which may not capture complex temporal dependencies. We propose HiCLR, a hierarchical contrastive learning framework that leverages multi-scale temporal encodings to learn robust and informative representations. By contrasting local and global patterns, HiCLR encourages the model to discover meaningful structures and relationships in the data. Experimental results on several benchmark datasets demonstrate the effectiveness of HiCLR in improving clustering, classification, and anomaly detection performance.