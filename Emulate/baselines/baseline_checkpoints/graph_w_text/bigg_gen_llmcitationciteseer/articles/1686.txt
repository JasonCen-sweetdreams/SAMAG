Sentiment analysis on multi-modal data, such as text, images, and videos, is a challenging task due to the complexity of integrating and modeling heterogeneous features. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) that learns to jointly represent and align modalities. HGAT employs a hierarchical structure to capture both local and global dependencies, and incorporates attention mechanisms to selectively focus on relevant modalities and features. Experimental results on several benchmark datasets demonstrate that HGAT outperforms state-of-the-art methods in multi-modal sentiment analysis, achieving significant improvements in accuracy and F1-score.