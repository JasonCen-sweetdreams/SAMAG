Gesture recognition is a crucial aspect of immersive virtual reality (VR) experiences. However, existing recognition systems often rely on a single modality, such as computer vision or inertial measurement units (IMUs), which can be prone to errors. This paper proposes a novel multimodal fusion framework that combines vision, IMU, and electromyography (EMG) data to recognize gestures in VR. Our approach leverages a deep neural network to fuse the modalities, achieving improved recognition accuracy and robustness to noise and variability. We evaluate our framework on a large-scale dataset of VR gestures, demonstrating significant performance gains over uni-modal approaches.