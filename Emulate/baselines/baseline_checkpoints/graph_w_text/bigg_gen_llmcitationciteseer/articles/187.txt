Recent advances in multimodal emotion recognition have leveraged transformer-based architectures to fuse and process visual and acoustic features. However, the robustness of these models to adversarial attacks remains understudied. This paper investigates the vulnerability of transformer-based multimodal emotion recognition models to targeted attacks, demonstrating that imperceptible perturbations can significantly degrade model performance. We propose a novel defense mechanism, 'Multimodal Adversarial Training', which incorporates adversarial examples into the training process to improve robustness. Experimental results on the CMU-MOSEI dataset show that our approach yields state-of-the-art performance under adversarial conditions, highlighting the importance of robustness in real-world emotion recognition applications.