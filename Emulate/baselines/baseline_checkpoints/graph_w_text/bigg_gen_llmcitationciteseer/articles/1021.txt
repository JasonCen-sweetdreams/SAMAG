Visual question answering (VQA) models often struggle to provide interpretable explanations for their predictions, particularly when requiring multi-hop reasoning. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages visual and linguistic cues to generate attention weights at multiple levels of abstraction. Our approach enables the model to focus on relevant regions of the image and corresponding question words, leading to improved performance on VQA tasks that demand complex reasoning. Experimental results on the CLEVR and GQA datasets demonstrate the effectiveness of HAN in generating accurate and explainable answers.