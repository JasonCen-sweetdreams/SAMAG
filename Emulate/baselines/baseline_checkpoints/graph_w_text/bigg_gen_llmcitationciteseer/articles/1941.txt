This paper presents EmoReact, a novel multimodal affective state recognition system that combines speech, facial expressions, and physiological signals to identify emotional states. Our approach leverages deep learning-based feature extractors and a hierarchical fusion framework to capture complex relationships between modalities. We evaluate EmoReact on a large-scale dataset of human-computer interactions, achieving state-of-the-art performance in recognizing emotions, including subtle and nuanced affective states. EmoReact has implications for developing more empathetic human-computer interfaces and enhancing user experience in various applications.