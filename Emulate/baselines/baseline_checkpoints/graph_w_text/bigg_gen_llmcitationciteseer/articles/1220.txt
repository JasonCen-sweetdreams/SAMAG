Augmented reality (AR) has transformed human-computer interaction, but existing input methods are limited. This paper presents a novel gaze-based interaction framework for AR environments, leveraging multimodal fusion of eye-tracking, head-pose, and hand-tracking data. Our approach enables intuitive and precise manipulation of virtual objects, exploiting the strengths of each modality to overcome individual limitations. We evaluate our system through a user study, demonstrating significant improvements in task completion time and user satisfaction compared to traditional controller-based interfaces.