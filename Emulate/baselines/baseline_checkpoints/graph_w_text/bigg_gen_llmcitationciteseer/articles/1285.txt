Multimodal data, which combines different data types such as images, text, and audio, has become increasingly prevalent in various applications. However, existing clustering algorithms struggle to handle such data due to the challenges of integrating heterogeneous features and scaling to large datasets. This paper proposes a novel hierarchical clustering approach, 'Deep Hierarchical Fusion' (DHF), which leverages deep embeddings to integrate multimodal features and enables efficient clustering through a scalable, distributed framework. Our experiments on several benchmark datasets demonstrate the effectiveness of DHF in discovering meaningful clusters and outperforming state-of-the-art methods.