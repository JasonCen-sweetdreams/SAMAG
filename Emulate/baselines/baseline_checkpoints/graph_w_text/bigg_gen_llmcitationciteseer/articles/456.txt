Dialogue state tracking (DST) is a crucial component of task-oriented dialogue systems. However, existing DST models often struggle to handle complex, multi-turn dialogues and lack interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages both word-level and turn-level attention to capture contextual dependencies in dialogue. We also introduce an explainability module that generates visualizations of attention weights, enabling users to understand the model's decision-making process. Experimental results on the MultiWOZ dataset demonstrate that our HAN model outperforms state-of-the-art DST models while providing insightful explanations.