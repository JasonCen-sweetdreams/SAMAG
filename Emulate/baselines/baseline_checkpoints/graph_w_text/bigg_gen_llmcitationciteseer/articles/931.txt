Coordinating multiple autonomous agents in complex, dynamic environments remains a significant challenge. This paper introduces a novel hierarchical graph reinforcement learning (HGRL) framework that enables agents to learn effective coordination strategies. By representing the agent interactions as a hierarchical graph, HGRL can capture both local and global dependencies, leading to improved coordination and adaptability. We demonstrate the effectiveness of HGRL in a simulated disaster response scenario, where agents must collaborate to rescue victims and allocate resources. Our results show that HGRL outperforms existing methods in terms of response time, resource utilization, and overall mission success rate.