Autonomous vehicles require efficient control policies to navigate complex scenarios. This paper presents a novel hierarchical reinforcement learning (HRL) framework, 'HierCtrl', which leverages a layered policy structure to balance exploration and exploitation. Our approach combines a high-level goal-directed policy with low-level, adaptive motion planners, enabling the vehicle to adapt to changing environments while minimizing compute resources. We demonstrate HierCtrl's effectiveness in simulated urban driving scenarios, showcasing improved navigation and reduced computation time compared to flat RL baselines.