Existing multimodal sentiment analysis approaches rely on complex fusion techniques, making it challenging to interpret the decision-making process. This paper introduces a hierarchical attention network (HAN) that leverages visual, acoustic, and textual features to predict sentiment scores. Our HAN model employs a novel attention mechanism that learns to weight modality-specific features based on their relevance to the sentiment prediction task. Experimental results on the CMU-MOSI dataset demonstrate that our approach outperforms state-of-the-art methods in terms of accuracy and F1-score, while providing transparent and interpretable results.