Conversational agents have become ubiquitous in various applications, but understanding human emotions in multimodal interactions remains a challenging task. This paper introduces a novel hierarchical attention network (HAN) architecture that jointly learns verbal and non-verbal cues from audio, text, and vision inputs. Our HAN model consists of modality-specific attention layers and a fusion module that adaptively weighs the importance of each modality. Experimental results on a large-scale multimodal emotion recognition dataset demonstrate the superiority of our approach over state-of-the-art methods, achieving an average F1-score improvement of 12.5%.