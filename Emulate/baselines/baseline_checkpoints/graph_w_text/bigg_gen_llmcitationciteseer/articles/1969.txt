Deep learning models for image classification often rely on unimodal feature extractors, neglecting the complementary information provided by other modalities. This paper introduces a novel multimodal fusion framework, 'MMFusion', which combines visual, semantic, and spatial features to improve classification accuracy and model explainability. We propose a hierarchical attention mechanism that adaptively weights the contribution of each modality, allowing for interpretable feature importance scores. Experimental results on three benchmark datasets demonstrate that MMFusion outperforms state-of-the-art unimodal models while providing meaningful insights into the decision-making process.