Accurate dialogue state tracking is crucial for task-oriented conversational AI systems. Existing approaches often rely on hand-crafted features or complex pipeline models, which can be brittle and computationally expensive. This paper proposes a novel graph attention network (GAT) framework for multi-modal dialogue state tracking, which integrates visual, spoken, and textual cues. Our model leverages graph-based representation learning to capture subtle dependencies between entities and context, achieving state-of-the-art performance on the DSTC2 benchmark. We also demonstrate the effectiveness of our approach in real-world conversational scenarios, showcasing its potential for deployment in human-AI interaction systems.