Voice assistants have become ubiquitous, but their interaction modalities often neglect the needs of people with disabilities. This paper presents an empathetic design approach for integrating emotional intelligence into voice assistants, focusing on users with visual impairments and autism spectrum disorder. We develop a multimodal affect recognition system that combines speech, prosody, and sentiment analysis to detect user emotions. Our user-centered evaluation with 20 participants shows that the proposed system improves the overall user experience, reduces frustration, and increases trust in voice assistants. We discuss implications for inclusive HCI design and future research directions.