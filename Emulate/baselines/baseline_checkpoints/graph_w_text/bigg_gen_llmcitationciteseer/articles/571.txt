Event extraction from multi-modal data (e.g., images, text, and videos) is a crucial task in various applications, including surveillance, healthcare, and social media analysis. This paper presents a novel hierarchical graph attention network (HGAN) that leverages the complementary information from different modalities to improve event extraction performance. Our HGAN model consists of two stages: intra-modal attention and inter-modal attention. The intra-modal attention module learns to focus on relevant regions within each modality, while the inter-modal attention module integrates the features from different modalities to capture cross-modal correlations. Experimental results on two benchmark datasets demonstrate the effectiveness of our approach in extracting complex events from multi-modal data.