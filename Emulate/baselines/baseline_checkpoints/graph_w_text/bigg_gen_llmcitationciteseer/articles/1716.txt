Virtual reality (VR) has revolutionized human-computer interaction, but the design of intuitive interfaces remains a significant challenge. This paper explores the application of embodied cognition principles to gesture-based interaction in VR. We propose a novel framework that leverages machine learning to recognize and interpret user gestures, thereby enhancing the sense of immersion and presence in VR environments. Our user study demonstrates significant improvements in user experience and task performance when compared to traditional controller-based interfaces. The results have important implications for the design of next-generation VR systems.