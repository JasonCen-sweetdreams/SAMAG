Multimodal sentiment analysis aims to identify sentiment from multiple sources such as text, images, and videos. Existing approaches often rely on early or late fusion strategies, which may not effectively capture cross-modal interactions. This paper proposes a hierarchical cross-modal attention framework, 'HCMANet', that learns to selectively focus on salient regions in each modality and align them to facilitate sentiment prediction. Experimental results on three benchmark datasets demonstrate that HCMANet outperforms state-of-the-art methods, achieving an average improvement of 4.2% in sentiment accuracy.