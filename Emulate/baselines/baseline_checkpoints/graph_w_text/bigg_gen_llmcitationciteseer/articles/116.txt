Distributed database systems have become increasingly popular due to their ability to scale and handle large amounts of data. However, optimizing queries in these systems remains a challenging task. This paper proposes a novel approach to query optimization using machine learning techniques. We develop a framework that utilizes neural networks to learn the optimization strategies from historical query data and adapt to changing workloads. Experimental results on a real-world dataset demonstrate that our approach can reduce query execution time by up to 30% compared to traditional optimization techniques.