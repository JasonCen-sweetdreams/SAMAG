Multimodal sentiment analysis has gained increasing attention in recent years, but existing approaches often struggle to effectively integrate and reason about multimodal cues. This paper proposes a novel deep hierarchical attention framework, 'MHAtt', which leverages self-attention mechanisms to jointly model intra-modal and inter-modal relationships. Our approach achieves state-of-the-art performance on three benchmark datasets, outperforming prior work by up to 5.2% in terms of sentiment classification accuracy. We also provide visualizations and ablation studies to demonstrate the effectiveness of our approach in capturing nuanced multimodal interactions.