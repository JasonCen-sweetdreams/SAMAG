Sentiment analysis in social media is a challenging task due to the multimodal nature of online content. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates visual and textual features to analyze sentiment in social media posts. Our approach employs a stacked attention mechanism to model complex interactions between modalities, enabling the network to selectively focus on relevant regions of the input data. Experimental results on a large-scale multimodal dataset demonstrate that HAN outperforms state-of-the-art methods in sentiment classification, particularly in cases where textual and visual cues are conflicting.