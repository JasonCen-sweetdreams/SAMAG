Gesture recognition systems have become pervasive in HCI applications, but their performance often degrades for users with motor disabilities. This paper presents a novel approach to designing inclusive gesture recognition systems that accommodate users with varying motor abilities. We propose a multi-modal fusion framework that combines computer vision, machine learning, and electromyography (EMG) signals to recognize gestures. Our approach is evaluated on a dataset of 20 users with motor disabilities, demonstrating improved recognition accuracy and reduced latency compared to existing systems.