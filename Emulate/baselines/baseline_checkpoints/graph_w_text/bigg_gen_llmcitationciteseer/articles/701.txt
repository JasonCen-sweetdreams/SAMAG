Emotion recognition is a crucial aspect of human-computer interaction (HCI), enabling systems to respond empathetically to users. This paper presents EmoReact, a multimodal emotion recognition system that fuses facial expressions, speech tone, and physiological signals to recognize emotions. We propose a novel attention-based fusion mechanism that adaptively weighs the modalities based on their reliability and emotional relevance. Our evaluations on a large, publicly available dataset demonstrate that EmoReact achieves state-of-the-art emotion recognition accuracy, outperforming unimodal and fusion-based baselines. We also explore the applicability of EmoReact in a real-world HCI scenario, showcasing its potential to enhance user experience.