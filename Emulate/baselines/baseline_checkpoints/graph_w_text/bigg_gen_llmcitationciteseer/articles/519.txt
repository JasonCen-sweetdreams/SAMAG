Human-robot collaboration (HRC) demands effective multimodal fusion to comprehend human intent and adapt to dynamic environments. This paper presents a novel hierarchical attention network (HAN) for HRC, which integrates visual, auditory, and linguistic cues to enable explainable decision-making. Our HAN model comprises modular attention mechanisms that selectively weight and combine modalities, facilitating interpretable fusion and improved task performance. We demonstrate the efficacy of our approach on a real-world HRC dataset, showcasing enhanced collaboration and trust between humans and robots.