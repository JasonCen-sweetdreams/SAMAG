Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a challenging task due to the inherent heterogeneity of data sources. This paper introduces a novel Hierarchical Graph Attention Network (HGAT) framework that integrates graph-based representation learning with attention mechanisms to effectively capture complex relationships between modalities. We demonstrate the superiority of HGAT over state-of-the-art methods on two benchmark datasets, achieving improvements of up to 12.5% in overall emotion recognition accuracy. Our ablation studies highlight the importance of hierarchical attention in modeling intra- and inter-modal interactions.