Virtual reality (VR) systems can benefit from gaze-based interaction, but current approaches are limited by high computational costs and inaccurate gaze tracking. This paper presents a novel foveated rendering technique that leverages visual attention models to optimize rendering resources. We conducted a user study to evaluate the effectiveness of our approach, which combines machine learning-based gaze prediction with a foveated rendering pipeline. Results show a significant reduction in computational overhead while maintaining user experience quality. Our approach enables efficient and accurate gaze-based interaction for VR applications.