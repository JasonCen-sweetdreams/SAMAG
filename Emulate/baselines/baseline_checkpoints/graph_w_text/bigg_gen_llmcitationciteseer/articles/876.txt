Multi-modal medical image analysis has become increasingly important for accurate disease diagnosis. However, the fusion of heterogeneous data sources (e.g., MRI, CT, and PET scans) remains a significant challenge. This paper proposes a novel hierarchical attention network (HAN) architecture that learns to adaptively weigh and combine features from different modalities. Our approach leverages self-attention mechanisms to capture complex relationships between modalities and utilizes a hierarchical fusion strategy to reduce computational overhead. Experimental results on a large-scale brain tumor segmentation dataset demonstrate that HAN outperforms state-of-the-art fusion methods, achieving a 12.5% improvement in Dice similarity coefficient.