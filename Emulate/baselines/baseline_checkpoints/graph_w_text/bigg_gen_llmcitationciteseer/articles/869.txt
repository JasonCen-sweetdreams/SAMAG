As AI systems become increasingly pervasive, there is a growing need to understand their decision-making processes. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) architecture for explainable AI decision-making. HGAT leverages graph attention mechanisms to identify relevant nodes and edges in complex graph-structured data, and hierarchically aggregates attention weights to generate interpretable explanations for AI-driven decisions. We evaluate HGAT on a suite of benchmarks and demonstrate its effectiveness in providing transparent and trustworthy AI decision-making in various applications.