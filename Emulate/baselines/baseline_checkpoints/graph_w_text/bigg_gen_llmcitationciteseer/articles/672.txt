Deep neural networks have achieved state-of-the-art performance in various machine learning tasks, but their complexity makes hyperparameter tuning a daunting task. Bayesian optimization with Gaussian processes has emerged as a promising approach, but its computational cost hinders its applicability to large datasets. This paper proposes an efficient Bayesian optimization method, 'GP-BO+', which leverages a surrogate model to approximate the objective function and adaptively selects the most informative hyperparameter configurations. We demonstrate the effectiveness of GP-BO+ on several benchmark datasets, showing significant speedups over existing methods while maintaining competitive performance.