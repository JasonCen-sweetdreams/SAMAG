Emotion recognition in conversational AI is a challenging task due to the complexity of human emotions and the multimodal nature of human communication. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) framework that jointly models auditory, visual, and textual features from conversational data. Our approach leverages graph attention mechanisms to capture contextual relationships between utterances and modalities, and hierarchical graph pooling to aggregate features at multiple scales. Experimental results on a large-scale multimodal dataset demonstrate that HGAT outperforms state-of-the-art methods in emotion recognition accuracy and robustness to noisy data.