Accurate affective state recognition is crucial in human-computer interaction. This paper presents EmoTract, a multimodal framework that integrates physiological signals (EEG, ECG, and skin conductance) with behavioral cues (facial expressions and body language) to recognize emotions. We propose a novel fusion technique that leverages the strengths of each modality, achieving improved recognition accuracy and robustness. Our approach is evaluated on a diverse dataset of 200 participants, demonstrating superior performance compared to unimodal and traditional multimodal approaches. EmoTract has implications for affective computing, mental health monitoring, and personalized human-computer interaction.