Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a challenging task due to the inherent complexity and variability of human emotions. This paper proposes a novel Hierarchical Attention Network (HAN) that selectively focuses on relevant modalities and features to improve emotion recognition accuracy and explainability. Our HAN model consists of three levels of attention: modality-level, feature-level, and instance-level, allowing it to adaptively weigh the importance of different modalities and features for each instance. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods in terms of emotion recognition accuracy and provides meaningful explanations for its predictions.