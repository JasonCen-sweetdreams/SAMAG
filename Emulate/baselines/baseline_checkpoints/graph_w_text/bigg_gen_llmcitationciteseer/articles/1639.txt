Self-supervised representation learning has shown promising results in various computer vision tasks. However, the robustness of these models against adversarial attacks remains largely unexplored. This paper presents a comprehensive analysis of the robustness of state-of-the-art self-supervised methods, including contrastive learning and generative models, against various types of adversarial attacks. Our experiments reveal that these models are vulnerable to attacks, leading to significant performance degradation. We further propose a novel defense strategy, 'AdvReg', which incorporates adversarial training into the self-supervised learning pipeline, resulting in improved robustness and generalizability.