Pre-trained language models have achieved state-of-the-art results in various natural language processing tasks. However, their performance degrades significantly when fine-tuning on low-resource languages. This paper proposes a novel meta-learning approach, 'MetaTune', which enables robust transfer learning for low-resource language models. By learning to adapt to new languages and tasks, MetaTune improves the model's ability to generalize across languages and achieves better performance on downstream tasks. We evaluate MetaTune on several low-resource languages and demonstrate its effectiveness in improving language model performance.