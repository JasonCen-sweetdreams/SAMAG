Virtual reality (VR) head-mounted displays often rely on manual controllers for interaction, which can be cumbersome and detract from immersion. This paper presents a machine learning-based approach to predictive eye movement, enabling gaze-based interaction in VR. We propose a novel deep learning model that integrates eye tracking data with scene semantics to predict user intent. Our approach achieves a significant reduction in interaction latency and improves overall user experience. We evaluate our method using a custom-built VR platform and demonstrate its applicability to various interactive tasks, including object selection and manipulation.