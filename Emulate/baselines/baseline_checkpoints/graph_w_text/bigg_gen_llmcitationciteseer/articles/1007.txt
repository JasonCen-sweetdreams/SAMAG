This paper introduces a novel approach to coordinated exploration in multi-agent systems, leveraging deep graph reinforcement learning. We propose a Graph Attention Network (GAT) based architecture that learns to represent agents' observations and actions as nodes in a graph, enabling the agents to share knowledge and coordinate their exploration strategies. Our approach is evaluated in a suite of multi-agent environments, demonstrating improved exploration efficiency and task performance compared to traditional single-agent exploration methods. We also provide theoretical guarantees on the convergence of our algorithm to an optimal joint policy.