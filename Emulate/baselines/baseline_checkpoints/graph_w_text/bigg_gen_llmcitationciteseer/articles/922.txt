Emotion recognition from multi-modal inputs (e.g., speech, text, facial expressions) is a challenging task in AI-powered human-computer interaction. This paper proposes a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and features, enabling more accurate and explainable emotion recognition. We introduce a novel attention mechanism that incorporates domain knowledge from psychology and cognitive science. Experimental results on the benchmark IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art methods in terms of recognition accuracy and interpretability.