Multimodal emotion recognition tasks often rely on fusion techniques that combine features from different modalities, such as audio, video, and text. However, these approaches can be limited by the quality of the features and the complexity of the fusion process. This paper proposes a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features at multiple levels of abstraction. Our experiments on the IEMOCAP dataset demonstrate that HAN outperforms state-of-the-art multimodal fusion methods, achieving an average F1-score improvement of 12.5% across six emotions.