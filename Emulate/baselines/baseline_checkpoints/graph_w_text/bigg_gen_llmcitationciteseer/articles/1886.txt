As multi-agent reinforcement learning (MARL) systems become increasingly prevalent in real-world applications, there is a growing need for transparent and interpretable decision-making processes. This paper proposes a novel attention mechanism, 'AttrAction', which enables explainable policy learning in cooperative MARL settings. By incorporating attention weights into the policy update rules, AttrAction provides insights into the interactions between agents and their environment. We evaluate AttrAction on a range of MARL benchmarks, demonstrating improved transparency and performance compared to state-of-the-art methods.