Autonomous vehicle platooning has the potential to significantly improve traffic efficiency and safety. This paper presents a decentralized cooperative multi-agent reinforcement learning (DCMRL) framework for platooning, where each vehicle learns to coordinate with its neighbors to achieve optimal speed and distance control. Our approach leverages a novel combination of graph neural networks and decentralized Q-learning to enable real-time decision-making in a distributed manner. Experimental results demonstrate that DCMRL outperforms traditional centralized and rule-based approaches in terms of fuel efficiency, stability, and adaptability to changing traffic conditions.