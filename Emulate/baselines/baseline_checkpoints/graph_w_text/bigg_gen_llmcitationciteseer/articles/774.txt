Visual question answering (VQA) models often rely on complex neural architectures, making it challenging to interpret their decision-making process. We propose a novel hierarchical attention network (HAN) that incorporates both visual and linguistic attention mechanisms. Our approach enables the model to focus on relevant regions of the image and question phrases, providing insights into its reasoning process. Experimental results on the VQA 2.0 dataset demonstrate that our HAN model achieves state-of-the-art performance while offering improved explainability through visualizations of attention weights.