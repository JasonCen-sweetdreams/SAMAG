Voice assistants have become ubiquitous, but their utility is hindered for non-native speakers due to difficulties in pronunciation recognition. This paper explores the design of inclusive voice assistants by developing a personalized pronunciation modeling approach tailored to individual non-native speakers' linguistic backgrounds. We propose a novel deep learning architecture that integrates phonetic, lexical, and prosodic features to improve pronunciation recognition. Our user study with 50 non-native speakers demonstrates significant improvements in recognition accuracy and user satisfaction, highlighting the potential for more inclusive voice assistants.