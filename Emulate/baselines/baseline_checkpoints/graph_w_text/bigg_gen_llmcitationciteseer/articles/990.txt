Multimodal sentiment analysis (MSA) aims to predict sentiment from heterogeneous data sources, such as text, images, and audio. However, real-world datasets often contain noisy labels, which can significantly degrade the performance of deep learning models. This paper proposes a robust MSA framework, 'NoiseShield', that leverages meta-learning and attention mechanisms to mitigate the impact of noisy labels. We introduce a novel label cleaning strategy that adaptively selects reliable samples for training, and demonstrate the effectiveness of NoiseShield on three benchmark datasets.