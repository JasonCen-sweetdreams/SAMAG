Virtual reality (VR) has transformed the way we interact with digital information, but current interfaces often neglect the emotional aspects of human communication. This paper presents EmoTouch, a novel affective gesture recognition system that leverages multimodal fusion of hand tracking, electromyography (EMG), and computer vision. Our approach enables accurate recognition of users' emotional states (e.g., excitement, frustration) in VR environments, enhancing the overall user experience. We evaluate EmoTouch on a dataset of 30 participants, achieving a recognition accuracy of 87.4% and demonstrating its potential for applications in gaming, education, and therapy.