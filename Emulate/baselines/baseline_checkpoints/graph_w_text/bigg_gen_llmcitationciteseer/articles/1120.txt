Multi-modal image classification tasks often involve processing large volumes of data from diverse sources. This paper proposes a novel tree-structured neural network (TNN) architecture, 'ModalTree', which adaptively selects and integrates feature representations from multiple modalities. We introduce a hierarchical attention mechanism that learns to weight the importance of each modality and its corresponding feature extractors. Experimental results on a benchmark dataset demonstrate that ModalTree outperforms state-of-the-art fusion methods while reducing computational costs by up to 40%. We further analyze the interpretability of our approach, highlighting the efficacy of TNNs in identifying informative modalities for image classification.