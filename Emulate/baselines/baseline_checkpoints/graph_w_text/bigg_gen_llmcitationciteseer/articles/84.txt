Accurate emotional state inference is crucial for empathetic human-computer interaction. This paper proposes a novel multimodal fusion framework, 'EmoFuse', which integrates facial expressions, speech patterns, and physiological signals to infer users' emotional states. We design a hierarchical attention mechanism to weigh the contributions of each modality and adapt to individual differences. Our evaluation on a large, diverse dataset shows that EmoFuse outperforms state-of-the-art unimodal and multimodal approaches, achieving a 15% improvement in emotional state recognition accuracy and a 20% reduction in inference latency.