Despite the success of gradient boosting machines in tabular data analysis, they often struggle to capture complex interactions between features. This paper proposes a novel hierarchical attention mechanism that selectively focuses on relevant feature subsets during gradient boosting. Our approach, 'HierAttnGB', adaptively learns attention weights at multiple scales, enabling the model to capture both local and global patterns. We demonstrate the effectiveness of HierAttnGB on several benchmark datasets, achieving state-of-the-art performance in terms of mean squared error and feature importance interpretability.