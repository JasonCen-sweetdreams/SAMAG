Voice assistants have become ubiquitous in smart homes, but their interfaces often exclude users with disabilities. This paper presents 'AccessibleVoice', a multimodal interaction framework that enables users with disabilities to interact with voice assistants using alternative input methods, such as gaze-based, gesture-based, or text-based inputs. We propose a novel interaction model that combines machine learning-based intent recognition with rule-based error handling to improve the accuracy and robustness of the system. A user study with 30 participants with disabilities demonstrates that AccessibleVoice significantly enhances user experience and satisfaction compared to traditional voice-only interfaces.