Distributed database systems are increasingly used to support large-scale data analytics. However, optimizing query performance in these systems remains a significant challenge. This paper proposes a novel adaptive join ordering algorithm, 'AJO', which dynamically adjusts the join order based on runtime statistics and data distribution. AJO leverages a combination of machine learning-based prediction models and cost-based optimization to minimize query latency. Experimental results on a real-world benchmark dataset show that AJO outperforms state-of-the-art query optimization techniques by up to 30% in terms of query execution time.