Multimodal information retrieval (MMIR) systems struggle to bridge the semantic gap between text and visual features. We propose a novel query expansion approach, 'NeuroEmb', which leverages neural embeddings to capture the semantic relationships between query terms and relevant visual concepts. Our method integrates a visual-semantic embedding space with a neural ranking model, enabling the system to generate more effective query expansions. Experimental results on the WikipediaMM dataset demonstrate significant improvements in retrieval performance, especially for queries with ambiguous or abstract concepts.