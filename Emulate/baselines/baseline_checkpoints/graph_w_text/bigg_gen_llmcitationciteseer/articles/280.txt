Multimodal emotion recognition (MER) systems must process and integrate features from various sources, such as speech, vision, and text. This paper presents a novel hierarchical attention network (HAN) architecture that selectively focuses on relevant modalities and regions within each modality to improve MER performance. Our HAN model comprises two stages: an intra-modality attention module that captures local patterns within each modality, and an inter-modality attention module that adaptively weights the modalities to form a unified representation. Experiments on the IEMOCAP and CMU-MOSEI datasets demonstrate that our approach outperforms state-of-the-art MER systems in terms of both accuracy and computational efficiency.