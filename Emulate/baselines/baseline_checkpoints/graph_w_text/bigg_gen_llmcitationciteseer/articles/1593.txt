Query expansion is a crucial step in text retrieval, but existing methods often rely on heuristics or manual feature engineering. This paper proposes a neural embedding-based approach, 'NEQE', which learns to expand queries by modeling the semantic relationships between words. Our method leverages a pre-trained language model to generate contextualized embeddings, which are then used to compute query-term similarities. Experimental results on several benchmark datasets demonstrate that NEQE outperforms state-of-the-art query expansion methods, achieving significant improvements in retrieval effectiveness. We also show that NEQE is robust to varying query lengths and document collections.