Conversational agents require accurate emotion recognition to provide empathetic and personalized responses. This paper introduces HANER, a hierarchical attention network that integrates acoustic, linguistic, and visual features to recognize emotions in multi-modal conversations. HANER employs a dynamic attention mechanism that adaptively weights the importance of each modality based on the conversation context. Experimental results on the IEMOCAP and CMU-MOSEI datasets demonstrate that HANER outperforms state-of-the-art methods in emotion recognition accuracy, especially in ambiguous or noisy conversations.