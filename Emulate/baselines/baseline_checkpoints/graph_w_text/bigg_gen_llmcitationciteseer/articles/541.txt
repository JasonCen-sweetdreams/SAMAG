Emotion recognition in conversational agents is a crucial aspect of human-computer interaction. Existing approaches typically rely on a single modality, such as speech or text, and struggle to generalize across diverse contexts. We propose a novel Hierarchical Attention Network (HAN) that integrates multi-modal features from speech, text, and visual cues. Our HAN model employs a hierarchical structure to capture both local and global dependencies across modalities, enabling more accurate emotion recognition. Experimental results on the IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art methods, achieving a 12.5% improvement in F1-score.