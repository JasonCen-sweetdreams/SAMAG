This paper addresses the problem of distributed resource allocation in multi-agent systems, where agents must coordinate to maximize global utility. We propose a hierarchical reinforcement learning framework, 'HRL-MA', which enables agents to learn both local and global policies. The framework consists of two layers: a lower layer where agents learn to allocate resources based on local observations, and an upper layer where agents coordinate to share resources and optimize global utility. Experimental results on a simulated grid network demonstrate that HRL-MA outperforms traditional methods in terms of resource utilization and overall system performance.