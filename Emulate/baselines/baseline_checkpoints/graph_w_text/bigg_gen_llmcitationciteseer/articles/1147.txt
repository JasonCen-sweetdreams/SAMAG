Multimodal sentiment analysis has gained increasing attention in recent years, leveraging the complementary information from text, image, and audio inputs. However, most existing approaches treat each modality separately or rely on early fusion, neglecting the complex interactions between them. In this paper, we propose a Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and their interactions. Our HAN model consists of two stages: intra-modal attention and inter-modal attention, enabling the capture of both local and global dependencies. Experimental results on the CMU-MOSI and ICT-MMM datasets demonstrate the superiority of our approach over state-of-the-art methods, achieving a 5.3% improvement in sentiment prediction accuracy.