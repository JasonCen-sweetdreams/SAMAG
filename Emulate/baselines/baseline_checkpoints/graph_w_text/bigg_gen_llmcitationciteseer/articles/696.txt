Assisted living technologies have transformed the way we support individuals with disabilities. However, existing systems often rely on explicit user input, which can be cognitively demanding. This paper proposes a novel gaze-based intention recognition system, 'GazeIntend', that leverages deep learning to infer user goals from eye movement patterns. Our approach combines convolutional neural networks (CNNs) for feature extraction with recurrent neural networks (RNNs) for intention modeling. We evaluate GazeIntend on a dataset of 20 participants with mixed abilities, achieving an average accuracy of 87.4% in recognizing user intentions. The results demonstrate the potential of gaze-based interfaces for enhancing autonomy and independence in assisted living environments.