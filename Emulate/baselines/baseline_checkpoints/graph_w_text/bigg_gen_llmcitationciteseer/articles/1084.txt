Emotion recognition in human-computer interaction (HCI) is a crucial task for developing empathetic interfaces. This paper presents a novel hierarchical attention network (HAN) architecture that leverages multi-modal data from facial expressions, speech, and physiological signals to recognize emotions. Our HAN model learns to selectively focus on relevant modalities and features, achieving state-of-the-art performance on the EmoReact dataset. We also conduct an extensive ablation study to demonstrate the effectiveness of our approach in handling noisy or missing data. The proposed framework has promising applications in affective computing, virtual assistants, and mental health monitoring.