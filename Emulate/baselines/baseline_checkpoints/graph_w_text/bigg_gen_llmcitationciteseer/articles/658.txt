Emotion recognition in conversational dialogue is a challenging task due to the complexity of human emotions and the variability of expression across different modalities. This paper proposes a novel hierarchical attention network (HAN) that integrates acoustic, linguistic, and visual features to recognize emotions in multi-modal dialogue. Our HAN model employs a multi-level attention mechanism to selectively focus on relevant modalities and time segments, leading to improved emotion recognition accuracy. Experimental results on the IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from conversational dialogue.