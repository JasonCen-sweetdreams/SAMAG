Emotion recognition from multimodal cues is a challenging task, particularly when dealing with real-world datasets exhibiting diverse emotional expressions. This paper proposes a novel Hierarchical Attention Network (HAN) that facilitates explainable emotion recognition by learning to focus on relevant modalities and temporal segments. Our approach integrates a hierarchical attention mechanism with a multimodal fusion module, enabling the model to selectively weigh and combine features from speech, text, and visual cues. Experimental results on the IEMOCAP and SEMAINE datasets demonstrate that HAN outperforms state-of-the-art methods in terms of emotion recognition accuracy and provide insights into the decision-making process.