Main-memory database systems have revolutionized real-time analytics by providing low-latency query execution. However, optimizing query performance in these systems remains a challenging task. This paper presents a novel query optimization framework, 'RTOpt', which leverages machine learning-based cardinality estimation and cost modeling to identify the most efficient query plans. Our approach incorporates a feedback loop that continuously refines the optimization process based on runtime query statistics. Experimental results on a popular real-time analytics benchmark demonstrate that RTOpt outperforms state-of-the-art query optimizers by up to 3.5x in terms of query response time.