Multi-modal emotion recognition has gained significant attention in recent years, but existing approaches often suffer from high computational complexity and limited interpretability. This paper proposes a novel hierarchical attention-based neural network (HAN) framework for efficient multi-modal emotion recognition. Our approach leverages self-attention mechanisms to selectively focus on relevant modalities and regions, reducing computational overhead while improving recognition accuracy. Experimental results on multiple benchmark datasets demonstrate the superiority of HAN over state-of-the-art methods, achieving an average improvement of 12.5% in F1-score with 30% reduction in inference time.