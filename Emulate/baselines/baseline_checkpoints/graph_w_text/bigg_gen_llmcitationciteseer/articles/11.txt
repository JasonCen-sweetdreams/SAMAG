Emotion recognition from multi-modal inputs, such as speech, text, and vision, is a challenging task in AI. We propose a novel Hierarchical Attention Network (HAN) that leverages the strengths of each modality to recognize emotions in a more interpretable and accurate manner. Our HAN model consists of modality-specific attention modules that learn to focus on relevant features, followed by a hierarchical fusion mechanism that combines the outputs. We evaluate our approach on three benchmark datasets and demonstrate significant improvements over state-of-the-art methods. Our ablation studies and visualizations provide insights into the decision-making process of the model, making it more transparent and trustworthy.