Sentiment analysis has become increasingly important in understanding user opinions and preferences. However, existing methods often struggle to handle multi-modal data, where text, images, and audio are intertwined. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) architecture that effectively integrates and weighs the contributions of different modalities. HGAT uses a hierarchical fusion mechanism to learn modality-specific and shared representations, and incorporates graph attention to model relationships between modalities. Experimental results on a newly curated multi-modal dataset demonstrate the superiority of HGAT over state-of-the-art methods, achieving a 12% improvement in sentiment classification accuracy.