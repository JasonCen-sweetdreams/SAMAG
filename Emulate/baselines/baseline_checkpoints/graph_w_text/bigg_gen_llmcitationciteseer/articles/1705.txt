Emotion recognition from multi-modal data (e.g., speech, text, vision) is a challenging task, especially when requiring explainability. We propose a hierarchical attention network (HAN) that leverages both intra-modal and inter-modal relationships to improve emotion recognition accuracy. Our HAN architecture consists of modality-specific attention modules and a fusion layer that adaptively weights the modalities. We evaluate our approach on the CMU-MOSEI dataset and demonstrate significant improvements over state-of-the-art methods. Notably, our model provides interpretable attention weights, enabling explanation of the decision-making process.