Multi-modal representation learning has emerged as a crucial aspect of computer vision, enabling models to jointly process and correlate disparate data sources. However, existing methods often struggle to effectively integrate and cluster features from diverse modalities. This paper introduces a novel hierarchical clustering framework, 'ModalTree', which adaptively partitions and merges features from individual modalities to form a unified, hierarchical representation. Our experiments on several benchmark datasets demonstrate that ModalTree outperforms state-of-the-art methods in terms of clustering quality, feature correlation, and downstream task performance.