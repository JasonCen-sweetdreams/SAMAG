In many real-world applications, machine learning models face non-stationary environments, where the underlying data distribution changes over time. This paper proposes an efficient online learning algorithm, 'AdaReg', that leverages adaptive regularization to tackle concept drift. By dynamically adjusting the regularization strength based on the local error landscape, AdaReg achieves improved stability and adaptability in non-stationary settings. We provide theoretical guarantees for AdaReg's convergence and demonstrate its effectiveness on several benchmarks, including weather forecasting and traffic prediction.