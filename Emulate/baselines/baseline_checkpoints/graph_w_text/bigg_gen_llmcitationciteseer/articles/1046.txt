Multimodal information retrieval (MMIR) systems are increasingly important for effective search and filtering of multimedia content. This paper proposes a novel neural ranking model, 'MMRank', which combines visual and textual features to improve retrieval performance. We introduce a multimodal fusion layer that adaptively weighs and combines the output of separate visual and textual encoders, allowing the model to capture complex relationships between modalities. Experimental results on a large-scale MMIR benchmark dataset demonstrate that MMRank outperforms state-of-the-art models while requiring fewer parameters and computations.