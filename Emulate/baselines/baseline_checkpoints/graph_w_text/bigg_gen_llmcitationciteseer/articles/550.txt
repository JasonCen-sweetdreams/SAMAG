Deep reinforcement learning (DRL) has achieved significant success in various applications, but its vulnerability to adversarial attacks poses a major concern. This paper proposes a novel approach to detect adversarial attacks on DRL policies using graph neural networks (GNNs). We design a GNN-based architecture that learns to represent the policy's decision-making process as a graph and identifies anomalies indicative of adversarial attacks. Our experiments on common DRL benchmarks demonstrate the effectiveness of our approach in detecting various types of attacks, including poisoning and evasion attacks, with high accuracy and robustness.