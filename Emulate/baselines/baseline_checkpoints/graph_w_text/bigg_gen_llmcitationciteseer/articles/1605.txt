Recent advancements in transformers have revolutionized natural language processing (NLP). However, their robustness against adversarial attacks remains a significant concern. This paper investigates the vulnerability of transformer-based models to targeted attacks on the input text. We propose a novel method, 'AdvGrid', which leverages a grid-based search to identify the most critical input features and craft perturbations that maximize the attack's impact. Our experiments on popular NLP benchmarks demonstrate that AdvGrid outperforms existing attack methods, highlighting the need for more robust transformer architectures and defense strategies.