Multi-modal learning has become increasingly important in computer vision tasks, where models need to process and integrate information from disparate sources such as images, text, and audio. However, existing approaches often suffer from high computational complexity and limited scalability. This paper proposes a novel hierarchical attention network (HAN) that leverages self-attention mechanisms to selectively focus on relevant modalities and features. Our HAN model achieves state-of-the-art performance on several benchmarks, including the multimodal sentiment analysis and visual question answering tasks, while reducing computational overhead by up to 40% compared to existing methods.