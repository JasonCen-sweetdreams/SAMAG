Deep reinforcement learning (DRL) has achieved remarkable success in various domains, but the lack of transparency in learned policies hinders trust and understanding. This paper introduces 'RL-Explain', a novel framework that leverages model-agnostic explanations to provide insights into DRL policies. We propose a hierarchical attribution method that decomposes policy decisions into feature importance and temporal relevance, enabling the identification of critical state features and action sequences. Our experiments on Atari games and robotic control tasks demonstrate that RL-Explain improves policy interpretability without sacrificing performance.