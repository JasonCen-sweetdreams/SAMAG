In multi-agent systems, decision-making often relies on complex interactions between agents. While deep learning models can learn effective policies, they typically lack transparency and interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that enables explainable decision-making in multi-agent environments. By incorporating attention mechanisms at both the agent and system levels, HANs can identify influential agents and highlight key interactions that drive decision-making. Experimental results on a simulated traffic control scenario demonstrate improved policy performance and enhanced interpretability compared to state-of-the-art methods.