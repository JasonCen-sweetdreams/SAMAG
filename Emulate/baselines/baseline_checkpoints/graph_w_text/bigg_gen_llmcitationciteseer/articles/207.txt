In multi-agent reinforcement learning, coordinating exploration among agents is crucial to achieve efficient learning and avoid redundant exploration. This paper proposes a novel graph-based communication framework, Graph-CoEx, which enables agents to share their exploration experiences and adapt their exploration strategies accordingly. We introduce a graph neural network-based communication module that learns to represent the exploration landscape and facilitates coordination among agents. Experimental results on several multi-agent environments demonstrate that Graph-CoEx significantly improves the learning efficiency and stability compared to existing methods.