Autonomous vehicles rely on reinforcement learning (RL) to learn control policies from interactions with their environment. However, existing methods often neglect uncertainty in the policy, leading to overconfidence and suboptimal decisions. We propose a novel uncertainty-aware RL framework, 'UARL', that incorporates Bayesian neural networks to model epistemic uncertainty in the policy. We demonstrate that UARL improves safety and robustness in simulated autonomous driving scenarios, particularly in situations with incomplete or noisy sensor data. Our results show that UARL outperforms state-of-the-art RL methods in terms of crash avoidance and smoothness of control.