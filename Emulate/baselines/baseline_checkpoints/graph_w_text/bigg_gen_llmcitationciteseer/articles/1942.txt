Skeleton-based action recognition has gained popularity due to its applications in human-computer interaction and healthcare. However, existing methods struggle to capture complex spatial-temporal relationships between body joints. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) that leverages multi-scale graph attention to model joint interactions at various semantic levels. Our experiments on the NTU-RGB+D and Kinetics datasets demonstrate that HGAT outperforms state-of-the-art methods in recognizing actions from skeleton data, especially in scenarios with partial occlusions or noisy data.