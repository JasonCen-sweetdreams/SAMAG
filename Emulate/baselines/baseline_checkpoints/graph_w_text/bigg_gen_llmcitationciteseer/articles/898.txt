Decentralized task allocation in multi-agent systems is a challenging problem due to the need for agents to coordinate and adapt to changing environments. This paper proposes a novel framework, 'Dec-MARL', which combines multi-agent reinforcement learning with decentralized communication protocols. We introduce a hierarchical architecture where agents learn to allocate tasks based on local observations and communicate with neighbors to resolve conflicts. Experimental results on a simulated disaster response scenario demonstrate that Dec-MARL outperforms traditional centralized approaches in terms of task completion rate and adaptability to changing agent availability.