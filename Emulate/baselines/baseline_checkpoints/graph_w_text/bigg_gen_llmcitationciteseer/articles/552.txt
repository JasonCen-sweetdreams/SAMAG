Emotion recognition is a crucial component of conversational systems, but it remains a challenging task due to the complexity of human emotions and the scarcity of annotated data. This paper proposes a novel hierarchical multi-task learning framework, 'EmoHMTL', which jointly learns to recognize emotions, sentiment, and speaker traits from conversational text. We introduce a task-adaptive attention mechanism that dynamically weights the importance of each task-specific feature, leading to improved performance and robustness. Experimental results on three benchmark datasets demonstrate the effectiveness of EmoHMTL in recognizing emotions and sentiment in conversational systems, outperforming state-of-the-art single-task models.