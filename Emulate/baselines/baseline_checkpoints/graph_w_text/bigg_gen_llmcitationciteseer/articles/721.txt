Explainable recommendation systems (ERS) have gained significant attention in recent years. However, existing ERS models often rely on shallow fusion of multimodal data, leading to suboptimal performance. This paper proposes a novel deep multimodal fusion framework, 'MMF-ERS', which leverages transformers to jointly model user-item interactions, visual, and textual features. Our approach enables the generation of interpretable explanations for recommended items, improving user trust and satisfaction. Experimental results on a large-scale e-commerce dataset demonstrate the effectiveness of MMF-ERS in enhancing recommendation accuracy and explanation quality.