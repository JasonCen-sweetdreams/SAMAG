Deep learning models have achieved state-of-the-art performance on various multimodal tasks, but their lack of transparency hinders their adoption in high-stakes applications. This paper proposes a novel Hierarchical Attention Network (HAN) framework that integrates attention mechanisms across multiple modalities to provide insights into the decision-making process. We introduce a modular architecture that comprises modality-specific attention modules and a hierarchical fusion layer. Experiments on a benchmark multimodal dataset demonstrate that HAN improves model interpretability while maintaining competitive performance compared to existing multimodal fusion techniques.