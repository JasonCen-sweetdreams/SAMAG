Deep learning models have become increasingly computationally expensive, making them challenging to deploy on resource-constrained devices. This paper proposes an adaptive batch normalization (AdaBN) technique that dynamically adjusts the batch normalization statistics based on the input data distribution. By doing so, AdaBN reduces the computational overhead of batch normalization, leading to significant speedups on devices with limited resources. We evaluate AdaBN on various deep learning architectures and demonstrate its effectiveness in improving inference speed while maintaining model accuracy.