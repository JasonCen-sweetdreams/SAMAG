Open-domain question answering (ODQA) systems rely on effective document retrieval to answer user queries. However, the traditional approach of fixed-length document snippets often leads to suboptimal performance. This paper presents a novel query-adaptive document expansion (QADE) method that dynamically selects relevant sentences from retrieved documents based on query-specific importance weights. Our approach leverages a hierarchical attention mechanism to estimate the relevance of each sentence to the query, and then generates a weighted document representation for improved answer extraction. Experimental results on the SQuAD dataset demonstrate that QADE outperforms state-of-the-art ODQA systems by 3.5% in terms of exact match score.