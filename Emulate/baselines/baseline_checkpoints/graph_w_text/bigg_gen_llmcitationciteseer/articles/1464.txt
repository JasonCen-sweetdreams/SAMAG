Deep neural networks have achieved state-of-the-art performance in various applications, but their lack of transparency hinders trust and adoption. This paper proposes a novel attention-based feature importance (ABFI) method for explaining deep neural network decisions. ABFI leverages attention mechanisms to identify crucial input features contributing to the model's predictions. We demonstrate the efficacy of ABFI on various benchmark datasets, showing that it outperforms existing explainability techniques in terms of accuracy and computational efficiency. Furthermore, we integrate ABFI with a visual analytics framework to facilitate human-in-the-loop model debugging and improvement.