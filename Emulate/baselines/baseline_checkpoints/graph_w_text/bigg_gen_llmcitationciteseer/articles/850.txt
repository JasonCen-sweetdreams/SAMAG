Deep reinforcement learning (DRL) agents have achieved remarkable success in various applications, but their vulnerability to adversarial attacks remains a significant concern. This paper presents a comprehensive robustness analysis of DRL agents under different types of attacks, including model-based and model-free attacks. We develop a novel evaluation framework that assesses the robustness of DRL agents across various attack scenarios and propose a set of defense mechanisms to improve their resilience. Experimental results on several benchmark environments demonstrate the effectiveness of our approach in enhancing the robustness of DRL agents against adversarial attacks.