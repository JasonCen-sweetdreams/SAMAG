Coordinating heterogeneous multi-agent systems (MAS) is a complex task, especially in dynamic environments. This paper proposes a novel decentralized reinforcement learning (DRL) approach, 'HetMAS-DRL', which enables agents to learn cooperative policies without explicit communication. We introduce a graph-based attention mechanism that allows agents to selectively focus on relevant neighbors and adapt to changes in the environment. Experimental results on a variety of MAS scenarios demonstrate that HetMAS-DRL outperforms traditional centralized and decentralized learning methods in terms of coordination efficiency and adaptability.