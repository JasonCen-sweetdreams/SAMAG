Multimodal sentiment analysis (MSA) has gained popularity with the increasing availability of multimedia data. However, existing MSA models suffer from a lack of interpretability, making it challenging to understand the decision-making process. This paper proposes a hierarchical attention mechanism (HAM) that integrates visual and textual features to generate explainable sentiment predictions. Our approach utilizes a novel attention hierarchy that captures both local and global feature interactions, enabling the model to focus on relevant multimodal cues. Experimental results on three benchmark datasets demonstrate that HAM outperforms state-of-the-art MSA models in terms of accuracy and provides meaningful visualizations for sentiment explanation.