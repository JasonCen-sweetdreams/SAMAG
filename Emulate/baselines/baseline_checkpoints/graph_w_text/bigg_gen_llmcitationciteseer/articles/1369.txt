Accurate emotional state recognition is crucial for human-computer interaction (HCI) applications. This paper presents a novel multimodal approach that combines eye-tracking and electroencephalography (EEG) signals to recognize emotional states. Our proposed framework, 'EmoFusion', employs a convolutional neural network (CNN) to extract features from eye-tracking data and a recurrent neural network (RNN) to analyze EEG signals. The fusion of these modalities using a Bayesian approach improves recognition accuracy compared to unimodal approaches. Experimental results on a dataset of 50 participants demonstrate that EmoFusion achieves an average accuracy of 87.2% in recognizing four emotional states: happiness, sadness, anger, and fear.