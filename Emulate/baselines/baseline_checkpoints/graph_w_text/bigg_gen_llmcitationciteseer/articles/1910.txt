Contrastive self-supervised learning has shown remarkable success in learning visual representations. However, existing methods rely on computationally expensive contrastive loss functions and require large batches. This paper presents 'EffiCLR', a novel approach that leverages a momentum-based caching mechanism to reduce the computational overhead of contrastive learning. We also propose a hierarchical clustering-based strategy to construct more informative positive and negative pairs. Extensive experiments on ImageNet and CIFAR-10 demonstrate that EffiCLR achieves competitive performance to state-of-the-art methods while reducing computational costs by up to 30%.