This paper addresses the challenge of coordinating autonomous agents in dynamic environments. We propose a novel multi-agent reinforcement learning framework, 'MA-RL-Coor', which enables agents to learn cooperative policies that adapt to changing environmental conditions. MA-RL-Coor integrates a decentralized actor-critic architecture with a communication mechanism that allows agents to share local observations and coordinate their actions. Experimental results in a simulated robotic soccer environment demonstrate that MA-RL-Coor outperforms existing decentralized and centralized approaches in terms of task completion rate and adaptability to environmental changes.