Recent advances in graph neural networks (GNNs) have shown promising results in recommendation systems. However, most existing methods suffer from scalability issues when dealing with large graphs. This paper proposes a novel hierarchical attention-based GNN framework, 'HAT', which leverages a hierarchical clustering approach to reduce the graph size while preserving crucial structural information. We introduce a attention mechanism that adaptively weights the importance of different clusters based on user preferences. Experimental results on several real-world datasets demonstrate that HAT achieves state-of-the-art performance while reducing computational costs by up to 60% compared to existing GNN-based methods.