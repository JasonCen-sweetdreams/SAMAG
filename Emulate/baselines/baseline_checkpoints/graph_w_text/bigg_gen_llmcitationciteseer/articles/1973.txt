Voice assistants have become ubiquitous, but their adoption by users with disabilities is hindered by limitations in current interaction modalities. This paper presents 'EchoAccess', a novel multimodal framework that enables users with disabilities to interact with voice assistants using a combination of speech, gesture, and gaze inputs. We propose a probabilistic fusion model that dynamically adapts to the user's abilities and preferences, ensuring a more inclusive and personalized interaction experience. A user study with 30 participants demonstrates significant improvements in task completion rates and user satisfaction compared to traditional voice-only interfaces.