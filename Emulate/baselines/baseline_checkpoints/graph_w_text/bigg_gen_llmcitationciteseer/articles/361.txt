Multimodal sentiment analysis has gained increasing attention in recent years, but existing approaches often neglect the complex relationships between different modalities. This paper proposes a novel Hierarchical Explainable Attention Network (HEAT) that integrates multimodal features through a hierarchical attention mechanism. Our approach learns to selectively focus on relevant regions of input data, providing interpretable results. We evaluate HEAT on two benchmark datasets and demonstrate significant improvements in sentiment analysis accuracy and explainability compared to state-of-the-art methods.