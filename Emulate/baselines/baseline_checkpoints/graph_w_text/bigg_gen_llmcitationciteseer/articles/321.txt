Virtual reality (VR) has immense potential for enhancing accessibility in various domains, including education and healthcare. However, existing VR systems often neglect the needs of users with disabilities, leading to a suboptimal experience. This paper presents a novel gaze-aware adaptive interface framework, 'GAIA', which utilizes machine learning-based gaze estimation to dynamically adapt VR content and interaction modalities to individual users' abilities. We evaluate GAIA through a user study involving participants with visual impairments, demonstrating significant improvements in navigation efficiency and overall user satisfaction.