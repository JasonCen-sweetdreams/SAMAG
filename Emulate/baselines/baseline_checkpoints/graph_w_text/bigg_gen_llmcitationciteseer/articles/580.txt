Deep neural networks have been increasingly vulnerable to adversarial attacks, which can manipulate model predictions. This paper proposes a novel approach to detect such attacks using graph-based anomaly detection. We represent the neural network's activation patterns as a graph and identify anomalies in the graph structure. Our method, 'GraphGuard', leverages graph convolutional networks to learn a robust representation of the graph and detect deviations from the expected behavior. Experimental results on several benchmark datasets demonstrate the effectiveness of GraphGuard in detecting adversarial attacks, outperforming existing methods in terms of detection accuracy and robustness.