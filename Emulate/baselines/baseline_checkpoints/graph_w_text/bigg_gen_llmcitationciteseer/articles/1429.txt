Augmented reality (AR) has the potential to revolutionize human-computer interaction, but current interfaces often rely on cumbersome controllers or gestures. This paper explores gaze-based interaction as a more natural and intuitive approach. We conducted a user study to investigate the relationship between foveal vision and eye movement in AR environments, and developed a novel algorithm that leverages this understanding to enable accurate and efficient gaze-based selection. Our results show significant improvements in interaction speed and accuracy compared to traditional methods, and highlight the potential for gaze-based interaction to become a cornerstone of future AR interfaces.