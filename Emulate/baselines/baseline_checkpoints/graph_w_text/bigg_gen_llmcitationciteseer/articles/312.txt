Node classification is a fundamental task in graph-structured data analysis. However, existing methods often suffer from high computational complexity and limited scalability. This paper introduces HiGAT, a novel hierarchical graph attention network that leverages a coarse-to-fine node representation strategy to reduce computational costs. By iteratively applying graph attention mechanisms at multiple scales, HiGAT learns to focus on relevant nodes and their relationships, leading to improved classification performance. Experimental results on several benchmark datasets demonstrate that HiGAT achieves state-of-the-art results with significant speedups over existing methods.