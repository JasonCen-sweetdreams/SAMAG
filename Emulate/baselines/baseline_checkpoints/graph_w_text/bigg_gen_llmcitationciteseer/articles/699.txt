Deep reinforcement learning (DRL) has achieved remarkable success in various domains, but the lack of explainability hinders trust and adoption. This paper presents a novel approach to enhance explainability in DRL using hierarchical attention mechanisms. Our method, HierAttn, disentangles the decision-making process into multiple levels, enabling the identification of critical state features and action influences. We evaluate HierAttn on several Atari games and a real-world robotics task, demonstrating improved explainability without compromising policy performance. Furthermore, we provide a theoretical analysis of the attention mechanisms, shedding light on their role in promoting interpretability.