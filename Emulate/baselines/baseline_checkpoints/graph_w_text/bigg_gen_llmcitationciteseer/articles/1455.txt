Open-domain question answering (ODQA) models struggle to retrieve relevant passages from a large corpus, especially when the question is ambiguous or has multiple answers. We propose a neural retrieval approach, called NERD, which incorporates document expansion to enrich the context of relevant passages. Our model uses a transformer-based encoder to jointly learn question and passage representations, and a neural ranker to re-score the passages based on their expanded contexts. Experimental results on the Natural Questions dataset demonstrate that NERD outperforms state-of-the-art ODQA models, achieving a 12% improvement in retrieval recall and a 9% improvement in answer accuracy.