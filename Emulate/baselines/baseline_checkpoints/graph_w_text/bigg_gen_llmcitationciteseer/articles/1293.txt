Graph neural networks (GNNs) have demonstrated remarkable performance in various graph-based tasks. However, their vulnerability to adversarial attacks has received limited attention. This paper investigates node-level vulnerabilities in GNNs, proposing a novel attack framework, 'NodeFool', which perturbs node features to deceive GNN predictions. We analyze the attack's effectiveness on several benchmark datasets, revealing that NodeFool can significantly degrade GNN performance. Furthermore, we develop a defense mechanism, 'GraphShield', which incorporates adversarial training and graph attention to improve GNN robustness against node-level attacks.