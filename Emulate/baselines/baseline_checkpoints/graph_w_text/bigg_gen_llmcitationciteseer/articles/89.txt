Emotion recognition systems often rely on a single modality, such as facial expressions or speech patterns. However, humans use multiple cues to infer emotional states. This paper introduces a hierarchical attention network (HAN) that integrates acoustic, visual, and linguistic features to recognize emotions in a multi-modal setting. Our HAN model employs attention mechanisms at both the feature and modality levels to adaptively weight the importance of each modality and feature type. Experimental results on the EMOTIC dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from facial expressions, speech, and text.