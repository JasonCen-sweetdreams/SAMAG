This paper explores the relationship between embodied cognition and cognitive load in virtual reality (VR) environments. We designed an experiment to investigate how gesture recognition affects cognitive load in VR-based learning tasks. Our results show that the use of embodied gestures can reduce cognitive load and improve learning outcomes in VR. We also propose a novel gesture recognition model that leverages machine learning techniques to classify gestures in real-time. The implications of our findings are discussed in the context of HCI and education.