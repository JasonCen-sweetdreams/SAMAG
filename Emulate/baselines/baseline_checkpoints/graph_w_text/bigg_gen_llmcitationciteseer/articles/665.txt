Neural retrieval models have shown promising results in information retrieval tasks, but their ranking consistency often suffers due to the lack of explicit document relationships. This paper proposes a self-supervised learning approach to learn document embeddings, which are then incorporated into a neural ranking model to improve ranking consistency. Our approach, dubbed 'DocEmb', leverages a contrastive learning objective to learn document embeddings that capture semantic similarities. Experimental results on several benchmark datasets demonstrate that DocEmb significantly improves ranking consistency and retrieval performance compared to state-of-the-art neural retrieval models.