Multimodal sentiment analysis involves identifying sentiment from multiple sources such as text, images, and audio. Existing approaches often rely on early fusion or late fusion, which can lead to information loss or misaligned feature representations. We propose a novel hierarchical attention neural network (HATNet) that leverages self-attention mechanisms to selectively focus on relevant modalities and features. Our experiments on the CMU-MOSI and Multimodal Sentiment Analysis datasets demonstrate that HATNet outperforms state-of-the-art methods in terms of accuracy and robustness, particularly in scenarios with noisy or missing modalities.