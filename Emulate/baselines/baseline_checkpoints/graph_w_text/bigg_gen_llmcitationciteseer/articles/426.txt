This paper presents a novel multi-agent reinforcement learning (MARL) approach to coordinate autonomous vehicles in complex urban scenarios. We propose a decentralized framework, 'CoAV', where each vehicle learns to optimize its navigation policy based on local observations and communication with neighboring agents. Our method integrates a attention-based neural network to model inter-agent relationships and a hierarchical reward function to balance individual and collective objectives. Experimental results using a realistic traffic simulator demonstrate that CoAV outperforms traditional decentralized control methods in terms of traffic flow and safety.