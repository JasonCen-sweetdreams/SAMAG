Visual question answering (VQA) models often struggle to effectively integrate information from different modalities, such as images and text. This paper presents a novel hierarchical attention network (HAN) architecture that addresses this challenge. Our approach iteratively refines attention weights across modalities, allowing the model to selectively focus on relevant regions and tokens. We demonstrate the effectiveness of HAN on several benchmark datasets, achieving state-of-the-art performance while reducing computational overhead. Furthermore, we provide a comprehensive analysis of attention patterns, offering insights into the decision-making process of VQA models.