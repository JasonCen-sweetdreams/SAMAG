Few-shot relation extraction (FSRE) aims to extract novel relations from a limited number of examples. Existing FSRE models rely on complex meta-learning strategies or rely heavily on pre-trained language models. We propose a hierarchical attention network (HAN) that leverages both entity-level and relation-level attention mechanisms to effectively capture contextual dependencies. Our experiments on the FewRel benchmark show that HAN achieves state-of-the-art performance on FSRE tasks, outperforming existing models by up to 15% in accuracy. We further analyze the effectiveness of our approach on different types of relations and datasets.