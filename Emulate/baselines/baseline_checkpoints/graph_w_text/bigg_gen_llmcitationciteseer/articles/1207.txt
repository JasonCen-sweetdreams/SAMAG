Virtual reality (VR) systems heavily rely on manual input devices, limiting the sense of immersion and interaction. This paper presents a novel gaze-based interaction framework for VR, leveraging machine learning (ML) to predict user intentions from eye movements. Our approach uses a deep neural network to classify gaze patterns into distinct actions, such as selection, manipulation, and navigation. We evaluate our framework using a custom-built VR setup and demonstrate significant improvements in interaction accuracy and user experience compared to traditional manual input methods.