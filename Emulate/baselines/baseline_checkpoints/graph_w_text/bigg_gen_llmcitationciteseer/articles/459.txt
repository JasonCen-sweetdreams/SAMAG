Visual Question Answering (VQA) has witnessed significant advancements with the advent of deep learning models. However, these models often lack transparency and interpretability, making it challenging to understand their decision-making processes. This paper proposes a novel multi-modal fusion framework, 'XplainVQA', which integrates visual, linguistic, and cognitive modalities to provide explainable VQA. We introduce a hierarchical graph attention mechanism that captures complex relationships between image regions, question keywords, and answer choices. Extensive experiments on the VQA-X and COCO-QA datasets demonstrate that XplainVQA achieves state-of-the-art performance while providing insightful explanations for its predictions.