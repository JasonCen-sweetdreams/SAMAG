Reinforcement learning (RL) has achieved remarkable success in robotics, but its lack of transparency hinders trust and adaptability in real-world applications. This paper proposes a novel Explainable RL (XRL) framework that leverages causal graphs to model the relationships between actions, states, and rewards. We introduce a graph-based attention mechanism that highlights the most critical factors contributing to the agent's decision-making process. Experimental results on a robotic arm platform demonstrate that our XRL approach improves interpretability, robustness, and adaptability in dynamic environments, while maintaining competitive performance with state-of-the-art RL methods.