Deep reinforcement learning (DRL) has achieved remarkable success in complex decision-making tasks, but the lack of interpretability hinders its adoption in high-stakes applications. This paper proposes a novel approach to explaining DRL agents' decisions by abstracting the state space using a learned, probabilistic model. Our method, dubbed 'ExplainRL', leverages the abstraction to generate compact, symbolic representations of the agent's policy, which can be easily understood by humans. We evaluate ExplainRL on a suite of Atari games and demonstrate significant improvements in explanation quality and computational efficiency compared to existing methods.