Multi-modal learning has gained significant attention in recent years, but existing approaches often suffer from high computational costs and limited scalability. In this paper, we propose a novel hierarchical contrastive learning framework, 'HiCLR', which leverages a hierarchical representation learning strategy to efficiently learn multi-modal representations. By exploiting the inherent structures in different modalities, HiCLR can adapt to various modalities and tasks while reducing the computational overhead. Our experiments on several benchmarks demonstrate the effectiveness of HiCLR in learning robust and transferable multi-modal representations, outperforming state-of-the-art methods in various downstream tasks.