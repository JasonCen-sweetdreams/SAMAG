Document classification is a fundamental task in natural language processing, but existing deep learning models often require significant computational resources and memory. This paper proposes a novel hierarchical attention network (HAN) architecture that leverages both local and global document semantics to improve classification efficiency. Our HAN model consists of a two-level attention mechanism that selectively focuses on relevant sentences and words, reducing the input dimensionality and computational cost. Experimental results on several benchmark datasets demonstrate that our approach achieves competitive accuracy with state-of-the-art methods while requiring up to 50% fewer parameters and 30% less computation.