Autonomous robot navigation in complex, dynamic environments remains a significant challenge. This paper presents a novel deep hierarchical reinforcement learning (HRL) framework, 'HierNav', which leverages a hierarchical policy structure to learn both high-level navigation strategies and low-level control primitives. Our approach incorporates a novel state-abstraction mechanism that enables the robot to reason about the environment at multiple levels of granularity. Experimental results on a range of simulated and real-world navigation tasks demonstrate that HierNav outperforms state-of-the-art flat reinforcement learning methods in terms of navigation efficiency and robustness.