Sparse queries in Information Retrieval (IR) systems often lead to inadequate retrieval performance. Query expansion techniques can alleviate this issue, but existing methods rely on term co-occurrence statistics or manual thesauri, which may not capture semantic relationships between terms. This paper proposes a neural embedding-based query expansion approach, 'Neuxpand', which leverages pre-trained language models to learn dense vector representations of terms. Our method adapts to the query context by generating context-aware embeddings, allowing for more precise expansion. Experimental results on the TREC Robust04 dataset demonstrate significant improvements in retrieval performance over state-of-the-art methods.