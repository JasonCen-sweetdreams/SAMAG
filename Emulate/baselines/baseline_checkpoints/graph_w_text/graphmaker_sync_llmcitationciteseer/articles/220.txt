Few-shot natural language processing (NLP) tasks pose significant challenges due to the limited availability of labeled training data. This paper presents a novel meta-learning approach that leverages hierarchical task embeddings to adapt to new NLP tasks with only a few examples. Our method, called HiTE, learns to encode tasks in a hierarchical structure, capturing both task-specific and task-agnostic knowledge. We evaluate HiTE on a range of few-shot NLP tasks, including text classification, sentiment analysis, and question answering, and demonstrate state-of-the-art performance. Our results show that HiTE can effectively generalize to new tasks and outperforms existing few-shot NLP approaches.