This paper presents EmoTract, a novel multimodal affective state tracking system for human-computer interaction. EmoTract leverages computer vision, speech recognition, and physiological signal processing to detect and recognize users' emotions in real-time. Our approach employs a hybrid deep learning architecture, combining convolutional neural networks (CNNs) for facial expression analysis with recurrent neural networks (RNNs) for speech and physiological signal processing. We evaluate EmoTract on a dataset of 100 users, demonstrating high accuracy in recognizing six basic emotions (happiness, sadness, anger, fear, surprise, and disgust) with an average F1-score of 0.92.