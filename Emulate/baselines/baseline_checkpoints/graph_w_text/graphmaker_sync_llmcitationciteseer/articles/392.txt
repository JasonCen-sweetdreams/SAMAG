Decentralized task allocation in multi-agent systems is a challenging problem due to the lack of centralized control and limited communication between agents. This paper proposes a graph reinforcement learning framework that enables agents to learn to allocate tasks in a decentralized manner. We model the task allocation problem as a graph, where agents and tasks are represented as nodes, and edges represent the availability of agents for tasks. Our approach uses a decentralized actor-critic architecture, where each agent learns to optimize a local objective function that takes into account the global system's performance. Experiments on a variety of task allocation scenarios demonstrate that our approach outperforms traditional decentralized task allocation methods in terms of system efficiency and fairness.