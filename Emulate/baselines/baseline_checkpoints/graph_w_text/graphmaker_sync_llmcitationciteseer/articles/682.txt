Graph Convolutional Networks (GCNs) have achieved state-of-the-art performance in node classification tasks. However, their vulnerability to adversarial attacks hinders their deployment in real-world applications. This paper proposes a novel adversarial training framework for GCNs, which incorporates a learnable perturbation generator to simulate attacks on the graph structure and node features. We demonstrate that our approach, 'AdvGCN', significantly improves the robustness of GCNs against various types of attacks, including node addition, deletion, and feature manipulation. Experiments on several benchmark datasets show that AdvGCN outperforms existing defense methods, achieving an average improvement of 12.5% in classification accuracy under attack.