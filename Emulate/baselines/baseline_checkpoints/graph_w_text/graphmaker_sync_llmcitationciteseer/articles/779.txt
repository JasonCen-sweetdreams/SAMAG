Time series forecasting is a crucial task in various domains, but existing methods often struggle with handling long-range dependencies and high-dimensional data. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages self-attention mechanisms at multiple scales to capture complex temporal patterns. Our approach integrates a pyramidal structure to efficiently process long-range dependencies, and a feature importance module to adaptively select relevant inputs. Experiments on four real-world datasets demonstrate that HAN outperforms state-of-the-art methods in terms of prediction accuracy and computational efficiency.