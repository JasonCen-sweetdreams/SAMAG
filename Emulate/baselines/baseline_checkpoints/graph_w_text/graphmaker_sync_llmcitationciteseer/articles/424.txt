Ad-hoc retrieval suffers from the vocabulary mismatch problem, where the query and document terms do not align. We propose a neural query expansion approach, 'CrossQA', that leverages cross-modal attention to incorporate visual features from images and videos into the query representation. Our model consists of a multimodal encoder that generates a joint query-document embedding space, and a query expansion module that predicts relevant terms based on the attention weights. Experimental results on the TREC-CAR dataset show that CrossQA significantly outperforms state-of-the-art query expansion methods, achieving a 15% improvement in mean average precision.