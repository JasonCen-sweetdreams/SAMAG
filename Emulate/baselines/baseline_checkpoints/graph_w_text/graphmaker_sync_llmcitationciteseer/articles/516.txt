This paper presents a novel approach to error detection in multimodal human-computer interaction using gaze-based metrics. We develop a machine learning model that leverages eye-tracking data to identify instances of user frustration and confusion, enabling proactive system interventions to prevent errors. Our approach is evaluated on a dataset of users performing a range of tasks on a multimodal interface, demonstrating a significant reduction in error rates and user frustration compared to traditional interaction methods. The proposed method has implications for the design of more intuitive and user-friendly interfaces in various domains.