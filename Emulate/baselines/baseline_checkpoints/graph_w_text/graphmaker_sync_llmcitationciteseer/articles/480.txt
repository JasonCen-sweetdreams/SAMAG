Backdoor attacks pose a significant threat to the reliability of deep neural networks (DNNs) by injecting hidden triggers that mislead the model. Existing defense mechanisms often rely on heuristic methods that fail to generalize. This paper proposes a novel adversarial training framework, 'BackdoorShield', which leverages the concept of worst-case perturbations to fortify DNNs against backdoor attacks. We devise a systematic approach to generate diverse backdoor patterns, and incorporate them into the training process to enhance the model's robustness. Experimental results on multiple benchmark datasets demonstrate that BackdoorShield significantly improves the resilience of DNNs against various backdoor attacks, outperforming state-of-the-art defense methods.