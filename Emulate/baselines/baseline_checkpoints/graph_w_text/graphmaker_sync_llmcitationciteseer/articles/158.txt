Emotion recognition in conversational agents is a challenging task due to the complexity of human emotions and the variability of multimodal cues. This paper introduces a novel deep hierarchical fusion framework, 'MultiEmo', which leverages the strengths of vision, speech, and language modalities to recognize emotions in conversations. Our approach uses a hierarchical attention mechanism to selectively focus on relevant modalities and fuse their representations at multiple levels. Experimental results on the IEMOCAP dataset demonstrate that MultiEmo outperforms state-of-the-art methods in recognizing emotions from multimodal inputs, achieving a significant improvement in F1-score and correlation coefficient.