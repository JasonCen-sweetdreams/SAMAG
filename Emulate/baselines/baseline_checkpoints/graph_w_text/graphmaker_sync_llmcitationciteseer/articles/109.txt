In dynamic environments, task allocation among multiple agents is a challenging problem due to the uncertainty and volatility of task requirements. This paper proposes a decentralized multi-agent reinforcement learning (MARL) framework that enables agents to learn optimal task allocation policies without explicit communication. We introduce a novel graph-based attention mechanism that captures task relationships and agent capabilities, facilitating efficient task allocation. Experimental results on a simulated disaster response scenario demonstrate the effectiveness of our approach in adapting to changing task demands and improving overall system efficiency.