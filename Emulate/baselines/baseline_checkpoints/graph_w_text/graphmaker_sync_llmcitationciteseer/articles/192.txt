Real-time traffic signal control is a complex problem that requires coordination among multiple agents. This paper proposes a novel multi-agent reinforcement learning framework, 'TrafficSync', which leverages graph neural networks to model traffic dynamics and coordination graphs to enable efficient communication among agents. We introduce a hierarchical learning mechanism that decomposes the global reward function into local objectives, allowing agents to learn in parallel and adapt to changing traffic conditions. Experimental results on a realistic simulation platform demonstrate that TrafficSync reduces congestion by 25% and travel times by 18% compared to traditional traffic signal control methods.