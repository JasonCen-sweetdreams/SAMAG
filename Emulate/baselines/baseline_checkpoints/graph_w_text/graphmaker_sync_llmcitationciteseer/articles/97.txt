Neural search engines have gained popularity due to their ability to capture complex query-document relationships. However, they often struggle to generalize to out-of-vocabulary queries. This paper proposes a novel query expansion method, 'ConQE', which leverages contrastive learning to learn robust query representations. By training a contrastive model to distinguish between relevant and non-relevant documents, ConQE generates effective expansion terms that improve the retrieval performance of neural search engines. Experimental results on the TREC CAR dataset demonstrate that ConQE outperforms state-of-the-art query expansion methods, achieving a 15% increase in mean average precision.