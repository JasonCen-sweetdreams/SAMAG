Autonomous vehicles (AVs) operating in dynamic environments require real-time decision-making and adaptability to navigate safely and efficiently. This paper presents a novel multi-agent reinforcement learning (MARL) framework for AV navigation, where each agent represents a distinct vehicle or traffic entity. We introduce a hierarchical graph-based architecture that integrates local and global state representations, enabling agents to learn cooperative behaviors and respond to changing environmental conditions. Experimental results on a simulated urban traffic scenario demonstrate improved navigation performance and reduced collision rates compared to traditional single-agent approaches.