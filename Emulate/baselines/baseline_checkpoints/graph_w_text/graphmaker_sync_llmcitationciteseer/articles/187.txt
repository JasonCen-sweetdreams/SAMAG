Deep learning models have been shown to be vulnerable to adversarial attacks, which can significantly impact their performance. This paper proposes a novel approach to analyze the robustness of deep learning models against such attacks using Bayesian neural networks. We develop a framework that leverages Bayesian inference to quantify the uncertainty of the model's predictions and identify the most vulnerable inputs. Our experiments on several benchmark datasets demonstrate that our approach can effectively detect and mitigate adversarial attacks, providing a robustness score that can guide model development and deployment.