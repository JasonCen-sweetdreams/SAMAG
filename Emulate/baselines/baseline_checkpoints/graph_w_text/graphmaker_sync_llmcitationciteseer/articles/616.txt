Multimodal sentiment analysis is a challenging task, particularly in low-resource domains where annotated data is scarce. This paper proposes a novel deep transfer learning approach, 'ModaTL', which leverages pre-trained multimodal encoders and fine-tunes them on a small set of target domain data. We introduce a domain adaptation module that aligns the feature distributions of the source and target domains, enabling effective knowledge transfer. Experimental results on three low-resource datasets demonstrate that ModaTL outperforms state-of-the-art methods by up to 12% in terms of overall sentiment accuracy, while requiring significantly less target domain data.