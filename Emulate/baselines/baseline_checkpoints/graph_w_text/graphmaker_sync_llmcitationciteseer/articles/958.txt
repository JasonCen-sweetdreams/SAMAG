Zero-shot learning (ZSL) has gained significant attention in recent years, but existing methods often struggle to scale to large datasets and complex tasks. This paper proposes a novel hierarchical graph attention network (HGAT) architecture, which leverages graph-based attention mechanisms to model relationships between seen and unseen classes. HGAT learns to selectively focus on relevant semantic features and adaptively transfer knowledge from seen classes to unseen ones. Experimental results on four benchmark datasets demonstrate that HGAT achieves state-of-the-art performance in ZSL tasks, outperforming existing methods by a significant margin.