Explainable reinforcement learning (XRL) has gained popularity in recent years, enabling model transparency and accountability in high-stakes decision-making domains. However, the vulnerability of XRL policies to adversarial attacks remains understudied. This paper investigates the susceptibility of popular XRL algorithms to perturbation-based attacks, which can manipulate policy explanations to alter agent behavior. We propose a novel attack framework, 'XRL-Attack', that leverages gradient-based optimization to craft perturbations targeting specific policy features. Experimental results on several Atari games demonstrate that XRL-Attack can significantly degrade policy performance and compromise explanation quality, highlighting the need for robustness measures in XRL.