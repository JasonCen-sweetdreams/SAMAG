Virtual reality (VR) systems often rely on gaze-based input for user interaction. However, eye movement errors can significantly degrade the user experience. This paper proposes a novel error reduction technique based on machine learning and computer vision. We develop a gaze correction model that leverages pupil center corneal reflection tracking and adaptive filtering to improve the accuracy of gaze-based input. Our user study with 30 participants shows that the proposed method reduces eye movement errors by 35% compared to traditional gaze-based input methods, resulting in improved user performance and satisfaction in VR applications.