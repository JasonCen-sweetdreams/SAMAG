Multi-modal sentiment analysis (MSA) has become increasingly important in understanding user opinions from diverse data sources. However, existing approaches often suffer from high computational costs and limited scalability. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that efficiently fuses and selects relevant features from text, image, and audio modalities. Our approach leverages a multi-level attention mechanism that adaptively weights features based on their sentiment relevance, resulting in improved performance and reduced inference time. Experiments on three benchmark MSA datasets demonstrate the effectiveness of our approach in achieving state-of-the-art results with significant speedup.