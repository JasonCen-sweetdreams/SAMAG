Accurate emotion recognition is crucial for developing empathetic human-computer interfaces. Existing approaches rely on a single modality, such as facial expressions or speech, which can be limiting in real-world scenarios. This paper presents EmoTract, a novel multimodal framework that integrates computer vision, audio processing, and physiological signal analysis to recognize emotions in HCI. EmoTract employs a hierarchical fusion strategy, combining the strengths of individual modalities to improve recognition accuracy. We evaluate EmoTract on a large, diverse dataset and demonstrate its superior performance compared to state-of-the-art unimodal approaches.