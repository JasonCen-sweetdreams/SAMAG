Emotion recognition from multi-modal inputs, such as speech, text, and vision, is a challenging task due to the complexity of human emotions and the need to integrate heterogeneous data sources. This paper presents a novel hierarchical attention network (HAN) architecture that learns to selectively focus on relevant modalities and time segments to improve emotion recognition performance. Our approach leverages a hierarchical structure to capture both local and global contextual information and incorporates a modality-aware attention mechanism to adaptively weight the contributions of each modality. Experimental results on the multimodal emotion recognition benchmark dataset show that our proposed HAN outperforms state-of-the-art approaches by 12.5% in terms of weighted F1-score.