Emotion recognition from multimodal data (e.g., speech, text, vision) is a challenging task due to the inherent heterogeneity of modalities. This paper introduces a Hierarchical Attention Network (HAN) that leverages the strengths of each modality to improve emotion recognition. Our HAN model consists of modality-specific attention modules that learn to fuse relevant features from each modality, followed by a hierarchical fusion layer that combines the outputs from each modality. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art multimodal fusion techniques, achieving an F1-score of 0.83 for emotion classification.