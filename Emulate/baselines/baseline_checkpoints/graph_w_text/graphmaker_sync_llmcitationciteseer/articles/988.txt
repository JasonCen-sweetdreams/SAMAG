Ad-hoc retrieval systems struggle to effectively handle complex, multi-modal queries. This paper proposes a novel deep relevance matching framework, 'MultiMatch', which leverages query embeddings to jointly model textual, visual, and semantic features. Our approach extends the transformer architecture to incorporate multi-modal attention mechanisms, enabling the capture of nuanced relationships between query elements. Experimental results on the TREC-VID dataset demonstrate significant improvements in retrieval performance, particularly for queries with multiple media types. We also show that our method generalizes well to unseen query types, outperforming state-of-the-art methods in zero-shot retrieval scenarios.