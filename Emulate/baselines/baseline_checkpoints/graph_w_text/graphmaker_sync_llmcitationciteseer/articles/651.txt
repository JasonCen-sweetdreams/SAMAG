Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the inherent complexity and heterogeneity of the data. This paper proposes a Hierarchical Bayesian Neural Network (H-BNN) framework that models the uncertainty in the data and the relationships between different modalities. Our approach leverages Bayesian deep learning to learn modality-specific and shared representations, and performs hierarchical inference to recognize emotions. Experimental results on the CMU-MOSEI dataset demonstrate the effectiveness of H-BNN in achieving state-of-the-art performance in multi-modal emotion recognition tasks.