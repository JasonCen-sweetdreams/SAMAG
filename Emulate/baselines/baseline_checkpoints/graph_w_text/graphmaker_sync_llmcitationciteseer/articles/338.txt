Knowledge graph embedding models have achieved state-of-the-art performance in various applications, but struggle to scale to large, multi-relational graphs. We propose a novel hierarchical graph attention network (HGAN) architecture that learns to selectively focus on relevant subgraphs and relations. Our approach combines node-level attention with hierarchical graph pooling to capture complex dependencies between entities. Experiments on multiple benchmark datasets demonstrate that HGAN outperforms existing state-of-the-art methods in terms of link prediction and entity classification, while reducing computational costs by up to 50%