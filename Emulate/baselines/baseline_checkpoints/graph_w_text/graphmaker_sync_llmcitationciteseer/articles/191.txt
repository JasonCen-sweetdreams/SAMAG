Multi-modal retrieval has gained significant attention in recent years, but existing approaches often struggle with the vocabulary mismatch problem. This paper proposes a novel query expansion framework that leverages deep learning to incorporate visual and textual features. Our approach, dubbed 'MMQE', uses a multi-task learning objective to jointly optimize query representation and expansion. We further introduce a novel attention mechanism to selectively incorporate relevant terms from the visual modality. Experiments on a large-scale dataset demonstrate that MMQE outperforms state-of-the-art methods in terms of retrieval accuracy and efficiency.