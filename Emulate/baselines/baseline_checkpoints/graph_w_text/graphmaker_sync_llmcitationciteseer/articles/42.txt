This paper presents a novel approach to designing gesture-based interfaces that adapt to the abilities of individuals with motor impairments. We propose a machine learning-based framework that utilizes a combination of computer vision and sensor data to recognize and classify gestures. Our approach enables users to customize the interface to their individual needs, allowing for more accurate and efficient interaction. We evaluate our approach through a user study with participants with varying levels of motor impairment, demonstrating significant improvements in interaction accuracy and user satisfaction.