In multi-agent systems, efficient task allocation is crucial for achieving collective goals. This paper proposes a novel distributed task allocation framework that leverages reinforcement learning to learn cooperative policies. Our approach, called 'MASTAR', enables agents to learn from their experiences and adapt to changing task requirements and environmental conditions. We demonstrate the effectiveness of MASTAR in a simulated disaster response scenario, where it outperforms traditional centralized allocation methods in terms of task completion time and resource utilization.