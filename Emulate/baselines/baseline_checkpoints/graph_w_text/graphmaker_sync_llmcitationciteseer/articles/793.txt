Real-time data analytics has become a crucial aspect of modern applications, necessitating efficient query optimization techniques for distributed database systems. This paper presents a novel approach, 'QOpt', which leverages machine learning-based models to predict query execution times and optimize query plans accordingly. We propose a hierarchical clustering algorithm to group queries based on their execution patterns and develop a cost-based optimization framework that incorporates these predictions. Experimental results on a large-scale distributed database system demonstrate that QOpt achieves an average query response time reduction of 35% compared to state-of-the-art optimization techniques.