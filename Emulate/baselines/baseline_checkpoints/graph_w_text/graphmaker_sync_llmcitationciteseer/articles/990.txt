Multi-agent reinforcement learning (MARL) has emerged as a crucial paradigm for tackling complex, real-world problems. However, the curse of dimensionality and partial observability in MARL domains hinder the scalability and efficiency of existing methods. We propose a novel hierarchical attention network (HAT) architecture that addresses these challenges by selectively focusing on relevant agents, observations, and actions. HAT enables more effective exploration, improves coordination, and reduces communication overhead among agents. Our experiments on several MARL benchmarks demonstrate that HAT outperforms state-of-the-art methods in terms of learning speed, convergence, and overall performance.