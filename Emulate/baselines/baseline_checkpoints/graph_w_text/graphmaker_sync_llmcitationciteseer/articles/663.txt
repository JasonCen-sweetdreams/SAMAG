Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is crucial for human-computer interaction. This paper presents a novel Hierarchical Attention Network (HAN) architecture to tackle this problem. HAN leverages attention mechanisms at multiple scales to selectively focus on relevant modalities, features, and temporal segments. We demonstrate the effectiveness of HAN on three benchmark datasets, achieving state-of-the-art performance in recognizing emotions from multi-modal inputs. Our analysis reveals that HAN's hierarchical attention structure enables it to capture complex cross-modal interactions and contextual dependencies.