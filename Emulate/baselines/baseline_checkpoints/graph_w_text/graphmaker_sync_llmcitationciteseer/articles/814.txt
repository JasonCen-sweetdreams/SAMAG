Virtual reality (VR) systems increasingly rely on gesture recognition to provide an immersive user experience. However, the cognitive load imposed by VR environments can affect the accuracy of gesture recognition systems. This paper presents a comprehensive study on the impact of cognitive load on gesture recognition in VR. We design a novel experimental framework that manipulates cognitive load using a dual-task paradigm and evaluates its effect on gesture recognition performance. Our results show that increased cognitive load leads to a significant decline in gesture recognition accuracy, and that this decline is more pronounced for complex gestures. We discuss the implications of our findings for the design of more robust and user-centered VR systems.