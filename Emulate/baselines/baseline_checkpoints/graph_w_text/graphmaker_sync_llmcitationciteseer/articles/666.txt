Deep neural networks are vulnerable to adversarial attacks, which can mislead them to make incorrect predictions. To address this issue, we propose a novel defense mechanism based on hierarchical reinforcement learning (HRL). Our approach, called 'AdvHR', consists of two stages: (1) a low-level policy that generates perturbations to the input data, and (2) a high-level policy that selects the most effective perturbations to robustify the network. We evaluate AdvHR on several benchmark datasets and demonstrate its effectiveness in improving the robustness of deep neural networks against various types of adversarial attacks.