Emotional intelligence is a crucial aspect of human-computer interaction, enabling machines to empathize with users' emotional states. This paper presents EmoReact, a novel framework for affective gesture recognition using computer vision and machine learning. We propose a hierarchical attention mechanism to extract salient features from hand and facial gestures, which are then classified using a multimodal fusion approach. Our results show that EmoReact achieves state-of-the-art performance on a benchmark dataset, with an average F1-score of 0.92 across six emotions. We discuss the implications of EmoReact for enhancing emotional intelligence in HCI applications, such as virtual assistants and affective computing.