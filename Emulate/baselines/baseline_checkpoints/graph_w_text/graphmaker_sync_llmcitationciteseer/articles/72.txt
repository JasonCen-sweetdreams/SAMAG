Virtual assistants are increasingly prevalent, but their inability to recognize users' emotional states limits their effectiveness. This paper presents a novel gaze-based approach to infer emotional states using a wearable eye-tracking device. We propose a machine learning model that leverages gaze patterns, pupil dilation, and facial feature extraction to recognize six primary emotions. Our user study with 30 participants demonstrates that our system achieves an accuracy of 85.6% in classifying emotional states, outperforming existing audio-based methods. We discuss the implications of our approach for creating more empathetic and personalized virtual assistants.