Virtual assistants have become ubiquitous, yet they often struggle to understand users' emotional states, leading to inadequate responses. This paper presents 'EmoFusion', a novel approach that integrates multimodal cues (speech, text, and facial expressions) to enhance emotional intelligence in virtual assistants. Our system uses a deep neural network to fuse features from each modality, enabling the assistant to recognize and respond empathetically to users' emotional needs. We evaluated EmoFusion in a user study, demonstrating significant improvements in user satisfaction and emotional understanding compared to traditional interfaces.