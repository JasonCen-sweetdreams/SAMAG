Hierarchical reinforcement learning (HRL) has shown promise in solving complex tasks, but exploring large state spaces remains a significant challenge. This paper proposes a novel approach, 'Graph-HRL', which leverages graph neural networks (GNNs) to efficiently explore hierarchical representations of state spaces. By learning to encode subgoals as graph nodes, our method enables the agent to reason about abstract states and focus exploration on promising regions. Experimental results on a range of benchmark tasks demonstrate that Graph-HRL outperforms state-of-the-art HRL methods in terms of exploration efficiency and overall performance.