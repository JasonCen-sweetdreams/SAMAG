Autonomous systems rely on reinforcement learning (RL) to make decisions in complex, dynamic environments. However, the lack of explainability and uncertainty awareness in RL models hinders their deployment in safety-critical applications. This paper proposes 'UAREL', an uncertainty-aware explainable RL framework that integrates Bayesian neural networks with model-based RL. We develop a novel uncertainty quantification method that propagates epistemic uncertainty through the decision-making process, enabling the agent to reason about its own uncertainty. Experimental results on a simulated autonomous driving scenario demonstrate that UAREL improves decision-making robustness and provides interpretable explanations for its actions.