Multimodal sentiment analysis (MSA) aims to predict sentiment from heterogeneous data sources (e.g., text, images, audio). Existing deep learning approaches often rely on complex models, making it challenging to interpret the decision-making process. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) for MSA, which integrates graph attention mechanisms to selectively focus on relevant multimodal features. We introduce a hierarchical feature fusion strategy, enabling the model to capture both local and global dependencies across modalities. Our experiments on three datasets demonstrate that HGAT outperforms state-of-the-art models while providing explainable insights into the sentiment formation process.