Virtual reality (VR) systems often rely on hand-held controllers or gestures, which can be challenging or impossible for users with mobility impairments. This paper presents 'GazeGuide', a gaze-based interface that enables accessible gestural input in VR. Our approach leverages machine learning-based gaze tracking and gesture recognition to allow users to interact with virtual objects using only their eyes. We evaluate GazeGuide in a user study with participants with and without motor impairments, demonstrating improved accessibility and user experience. The results suggest that gaze-based interfaces can provide a more inclusive and immersive VR experience for users with diverse abilities.