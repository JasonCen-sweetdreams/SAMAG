Distributed relational databases are increasingly used to support large-scale applications, but query processing remains a bottleneck. This paper proposes an adaptive join ordering technique, 'AJO', which dynamically adjusts the join order based on runtime statistics and data distribution. AJO integrates a machine learning-based cost model with a probabilistic optimization framework to minimize query execution time. Our experiments on the TPC-DS benchmark demonstrate that AJO outperforms state-of-the-art query optimizers in terms of query execution time and scalability, especially for complex queries with multiple joins.