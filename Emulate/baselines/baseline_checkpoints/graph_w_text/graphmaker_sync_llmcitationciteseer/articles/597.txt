Virtual reality (VR) systems often rely on manual input devices, limiting the sense of immersion. This paper presents a novel gaze-based interaction model, 'GazeVR', which leverages eye-tracking technology to enable intuitive user interactions in VR environments. We develop a machine learning-based algorithm that infers user intent from gaze patterns, and demonstrate its effectiveness in a range of VR applications, including object selection, navigation, and manipulation. Our user study shows that GazeVR significantly improves user experience and reduces cognitive load compared to traditional input methods.