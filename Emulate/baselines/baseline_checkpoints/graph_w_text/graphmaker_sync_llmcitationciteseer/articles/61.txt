Voice assistants have become ubiquitous, but their interactions can perpetuate and even amplify cognitive biases. This paper presents a novel multimodal framework for designing inclusive voice assistants that mitigate biases in human-computer interaction. Our approach combines natural language processing, affective computing, and cognitive modeling to detect and adapt to users' emotional and cognitive states. A user study with 120 participants demonstrates that our approach reduces bias in decision-making tasks and improves user satisfaction. We discuss implications for designing more inclusive and empathetic voice assistants.