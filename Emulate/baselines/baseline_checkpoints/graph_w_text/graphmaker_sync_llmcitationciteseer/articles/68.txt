As AI-driven time-series forecasting models become increasingly prevalent, it is essential to develop techniques that provide insights into their decision-making processes. This paper proposes a novel attention-based explainability framework, 'AttExplainer', which generates interpretable explanations for complex forecasting models. AttExplainer leverages self-attention mechanisms to identify key input features and their relationships, enabling the identification of influential factors driving forecasting errors. Our experiments on real-world datasets demonstrate that AttExplainer outperforms existing explainability methods in terms of accuracy and computational efficiency, paving the way for more trustworthy AI-driven forecasting applications.