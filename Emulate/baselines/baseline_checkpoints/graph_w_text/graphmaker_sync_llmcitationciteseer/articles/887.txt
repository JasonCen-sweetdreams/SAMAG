Emotion recognition from multi-modal inputs (e.g., speech, text, vision) has seen significant progress, but existing approaches often rely on complex, computationally expensive models. This paper introduces Hierarchical Attention Networks (HANs), a novel architecture that leverages attention mechanisms to selectively fuse modalities and recognize emotions. HANs achieve state-of-the-art performance on benchmark datasets while reducing computational overhead by 30%. We attribute this improvement to the hierarchical attention structure, which adaptively weights modalities based on their emotional relevance. Our approach has implications for real-time, low-latency emotion recognition in human-computer interaction and affective computing applications.