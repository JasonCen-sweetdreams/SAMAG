Multimodal sentiment analysis (MSA) is a challenging task that requires integrating information from multiple sources, such as text, images, and videos. This paper presents a novel hierarchical attention network (HAN) architecture for MSA, which learns to selectively focus on relevant modalities and features at multiple levels. Our approach leverages a multimodal fusion module to combine modalities, followed by a hierarchical attention mechanism that weights the importance of each modality and feature. Experimental results on the MM-IMDb dataset demonstrate that our method outperforms state-of-the-art MSA models, achieving an improvement of 3.5% in accuracy and 2.1% in F1-score.