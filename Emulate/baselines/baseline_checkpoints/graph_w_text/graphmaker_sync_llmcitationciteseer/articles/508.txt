Traditional ranking models in information retrieval (IR) often rely on complex feature engineering and exhaustive document matching, leading to high computational costs. This paper presents a novel hierarchical neural ranking model, HNR, which leverages self-attention mechanisms to capture contextualized document representations. HNR employs a two-stage ranking framework, where a coarse-grained ranking module first filters out irrelevant documents, followed by a fine-grained ranking module that refines the scores using hierarchical attention. Experimental results on the TREC-2004 dataset demonstrate that HNR achieves state-of-the-art performance while reducing computational costs by up to 40% compared to existing neural ranking models.