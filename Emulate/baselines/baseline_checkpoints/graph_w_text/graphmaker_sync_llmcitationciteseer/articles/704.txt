Visual question answering (VQA) tasks require models to jointly process visual and textual inputs. Existing approaches often concatenate or fuse features using simple concatenation or element-wise multiplication, neglecting the complex relationships between modalities. We propose a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant regions in images and words in questions, enabling more effective multi-modal fusion. Our experiments on the VQA 2.0 dataset demonstrate that HAN outperforms state-of-the-art methods, achieving a 3.2% absolute improvement in accuracy while reducing computational costs by 25%