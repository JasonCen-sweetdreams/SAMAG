Multimodal sentiment analysis has become increasingly important in various applications, including customer service chatbots and social media analysis. However, existing deep learning models often struggle to provide interpretable results. This paper proposes a novel hierarchical attention network (HAN) that integrates visual, textual, and acoustic features for multimodal sentiment analysis. Our HAN model learns to selectively focus on relevant modalities and features, providing explainable sentiment predictions. Experimental results on the CMU-Multimodal Opinion dataset demonstrate that our approach outperforms state-of-the-art models in terms of accuracy and interpretability.