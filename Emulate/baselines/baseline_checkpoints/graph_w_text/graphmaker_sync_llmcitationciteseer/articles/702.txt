Virtual reality (VR) interfaces have the potential to revolutionize human-computer interaction, but they also introduce significant cognitive load challenges. This paper presents a novel design framework for adaptive visualizations that dynamically adjust to the user's cognitive state. We propose a real-time cognitive load assessment model that combines eye-tracking, EEG, and performance metrics. Our approach is evaluated through a user study, demonstrating improved task performance and reduced mental fatigue in a VR-based problem-solving scenario. The results have implications for the design of more inclusive and user-centered VR interfaces.