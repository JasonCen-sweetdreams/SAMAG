Time-series forecasting is a crucial task in many applications, but existing methods often lack interpretability. This paper proposes a novel hierarchical attention network (HAN) architecture for explainable time-series forecasting. Our approach leverages self-attention mechanisms to identify relevant patterns at multiple scales, and then combines them using a hierarchical fusion strategy. We demonstrate the effectiveness of HAN on several benchmark datasets, showcasing improved forecasting accuracy and interpretable feature importance. Ablation studies and visualizations provide insights into the learned attention patterns, enabling better understanding of the underlying time-series dynamics.