Multi-modal queries, comprising both text and images, have become increasingly prevalent in modern information retrieval systems. However, existing document retrieval models struggle to effectively fuse and weigh the relevance of different modalities. This paper presents a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant regions of the query image and corresponding text fragments. Our experiments on the WikiImage dataset demonstrate significant improvements in retrieval accuracy and robustness, particularly for queries with ambiguous or abstract semantics. We also explore the interpretability of our HAN model, visualizing attention patterns to provide insights into the decision-making process.