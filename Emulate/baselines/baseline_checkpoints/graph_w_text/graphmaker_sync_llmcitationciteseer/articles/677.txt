Question answering over knowledge graphs (KGs) has gained significant attention in recent years. However, existing methods often suffer from the curse of dimensionality and inadequate modeling of complex relationships. This paper proposes a novel KG embedding approach, 'HieraQA', which leverages hierarchical attention to selectively focus on relevant entities and relations. We develop a multi-level attention mechanism that captures both local and global dependencies in the KG. Experimental results on three benchmark datasets demonstrate that HieraQA outperforms state-of-the-art methods in terms of answer accuracy and computational efficiency.