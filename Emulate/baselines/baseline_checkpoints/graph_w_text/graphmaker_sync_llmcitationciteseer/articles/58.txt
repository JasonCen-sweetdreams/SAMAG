The increasing adoption of deep learning models in real-world applications has raised concerns about their vulnerability to adversarial attacks. This paper presents a novel approach to detect such attacks using explainable AI techniques. Our method, 'ExpAd', analyzes the feature importance scores generated by SHAP values to identify anomalous input patterns that are indicative of adversarial attacks. We evaluate ExpAd on several benchmark datasets and demonstrate its effectiveness in detecting attacks with high accuracy, outperforming existing detection methods. Our approach has significant implications for the development of robust AI systems in security-critical applications.