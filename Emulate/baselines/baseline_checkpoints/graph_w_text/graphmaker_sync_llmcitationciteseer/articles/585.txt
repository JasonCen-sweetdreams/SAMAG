Voice-based interfaces have become ubiquitous, but individuals with speech disabilities, such as apraxia or dysarthria, often struggle to interact with these systems. This paper presents a novel framework for designing inclusive voice-based interfaces that accommodate diverse speech patterns. We introduce a machine learning-based approach that learns to recognize and adapt to individual speech characteristics, enabling more accurate and efficient interactions. Our user study with 20 participants with speech disabilities demonstrates significant improvements in interaction speed and accuracy compared to traditional speech recognition systems.