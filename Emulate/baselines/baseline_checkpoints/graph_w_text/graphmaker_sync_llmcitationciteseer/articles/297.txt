Deep neural networks (DNNs) are vulnerable to adversarial attacks, which can be detrimental in safety-critical applications. This paper presents a novel approach for detecting such attacks by leveraging Bayesian uncertainty estimates. We propose a probabilistic framework, 'BayesDefend', that models the uncertainty of DNN predictions and identifies abnormal input samples. Our method achieves state-of-the-art detection performance on several benchmark datasets, outperforming existing methods by up to 15% in terms of area under the receiver operating characteristic curve (AUC-ROC). We also demonstrate the effectiveness of BayesDefend in real-world scenarios, including image classification and natural language processing tasks.