Link prediction in knowledge graphs is a fundamental task in various AI applications. While existing methods focus on flat graph structures, real-world graphs often exhibit hierarchical relationships. We propose a novel knowledge graph embedding framework, 'HATKE', which leverages hierarchical attention mechanisms to capture multi-relational semantics. Our approach learns entity and relation embeddings that adapt to the hierarchical structure, improving link prediction performance. Experimental results on benchmark datasets demonstrate that HATKE outperforms state-of-the-art methods, particularly in scenarios with complex relational patterns.