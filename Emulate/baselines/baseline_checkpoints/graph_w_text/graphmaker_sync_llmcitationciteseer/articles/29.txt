Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a crucial task in human-computer interaction. While deep learning models have achieved high accuracy, they often lack interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates attention mechanisms at multiple levels to selectively focus on relevant modalities and features. We demonstrate that HAN outperforms state-of-the-art methods on the CMU-MOSEI dataset and provide visualizations to illustrate the attention patterns, enabling explainable emotion recognition.