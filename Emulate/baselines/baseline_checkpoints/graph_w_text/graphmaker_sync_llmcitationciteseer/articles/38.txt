Neural architecture search (NAS) has emerged as a promising approach for automated machine learning model design. However, existing methods often rely on computationally expensive evaluation metrics or surrogate models. This paper proposes a novel evolutionary multi-objective optimization (EMO) framework for NAS, which simultaneously optimizes multiple conflicting objectives such as model accuracy, latency, and energy consumption. Our approach leverages a population-based search strategy with adaptive mutation and crossover operators, enabling efficient exploration of the vast architecture space. Experiments on benchmark image classification tasks demonstrate that our EMO-NAS method yields competitive performance to state-of-the-art NAS algorithms at a significantly reduced computational cost.