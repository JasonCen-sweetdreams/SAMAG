Deep neural networks (DNNs) have become increasingly prevalent in various machine learning applications. However, their training process is often computationally expensive and time-consuming. This paper proposes a novel optimization method, 'Stochastic Weight Averaging with Momentum' (SWAM), which leverages stochastic weight averaging and momentum-based gradient descent to accelerate the training process. We theoretically analyze the convergence properties of SWAM and demonstrate its empirical effectiveness on several benchmark datasets. Experimental results show that SWAM achieves faster convergence rates and improved generalization performance compared to state-of-the-art optimization methods.