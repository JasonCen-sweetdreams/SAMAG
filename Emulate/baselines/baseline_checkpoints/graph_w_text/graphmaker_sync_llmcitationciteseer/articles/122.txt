Individuals with severe motor impairments face significant challenges interacting with digital systems. This paper presents EyeCare, a novel gaze-based interface that enables users to interact with graphical user interfaces using only their eye movements. EyeCare employs a machine learning-based gaze estimation model that leverages deep learning techniques to accurately detect and interpret user intentions. A user study with 15 participants demonstrates that EyeCare achieves an average accuracy of 92.5% in selecting UI elements, significantly outperforming existing state-of-the-art systems. EyeCare has the potential to greatly improve the quality of life for individuals with severe motor impairments.