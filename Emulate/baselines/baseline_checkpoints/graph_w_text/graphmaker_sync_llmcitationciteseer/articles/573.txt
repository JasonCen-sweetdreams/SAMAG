Dialogue state tracking (DST) is a crucial component in task-oriented dialogue systems, requiring the ability to accurately track user goals and preferences across multiple turns. This paper presents a novel hierarchical attention framework, 'HAT-DST', which leverages multi-modal inputs (text, speech, and vision) to improve DST performance. Our approach employs a hierarchical attention mechanism to selectively focus on relevant context and modalities, while incorporating contextualized embeddings to capture nuanced user intentions. Experimental results on the MultiWOZ 2.1 dataset demonstrate significant improvements in DST accuracy and robustness, especially in scenarios with noisy or ambiguous user input.