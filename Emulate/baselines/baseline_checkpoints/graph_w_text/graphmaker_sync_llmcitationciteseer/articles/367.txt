Emotional intelligence (EI) has become a critical aspect of human-computer interaction (HCI), as it enables systems to better understand and respond to users' emotional states. This paper presents a novel multimodal fusion approach for evaluating EI in HCI, combining facial expression analysis, speech patterns, and physiological signals. We conducted a user study with 50 participants, analyzing their interactions with an affective chatbot. Our results show that our approach significantly outperforms single-modality EI evaluation methods, achieving an accuracy of 87.2% in detecting users' emotional states. We discuss the implications of our findings for designing more empathetic and human-centered interfaces.