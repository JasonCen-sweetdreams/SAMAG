Anomaly detection in time-series data is a crucial task in various domains, including finance, healthcare, and manufacturing. This paper proposes a novel self-supervised representation learning approach, 'TS-Rep', which leverages the inherent temporal dependencies in time-series data to learn effective anomaly detectors. TS-Rep uses a contrastive loss function to learn a robust representation space, where anomalies are distinguished from normal patterns. Our experiments on several benchmark datasets demonstrate that TS-Rep outperforms state-of-the-art anomaly detection methods, achieving an average improvement of 12.5% in terms of F1-score.