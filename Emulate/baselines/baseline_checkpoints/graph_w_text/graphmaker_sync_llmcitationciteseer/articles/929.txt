Visual question answering (VQA) requires the integration of visual and linguistic features to accurately answer questions about images. This paper proposes a hierarchical graph attention network (HGAT) that leverages multi-modal feature fusion to improve VQA performance. Our approach uses graph attention to model relationships between visual regions, objects, and question words, and hierarchically aggregates features to capture complex contextual information. Experimental results on the VQA 2.0 dataset demonstrate that HGAT outperforms state-of-the-art models, achieving a 4.5% absolute improvement in accuracy.