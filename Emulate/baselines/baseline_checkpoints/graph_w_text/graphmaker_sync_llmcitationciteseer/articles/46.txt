Emotion recognition in human-computer interaction (HCI) involves analyzing diverse modalities of user data, including speech, text, and facial expressions. This paper presents a novel Hierarchical Attention Network (HAN) architecture that integrates multi-modal features to improve emotion recognition accuracy. Our HAN model employs modality-specific attention mechanisms to selectively focus on relevant features from each modality, and a hierarchical fusion strategy to combine modality representations. Experiment results on a benchmark HCI dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, with significant improvements in F1-score and correlation coefficient.