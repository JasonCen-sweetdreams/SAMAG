Emotion recognition from multi-modal data, such as text, audio, and vision, is a challenging task due to the complexity of modeling relationships between modalities. This paper proposes a novel hierarchical graph attention framework, 'HGA-EMO', which leverages graph neural networks to capture both local and global dependencies across modalities. Experimental results on the CMU-MOSEI dataset demonstrate that HGA-EMO outperforms state-of-the-art methods in recognizing emotions from multi-modal data, with improved scalability and interpretability.