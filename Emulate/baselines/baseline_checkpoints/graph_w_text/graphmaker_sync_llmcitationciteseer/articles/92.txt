Accurate emotion recognition is crucial in human-computer interaction (HCI) for developing empathetic systems. This paper proposes a novel hierarchical attention framework, 'HAT-Emo', for multi-modal emotion recognition. HAT-Emo integrates audio, visual, and textual features using attention mechanisms at multiple levels, capturing both local and global dependencies. Experimental results on the SEMAINE and EmoReact datasets demonstrate that HAT-Emo outperforms state-of-the-art methods in recognizing emotions from facial expressions, speech, and text. Our approach enables the development of more effective affective computing systems for HCI applications.