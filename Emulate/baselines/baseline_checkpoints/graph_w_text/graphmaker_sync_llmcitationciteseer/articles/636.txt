Explainable recommendation systems (ERS) have gained significant attention in recent years, but most existing approaches focus on either visual or textual explanations. This paper proposes a novel hierarchical attention network (HAN) that integrates multi-modal information from user reviews, item images, and contextual data to generate personalized explanations for recommendations. Our HAN model learns to selectively attend to relevant features across modalities, producing more accurate and informative explanations. Experimental results on a large-scale e-commerce dataset demonstrate that our approach outperforms state-of-the-art ERS methods in terms of both recommendation accuracy and explanation quality.