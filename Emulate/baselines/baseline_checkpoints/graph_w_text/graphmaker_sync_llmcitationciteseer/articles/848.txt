Autonomous drones require efficient navigation in uncertain environments, which is challenging due to the complexities of real-world scenarios. This paper presents a hierarchical reinforcement learning (HRL) framework, 'DroneHRL', which integrates high-level mission planning with low-level control policies. We propose a novel abstraction mechanism that enables the agent to reason about the environment at different levels of granularity, allowing for more effective navigation in uncertain scenarios. Experimental results demonstrate that DroneHRL outperforms state-of-the-art methods in terms of navigation efficiency and robustness to environmental uncertainty.