Graph neural networks (GNNs) have emerged as a powerful tool for graph-structured data analysis. However, their scalability and accuracy are limited by the batch processing mechanism, which can lead to high computational costs and memory usage. This paper proposes a novel batch processing framework, 'BatchGN', which leverages adaptive batch sizing, graph partitioning, and cache-friendly data layout to minimize memory access patterns and optimize computation. Experimental results on large-scale graph benchmarks demonstrate that BatchGN achieves up to 5x speedup and 10% accuracy improvement compared to existing state-of-the-art GNN implementations.