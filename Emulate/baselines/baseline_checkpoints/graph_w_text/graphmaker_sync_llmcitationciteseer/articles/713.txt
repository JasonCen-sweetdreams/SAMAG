Deep neural networks are vulnerable to adversarial attacks, which can mislead them into making incorrect predictions. This paper presents a novel approach to robustifying deep neural networks against such attacks using meta-learning. Our method, called 'MetaShield', learns to generate adversarial examples that are used to train a network to be robust against a wide range of attacks. We demonstrate the effectiveness of MetaShield on several benchmark datasets, including ImageNet and CIFAR-10, and show that it outperforms state-of-the-art defenses in terms of robustness and accuracy.