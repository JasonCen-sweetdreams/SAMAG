Contrastive learning has emerged as a powerful method for self-supervised representation learning in computer vision. However, existing approaches often rely on computationally expensive and memory-intensive techniques, limiting their applicability to large-scale datasets. This paper introduces Hierarchical Contrastive Learning (HCL), a novel framework that leverages a hierarchical representation of images to efficiently learn visual features. HCL employs a pyramidal architecture to capture multi-scale context and a momentum-based approach to maintain consistency across different scales. Our experiments on ImageNet and CIFAR-10 demonstrate that HCL outperforms state-of-the-art contrastive learning methods while reducing computational costs by up to 30%.