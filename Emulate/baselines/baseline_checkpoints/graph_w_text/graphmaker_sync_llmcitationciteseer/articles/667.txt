Multi-modal sentiment analysis (MSA) involves analyzing sentiment from diverse data sources such as text, images, and videos. While deep learning approaches have shown promising results, they often lack transparency and interpretability. This paper proposes a novel hierarchical attention network (HAN) architecture for MSA, which uses attention mechanisms to selectively focus on relevant modalities and features. We introduce a multi-modal fusion module that adaptively weights the importance of each modality based on the input data. Experimental results on a benchmark MSA dataset demonstrate that our approach outperforms state-of-the-art methods while providing interpretable attention weights for sentiment classification.