Upper limb impairments can significantly impact an individual's ability to interact with digital systems. This paper presents a novel approach to designing gesture-based interfaces that are inclusive of individuals with upper limb impairments. We conducted a participatory design study with 15 participants to identify common challenges and preferences in gesture interaction. Our findings informed the development of 'EasyGest', a gesture recognition system that uses machine learning to adapt to individual differences in motor abilities. A user study with 30 participants demonstrated that EasyGest significantly improves the accuracy and usability of gesture input for individuals with upper limb impairments.