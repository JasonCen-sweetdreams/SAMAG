With the growing adoption of virtual reality (VR) technology, there is a need for more intuitive and natural interaction mechanisms. This paper presents a novel gaze-based interaction system that leverages deep learning to accurately track and interpret user gazes in VR environments. Our approach uses a convolutional neural network (CNN) to analyze eye movement data from VR headsets and generates a probabilistic gaze map, which is then used to infer user intent. We evaluate our system through a user study, demonstrating significant improvements in interaction accuracy and user experience compared to traditional controller-based interfaces.