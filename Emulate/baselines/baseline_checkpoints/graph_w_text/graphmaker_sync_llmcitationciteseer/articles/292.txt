This paper addresses the problem of coordinated exploration in multi-agent systems, where agents must balance individual learning and collaboration to achieve a common goal. We propose a novel approach using deep Q-networks (DQN) to learn cooperative policies in partially observable environments. Our method, called Coordinated Exploration with DQN (CEDQN), incorporates a decentralized exploration strategy that adaptively adjusts the exploration rate based on the agents' observations and rewards. We evaluate CEDQN in a decentralized sensor network scenario, demonstrating improved convergence and robustness compared to existing decentralized reinforcement learning methods.