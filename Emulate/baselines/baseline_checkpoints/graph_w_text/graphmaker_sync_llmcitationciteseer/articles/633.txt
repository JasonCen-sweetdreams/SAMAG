The quest for efficient natural language processing (NLP) models has led to the development of various neural architectures. However, manual architecture design is time-consuming and requires significant expertise. This paper proposes a novel deep neural architecture search (NAS) method, 'LinguaNAS', which leverages reinforcement learning to automatically discover efficient NLP models. LinguaNAS uses a hierarchical search space and a novel reward function that balances model accuracy and computational cost. Experiments on several NLP benchmarks demonstrate that LinguaNAS discovers models that outperform manually designed counterparts while reducing inference latency by up to 3x.