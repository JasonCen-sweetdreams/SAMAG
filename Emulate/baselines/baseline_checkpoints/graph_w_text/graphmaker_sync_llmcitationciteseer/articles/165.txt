Reinforcement learning (RL) has achieved remarkable success in various domains, but the lack of interpretability hinders its applicability in high-stakes decision-making scenarios. This paper proposes a novel approach to explainable RL by incorporating contrastive regularization into the policy optimization process. Our method, Contrastive-RL, encourages the agent to learn a compact and meaningful representation of the state space, enabling the identification of key factors driving its decision-making. We demonstrate the effectiveness of Contrastive-RL on a range of Atari games and a real-world robotics task, showcasing improved transparency and performance.