Effective human-computer collaboration relies on the system's ability to understand users' emotional states. This paper introduces EmoTract, a multimodal framework that infers users' affective states from interaction traces, including facial expressions, speech patterns, and keyboard/mouse dynamics. We propose a novel fusion approach that combines the strengths of machine learning and psychological models to improve inference accuracy. Evaluation on a real-world dataset shows that EmoTract outperforms state-of-the-art methods in recognizing emotional states, enabling more empathetic and personalized human-computer interactions.