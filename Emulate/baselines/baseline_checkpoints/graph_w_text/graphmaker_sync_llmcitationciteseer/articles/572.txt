Node classification is a fundamental task in graph-based learning, but existing methods often struggle to capture complex relationships between nodes. This paper proposes a novel hierarchical attention-based graph neural network (HAGNN) architecture, which leverages multi-scale node representations and attention weights to selectively focus on relevant neighbors. We demonstrate the effectiveness of HAGNN on several benchmark datasets, achieving state-of-the-art performance in node classification tasks. Moreover, we provide empirical evidence of the benefits of hierarchical attention in capturing long-range dependencies and improving robustness to noisy graph structures.