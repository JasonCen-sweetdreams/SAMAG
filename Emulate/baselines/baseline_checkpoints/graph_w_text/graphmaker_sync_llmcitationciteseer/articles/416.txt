Emotion recognition from multi-modal inputs, such as speech, text, and vision, has numerous applications in human-computer interaction and affective computing. However, existing models often lack interpretability, making it difficult to understand the decision-making process. This paper proposes a novel Hierarchical Attention Network (HAN) that incorporates attention mechanisms at multiple levels to selectively focus on relevant modalities and features. Our experiments on the CMU-MOSEI dataset demonstrate improved emotion recognition accuracy and provide insights into the attention patterns, enabling explainable AI for real-world applications.