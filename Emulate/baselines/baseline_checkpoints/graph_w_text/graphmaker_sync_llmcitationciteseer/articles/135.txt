Emotion recognition is a crucial aspect of human-computer interaction, with applications in fields such as mental health, customer service, and entertainment. This paper presents a novel multimodal approach that leverages audio, video, and text inputs to recognize emotions in real-time. Our proposed architecture, called 'EmoFusion', employs attention-based deep neural networks to selectively focus on relevant features from each modality, leading to improved recognition accuracy and robustness. Experimental results on a large, publicly available dataset demonstrate the effectiveness of EmoFusion in recognizing emotions in diverse contexts, outperforming state-of-the-art unimodal and multimodal approaches.