In multi-agent cooperation, understanding the decision-making process is crucial for trust and accountability. This paper introduces a novel Hierarchical Attention Network (HAN) framework that enables explainable cooperation among agents. HAN learning agents attend to relevant information from both local observations and communication with other agents, allowing for interpretable decision-making. We demonstrate the effectiveness of HAN in a variety of cooperative tasks, including joint navigation and resource allocation, and show that it outperforms existing methods in terms of both performance and explainability.