The proliferation of edge computing has introduced significant challenges in resource allocation, particularly in scenarios with dynamic workloads and heterogeneous devices. This paper proposes a deep reinforcement learning (DRL) framework, 'EdgeOpt', which learns to optimize resource allocation in edge computing environments. EdgeOpt employs a hierarchical architecture that combines graph neural networks with deep Q-networks, enabling efficient allocation of computing resources, memory, and bandwidth. Experimental results on a real-world edge computing testbed demonstrate that EdgeOpt outperforms state-of-the-art heuristic-based approaches, achieving improved response times, reduced latency, and enhanced resource utilization.