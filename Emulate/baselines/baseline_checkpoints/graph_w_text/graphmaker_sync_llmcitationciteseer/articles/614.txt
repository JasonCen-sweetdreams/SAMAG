Multimodal sentiment analysis is a challenging task that requires modeling complex relationships between visual, textual, and acoustic features. This paper proposes a novel hierarchical graph attention network (HGAT) that captures both local and global dependencies across modalities. Our approach leverages graph attention mechanisms to learn modality-specific and multimodal representations, which are then fused using a hierarchical pooling strategy. Experimental results on the MMIMDB and CMU-MOSEI datasets demonstrate that HGAT outperforms state-of-the-art methods in multimodal sentiment analysis tasks, achieving significant improvements in accuracy and F1-score.