With the increasing prevalence of multi-modal documents, such as those containing images, videos, and text, traditional information retrieval systems are struggling to efficiently retrieve relevant documents. This paper proposes a novel approach that leverages hierarchical neural embeddings to jointly model the semantic relationships between different modalities. Our method, called Hierarchical Multi-Modal Retrieval (HMMR), learns a hierarchical representation of documents by recursively applying a neural embedding module to each modality. Experiments on a large-scale dataset show that HMMR outperforms state-of-the-art methods in terms of retrieval efficiency and effectiveness, while also providing interpretable results.