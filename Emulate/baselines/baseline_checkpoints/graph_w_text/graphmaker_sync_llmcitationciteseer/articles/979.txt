This paper proposes a hierarchical multi-agent reinforcement learning (MARL) framework for coordinating autonomous vehicles in complex urban scenarios. Our approach, called HiMAC, consists of two levels: a high-level planner that assigns tasks to individual vehicles, and a low-level controller that executes the tasks using deep reinforcement learning. We demonstrate the effectiveness of HiMAC in a simulated environment, where it outperforms traditional distributed control methods in terms of traffic flow, safety, and fuel efficiency. Our results have implications for the development of large-scale autonomous transportation systems.