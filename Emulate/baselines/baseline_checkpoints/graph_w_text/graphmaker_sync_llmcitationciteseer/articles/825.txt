Deep neural networks (DNNs) are increasingly deployed on resource-constrained devices, where model size and computational complexity are critical concerns. This paper presents a novel regularization technique, 'AdaReg', which adaptively adjusts the pruning rate of DNNs based on the availability of computational resources. Our approach leverages a reinforcement learning framework to optimize the trade-off between model accuracy and compression ratio. Experimental results on several benchmark datasets demonstrate that AdaReg outperforms existing pruning methods in terms of model size reduction while preserving accuracy, making it suitable for real-world applications with varying resource constraints.