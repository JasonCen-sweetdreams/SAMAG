Voice assistants have become ubiquitous in modern life, but their usefulness is severely limited for individuals with dysarthria, a speech disorder characterized by difficulty articulating words. This paper presents a novel framework for designing accessible voice assistants that can better understand and respond to users with dysarthria. We develop a speaker-dependent acoustic model that leverages transfer learning from a large corpus of healthy speech data and fine-tunes on a small dataset of dysarthric speech. Our evaluation shows that the proposed system significantly improves transaction success rates and user satisfaction compared to commercial voice assistants. We also provide design recommendations for voice assistant interfaces to facilitate more effective communication with users with dysarthria.