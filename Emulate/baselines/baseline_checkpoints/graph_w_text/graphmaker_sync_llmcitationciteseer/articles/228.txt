Understanding users' emotional states is crucial for designing empathetic and personalized human-computer interaction systems. This paper presents a novel multimodal fusion framework that integrates facial expression, speech, and physiological signals to infer users' emotional states in real-time. Our approach leverages a deep neural network to learn a shared representation across modalities, and a probabilistic graphical model to reason about the uncertainty of emotional state inference. Experimental results on a large-scale HCI dataset demonstrate the effectiveness of our approach in improving the accuracy and robustness of emotional state inference, compared to traditional unimodal and early fusion approaches.