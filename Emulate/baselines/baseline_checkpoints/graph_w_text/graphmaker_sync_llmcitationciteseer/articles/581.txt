Distributed databases have become increasingly prevalent in modern data management systems. However, optimizing query performance in these systems remains a significant challenge. This paper proposes a novel approach that leverages machine learning to optimize query execution plans. We develop a learning-based cost model that accurately predicts query execution times and a reinforcement learning-based optimizer that adapts to changing database workloads. Experimental results on a real-world distributed database benchmark demonstrate that our approach outperforms state-of-the-art query optimizers by up to 30% in terms of query latency and 25% in terms of resource utilization.