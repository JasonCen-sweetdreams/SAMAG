Deep learning models have been shown to be vulnerable to various types of attacks, including image-based and audio-based attacks. This paper investigates the robustness of deep learning models against multi-modal attacks, where an attacker manipulates both images and audio inputs to deceive the model. We propose a novel framework, 'MM-Defense', that leverages adversarial training and input pre-processing to improve the robustness of deep learning models against multi-modal attacks. Our experiments demonstrate that MM-Defense achieves significant improvements in robustness against state-of-the-art attacks, while maintaining accuracy on clean data.