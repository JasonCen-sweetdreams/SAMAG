Deep neural networks are prone to exploding gradients, which can lead to unstable training and poor generalization. This paper proposes an online learning algorithm, 'AdaClip', that adaptively clips gradients based on the local curvature of the loss landscape. We introduce a novel metric, 'gradient fragility', to quantify the sensitivity of gradients to parameter updates. Experimental results on CIFAR-10 and ImageNet datasets demonstrate that AdaClip converges faster and achieves improved robustness to adversarial attacks compared to state-of-the-art gradient clipping methods.