Real-time object detection is a critical component of autonomous vehicle systems. This paper presents a novel hierarchical attention-based YOLO (HAT-YOLO) framework that improves detection accuracy and efficiency in complex scenes. Our approach integrates a hierarchical attention mechanism that adaptively focuses on regions of interest, reducing computational overhead and improving robustness to occlusions and varying lighting conditions. Experimental results on the KITTI and Cityscapes datasets demonstrate that HAT-YOLO outperforms state-of-the-art detectors, achieving an average precision of 92.1% at 30 FPS on a single NVIDIA GPU.