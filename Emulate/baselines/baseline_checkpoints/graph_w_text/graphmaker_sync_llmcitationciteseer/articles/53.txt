Multimodal emotion recognition (MER) involves analyzing human emotions from various modalities such as speech, text, and vision. Existing MER approaches often rely on early fusion or late fusion strategies, which may not effectively capture complex relationships between modalities. This paper proposes a hierarchical graph attention network (HGAT) that leverages graph neural networks and attention mechanisms to model multimodal interactions. Our HGAT framework consists of a modality-specific encoder, a graph attention module, and a fusion layer. Experiments on the CMU-MOSEI dataset demonstrate that HGAT outperforms state-of-the-art MER models, achieving an average F1-score of 83.2% for emotion classification.