Deep reinforcement learning (DRL) has achieved state-of-the-art performance in autonomous vehicle control tasks. However, recent studies have shown that DRL policies are vulnerable to adversarial attacks, which can compromise safety and reliability. This paper investigates the robustness of DRL policies in autonomous vehicles against targeted attacks on sensor inputs. We propose a novel attack framework, 'AV-Attack', which leverages generative models to craft perturbations that deceive the DRL policy into making unsafe decisions. Our experiments demonstrate the effectiveness of AV-Attack on various DRL algorithms, highlighting the need for robustness evaluation and adversarial training in autonomous vehicle development.