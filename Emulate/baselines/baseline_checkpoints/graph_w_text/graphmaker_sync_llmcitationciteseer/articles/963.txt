Deep neural networks (DNNs) have achieved state-of-the-art performance in various applications, but their computational complexity and memory requirements hinder their deployment on resource-constrained devices. This paper proposes a novel meta-pruning approach, which adaptively prunes filters in DNNs based on their importance, measured by a learnable meta-model. Our method, called Meta-Prune, dynamically adjusts the pruning ratio and filter importance during training, resulting in more efficient models with minimal accuracy loss. Experimental results on ImageNet and CIFAR-10 datasets demonstrate that Meta-Prune outperforms existing pruning methods, achieving up to 5.2x compression ratio and 2.3x speedup on mobile devices.