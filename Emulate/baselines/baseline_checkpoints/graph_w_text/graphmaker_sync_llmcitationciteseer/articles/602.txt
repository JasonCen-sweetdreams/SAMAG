Emotion recognition is a crucial aspect of human-computer interaction, but existing approaches often rely on a single modality, such as speech or text. We propose a hierarchical attention network (HAN) that fuses multimodal inputs from speech, text, and vision to recognize emotions more accurately. Our HAN model consists of two stages: intra-modal attention, which captures local dependencies within each modality, and inter-modal attention, which integrates information across modalities. Experimental results on the IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art single-modality models and achieves a 15% improvement in emotion recognition accuracy.