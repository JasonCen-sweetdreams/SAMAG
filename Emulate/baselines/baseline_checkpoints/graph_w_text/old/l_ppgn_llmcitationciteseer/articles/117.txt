This paper presents a novel approach to cooperative task allocation for multi-agent systems using distributed reinforcement learning. We propose a decentralized framework, called MARL-TA, which enables agents to learn and adapt to changing task requirements and environment dynamics. MARL-TA employs a hierarchical architecture, where agents learn to allocate tasks based on local observations and communicate with neighbors to resolve conflicts and achieve global optimality. Experimental results show that MARL-TA outperforms existing methods in terms of task completion rate, makespan, and adaptability to changing scenarios.