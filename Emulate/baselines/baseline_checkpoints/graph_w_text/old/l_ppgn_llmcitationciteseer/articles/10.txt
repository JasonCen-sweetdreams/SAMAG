Visual question answering (VQA) models often rely on opaque attention mechanisms, making it challenging to interpret their decision-making processes. This paper presents a novel hierarchical attention framework, HARE, that incorporates explainable reasoning into VQA models. HARE leverages a multi-scale attention mechanism to focus on relevant regions of the image and generate attention-based explanations for the predicted answer. We demonstrate the effectiveness of HARE on several VQA benchmarks, achieving state-of-the-art results while providing transparent and interpretable explanations. Our approach has significant implications for building trustworthy AI systems in real-world applications.