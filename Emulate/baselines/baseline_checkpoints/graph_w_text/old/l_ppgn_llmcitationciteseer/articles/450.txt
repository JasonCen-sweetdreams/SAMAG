Emotion recognition in human-computer interaction is a crucial task, as it enables systems to respond empathetically to users. However, existing methods rely on a single modality (e.g., facial expressions or speech) and often struggle with multimodal fusion. We propose a hierarchical attention network (HAN) that integrates visual, audio, and linguistic features to recognize emotions. Our HAN employs a novel attention mechanism that adaptively weights modality-specific features based on their relevance to the emotional context. Experiments on a large multimodal dataset show that our approach outperforms state-of-the-art methods in recognizing emotions, achieving an F1-score of 0.92.