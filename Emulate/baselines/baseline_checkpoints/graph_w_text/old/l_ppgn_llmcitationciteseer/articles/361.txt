Multimodal sentiment analysis (MSA) aims to jointly analyze visual, textual, and acoustic features to infer sentiment from user-generated content. Existing methods often rely on feature fusion or early fusion, which may not effectively capture complex cross-modal interactions. We propose a novel Hierarchical Attention Network (HAN) that models intra-modal and inter-modal attention to selectively focus on relevant features. Our HAN framework comprises a multimodal encoder, a hierarchical attention mechanism, and a sentiment predictor. Experimental results on the CMU-MOSI dataset demonstrate that HAN outperforms state-of-the-art methods in MSA tasks, achieving an average improvement of 3.2% in sentiment accuracy.