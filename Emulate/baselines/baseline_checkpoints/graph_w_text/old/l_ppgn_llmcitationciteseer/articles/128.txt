Recognizing emotions from multimodal inputs, such as speech, text, and facial expressions, is a challenging task. This paper proposes a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features for emotion recognition. Our HAN model consists of three levels of attention: modality-level, feature-level, and instance-level. We evaluate our approach on the Multimodal Emotion Recognition benchmark and achieve state-of-the-art performance, outperforming existing multimodal fusion methods by 5.2% in terms of weighted F1-score.