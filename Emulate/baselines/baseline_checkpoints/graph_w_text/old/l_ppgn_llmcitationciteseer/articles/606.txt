Emotion recognition in conversational dialogue systems is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates acoustic, linguistic, and visual features from speech, text, and facial expressions. Our HAN model learns to selectively focus on salient features across modalities and temporal segments, outperforming state-of-the-art approaches on the IEMOCAP and MELD datasets. We demonstrate the effectiveness of our approach in improving the overall emotion recognition accuracy and its robustness to noisy or missing modalities.