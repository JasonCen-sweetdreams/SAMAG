In complex, dynamic environments, task allocation among agents is a challenging problem. This paper proposes a novel multi-agent reinforcement learning framework, 'DyMAT', which enables agents to learn distributed task allocation strategies. DyMAT utilizes a decentralized, asynchronous learning approach, where each agent learns to allocate tasks based on local observations and communication with neighboring agents. We evaluate DyMAT in a simulated disaster response scenario and demonstrate improved task allocation efficiency and adaptability compared to traditional, centralized approaches.