Deep learning models have achieved state-of-the-art performance in various applications, but their success heavily relies on careful hyperparameter tuning. This paper proposes a novel Bayesian optimization framework, 'HyperBO', which leverages a probabilistic model to efficiently search the hyperparameter space. We introduce a new acquisition function that balances exploration and exploitation, and demonstrate its effectiveness on several benchmark datasets. Our experiments show that HyperBO outperforms existing methods, achieving better model performance with significantly reduced computational cost.