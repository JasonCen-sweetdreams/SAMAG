Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is crucial for human-computer interaction and affective computing. However, existing approaches often suffer from high computational costs and limited scalability. This paper presents a novel Hierarchical Attention Network (HAN) architecture that incorporates modal-specific attention mechanisms to selectively focus on salient features from each input modality. We demonstrate that HAN outperforms state-of-the-art methods on a benchmark dataset, achieving improved emotion recognition accuracy while reducing computational overhead by up to 30%. Our approach has significant implications for real-world applications, such as affective robots and empathetic virtual assistants.