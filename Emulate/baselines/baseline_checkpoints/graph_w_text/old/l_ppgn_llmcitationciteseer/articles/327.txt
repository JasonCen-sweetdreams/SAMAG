Virtual reality (VR) systems often rely on manual input devices, limiting the immersiveness and accessibility of VR experiences. This paper presents a novel gaze-based interaction method, 'GazeNet', which leverages convolutional neural networks (CNNs) to detect and track user gaze in real-time. Our approach uses a CNN-based gaze estimation model, trained on a large dataset of eye movement recordings, to predict the user's intended target in the virtual environment. We evaluate GazeNet in a series of user studies, demonstrating improved interaction accuracy and reduced user fatigue compared to traditional manual input methods.