Emotion recognition is a crucial aspect of human-robot interaction, enabling robots to respond empathetically to users. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates multi-modal inputs from facial expressions, speech, and physiological signals. Our HAN model learns to selectively focus on relevant modalities and features, achieving state-of-the-art performance on the EMotion Recognition in Human-Robot Interaction (EMR-HRI) benchmark. We also demonstrate the effectiveness of our approach in a real-world HRI scenario, where a robot employs emotional intelligence to provide personalized support to users.