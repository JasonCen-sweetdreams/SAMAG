Transformer-based models have achieved state-of-the-art performance in various natural language processing (NLP) tasks. However, their robustness to adversarial attacks remains a concern. This paper investigates the susceptibility of popular transformer architectures to input perturbations and proposes a novel defense mechanism. We develop a regularization technique that encourages the model to learn more robust representations by penalizing the similarity between clean and adversarial input embeddings. Our experiments on several NLP benchmarks demonstrate that the proposed approach improves the model's resistance to attacks while maintaining its performance on clean data.