Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the complexities of human emotions and the variability of modalities. This paper proposes a novel hierarchical attention-based neural network (HATNet) that learns to selectively focus on relevant modalities and fuse their representations for improved emotion recognition. We evaluate HATNet on three benchmark datasets and demonstrate its superiority over state-of-the-art models in terms of recognition accuracy and robustness to noisy inputs.