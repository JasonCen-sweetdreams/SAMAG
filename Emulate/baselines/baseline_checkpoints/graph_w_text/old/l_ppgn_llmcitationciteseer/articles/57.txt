Neural architecture search (NAS) has revolutionized the design of deep neural networks. However, the search process is often computationally expensive and requires significant domain expertise. This paper proposes a novel graph-based reinforcement learning approach, 'GraphNAS', which leverages the structural properties of neural architectures to guide the search process. By representing architectures as graphs, we can efficiently explore the vast search space using graph-based reinforcement learning algorithms. Experimental results on popular benchmark datasets demonstrate that GraphNAS achieves state-of-the-art performance with significantly reduced computational resources and search time.