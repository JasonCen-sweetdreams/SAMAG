Reinforcement learning (RL) has shown remarkable success in complex decision-making tasks, but its opacity hinders trust and reliability. This paper presents a novel architecture, 'Explainable-DQN', that integrates model-based RL with attention-based explanations. By leveraging environment dynamics, our approach generates interpretable policies and explicit state importance scores. Experimental results on Atari games and real-world robotics tasks demonstrate improved explainability, policy robustness, and adaptation to changing environments.