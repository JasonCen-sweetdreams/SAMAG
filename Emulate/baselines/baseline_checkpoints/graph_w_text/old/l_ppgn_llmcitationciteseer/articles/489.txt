Emotion recognition from multimodal data, such as speech, text, and vision, remains a challenging task due to the complexity of human emotions and the varying importance of different modalities. This paper proposes a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features at multiple levels of abstraction. Our HAN model outperforms state-of-the-art approaches on the IEMOCAP and CMU-MOSEI datasets, achieving improved recognition accuracy and robustness to noisy or missing data. We also provide insights into the attention patterns learned by our model, revealing interesting correlations between modalities and emotions.