Emotion recognition from multi-modal inputs, such as speech, text, and facial expressions, remains a challenging task in affective computing. We propose a novel hierarchical attention network (HAN) that adaptively fuses features from different modalities and captures complex contextual relationships. Our HAN model consists of modal-specific attention modules, a cross-modal fusion layer, and a hierarchical attention mechanism that selectively focuses on salient modalities and temporal segments. Experimental results on the MLEEP dataset demonstrate the superiority of our approach over state-of-the-art methods, achieving an average F1-score improvement of 8.2% on emotion classification tasks.