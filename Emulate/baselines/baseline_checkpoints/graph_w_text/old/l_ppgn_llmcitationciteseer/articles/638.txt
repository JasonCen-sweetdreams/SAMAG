Emotion recognition from multi-modal data (e.g., speech, text, and vision) has gained increasing attention in human-computer interaction. However, existing approaches often suffer from high computational costs and limited scalability. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that efficiently integrates information from multiple modalities. Our HAN model employs a hierarchical attention mechanism to selectively focus on relevant features across modalities, reducing the computational complexity while improving recognition accuracy. Extensive experiments on the CMU-MOSI dataset demonstrate the effectiveness of our approach, achieving a 12.5% relative improvement in F1-score compared to state-of-the-art methods.