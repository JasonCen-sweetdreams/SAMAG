Emotion recognition is a complex task that involves understanding subtle cues from various modalities. This paper presents a novel hierarchical attention network (HAN) for multimodal emotion recognition, which combines convolutional neural networks (CNNs) for feature extraction and recurrent neural networks (RNNs) for temporal modeling. Our HAN architecture learns to selectively focus on relevant modalities and timestamps, leading to improved recognition performance. Experiments on the Multimodal Emotion Recognition Challenge (MERC) dataset show that our approach outperforms state-of-the-art methods by 5.2% in terms of weighted F1-score.