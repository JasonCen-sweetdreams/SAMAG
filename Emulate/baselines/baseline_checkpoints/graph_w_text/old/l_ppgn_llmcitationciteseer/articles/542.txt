Explainability is crucial in multi-agent systems, where autonomous decision-making can have significant consequences. This paper proposes a novel hierarchical attention network (HAN) architecture that interprets the decision-making process of multiple agents interacting in a shared environment. Our HAN model learns to focus on relevant agent-agent and agent-environment interactions, generating attention weights that provide insight into the decision-making process. Experimental results on a real-world autonomous driving dataset demonstrate improved explainability and decision-making performance compared to state-of-the-art reinforcement learning approaches.