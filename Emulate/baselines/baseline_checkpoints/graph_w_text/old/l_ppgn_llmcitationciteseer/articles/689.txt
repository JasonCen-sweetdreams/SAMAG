Multimodal dialogue systems require efficient processing of visual and textual inputs to generate coherent responses. We propose a novel attention mechanism, 'Hierarchical Cross-Modal Fusion' (HCMF), which leverages the strengths of both modalities to improve response generation. HCMF uses a hierarchical encoder to jointly represent visual and textual features, and a multimodal attention module to selectively focus on relevant input regions. Experimental results on the DSTC7 benchmark demonstrate that HCMF outperforms state-of-the-art models in terms of response quality and inference speed, making it suitable for real-world applications.