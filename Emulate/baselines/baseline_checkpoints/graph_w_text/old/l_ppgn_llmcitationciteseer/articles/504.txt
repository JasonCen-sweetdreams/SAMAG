Deep reinforcement learning has shown remarkable success in various robotics tasks, but its application to real-world scenarios is often hindered by sample inefficiency and exploration-exploitation trade-offs. This paper presents Hierarchical Deep Deterministic Policy Gradients (HDDPG), a novel hierarchical RL framework that leverages a two-level abstraction to tackle these challenges. HDDPG learns a high-level policy to select sub-goals and a low-level policy to achieve them, resulting in improved exploration efficiency and adaptability to changing environments. We demonstrate HDDPG's effectiveness in a range of robotic tasks, including robotic arm manipulation and autonomous driving.