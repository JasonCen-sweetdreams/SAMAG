Emotion recognition in human-robot interaction (HRI) is crucial for designing empathetic and socially intelligent robots. This paper presents a novel hierarchical attention network (HAN) that fuses multi-modal cues from speech, text, and vision to recognize emotions in HRI. Our HAN model consists of modal-specific attention modules and a hierarchical fusion layer that adaptively weights the importance of each modality. We evaluate our approach on a benchmark HRI dataset and demonstrate significant improvements in emotion recognition accuracy compared to state-of-the-art methods. Our results suggest that HAN is a promising approach for enabling robots to recognize and respond to human emotions in a more human-like manner.