Emotion recognition from multi-modal inputs, such as speech, text, and vision, is a challenging task due to the varying importance of each modality in different contexts. We propose a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and features at multiple scales. Our approach leverages a hierarchical structure to model relationships between modalities and a multi-head attention mechanism to adaptively weight features. Experimental results on the IEMOCAP dataset demonstrate that our HAN model outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an average F1-score improvement of 12.5%.