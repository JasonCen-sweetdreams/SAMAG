Traditional ranking models in information retrieval (IR) often rely on hand-crafted features, which can be limited in capturing complex document relationships. This paper proposes a hierarchical neural ranking model, HierRank, which leverages pre-trained language models and learns to represent documents as hierarchical structures. We demonstrate that HierRank outperforms state-of-the-art neural ranking models on several benchmark datasets, including the WikiQA and TREC-CAR datasets. Furthermore, we show that HierRank is more efficient in terms of computational resources and query latency, making it suitable for large-scale IR applications.