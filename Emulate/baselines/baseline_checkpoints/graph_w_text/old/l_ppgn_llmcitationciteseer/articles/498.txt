Deep reinforcement learning (DRL) has achieved remarkable successes in complex, partially observable environments. However, the lack of transparency in DRL models hinders their deployment in high-stakes applications. This paper proposes a novel explainability framework, 'POEXP', which leverages attention mechanisms and model-based reasoning to provide interpretable explanations for DRL policies. We demonstrate the effectiveness of POEXP in a series of simulated robotics experiments, showcasing its ability to identify critical state features and action sequences that contribute to policy decisions. Our approach enables more informed decision-making and facilitates the development of more trustworthy AI systems.