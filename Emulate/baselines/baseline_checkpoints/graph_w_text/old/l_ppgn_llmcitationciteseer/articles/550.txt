Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the complexity of human emotions and the heterogeneity of modalities. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) for multi-modal emotion recognition. HGAT leverages graph attention mechanisms to model relationships between modalities and hierarchically fuse features from different levels. Experimental results on the MMEmo database demonstrate that HGAT outperforms state-of-the-art methods in recognizing emotions from multi-modal data, achieving an F1-score of 83.2%.