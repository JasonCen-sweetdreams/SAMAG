Emotion recognition from multi-modal data, such as speech, text, and vision, remains a challenging task due to the complexity of human emotions and the heterogeneity of modalities. This paper proposes a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and features at multiple levels of abstraction. Our approach achieves state-of-the-art performance on the IEMOCAP and CMU-MOSEI datasets, outperforming existing fusion-based methods. We also provide insights into the interpretability of our model, demonstrating its ability to identify salient features and modalities that contribute to emotion recognition.