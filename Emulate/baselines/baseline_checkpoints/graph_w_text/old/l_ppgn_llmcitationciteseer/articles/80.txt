Individuals with motor impairments face significant challenges when interacting with touch-based devices. This paper presents a novel gesture recognition system that adapts to the unique abilities and needs of each user. Our approach combines machine learning with participatory design, involving users with motor impairments in the development and testing of the system. We introduce a new gesture representation framework that incorporates temporal and spatial features, enabling more accurate recognition of gestures performed by users with varying levels of motor control. Results from a user study demonstrate improved gesture recognition accuracy and user satisfaction compared to existing approaches.