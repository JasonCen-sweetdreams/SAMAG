Recommendation systems have become ubiquitous in modern online platforms, but their opacity has raised concerns about fairness, accountability, and user trust. This paper proposes a novel graph attention-based approach, 'ExplainRec', which leverages item relationships and user behaviors to generate interpretable recommendations. By learning attention weights on the graph, our model provides insight into the reasoning behind each recommendation. Experiments on real-world datasets demonstrate that ExplainRec outperforms state-of-the-art models in terms of recommendation accuracy while providing transparent explanations.