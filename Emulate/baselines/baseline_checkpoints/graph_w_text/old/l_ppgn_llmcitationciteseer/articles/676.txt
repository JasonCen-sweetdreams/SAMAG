Multi-hop question answering (MHQA) requires models to reason over multiple documents to answer complex questions. While existing approaches often rely on black-box neural networks, they lack transparency and interpretability. This paper proposes a novel hierarchical attention-based model, HARE, which incorporates explainable reasoning mechanisms to identify relevant documents, sentences, and entities. HARE leverages a graph-based attention framework to model relationships between entities and documents, enabling the model to provide explicit reasoning paths for its answers. Experimental results on the HotPotQA benchmark demonstrate that HARE outperforms state-of-the-art models while providing interpretable explanations for its answers.