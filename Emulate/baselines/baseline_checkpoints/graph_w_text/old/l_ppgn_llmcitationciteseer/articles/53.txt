Time series forecasting is a crucial task in many applications, but modeling long sequences poses significant challenges. This paper introduces Hierarchical Attention Networks (HAN), a novel architecture that leverages hierarchical representations and attention mechanisms to capture complex patterns in long sequences. HAN is designed to scale efficiently to long sequences by adaptively selecting relevant segments and encoding them into a compact representation. We evaluate HAN on several benchmark datasets and demonstrate its superiority over state-of-the-art methods in terms of accuracy and computational efficiency.