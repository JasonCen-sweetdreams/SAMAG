Emotion recognition in human-computer interaction (HCI) is a challenging task due to the complexity of human emotions and the variability of input modalities. This paper proposes a novel hierarchical attention network (HAN) that integrates multi-modal features from speech, text, and vision to recognize emotions in HCI. Our HAN model employs a hierarchical structure to capture both local and global contextual information, and attention mechanisms to selectively focus on relevant features. Experimental results on a benchmark HCI dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an accuracy of 92.1%.