Deep reinforcement learning (DRL) has achieved remarkable success in various domains, but its lack of transparency hinders its adoption in high-stakes applications. This paper presents a novel approach to explainability in DRL, introducing a model-agnostic saliency map technique that highlights critical state features contributing to the agent's decision-making process. By leveraging the concept of visual attention, our method provides insightful explanations for complex DRL policies, enabling the identification of biases and vulnerabilities. We demonstrate the effectiveness of our approach on several Atari games and a real-world robotics task.