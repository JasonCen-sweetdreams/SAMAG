Emotion recognition from multimodal inputs (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the heterogeneity of modalities. We propose a novel Hierarchical Attention Network (HAN) architecture that leverages the strengths of each modality to improve emotion recognition accuracy. Our HAN model consists of modality-specific attention modules that hierarchically refine feature representations, enabling the model to focus on relevant emotional cues. We demonstrate the effectiveness of our approach on three benchmark datasets, achieving state-of-the-art performance and providing interpretable attention weights for explainable emotion recognition.