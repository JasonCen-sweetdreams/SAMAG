Deep reinforcement learning (DRL) has achieved remarkable success in complex decision-making tasks, but its lack of transparency hinders its adoption in high-stakes applications. This paper proposes a novel approach to explaining DRL policies via model-based state abstraction. Our method, called 'AbstrEx', learns a compact, symbolic representation of the environment that is amenable to human understanding. By abstracting away task-irrelevant details, AbstrEx enables the generation of concise, actionable explanations for DRL decisions. We demonstrate the effectiveness of AbstrEx on a range of Atari games and a real-world robotics task, showcasing improved explainability without sacrificing policy performance.