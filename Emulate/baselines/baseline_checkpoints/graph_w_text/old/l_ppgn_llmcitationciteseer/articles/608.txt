Autonomous drone swarming has the potential to revolutionize various applications, including search and rescue, environmental monitoring, and aerial surveillance. However, the complexity of swarm control and the need for real-time decision-making pose significant challenges. This paper presents a hierarchical reinforcement learning framework for autonomous drone swarming, which decomposes the control problem into high-level task allocation and low-level motion planning. We leverage a multi-agent actor-critic architecture to learn cooperative policies that adapt to dynamic environments and uncertain agent states. Experimental results demonstrate improved swarm performance and resilience in simulated and real-world scenarios.