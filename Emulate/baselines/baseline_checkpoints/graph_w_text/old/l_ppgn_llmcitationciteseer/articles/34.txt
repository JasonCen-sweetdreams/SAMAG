Neural architecture search (NAS) has revolutionized the design of deep neural networks, but existing methods often rely on computationally expensive search strategies or surrogate models. This paper introduces EvoNAS, a novel evolutionary multi-objective optimization framework for NAS that balances model accuracy, latency, and energy efficiency. EvoNAS utilizes a probabilistic encoding scheme and a customized evolutionary algorithm to efficiently explore the vast architecture space. Experimental results on various benchmark datasets demonstrate that EvoNAS discovers competitive architectures with reduced computational overhead and improved trade-offs between conflicting objectives.