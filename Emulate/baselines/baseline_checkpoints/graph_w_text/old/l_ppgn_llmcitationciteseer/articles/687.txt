Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the variability and complexity of human emotions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features to improve emotion recognition performance. Our HAN model consists of two stages: modality attention and feature attention, which enable the model to adaptively weight the importance of different modalities and features for emotion recognition. Experimental results on the IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art methods, achieving a significant improvement in F1-score and accuracy.