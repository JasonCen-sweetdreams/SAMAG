In human-computer interaction, recognizing emotions from multi-modal inputs (e.g., speech, text, facial expressions) is crucial for developing empathetic and responsive systems. This paper proposes a hierarchical attention network (HAN) that integrates multiple modalities to recognize emotions more accurately. Our HAN framework consists of modality-specific attention modules that learn to focus on relevant input features, followed by a higher-level attention module that weights the outputs from each modality. Experimental results on a benchmark dataset show that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, with significant improvements in recognizing subtle emotional cues.