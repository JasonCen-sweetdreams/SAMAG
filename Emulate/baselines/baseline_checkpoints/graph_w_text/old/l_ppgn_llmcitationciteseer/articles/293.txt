Sentiment analysis on multi-modal data, such as text, images, and videos, is a challenging task due to the inherent heterogeneity and complexity of the data. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) that effectively integrates and selectively focuses on relevant modalities and their interactions. Our HGAT model consists of two stages: intra-modality graph attention and inter-modality graph fusion. Experimental results on two benchmark datasets demonstrate the superiority of HGAT over state-of-the-art methods, achieving significant improvements in sentiment analysis accuracy and robustness.