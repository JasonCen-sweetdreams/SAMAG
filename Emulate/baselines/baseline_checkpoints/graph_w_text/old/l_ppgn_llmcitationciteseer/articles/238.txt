Emotion recognition from multi-modal inputs, such as audio, video, and text, is a challenging task due to the complexity of human emotions and the varying importance of each modality. This paper proposes a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features to improve emotion recognition accuracy. Our HAN model consists of three stages: modality-level attention, feature-level attention, and decision-level fusion. Experimental results on the IEMOCAP dataset show that our approach outperforms state-of-the-art methods by 12.5% in terms of weighted F1-score, demonstrating the effectiveness of hierarchical attention in multi-modal emotion recognition.