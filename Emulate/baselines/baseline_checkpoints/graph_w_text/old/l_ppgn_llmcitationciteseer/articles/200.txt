Voice assistants have become ubiquitous, but users with dysarthria often struggle to interact with these systems due to speech recognition errors. This paper presents a novel approach to designing inclusive voice assistants that can better understand and respond to users with dysarthria. We propose a multimodal interface that combines speech, gesture, and gaze inputs to improve recognition accuracy. Our user study with 20 participants with dysarthria shows that our approach significantly reduces error rates and enhances user experience. We discuss implications for accessible HCI design and future directions for improving voice assistant technology.