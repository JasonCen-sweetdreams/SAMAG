Multi-modal emotion recognition has gained significant attention in human-computer interaction and affective computing. However, existing approaches suffer from high computational costs and neglect the hierarchical relationships between modalities. We propose a novel Hierarchical Attention Network (HAN) that leverages self-attention mechanisms to selectively focus on relevant modalities and features. Our experiments on the popular CMU-MOSEI dataset demonstrate that HAN outperforms state-of-the-art methods in terms of accuracy and F1-score, while reducing computational costs by up to 30%. We further provide insights into the learned attention patterns, revealing valuable information for affective computing applications.