Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is crucial for human-computer interaction applications. Existing approaches often rely on complex fusion techniques or require large annotated datasets. We propose a Hierarchical Attention Network (HAN) that leverages intra-modal and inter-modal attention mechanisms to selectively focus on relevant features across modalities. Our experiments on the IEMOCAP and CMU-Multimodal datasets demonstrate that HAN outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, while reducing computational complexity and requiring less annotated data.