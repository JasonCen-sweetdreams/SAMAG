Visual question answering (VQA) requires the effective integration of visual and language modalities. Existing approaches often rely on flat, sequential fusion methods that fail to capture hierarchical relationships between modalities. This paper proposes a novel hierarchical attention mechanism, 'HATT', which adaptively weighs and fuses multi-modal features at different levels of abstraction. We demonstrate that HATT achieves state-of-the-art performance on the VQA 2.0 benchmark, outperforming popular fusion methods while reducing computational overhead. Our ablation studies provide insights into the importance of hierarchical attention for robust VQA.