Recognizing emotions from multi-modal inputs (e.g., text, speech, vision) is a challenging task in affective computing. Existing approaches often rely on early fusion or late fusion strategies, which can lead to suboptimal performance. This paper presents a novel Hierarchical Attention Network (HAN) architecture that leverages the strengths of both early and late fusion. Our model employs attention mechanisms at multiple levels to selectively weigh the importance of each modality and capture complex inter-modal relationships. Experimental results on the CMU-MOSEI dataset demonstrate that HAN outperforms state-of-the-art approaches in terms of emotion recognition accuracy and efficiency.