Affective computing has numerous applications in HCI, including emotion-aware interfaces and mental health monitoring. This paper presents EmoTract, a novel framework for tracking affective states in real-time using multimodal fusion of speech, facial expressions, and physiological signals. We propose a hierarchical Bayesian model that integrates features from each modality, leveraging their complementary strengths to improve affect recognition accuracy. Evaluation on a large, diverse dataset demonstrates EmoTract's superiority over unimodal and state-of-the-art multimodal approaches, achieving a mean accuracy of 87.2% in classifying six basic emotions.