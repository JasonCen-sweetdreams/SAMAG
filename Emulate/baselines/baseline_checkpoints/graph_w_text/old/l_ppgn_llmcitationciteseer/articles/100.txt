Emotion recognition in human-computer interaction is crucial for developing empathetic conversational AI systems. This paper presents a novel hierarchical attention network (HAN) architecture that leverages multi-modal inputs from speech, text, and facial expressions to recognize emotions. Our HAN model consists of modality-specific attention layers and a fusion layer that learns to weight and combine the outputs from each modality. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms existing state-of-the-art methods, achieving an accuracy of 87.2% on the emotion recognition task.