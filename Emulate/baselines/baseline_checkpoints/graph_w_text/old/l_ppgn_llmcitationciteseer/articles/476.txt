Multi-modal sentiment analysis has gained popularity in recent years, but the search for optimal neural architectures remains a challenging task. This paper proposes a novel neural architecture search (NAS) framework, 'MMNAS', which leverages reinforcement learning and graph neural networks to efficiently explore the architecture space. We introduce a multi-modal encoding scheme that integrates visual and textual features, and demonstrate improved sentiment analysis performance on three benchmark datasets. Experimental results show that MMNAS outperforms hand-crafted architectures and existing NAS methods, with a significant reduction in search time and computational resources.