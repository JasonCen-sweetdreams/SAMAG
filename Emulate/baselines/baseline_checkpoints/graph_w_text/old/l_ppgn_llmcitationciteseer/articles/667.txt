Emotion recognition from multi-modal inputs, such as speech, text, and facial expressions, remains a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel Hierarchical Graph Attention Network (HiGAT) that learns to selectively focus on relevant modalities and capture long-range dependencies between them. HiGAT consists of a hierarchical graph attention mechanism that integrates information from different modalities and a graph convolutional layer that refines the learned representations. Experimental results on the CMU-MOSEI dataset demonstrate that HiGAT outperforms state-of-the-art methods in multi-modal emotion recognition tasks, achieving an average F1-score improvement of 5.2%.