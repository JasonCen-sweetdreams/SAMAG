Transparent decision-making is crucial in multi-agent systems, but existing methods often lack interpretability. This paper presents 'HATMAN', a hierarchical attention network architecture that disentangles agent interactions and provides explicit explanations for joint decisions. By recursively applying attention mechanisms, HATMAN captures complex dependencies between agents and infers their intentions. We evaluate HATMAN on a real-world autonomous driving dataset and demonstrate its ability to generate human-understandable explanations for multi-agent decision-making, outperforming state-of-the-art methods in terms of explanation fidelity and decision accuracy.