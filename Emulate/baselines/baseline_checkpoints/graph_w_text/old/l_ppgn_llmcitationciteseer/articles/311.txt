Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task, particularly when dealing with ambiguous or conflicting cues. This paper introduces HAN-EMO, a hierarchical attention network that selectively focuses on relevant modalities and features to improve emotion recognition accuracy. Our approach leverages a novel attention mechanism that adaptively weighs the importance of each modality and feature, providing insights into the decision-making process. Experimental results on several benchmark datasets demonstrate the superior performance of HAN-EMO over state-of-the-art methods, while also offering interpretable explanations for its predictions.