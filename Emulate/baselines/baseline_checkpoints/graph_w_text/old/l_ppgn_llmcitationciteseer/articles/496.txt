Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a crucial task in affective computing. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that effectively fuses and weights features from different modalities. Our approach incorporates a self-attention mechanism to model intra-modal relationships and a hierarchical fusion strategy to capture inter-modal dependencies. Experimental results on three benchmark datasets demonstrate that our HAN model outperforms state-of-the-art methods in terms of recognition accuracy and computational efficiency, paving the way for real-world applications in human-computer interaction and affective computing.