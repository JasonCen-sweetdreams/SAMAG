Deep reinforcement learning (DRL) agents are vulnerable to adversarial attacks, which can compromise their decision-making in critical applications. This paper proposes a novel approach to robustify DRL agents by incorporating adversarial experience replay (AER) into their training process. AER generates a curated set of adversarial experiences that the agent can learn from, improving its robustness to unseen attacks. We present empirical results on several Atari games, demonstrating that AER outperforms state-of-the-art DRL algorithms in the presence of adversarial perturbations, while maintaining comparable performance in the absence of attacks.