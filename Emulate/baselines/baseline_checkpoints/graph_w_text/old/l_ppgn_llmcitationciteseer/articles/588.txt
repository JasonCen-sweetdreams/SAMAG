Visual Question Answering (VQA) models have achieved impressive performance, but their decision-making processes remain opaque. This paper introduces Hierarchical Attention Networks (HANs) for explainable VQA, which disentangle the reasoning process into multiple stages. Our HANs employ a novel attention mechanism that recursively focuses on relevant regions of the image and question, generating a hierarchical representation of the question's context. We demonstrate that HANs outperform state-of-the-art VQA models while providing visualizations that reveal the model's attention patterns, thereby enhancing model interpretability.