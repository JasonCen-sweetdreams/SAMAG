Multi-agent systems have become increasingly prevalent in various domains, including robotics, autonomous vehicles, and smart grids. However, coordinating these systems to achieve global objectives remains a significant challenge. This paper presents a decentralized reinforcement learning framework, 'DRL-MAS', which enables heterogeneous agents to learn cooperative policies without requiring explicit communication or centralized control. We leverage graph neural networks to model agent interactions and incorporate a novel credit assignment mechanism to facilitate policy learning. Experimental results on a range of benchmark scenarios demonstrate the effectiveness of DRL-MAS in achieving efficient and scalable coordination in complex multi-agent systems.