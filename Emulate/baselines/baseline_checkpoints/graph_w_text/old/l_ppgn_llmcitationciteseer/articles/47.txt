Emotion recognition from multi-modal inputs (e.g., speech, text, vision) remains a challenging task in human-computer interaction. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features for emotion recognition. Our approach leverages self-attention mechanisms to capture intra-modal and inter-modal relationships, enabling the model to provide interpretable explanations for its predictions. Experimental results on the Multimedia Emotion Recognition Tasks (MERT) dataset demonstrate the effectiveness of HAN in improving recognition accuracy while providing insights into the decision-making process.