Emotion recognition from multi-modal data (e.g., speech, text, and vision) is crucial for human-computer interaction. However, existing methods struggle to effectively integrate and weigh the contributions of each modality. We propose a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and features. Our HAN consists of two stages: intra-modal attention, which extracts modality-specific features, and inter-modal attention, which fuses these features into a unified representation. We evaluate our approach on three benchmark datasets, achieving state-of-the-art performance in terms of emotion recognition accuracy and F1-score.