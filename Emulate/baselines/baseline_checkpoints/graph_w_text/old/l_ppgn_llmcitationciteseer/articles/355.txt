Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and capture subtle emotional cues. Our HAN architecture consists of multiple layers of attention mechanisms that adaptively weight the input modalities, allowing for interpretable and robust emotion recognition. Experimental results on a large-scale multi-modal emotion recognition dataset demonstrate the effectiveness of our approach, outperforming state-of-the-art methods by a significant margin.