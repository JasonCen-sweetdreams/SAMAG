Deep learning models have achieved state-of-the-art performance in multimodal sentiment analysis, but their lack of interpretability hinders trust and adoption. This paper proposes a novel hierarchical attention network (HAN) architecture that incorporates task-specific attention mechanisms to selectively focus on relevant modalities and regions. We demonstrate that HAN outperforms existing multimodal fusion methods on benchmark datasets while providing visual explanations for its predictions. Furthermore, we introduce a novel evaluation metric, 'Explainability Score', to quantify the model's ability to generate coherent and accurate explanations.