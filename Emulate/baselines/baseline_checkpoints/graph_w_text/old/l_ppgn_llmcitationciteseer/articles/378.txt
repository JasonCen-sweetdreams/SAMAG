Multimodal sentiment analysis has gained significant attention in recent years, but existing approaches often struggle to effectively integrate and weigh the contributions of different modalities. This paper proposes a hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and regions within each modality. Our approach leverages a novel multimodal fusion mechanism that adaptively combines the outputs of modality-specific encoders. Experimental results on the CMU-MOSI dataset demonstrate that our HAN outperforms state-of-the-art methods in multimodal sentiment analysis, achieving a 3.2% improvement in accuracy over the best baseline.