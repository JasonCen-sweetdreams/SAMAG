Deep reinforcement learning (DRL) has achieved remarkable success in complex decision-making tasks, but the lack of transparency in policy decisions hinders trust and adoption. This paper presents a novel model-agnostic explainability framework, 'RL-Explain', which leverages saliency maps and feature importance to provide insightful explanations for DRL policy decisions. We propose a hybrid approach that combines gradient-based and perturbation-based methods to identify relevant state features and quantify their contribution to the policy's output. Experimental results on various Atari games and real-world robotics scenarios demonstrate the effectiveness of RL-Explain in improving policy interpretability and facilitating error analysis.