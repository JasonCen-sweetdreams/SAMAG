Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging AI task. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages the complementary strengths of different modalities to recognize emotions in a more accurate and interpretable manner. Our HAN model uses a hierarchical attention mechanism to selectively focus on relevant modalities and features, enabling explainable emotion recognition. Experimental results on three benchmark datasets demonstrate the superiority of our approach over state-of-the-art methods in terms of both recognition accuracy and explainability.