Multi-task learning has become increasingly popular in various applications, but it often suffers from the curse of dimensionality and slow convergence. This paper proposes a novel hierarchical attention network (HAN) framework that efficiently learns shared representations across multiple tasks. Our approach leverages a hierarchical attention mechanism to selectively focus on relevant features and tasks, allowing for more effective knowledge sharing and improved performance. We evaluate HAN on several benchmark datasets, demonstrating significant improvements in convergence speed and accuracy compared to state-of-the-art multi-task learning methods.