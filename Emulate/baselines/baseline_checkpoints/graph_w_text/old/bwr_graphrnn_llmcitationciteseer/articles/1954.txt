Deep neural networks have achieved state-of-the-art performance in various machine learning tasks, but their success heavily relies on careful hyperparameter tuning. This paper proposes a novel Bayesian optimization approach using Gaussian processes (GP-BO) to efficiently search for optimal hyperparameters. We extend the traditional GP-BO framework by incorporating a surrogate model that captures the relationships between hyperparameters and the validation performance. Experimental results on several benchmark datasets demonstrate that our approach outperforms popular hyperparameter tuning methods, such as random search and grid search, in terms of both convergence speed and final performance.