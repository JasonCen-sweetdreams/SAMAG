Visual question answering (VQA) tasks require integrating information from multiple sources, including images, text, and knowledge graphs. This paper proposes a novel hypergraph attention network (HAN) architecture that effectively fuses these modalities to improve VQA performance. HAN leverages hypergraph structures to model complex relationships between entities and incorporates multi-head attention mechanisms to selectively focus on relevant features. Our experiments on the VQA 2.0 dataset demonstrate that HAN outperforms state-of-the-art models in both accuracy and interpretability, especially for questions requiring reasoning across multiple modalities.