Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) remains a challenging task due to the complexity of human emotions and the limited interpretability of current models. This paper proposes a novel Hierarchical Attention Network (HAN) framework that leverages the strengths of both local and global attention mechanisms to identify salient features across modalities. We introduce a multi-task learning objective that jointly optimizes emotion recognition and modality-specific feature importance, enabling explainable emotion recognition. Experimental results on the CMU-MOSEI dataset demonstrate the effectiveness of HAN in improving emotion recognition accuracy and providing interpretable insights into modality contributions.