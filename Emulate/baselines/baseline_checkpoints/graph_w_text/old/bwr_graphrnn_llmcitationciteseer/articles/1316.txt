In this paper, we propose a decentralized task allocation framework for heterogeneous multi-agent systems, where agents with different capabilities and resources need to collaborate to accomplish complex tasks. We leverage deep reinforcement learning to train agents to make decisions based on local observations and limited communication with neighboring agents. Our approach, called 'Hetero-MARL', uses a graph neural network to model agent interactions and a deep Q-network to learn task allocation policies. Experimental results on a variety of task allocation problems demonstrate the effectiveness and scalability of Hetero-MARL in achieving near-optimal solutions while adapting to dynamic environments.