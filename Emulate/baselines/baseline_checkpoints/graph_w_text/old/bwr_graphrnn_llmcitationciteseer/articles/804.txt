Time-series forecasting models often lack transparency, making it challenging to understand their predictions. This paper presents a novel Hierarchical Attention Network (HAN) architecture that integrates explainability into time-series forecasting. Our approach uses a hierarchical attention mechanism to selectively focus on relevant segments of the input sequence, generating attention weights that provide insights into the forecasting process. Experimental results on real-world datasets demonstrate that HAN outperforms state-of-the-art models in terms of forecasting accuracy while providing interpretable attention visualizations.