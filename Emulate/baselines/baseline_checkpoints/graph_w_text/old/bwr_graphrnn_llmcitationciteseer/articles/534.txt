Emotion recognition from multimodal data (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the variability of individual expression. This paper proposes a hierarchical attention network (HAN) to tackle this problem. Our HAN model consists of three stages: modality-specific encoders, cross-modal fusion, and emotion classification. We introduce a novel attention mechanism that adaptively weights the importance of different modalities and features, enabling the model to focus on the most informative cues. Experimental results on the IEMOCAP and CMU-MOSEI datasets demonstrate that our approach outperforms state-of-the-art methods in emotion recognition accuracy and robustness.