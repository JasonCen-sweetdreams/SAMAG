Emotion recognition from multi-modal data (e.g., speech, text, vision) is a challenging task due to the complexity of human emotions and the need for effective fusion of heterogeneous features. This paper presents a novel hierarchical attention network (HAN) that learns to focus on salient regions in each modality and adaptively combines them to improve emotion recognition performance. Experimental results on the CMU-MOSEI dataset demonstrate that our HAN outperforms state-of-the-art methods in terms of accuracy and computational efficiency, making it suitable for real-time applications.