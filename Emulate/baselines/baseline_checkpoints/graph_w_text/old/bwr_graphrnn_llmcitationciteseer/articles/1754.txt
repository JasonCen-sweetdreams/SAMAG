Explainability is crucial for real-world adoption of reinforcement learning (RL) in autonomous driving. This paper introduces 'HierExplain', a hierarchical attention-based RL framework that learns to focus on relevant input features and provides interpretable explanations for its decisions. Our approach leverages a novel, two-level attention mechanism that disentangles feature importance from state importance, enabling the model to identify key factors influencing its policy. We evaluate HierExplain on a range of autonomous driving scenarios and demonstrate improved explanation quality and policy performance compared to state-of-the-art baselines.