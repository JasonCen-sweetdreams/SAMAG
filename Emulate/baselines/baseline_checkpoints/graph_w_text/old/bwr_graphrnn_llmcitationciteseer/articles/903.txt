Autonomous vehicles require efficient decision-making algorithms to navigate complex scenarios. This paper presents a novel hierarchical reinforcement learning (HRL) framework for autonomous vehicle control, which combines the benefits of model-based and model-free RL approaches. Our method, 'Hierarchical Action Primitives' (HAP), learns to decompose tasks into primitive actions, enabling faster learning and improved robustness. We demonstrate HAP's effectiveness in simulation experiments, achieving reduced computation time and improved driving performance compared to state-of-the-art HRL methods.