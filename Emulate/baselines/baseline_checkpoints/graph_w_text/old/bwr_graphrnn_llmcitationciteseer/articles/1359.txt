Multimodal sentiment analysis, which involves understanding user sentiment from text, images, and videos, is an essential task in various applications. This paper proposes a novel hierarchical attention network (HAN) that leverages the strengths of both transformer-based and graph-based neural networks. Our approach introduces a multimodal fusion module that adaptively weights the importance of different modalities and a hierarchical attention mechanism that captures both local and global contextual information. Experimental results on three benchmark datasets demonstrate the superiority of our proposed HAN over state-of-the-art methods in terms of accuracy and robustness.