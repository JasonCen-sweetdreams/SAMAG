Explainability is crucial in multi-agent reinforcement learning (MARL) for understanding agent interactions and decision-making. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates attention mechanisms at both the agent and system levels. Our approach enables the identification of influential agents and their relationships, facilitating the explanation of complex emergent behaviors. We evaluate HAN on a suite of MARL benchmarks, demonstrating improved explainability and competitiveness with state-of-the-art methods. Our results have significant implications for the development of transparent and trustworthy AI systems in real-world applications.