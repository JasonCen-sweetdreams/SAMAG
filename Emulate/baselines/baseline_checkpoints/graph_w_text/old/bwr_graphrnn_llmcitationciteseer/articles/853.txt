In this paper, we present a novel approach to coordinated exploration in multi-agent systems using hierarchical deep reinforcement learning. We propose a hierarchical framework, where high-level agents learn to coordinate exploration policies, while low-level agents execute these policies to gather information about the environment. Our approach enables agents to balance exploration and exploitation, leading to improved performance in complex, dynamic environments. We evaluate our approach in a variety of scenarios, including robotic search and rescue, and show significant improvements over existing methods.