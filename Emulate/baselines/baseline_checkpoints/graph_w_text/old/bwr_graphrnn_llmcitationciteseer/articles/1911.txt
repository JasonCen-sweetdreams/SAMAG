Real-time database systems (RTDBS) require efficient and adaptive query optimization to meet stringent latency and throughput constraints. This paper introduces 'AQOA', a novel query optimization framework that leverages machine learning and statistical models to predict query execution times and optimize query plans. AQOA adaptively adjusts to changing workload patterns and data distributions, ensuring consistent performance and minimizing latency. Experiments on a large-scale RTDBS workload demonstrate AQOA's ability to reduce average query latency by up to 40% compared to state-of-the-art optimization techniques.