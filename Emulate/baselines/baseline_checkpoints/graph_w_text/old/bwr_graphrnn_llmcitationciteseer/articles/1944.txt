Multimodal event extraction involves identifying events from heterogeneous data sources, including images, videos, and text. This paper proposes a novel deep learning framework, 'MulEve', which leverages knowledge graph embeddings to jointly model the semantic relationships between entities, events, and multimodal features. We introduce a graph attention mechanism that adaptively weights the importance of different knowledge graph triples, enabling effective event extraction from noisy and incomplete data. Experiments on a large-scale multimodal dataset demonstrate that MulEve outperforms state-of-the-art methods in event extraction accuracy and robustness.