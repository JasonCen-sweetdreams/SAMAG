Emotion recognition from multi-modal data, such as speech, text, and vision, remains a challenging task due to the complexity of human emotions and the heterogeneity of modalities. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and time-steps to recognize emotions. Our HAN model consists of three levels of attention: modality-level, time-level, and token-level, allowing it to capture both local and global patterns in the data. Experimental results on the CMU-MOSI dataset demonstrate that our approach outperforms state-of-the-art methods in multi-modal emotion recognition, particularly in cases with noisy or missing data.