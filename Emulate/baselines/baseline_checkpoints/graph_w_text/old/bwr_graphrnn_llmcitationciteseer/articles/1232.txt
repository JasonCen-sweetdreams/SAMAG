This paper presents a novel decentralized multi-agent reinforcement learning (MARL) framework for dynamic task allocation in complex, partially observable environments. Our approach, dubbed 'MA-DTRL', leverages a hierarchical graph neural network (GNN) to learn adaptive task representations and a decentralized policy gradient method to optimize agent coordination. We evaluate MA-DTRL on a simulated logistics domain and demonstrate improved task completion rates and reduced communication overhead compared to traditional centralized and decentralized MARL approaches.