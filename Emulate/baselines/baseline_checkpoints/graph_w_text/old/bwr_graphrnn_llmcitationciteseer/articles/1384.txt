Emotion recognition from multimodal inputs (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the need to integrate information from diverse modalities. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and regions within each modality. Our approach achieves state-of-the-art performance on the IEMOCAP dataset, outperforming existing multimodal fusion methods by 12.1% in terms of weighted F1-score. We also provide an in-depth analysis of the attention weights, revealing insights into the emotional cues used by the model.