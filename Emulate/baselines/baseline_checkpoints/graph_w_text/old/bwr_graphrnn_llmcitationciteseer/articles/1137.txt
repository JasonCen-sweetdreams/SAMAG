This paper addresses the challenge of coordinating multiple agents in complex, dynamic environments. We propose a decentralized approach to Partially Observable Markov Decision Processes (POMDPs) that enables agents to make informed decisions despite limited knowledge of the system state. Our method, Dec-POMDP-C, utilizes a novel distributed inference algorithm to estimate the global state from local observations, and subsequently applies a decentralized planning strategy to optimize team performance. Experimental results in a simulated search-and-rescue scenario demonstrate that Dec-POMDP-C outperforms existing decentralized methods in terms of task completion efficiency and adaptability to changing environmental conditions.