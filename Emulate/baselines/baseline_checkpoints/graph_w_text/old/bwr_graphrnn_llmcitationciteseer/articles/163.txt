Emotion recognition from multi-modal inputs, such as speech, text, and vision, remains a challenging task due to the inherent complexities of human emotions and the need for interpretable models. We propose a novel Hierarchical Attention Network (HAN) that leverages attention mechanisms to selectively focus on relevant modalities and features. Our approach not only improves emotion recognition accuracy but also provides explainability through attention weights, enabling the identification of salient emotion cues. Experimental evaluations on several benchmark datasets demonstrate the effectiveness of HAN in capturing nuanced emotions and outperforming state-of-the-art methods.