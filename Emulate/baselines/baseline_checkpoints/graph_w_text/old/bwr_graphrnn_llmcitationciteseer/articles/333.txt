Explainability is crucial in multi-agent reinforcement learning (MARL) systems, where agents interact with each other and their environment. This paper introduces HAT-MARL, a hierarchical attention network that enables interpretable decision-making in MARL. HAT-MARL employs a two-level attention mechanism, focusing on both inter-agent and intra-agent interactions. We demonstrate the effectiveness of HAT-MARL in a cooperative navigation task, showcasing improved collaboration and interpretability compared to state-of-the-art MARL methods. The attention weights provide insights into the agents' decision-making processes, enabling better understanding and debugging of the system.