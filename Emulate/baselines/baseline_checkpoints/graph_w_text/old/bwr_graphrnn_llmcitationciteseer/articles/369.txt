In multi-agent systems, decision-making agents often rely on black-box models, hindering trust and interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) framework that enables explainable decision-making in multi-agent settings. HAN integrates attention mechanisms at both the agent and system levels, allowing for transparent attribution of agent contributions to collective decisions. We evaluate HAN on a simulated traffic management scenario, demonstrating improved decision quality and enhanced interpretability compared to state-of-the-art deep reinforcement learning baselines.