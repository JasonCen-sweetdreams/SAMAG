Conversational interfaces have become ubiquitous, but people with disabilities often face barriers in interacting with these systems. This paper proposes a novel multimodal fusion approach to design accessible conversational interfaces. We develop a framework that integrates computer vision, speech recognition, and natural language processing to enable users to interact using their preferred modality. Our approach uses a hierarchical fusion model to combine inputs from different modalities, allowing the system to adapt to individual user needs. We evaluate our approach through a user study with participants with disabilities, demonstrating significant improvements in interaction accuracy and user satisfaction.