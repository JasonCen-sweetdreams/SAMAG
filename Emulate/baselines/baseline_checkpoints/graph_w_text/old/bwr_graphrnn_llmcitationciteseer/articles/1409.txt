Neural search has become a promising approach for information retrieval, but it often suffers from the vocabulary mismatch problem. This paper proposes a novel query expansion method, 'ContraQE', which leverages contrastive learning to capture the semantic relationships between query terms and documents. ContraQE uses a dual-encoder architecture to learn dense representations of queries and documents, and then applies a contrastive loss function to align similar query-document pairs. Experimental results on several benchmarks demonstrate that ContraQE outperforms state-of-the-art query expansion methods, improving the retrieval performance of neural search models by up to 15%.