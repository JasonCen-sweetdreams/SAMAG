Emotion recognition from multi-modal inputs such as facial expressions, speech, and text remains a challenging task due to the complexity of human emotions and the variability of individual responses. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features at multiple levels of abstraction. Our approach achieves state-of-the-art performance on the CMU-MOSEI dataset, outperforming existing methods by 12.3% in terms of weighted F1-score. We also provide insights into the attention patterns learned by the model, shedding light on the importance of modality-specific and cross-modal features in emotion recognition.