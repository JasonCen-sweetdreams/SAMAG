Knowledge graph embedding (KGE) has become a crucial step in various AI applications, but existing methods often suffer from high computational costs and inadequate modeling of complex graph structures. This paper proposes a novel KGE approach, 'HGAN', which leverages hierarchical graph attention networks to capture multiple scales of graph dependencies. By recursively applying attention mechanisms at different graph levels, HGAN effectively encodes both local and global graph features, leading to improved performance on link prediction and entity classification tasks. Our experiments on several benchmark datasets demonstrate the efficacy and efficiency of HGAN compared to state-of-the-art KGE methods.