Multimodal sentiment analysis (MSA) has garnered significant attention in recent years due to its applications in human-computer interaction and affective computing. However, existing approaches often fail to effectively integrate and weigh the contributions of different modalities. This paper proposes a novel hierarchical attention network (HAN) architecture for MSA, which dynamically learns to focus on salient regions in images, utterances in audio, and phrases in text. Our experiments on a large-scale multimodal dataset demonstrate that HAN outperforms state-of-the-art methods, achieving a 12% improvement in sentiment classification accuracy and a 15% improvement in aspect-based sentiment analysis.