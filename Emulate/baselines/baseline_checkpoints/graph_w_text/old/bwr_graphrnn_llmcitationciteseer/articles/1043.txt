Emotion recognition from multi-modal data, such as speech, text, and facial expressions, remains a challenging task due to the complexity of human emotions and the variability of modalities. We propose a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features for emotion recognition. Our approach leverages a hierarchical attention mechanism to model the interactions between modalities and a feature attention module to adaptively weight feature importance. Experimental results on the IEMOCAP and SEMAINE datasets demonstrate that our HAN model outperforms state-of-the-art methods in terms of emotion recognition accuracy and computational efficiency.