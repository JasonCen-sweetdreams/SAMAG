Emotion recognition from multi-modal data, such as facial expressions, speech, and text, is a challenging task due to the complexity and variability of human emotional cues. We propose a novel hierarchical attention network (HAN) architecture that learns to selectively focus on relevant modalities and features for emotion recognition. Our HAN model consists of multiple attention layers, each capturing different levels of abstraction, from local feature extraction to global modality fusion. Experimental results on three benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in terms of emotion recognition accuracy and computational efficiency.