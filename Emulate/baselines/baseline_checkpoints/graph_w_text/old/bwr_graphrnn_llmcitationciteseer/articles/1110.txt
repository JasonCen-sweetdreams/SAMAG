Emotion recognition from multi-modal inputs (e.g., text, speech, vision) is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a Hierarchical Attention Network (HAN) that leverages the strengths of each modality to improve emotion recognition. Our HAN model consists of modality-specific attention mechanisms, which are integrated using a hierarchical fusion strategy. Experimental results on the CMU-MOSEI dataset show that our approach outperforms state-of-the-art methods, achieving an average F1-score of 0.83 across six emotions. We also demonstrate the effectiveness of our model in real-world applications, such as sentiment analysis and affective computing.