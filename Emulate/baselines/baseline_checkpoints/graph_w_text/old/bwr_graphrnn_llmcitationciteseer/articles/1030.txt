Knowledge graph embedding (KGE) is a fundamental task in AI, aiming to represent entities and relations in a low-dimensional vector space. However, existing KGE methods often struggle to capture complex hierarchical structures and long-range dependencies in large-scale graphs. We propose a novel approach, HierGAT, which leverages hierarchical graph attention networks to learn multi-scale representations of entities and relations. Our experiments on benchmark datasets demonstrate that HierGAT outperforms state-of-the-art KGE methods in link prediction and entity classification tasks, while reducing computational costs.