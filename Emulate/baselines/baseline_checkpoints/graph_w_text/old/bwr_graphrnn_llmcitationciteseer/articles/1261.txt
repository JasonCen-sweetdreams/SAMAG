In multi-agent systems, efficient resource allocation is crucial for optimal performance. This paper presents a novel hierarchical reinforcement learning framework for coordinating agents in a decentralized manner. Our approach, called 'Hierarchical Agent Coordination' (HAC), employs a two-level hierarchical structure, where higher-level agents learn to allocate resources and lower-level agents learn to optimize their tasks given the allocated resources. We evaluate HAC in a simulated resource-constrained environment and demonstrate significant improvements in task completion rate and resource utilization compared to state-of-the-art approaches.