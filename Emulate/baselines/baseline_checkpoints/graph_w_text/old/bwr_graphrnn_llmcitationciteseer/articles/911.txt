Emotion recognition from multimodal inputs (e.g., speech, text, vision) is a challenging problem due to the heterogeneity of features and the need to capture complex interactions between modalities. This paper proposes a hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features at multiple levels of abstraction. We demonstrate the effectiveness of HAN on two benchmark datasets, achieving state-of-the-art performance in multimodal emotion recognition tasks. Our results suggest that HAN can be a versatile framework for various multimodal applications, including human-computer interaction and affective computing.