Voice assistants have become ubiquitous, but their effectiveness is hindered for users with speech impairments. This paper presents a multimodal approach to designing inclusive voice assistants, incorporating gesture recognition, facial expression analysis, and eye-tracking to complement speech input. We conducted a user study with 30 participants, including individuals with cerebral palsy and Parkinson's disease, to evaluate the usability and accessibility of our system. Results show significant improvements in task completion rates and user satisfaction compared to traditional speech-only interfaces. Our approach has implications for designing more accessible HCI systems for diverse user populations.