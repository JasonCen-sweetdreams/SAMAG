Distributed database systems have become increasingly popular due to their ability to handle large-scale data and provide high-performance querying. However, optimizing queries in these systems remains a challenging task, especially with the growing complexity of queries and the increasing scale of data. This paper proposes a novel approach to query optimization using machine learning techniques. We design a reinforcement learning framework that learns to optimize queries based on the runtime statistics and query patterns. Our experiments show that the proposed approach outperforms traditional query optimization techniques by up to 30% in terms of query execution time and improves the overall system throughput by up to 25%.