This paper presents a decentralized multi-agent reinforcement learning framework for coordinating autonomous vehicles in complex urban scenarios. We propose a novel agent architecture that incorporates graph neural networks to model spatial relationships between vehicles and a hierarchical RL framework to learn both local and global coordination policies. Experimental results in a simulated environment demonstrate improved traffic flow and reduced congestion compared to traditional centralized approaches. We also analyze the impact of agent communication topologies on system performance and provide insights for real-world deployment.