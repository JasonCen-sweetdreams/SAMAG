Virtual reality (VR) experiences often rely on manual input devices, excluding individuals with motor impairments. This paper presents EmoReact, a novel affective gesture recognition system that leverages electromyography (EMG) and computer vision to detect emotional gestures in VR users. Our approach enables users to control VR interactions through subtle facial expressions, hand gestures, and emotional states. We evaluate EmoReact's performance using a dataset of 50 participants, demonstrating high accuracy and user satisfaction in a range of VR scenarios. EmoReact has significant implications for promoting inclusivity and accessibility in immersive computing.