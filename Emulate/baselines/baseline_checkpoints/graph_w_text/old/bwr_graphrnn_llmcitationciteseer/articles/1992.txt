Deep learning models have achieved state-of-the-art performance in sentiment analysis tasks, but their lack of transparency and interpretability hinders their adoption in high-stakes applications. This paper presents a novel hierarchical attention network (HAN) architecture that integrates visual and textual features for multi-modal sentiment analysis. Our approach leverages attention mechanisms to highlight salient regions in images and sentences that influence the model's sentiment predictions. Experimental results on a large-scale dataset demonstrate that our HAN model outperforms existing methods while providing insightful visualizations for explainable AI.