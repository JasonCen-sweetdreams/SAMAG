Reinforcement learning (RL) agents often struggle to generalize in partially observable environments, where the agent's observations do not fully capture the true state of the environment. To address this challenge, we propose a novel hierarchical attention network architecture, HARA, which integrates attention mechanisms at both the observation and action levels. HARA enables the agent to selectively focus on relevant observations and infer the underlying environment state. We evaluate HARA on several benchmark environments and demonstrate significant improvements in both performance and interpretability compared to state-of-the-art RL methods. Furthermore, we provide a detailed analysis of the learned attention patterns, offering insights into the decision-making process of the agent.