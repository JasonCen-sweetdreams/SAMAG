Decentralized task allocation in multi-agent systems is a challenging problem, particularly in dynamic environments. This paper proposes a novel approach using deep reinforcement learning to enable agents to learn effective task allocation strategies. We introduce a decentralized actor-critic framework, 'Dec-TAC', which utilizes graph neural networks to model agent interactions and coordination. Experimental results on a simulated disaster response scenario demonstrate that Dec-TAC outperforms traditional decentralized task allocation methods, achieving higher task completion rates and improved system efficiency.