Deep neural networks have achieved state-of-the-art performance in various machine learning tasks, but their success heavily relies on careful hyperparameter tuning. This process is often computationally expensive and requires significant domain expertise. We propose a Bayesian optimization framework, 'HyperBO', that leverages a probabilistic surrogate model to efficiently search the hyperparameter space. Our approach incorporates a novel acquisition function that balances exploration and exploitation, leading to faster convergence and improved model performance. Experiments on several benchmark datasets demonstrate the effectiveness of HyperBO in tuning hyperparameters for various neural network architectures.