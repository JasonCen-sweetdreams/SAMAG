Distributed database systems have become increasingly popular due to their ability to scale and handle large amounts of data. However, optimizing query performance in these systems remains a significant challenge. This paper proposes a novel approach that leverages machine learning to optimize query plans in distributed database systems. We develop a model that learns to predict the optimal query plan based on historical query patterns and system workload. Experimental results on a real-world dataset show that our approach outperforms traditional query optimization techniques by up to 30% in terms of query execution time.