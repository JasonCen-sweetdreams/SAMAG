Neural architecture search (NAS) has emerged as a promising technique for automating the design of deep neural networks. However, the computationally expensive nature of NAS limits its applicability to large-scale problems. This paper proposes a novel multi-fidelity Bayesian optimization approach, 'MF-BO', which leverages low-fidelity approximations of the NAS objective function to significantly reduce the computational overhead. We demonstrate the effectiveness of MF-BO in searching for efficient neural architectures on three benchmark datasets, achieving state-of-the-art results while reducing the search time by up to 5x compared to existing NAS methods.