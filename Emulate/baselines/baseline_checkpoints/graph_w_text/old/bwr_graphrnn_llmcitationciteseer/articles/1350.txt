Coordinating multi-agent systems is a challenging problem in real-world applications, such as traffic management and resource allocation. Reward shaping is a promising approach to improve coordination, but existing methods often rely on manual tuning or domain-specific knowledge. This paper proposes an adaptive reward shaping framework, 'ARS', which learns to shape rewards based on the agents' observations and actions. We demonstrate the effectiveness of ARS in a simulated traffic management scenario, where it outperforms state-of-the-art methods in terms of coordination efficiency and adaptability to changing environments.