Emotion recognition from multi-modal data remains a challenging task due to the complexity of human emotional expressions. This paper proposes a novel hierarchical attention network (HAN) architecture that efficiently integrates audio, visual, and textual features for robust emotion recognition. Our HAN model employs a hierarchical attention mechanism to selectively focus on relevant modalities and features, reducing the dimensionality of the input space. Experimental results on the Multimodal Emotion Recognition (MER) dataset demonstrate that our approach outperforms state-of-the-art methods in terms of recognition accuracy and computational efficiency.