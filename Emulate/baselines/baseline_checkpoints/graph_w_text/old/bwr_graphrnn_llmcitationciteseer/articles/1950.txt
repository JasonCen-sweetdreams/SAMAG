Augmented reality (AR) systems often rely on manual input methods, which can be cumbersome and limit user experience. This paper presents an eye-gaze based adaptive interaction framework for AR, which leverages machine learning models to predict user intent from eye movement patterns. We develop a novel gaze-driven interface that dynamically adjusts its interaction modalities based on user behavior, preferences, and task requirements. Our user study demonstrates significant improvements in interaction efficiency and user satisfaction, particularly for users with motor impairments.