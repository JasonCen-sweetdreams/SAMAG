Deep reinforcement learning (DRL) has achieved remarkable success in various applications, but its vulnerability to adversarial attacks hinders its deployment in safety-critical domains. In this paper, we propose a novel graph-based anomaly detection framework, 'GraphAD', to identify adversarial attacks on DRL agents. By modeling the agent's behavior as a graph, we leverage graph convolutional networks to learn a representation of normal behavior and detect anomalies indicative of attacks. Our approach outperforms existing detection methods on a range of DRL benchmarks, achieving an average detection rate of 95.6% with a false positive rate of 2.1%.