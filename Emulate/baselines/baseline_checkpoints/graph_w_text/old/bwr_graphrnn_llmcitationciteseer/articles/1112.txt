Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a crucial task in affective computing. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and temporal segments to improve emotion recognition. Our approach leverages a hierarchical fusion strategy, where modality-specific attention weights are computed and then combined to form a unified attention representation. We evaluate our HAN model on three benchmark datasets and demonstrate significant performance gains over state-of-the-art methods, achieving an average F1-score improvement of 8.5%.