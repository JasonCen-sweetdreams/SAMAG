Dialogue state tracking (DST) is a crucial component of task-oriented dialogue systems. While recent approaches have leveraged deep learning models, they often rely on a single modality (e.g., text or speech) and neglect the complementary information from other modalities. This paper introduces a novel hybrid attention mechanism that integrates visual, acoustic, and linguistic features for multi-modal DST. Our approach employs a hierarchical attention framework, where a primary attention module focuses on the most relevant modalities, and a secondary attention module refines the state tracking by incorporating cross-modal interactions. Experimental results on the MultiWOZ 2.1 dataset demonstrate that our approach achieves significant improvements over state-of-the-art systems, particularly in scenarios with noisy or incomplete input.