Visual question answering (VQA) has seen significant advances with the advent of deep learning. However, most state-of-the-art models rely on computationally expensive attention mechanisms that scale poorly with image size and question complexity. This paper presents a novel hierarchical attention network (HAN) that leverages multi-scale feature extraction and adaptive attention weights to improve VQA efficiency. Our approach achieves comparable accuracy to existing models while reducing inference time by up to 30% on popular benchmarks. We also demonstrate the effectiveness of HAN in real-world applications, such as robotics and healthcare.