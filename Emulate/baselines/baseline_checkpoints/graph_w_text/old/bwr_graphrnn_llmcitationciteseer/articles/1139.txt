Time-series forecasting models often struggle to provide interpretable insights into their predictions. We propose a novel hierarchical attention network (HAN) architecture that addresses this limitation. By incorporating attention mechanisms at multiple scales, our model learns to selectively focus on relevant input features and time-steps, providing a transparent and hierarchical representation of the forecasting process. Experimental results on several benchmark datasets demonstrate that our HAN model achieves state-of-the-art performance while offering improved explainability through attention-based feature importance scores.