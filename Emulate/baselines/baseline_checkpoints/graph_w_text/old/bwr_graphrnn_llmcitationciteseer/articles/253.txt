Explainability is crucial in multi-agent reinforcement learning (MARL) to understand agent interactions and decision-making processes. This paper proposes a novel hierarchical attention network (HAN) architecture to learn interpretable policies in MARL environments. Our HAN model incorporates both agent-level and joint-level attention mechanisms to capture complex relationships between agents. Experimental results on a range of MARL benchmarks demonstrate that our approach outperforms state-of-the-art methods in terms of both performance and explainability, providing insights into the decision-making processes of individual agents and their interactions.