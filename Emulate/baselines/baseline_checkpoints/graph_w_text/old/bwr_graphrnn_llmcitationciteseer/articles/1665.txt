Modern search engines rely on efficient document filtering to retrieve relevant results from massive indexes. This paper presents a novel approach that leverages semantic embeddings to improve filtering accuracy. We propose a dense passage retrieval model, 'SemFilter', which learns to encode documents and queries into a shared semantic space. By exploiting the geometric properties of this space, SemFilter achieves significant improvements in filtering accuracy and efficiency compared to traditional term-based approaches. Experiments on a large-scale web corpus demonstrate the effectiveness of our approach in reducing the number of candidate documents while preserving recall.