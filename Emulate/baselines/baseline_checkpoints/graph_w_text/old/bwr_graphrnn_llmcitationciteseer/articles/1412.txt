Multi-agent cooperation in dynamic environments is a challenging problem in artificial intelligence. This paper presents a deep hierarchical reinforcement learning framework, 'HRL-Coop', which enables agents to learn cooperative strategies in complex, partially observable environments. HRL-Coop leverages a hierarchical architecture to decompose the task into sub-tasks, and incorporates a novel attention mechanism to facilitate communication and coordination among agents. Experimental results on a variety of benchmark scenarios demonstrate that HRL-Coop outperforms state-of-the-art methods in terms of cooperation rate, reward, and adaptability to changing environments.