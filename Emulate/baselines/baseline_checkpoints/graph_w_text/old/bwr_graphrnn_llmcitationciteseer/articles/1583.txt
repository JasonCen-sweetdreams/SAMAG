This paper presents a novel framework for designing embodied conversational agents (ECAs) that can engage in multimodal human-robot interaction. We propose a hybrid approach combining rule-based and machine learning-based methods to generate context-aware, multimodal responses. Our ECA, 'RoboGuide', uses a combination of speech, gestures, and facial expressions to communicate with users in a museum setting. We evaluate RoboGuide's effectiveness in a user study, demonstrating significant improvements in user engagement and understanding compared to traditional text-based interfaces.