Multimodal emotion recognition has garnered significant attention in human-computer interaction, but existing approaches often lack interpretability. This paper proposes a novel hierarchical attention network (HAN) that integrates visual, acoustic, and linguistic features to recognize emotions in human-computer interactions. Our HAN model employs a bottom-up attention mechanism to selectively focus on salient features across modalities, enabling explainable emotion recognition. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods while providing insights into the emotional cues leveraged by the model.