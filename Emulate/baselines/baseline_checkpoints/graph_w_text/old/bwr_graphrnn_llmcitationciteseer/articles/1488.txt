Multimodal sentiment analysis aims to predict sentiment from heterogeneous data sources, such as text, images, and audio. However, existing approaches struggle to effectively fuse and weigh the contributions of each modality. We propose a hierarchical attention network (HAN) that learns to adaptively focus on relevant modalities and their interactions. Our HAN consists of modality-specific attention modules and a fusion layer that iteratively refines the attention weights. Experiments on three benchmark datasets demonstrate that our approach outperforms state-of-the-art methods, particularly in scenarios with noisy or missing modalities.