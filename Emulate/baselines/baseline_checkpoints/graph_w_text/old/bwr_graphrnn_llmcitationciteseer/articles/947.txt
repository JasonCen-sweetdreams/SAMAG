Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the variability and complexity of human emotions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and time-steps to recognize emotions more accurately and efficiently. Our approach leverages the strengths of both attention mechanisms and hierarchical feature extraction, achieving state-of-the-art performance on several benchmark datasets. We also demonstrate the effectiveness of our model in real-world applications, such as human-computer interaction and affective computing.