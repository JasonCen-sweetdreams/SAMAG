Emotion recognition is a crucial aspect of human-computer interaction, as it enables machines to respond empathetically to users. This paper proposes a novel hierarchical attention network (HAN) for multi-modal emotion recognition, combining speech, text, and facial expression features. Our HAN architecture consists of two attention layers: an intra-modal attention mechanism that weighs feature importance within each modality, and an inter-modal attention mechanism that fuses information across modalities. Experimental results on the SEMAINE database demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multimodal inputs, achieving an average F1-score of 0.92.