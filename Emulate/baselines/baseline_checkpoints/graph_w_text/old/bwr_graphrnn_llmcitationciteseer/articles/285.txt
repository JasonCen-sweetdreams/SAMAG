Multimodal sentiment analysis has gained significant attention in recent years, but the lack of explainability in current models hinders their adoption in real-world applications. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates vision and language modalities to analyze sentiment in images and text. Our approach utilizes attention weights to provide interpretable explanations of the sentiment prediction process, enabling the identification of influential modalities and features. Experimental results on the MM-IMDB dataset demonstrate the effectiveness of HAN in achieving state-of-the-art performance while providing transparent and interpretable sentiment analysis.