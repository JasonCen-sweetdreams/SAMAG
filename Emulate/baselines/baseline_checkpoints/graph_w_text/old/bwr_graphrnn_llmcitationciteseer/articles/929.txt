Individuals with motor impairments face significant challenges when interacting with virtual assistants. This paper presents 'GazeVA', a novel gaze-based interaction system that enables users to communicate with virtual assistants using only their eye movements. We develop a machine learning model that detects and interprets gaze patterns to identify user intent, and integrate it with a commercial virtual assistant platform. Our user study with 20 participants shows that GazeVA achieves an average accuracy of 92.5% in recognizing user commands, significantly outperforming existing gaze-based systems. We discuss the implications of our approach for enhancing accessibility in human-computer interaction.