Gesture recognition is a crucial aspect of human-computer interaction, but existing approaches often struggle with limited training data and variability in user behavior. This paper presents a novel framework, 'GestureFusion', which leverages transfer learning from large-scale image datasets and multimodal fusion of sensor data to improve recognition accuracy. Our approach adaptively combines convolutional neural networks (CNNs) with recurrent neural networks (RNNs) to capture both spatial and temporal patterns in gestures. Experiments on a diverse dataset of 100 users demonstrate a significant reduction in error rate (25.1% to 12.5%) compared to state-of-the-art methods, making GestureFusion a promising solution for real-world HCI applications.