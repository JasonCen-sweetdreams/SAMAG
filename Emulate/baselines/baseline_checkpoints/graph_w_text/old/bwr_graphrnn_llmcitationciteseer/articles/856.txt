Accurate affective state tracking is crucial in human-computer interaction to provide empathetic and personalized user experiences. This paper presents EmoTract, a novel multimodal system that combines computer vision, speech recognition, and physiological signal processing to track users' emotional states in real-time. EmoTract leverages a deep learning-based framework to fuse features from facial expressions, speech tone, and electrodermal activity, achieving a mean absolute error of 0.53 in valence-arousal space. We evaluate EmoTract in a user study with 30 participants, demonstrating its effectiveness in detecting emotional shifts during interactive tasks.