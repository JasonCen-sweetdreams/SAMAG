Neural search has shown promising results in various information retrieval tasks, but its performance often relies on the quality of the query representation. This paper proposes a novel approach to query expansion using contrastive learning, which learns to distinguish between relevant and non-relevant documents in a self-supervised manner. Our method, CLQE, leverages a neural network to generate multiple query representations and adapts to the search context by sampling hard negatives. Experiments on several benchmark datasets demonstrate that CLQE significantly improves the retrieval performance of neural search models, outperforming traditional query expansion techniques.