Cooperative task allocation is a crucial problem in multi-agent systems, where agents must work together to accomplish complex tasks in dynamic environments. This paper proposes a novel multi-agent reinforcement learning framework, 'CoopRL', which enables agents to learn cooperative policies through decentralized exploration and communication. We introduce a hierarchical task decomposition approach that adapts to changing environmental conditions, allowing agents to re-allocate tasks and adjust their policies accordingly. Experimental results on a simulated robotic search and rescue scenario demonstrate that CoopRL outperforms state-of-the-art methods in terms of task completion rate and communication efficiency.