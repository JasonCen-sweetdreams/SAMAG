Virtual reality (VR) systems often rely on manual input devices, which can be cumbersome and limit the sense of immersion. This paper explores the potential of gaze-based interaction as a more natural and intuitive alternative. We conduct a comprehensive study on eye movement patterns in VR, analyzing the saccadic behavior of 30 participants in various tasks. Our results show that gaze-based interaction can achieve high accuracy and reduce latency compared to traditional input methods. We also propose a novel gaze-tracking algorithm that incorporates machine learning-based models to improve the robustness and efficiency of the system.