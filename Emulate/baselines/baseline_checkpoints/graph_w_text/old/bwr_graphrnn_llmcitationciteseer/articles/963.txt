Virtual reality (VR) systems have the potential to revolutionize accessibility for individuals with disabilities. However, existing interaction methods often rely on manual input or voice commands, which can be limiting for users with mobility or speech impairments. This paper presents a novel gaze-based interaction system, 'GazeVR', that leverages eye-tracking technology to enable users to navigate and interact with virtual environments. We introduce a machine learning-based gaze classification model that accurately detects user intent and reduces false positives. A user study with 20 participants demonstrates that GazeVR outperforms traditional interaction methods in terms of task completion time and user satisfaction.