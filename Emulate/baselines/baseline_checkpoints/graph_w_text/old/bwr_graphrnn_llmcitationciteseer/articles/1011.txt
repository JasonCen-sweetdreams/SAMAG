Explainability is crucial in multi-agent reinforcement learning (MARL) to ensure trustworthy decision-making. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant agents and their interactions. Our approach models agent relationships using a graph attention mechanism and integrates it with a hierarchical reinforcement learning framework. Experimental results on a MARL benchmark demonstrate that HAN achieves state-of-the-art performance while providing interpretable explanations of agent decision-making processes.