In multi-agent systems, allocating tasks efficiently is crucial for achieving system-wide objectives. This paper presents a decentralized task allocation approach using deep reinforcement learning, where each agent learns to make task allocation decisions based on local observations and communication with neighboring agents. Our approach, called 'DRL-TA', leverages a multi-agent deep Q-network to learn a policy that maximizes system performance while minimizing communication overhead. Experimental results on a simulated disaster response scenario demonstrate that DRL-TA outperforms traditional centralized allocation methods in terms of task completion time and system adaptability.