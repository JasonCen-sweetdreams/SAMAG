Deep reinforcement learning (DRL) has achieved remarkable success in various domains, but its performance is highly dependent on the choice of hyperparameters. This paper proposes a novel Bayesian optimization framework, 'HyperDRL', to efficiently tune hyperparameters for DRL algorithms. We employ a Gaussian process surrogate model to approximate the objective function and a acquisition function to select the most informative hyperparameter settings. Experimental results on several benchmark tasks demonstrate that HyperDRL significantly outperforms manual tuning and other automated methods, achieving state-of-the-art performance in a fraction of the computational time.