Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a crucial task in affective computing. However, existing models often lack transparency in their decision-making processes, hindering trust and reliability. We propose a novel Hierarchical Attention Network (HAN) that integrates attention mechanisms across modalities and hierarchical layers. Our approach enables explainable emotion recognition by identifying salient features and modalities contributing to the prediction. Experimental results on the IEMOCAP and SEMAINE datasets demonstrate improved performance and interpretability compared to state-of-the-art methods.