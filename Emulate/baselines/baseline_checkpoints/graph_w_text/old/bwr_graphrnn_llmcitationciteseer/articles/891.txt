Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) architecture that leverages the strengths of graph neural networks and attention mechanisms. HGAT learns to selectively focus on relevant modalities and their interactions, enabling more accurate emotion recognition. Experimental results on two benchmark datasets demonstrate that HGAT outperforms state-of-the-art methods, achieving an average F1-score improvement of 8.5% over the best baseline.