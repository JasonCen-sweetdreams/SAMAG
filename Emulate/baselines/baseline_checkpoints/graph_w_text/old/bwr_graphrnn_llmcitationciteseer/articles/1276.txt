This paper presents a novel approach to personalized gesture recognition for elderly users, leveraging multimodal fusion and transfer learning. Our system combines data from computer vision, wearable sensors, and audio inputs to recognize gestures with high accuracy. We propose a transfer learning framework that adapts to individual users' patterns and variations, improving recognition rates by up to 25%. Our user study with 30 elderly participants demonstrates the effectiveness of our approach in enhancing the user experience and promoting independent living.