Emotion recognition from multi-modal data (e.g., speech, text, and vision) remains a challenging task due to the complexity of human emotions and the heterogeneity of modalities. This paper proposes a novel Hierarchical Attention Network (HAN) framework to address this problem. Our HAN model learns to selectively focus on relevant modalities and features at multiple levels, enabling more accurate emotion recognition. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods, achieving a 12.5% improvement in F1-score.