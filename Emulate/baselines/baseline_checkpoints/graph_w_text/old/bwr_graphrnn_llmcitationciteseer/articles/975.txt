Emotion recognition from multi-modal data (e.g., speech, text, and vision) is a challenging task, particularly when explaining the underlying decision-making process. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates attention mechanisms across modalities and features. Our approach enables the identification of salient features and modalities that contribute to emotion recognition. Experimental results on the CMU-MOSEI dataset demonstrate that HAN outperforms state-of-the-art methods in both emotion recognition accuracy and explainability, providing insights into the emotional cues utilized by the model.