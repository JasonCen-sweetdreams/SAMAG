Emotion recognition is a crucial aspect of human-computer interaction, but existing approaches often rely on expensive and time-consuming manual annotations. This paper presents a novel hierarchical attention network (HAN) for multi-modal emotion recognition, leveraging both audio and visual features. Our proposed architecture consists of modality-specific encoders, a hierarchical attention module, and a fusion layer. Experimental results on the IEMOCAP and SEMAINE datasets demonstrate that HAN outperforms state-of-the-art models in terms of accuracy and efficiency, while requiring fewer annotated samples. We also explore the interpretability of our model through attention visualization, providing insights into the emotional cues used for recognition.