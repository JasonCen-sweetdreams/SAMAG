Multimodal emotion recognition systems often struggle to provide transparent and interpretable decisions. This paper presents a novel hierarchical attention network (HAN) architecture that explicitly models the interactions between modalities and captures complex emotional patterns. Our approach leverages a hybrid attention mechanism that combines self-attention and cross-modal attention to selectively focus on relevant features. We demonstrate the effectiveness of HAN on two benchmark datasets, achieving state-of-the-art performance while providing visual explanations for the recognized emotions. Our results have implications for the development of more trustworthy and human-centered AI systems.