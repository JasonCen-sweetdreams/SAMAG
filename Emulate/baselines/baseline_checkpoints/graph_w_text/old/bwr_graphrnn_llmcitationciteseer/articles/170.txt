Conversational agents have become ubiquitous in various applications, but understanding user emotions remains a significant challenge. This paper proposes a novel Hierarchical Attention Network (HAN) architecture for multimodal emotion recognition in conversational agents. Our approach leverages attention mechanisms to fuse features from speech, text, and visual modalities, enabling the model to focus on relevant cues for emotion detection. We evaluate our approach on a large-scale multimodal dataset and demonstrate significant improvements over state-of-the-art methods. The proposed HAN architecture can be easily integrated into existing conversational agent frameworks, enhancing their emotional intelligence and user engagement.