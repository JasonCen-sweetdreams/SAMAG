Multimodal sentiment analysis (MSA) involves identifying sentiment from multiple sources, such as text, images, and audio. Existing approaches often rely on early fusion, which can lead to information loss. This paper proposes a hierarchical attention network (HAN) for MSA, which learns to selectively focus on relevant modalities and features at multiple levels. Our HAN model consists of modality-specific attention modules, followed by a fusion layer that adaptively weights the importance of each modality. Experimental results on the CMU-MOSI dataset show that our approach outperforms state-of-the-art methods in terms of accuracy and F1-score, particularly when dealing with noisy or incomplete data.