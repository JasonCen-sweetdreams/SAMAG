Multimodal sentiment analysis (MSA) involves analyzing sentiment from multiple sources (e.g., text, images, audio). While deep learning models have shown promising results, they often lack interpretability, making it difficult to understand the decision-making process. This paper proposes a novel Hierarchical Attention Network (HAN) framework for MSA, which incorporates explainable attention mechanisms at both the modality and feature levels. Our approach enables the identification of salient features contributing to the sentiment prediction, thereby providing insights into the model's decision-making process. Experimental results on a large-scale multimodal dataset demonstrate the effectiveness of our approach in improving both sentiment analysis accuracy and explainability.