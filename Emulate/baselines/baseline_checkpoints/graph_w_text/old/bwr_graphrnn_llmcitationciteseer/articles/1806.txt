Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a challenging task due to the complexity of human emotions and the variability of modalities. This paper presents a novel Hierarchical Attention Network (HAN) that adaptively fuses features from different modalities, capturing both intra- and inter-modal relationships. Our HAN model utilizes a hierarchical architecture to model emotions at multiple levels of granularity, from local features to global representations. Experimental results on the benchmark CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods in terms of emotion recognition accuracy and computational efficiency.