As in-vehicle infotainment systems become increasingly complex, gesture recognition has emerged as a promising modality for intuitive user interaction. However, existing systems often struggle with variability in user behavior, lighting conditions, and environmental factors. This paper presents 'Adapta Gesture', a machine learning-based framework that adaptively adjusts gesture recognition models to individual users and contexts. Our approach combines multimodal fusion of visual and inertial sensor data with online learning and transfer learning techniques to improve recognition accuracy and robustness. A user study with 30 participants demonstrates significant improvements in gesture recognition performance and user satisfaction compared to state-of-the-art systems.