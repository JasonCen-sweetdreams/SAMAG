Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) has gained significant attention in recent years. However, existing approaches often lack transparency and interpretability, making it challenging to understand the decision-making process. This paper presents a novel Hierarchical Attention Network (HAN) that leverages attention mechanisms to selectively focus on relevant modalities and features, enabling explainable emotion recognition. Our approach integrates a modality-aware attention module with a hierarchical fusion mechanism, allowing the model to adaptively weight and combine modalities based on their emotional relevance. Experimental results on the CMU-MOSEI dataset demonstrate the effectiveness of HAN in improving emotion recognition accuracy while providing interpretable insights into the decision-making process.