Emotion recognition from multi-modal data (e.g., audio, video, and text) is a challenging task due to the complex relationships between modalities. This paper proposes a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and features. Our HAN consists of two stages: intra-modality attention to capture local patterns and inter-modality attention to integrate information across modalities. We evaluate our approach on three benchmark datasets and demonstrate significant improvements in emotion recognition accuracy and computational efficiency compared to state-of-the-art methods.