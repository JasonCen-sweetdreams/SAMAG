Deep neural networks (DNNs) have achieved state-of-the-art performance in various machine learning tasks, but their hyperparameter tuning remains a challenging and time-consuming process. This paper proposes a novel Bayesian optimization approach, 'HyperBO', which leverages a probabilistic surrogate model to efficiently search the hyperparameter space. We introduce a new acquisition function that balances exploration and exploitation, and demonstrate its effectiveness in tuning DNNs for image classification tasks. Experimental results show that HyperBO outperforms existing methods in terms of convergence speed and final model performance.