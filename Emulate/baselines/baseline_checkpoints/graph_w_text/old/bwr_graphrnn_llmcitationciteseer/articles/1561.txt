Multi-modal dialogue systems require efficient processing of various input modalities, such as text, speech, and vision. This paper introduces a novel attention mechanism, 'ModalGate', which adaptively selects and fuses relevant modalities based on the context and task requirements. We demonstrate that ModalGate outperforms existing approaches in terms of response accuracy and latency, while reducing computational overhead. Evaluation on a benchmark dataset shows that our approach achieves state-of-the-art performance in multi-modal dialogue generation tasks, with significant improvements in user satisfaction ratings.