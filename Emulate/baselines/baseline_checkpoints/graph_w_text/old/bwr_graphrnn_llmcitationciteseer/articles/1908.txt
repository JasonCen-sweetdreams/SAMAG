Multi-modal sentiment analysis, which involves analyzing user opinions from both text and visual data, has become increasingly important in modern applications. In this paper, we propose a novel Hierarchical Graph Attention Network (HGAT) that leverages the graph structure of multi-modal data to capture complex relationships between modalities. Our approach utilizes self-attention mechanisms to learn modality-specific representations and hierarchical graph pooling to aggregate features across modalities. Experimental results on three benchmark datasets demonstrate that HGAT outperforms state-of-the-art methods in multi-modal sentiment analysis tasks, achieving an average improvement of 4.2% in accuracy.