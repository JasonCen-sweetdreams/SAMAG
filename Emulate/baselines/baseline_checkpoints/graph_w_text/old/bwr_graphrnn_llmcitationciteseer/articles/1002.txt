Multi-modal emotion recognition is a challenging task due to the complexity of human emotions and the heterogeneity of modalities. This paper proposes a novel hierarchical attention network (HAN) that leverages the strengths of individual modalities while mitigating their weaknesses. Our HAN model consists of modality-specific attention modules, followed by a hierarchical fusion mechanism that adaptively weights the importance of each modality. Experimental results on the benchmark CMU-MOSEI dataset demonstrate that our approach achieves state-of-the-art performance in recognizing emotions from facial expressions, speech, and text, while reducing computational costs by 30% compared to existing methods.