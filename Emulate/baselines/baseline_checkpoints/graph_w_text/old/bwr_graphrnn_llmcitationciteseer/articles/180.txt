Explainability is a crucial aspect of multi-agent systems, where agents interact and make decisions in complex environments. This paper presents a novel Hierarchical Attention Network (HAN) architecture that enables explainable decision-making in multi-agent settings. Our approach learns to focus on relevant agents and their interactions, providing interpretable attention weights. We evaluate HAN on a variety of multi-agent scenarios, demonstrating improved decision-making performance and enhanced explainability compared to state-of-the-art methods. Our results have significant implications for real-world applications, such as autonomous vehicles and smart grids.