Multi-modal emotion recognition (MER) has garnered significant attention in recent years, but existing approaches often suffer from high computational costs and limited generalizability. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages the complementary strengths of visual, acoustic, and textual modalities to improve MER performance. Our HAN model employs a hierarchical attention mechanism to selectively focus on relevant modalities and features, reducing the computational overhead and enhancing robustness to noisy or missing data. Experimental results on three benchmark datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance while requiring significantly fewer parameters and computations compared to existing MER models.