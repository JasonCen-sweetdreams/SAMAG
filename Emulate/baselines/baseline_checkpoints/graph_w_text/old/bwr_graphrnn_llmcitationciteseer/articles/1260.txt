Virtual reality (VR) interfaces often rely on gaze-based interaction, but incorrect gaze estimations can lead to user frustration and decreased immersion. This paper presents an eye-tracking-based adaptive gaze correction technique, 'EyeCorrect', which leverages machine learning to refine gaze estimates in real-time. Our approach combines convolutional neural networks (CNNs) with Kalman filters to predict and correct gaze errors, achieving an average error reduction of 37.4% in a user study. We demonstrate the effectiveness of EyeCorrect in various VR applications, including gaming and virtual training scenarios.