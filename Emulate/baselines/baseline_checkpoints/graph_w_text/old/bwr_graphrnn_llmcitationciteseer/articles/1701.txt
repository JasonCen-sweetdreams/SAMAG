Emotion recognition in human-computer interaction (HCI) is crucial for developing empathetic and personalized systems. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates multi-modal inputs from facial expressions, speech, and physiological signals. Our HAN model learns to selectively focus on relevant modalities and features, improving emotion recognition accuracy and robustness in real-world scenarios. Experimental results on a large-scale HCI dataset demonstrate significant performance gains over existing state-of-the-art approaches, particularly for nuanced emotions like boredom and frustration.