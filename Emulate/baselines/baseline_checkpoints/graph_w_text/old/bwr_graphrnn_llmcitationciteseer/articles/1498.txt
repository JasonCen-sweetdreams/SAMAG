Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) remains a challenging task due to the complexity of human emotions and the heterogeneity of modalities. We propose a novel hierarchical attention network (HAN) that adaptively fuses modalities at multiple levels, capturing both local and global context. Our HAN model consists of modality-specific attention modules, a cross-modal attention layer, and a hierarchical fusion mechanism. Experimental results on the benchmark CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an average F1-score of 83.1%.