Autonomous systems, such as self-driving cars, often rely on reinforcement learning (RL) to make decisions. However, the lack of interpretability in RL models hinders their adoption in safety-critical applications. This paper proposes a novel explainable RL framework, 'HierAtt', which integrates hierarchical attention mechanisms to focus on relevant state features and actions. We demonstrate the efficacy of HierAtt in a simulated autonomous driving environment, showing improved model interpretability and decision-making transparency. Our approach enables the identification of critical situations and provides insights into the decision-making process, paving the way for trustworthy autonomous systems.