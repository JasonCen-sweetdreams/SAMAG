Multimodal sentiment analysis is a challenging task that involves fusing and analyzing sentiment cues from text, images, and videos. This paper proposes a novel Hierarchical Attention Network (HAN) that incorporates both intra-modal and inter-modal attention mechanisms to selectively focus on salient features and modalities. Our approach enables explainable sentiment predictions by generating visual and textual rationales. Experimental results on three benchmark datasets demonstrate the effectiveness of HAN in improving sentiment accuracy and providing interpretable results.