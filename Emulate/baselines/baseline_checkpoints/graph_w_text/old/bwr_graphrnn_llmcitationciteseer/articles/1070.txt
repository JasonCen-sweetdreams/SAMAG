Emotion recognition from multi-modal inputs (e.g., speech, vision, text) is a crucial task in affective computing. While deep learning models have achieved state-of-the-art performance, they often lack interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) that leverages attention mechanisms to selectively focus on relevant input modalities and features, enabling explainable emotion recognition. Our experiments on a benchmark dataset demonstrate improved performance and provide insights into the attention patterns, revealing the importance of multimodal fusion and feature selection for emotion recognition.