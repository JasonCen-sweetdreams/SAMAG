Emotion recognition is a crucial aspect of human-computer interaction, enabling systems to respond empathetically and improve user experience. This paper presents EmoReact, a novel multimodal framework that leverages computer vision, natural language processing, and physiological signal processing to recognize emotions in real-time. EmoReact integrates a convolutional neural network (CNN) for facial expression analysis, a recurrent neural network (RNN) for sentiment analysis from text, and a Gaussian process regression model for electroencephalography (EEG) signal analysis. Experiments on a large, diverse dataset demonstrate EmoReact's superior performance over state-of-the-art unimodal approaches, achieving an accuracy of 93.2% in recognizing six basic emotions.