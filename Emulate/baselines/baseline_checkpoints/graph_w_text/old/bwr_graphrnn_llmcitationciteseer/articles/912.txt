Emotion recognition from multimodal inputs (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the variability of modality-specific features. This paper proposes a novel Hierarchical Attention Network (HAN) that integrates multiple modalities by exploiting their complementary information. Our HAN model consists of modality-specific attention modules and a hierarchical fusion mechanism that adaptively weights the importance of each modality. Experimental results on the IEMOCAP and CMU-MOSEI datasets demonstrate the superiority of our approach over state-of-the-art multimodal fusion methods, achieving an average F1-score improvement of 12.4% and 8.2%, respectively.