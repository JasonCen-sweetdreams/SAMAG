Multimodal sentiment analysis in online reviews poses a significant challenge due to the complexity of integrating heterogeneous data sources, such as text, images, and videos. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and aspects of the data. Our approach leverages a hierarchical attention mechanism to model the relationships between modalities, and a sentiment-aware fusion layer to integrate the outputs. Experimental results on a large-scale dataset of online reviews demonstrate the effectiveness of our approach, achieving state-of-the-art performance in terms of sentiment accuracy and aspect-based sentiment detection.