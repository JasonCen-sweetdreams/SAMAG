Emotion recognition in human-computer interaction (HCI) is a complex task, especially when dealing with multi-modal inputs such as speech, text, and facial expressions. This paper proposes a novel hierarchical attention network (HAN) architecture that learns to selectively focus on relevant modalities and features to recognize emotions. Our HAN model consists of a feature-level attention module and a modality-level attention module, which are jointly trained to optimize emotion recognition performance. Experimental results on a benchmark dataset show that our approach outperforms state-of-the-art methods, achieving an average F1-score of 0.92 across six emotions.