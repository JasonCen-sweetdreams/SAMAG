Emotion recognition in human-computer interaction (HCI) is crucial for developing empathetic AI systems. While deep learning models have achieved high accuracy, they often lack interpretability. This paper proposes a novel hierarchical attention network (HAN) for multimodal emotion recognition, which leverages both facial expression and speech features. Our HAN model learns to focus on salient regions of the face and spectro-temporal features of speech, providing an attention map that explains the emotion recognition process. Experimental results on the CMU-Multimodal SDK dataset demonstrate that our approach outperforms state-of-the-art methods while offering insights into the decision-making process.