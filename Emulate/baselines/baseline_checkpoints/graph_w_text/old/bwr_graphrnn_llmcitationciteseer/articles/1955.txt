Deep neural networks (DNNs) are vulnerable to adversarial attacks, which can compromise their performance and reliability. This paper presents a novel defense mechanism, Hierarchical Feature Alignment (HFA), which leverages the hierarchical structure of DNNs to align features across different layers. By aligning features, HFA reduces the sensitivity of DNNs to adversarial perturbations, thereby improving their robustness. We evaluate HFA on several benchmark datasets and demonstrate its effectiveness against various attack strategies, including Projected Gradient Descent and Fast Gradient Sign Method. Our results show that HFA outperforms state-of-the-art defense methods in terms of both accuracy and robustness.