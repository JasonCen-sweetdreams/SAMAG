Multi-modal emotion recognition is a challenging task that requires processing and fusing information from various modalities, such as speech, text, and vision. In this paper, we propose a neural architecture search (NAS) approach to automatically design efficient and effective models for multi-modal emotion recognition. Our method, called 'EMO-NAS', uses a reinforcement learning-based search strategy to explore a vast space of possible neural architectures and identify the best ones for a given emotion recognition task. We evaluate EMO-NAS on several benchmark datasets and demonstrate its ability to discover models that outperform hand-crafted baselines while requiring fewer parameters and computations.