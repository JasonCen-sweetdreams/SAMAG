Social media platforms generate vast amounts of multimodal data, comprising text, images, and videos. Effective sentiment analysis in this domain requires capturing complex relationships between modalities. This paper introduces a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and regions within each modality. Our HAN model consists of modality-specific encoders, a multimodal fusion layer, and a sentiment prediction module. Experimental results on a large-scale social media dataset demonstrate that our approach outperforms state-of-the-art methods in multimodal sentiment analysis, achieving an F1-score of 0.842.