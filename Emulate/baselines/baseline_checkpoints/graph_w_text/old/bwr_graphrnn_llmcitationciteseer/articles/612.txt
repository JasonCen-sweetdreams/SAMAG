Deep reinforcement learning (DRL) has achieved remarkable success in complex tasks, but the lack of transparency and accountability hinders its adoption in high-stakes applications. We propose a novel approach to explainable DRL, which leverages model-based critic-regularization to provide insightful explanations for agent decision-making. Our method, dubbed 'CriticExplainer', integrates a probabilistic critic network with a generative model to produce actionable explanations of state-value functions. Experimental results on Atari games and real-world robotics demonstrate that CriticExplainer improves transparency and interpretability without compromising policy performance.