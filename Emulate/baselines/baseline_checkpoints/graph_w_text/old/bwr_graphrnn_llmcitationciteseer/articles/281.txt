This paper presents a novel approach to gaze-based intention recognition, enabling more natural and efficient human-computer interaction. Our method leverages deep learning-based gaze estimation and intention inference from multimodal sensor data. We introduce a new dataset, GazeInt, comprising annotated gaze patterns and intention labels from 30 participants. Experimental results show that our approach outperforms state-of-the-art methods in recognizing user intentions, achieving an average accuracy of 92.5%. We demonstrate the effectiveness of our approach in a real-world application, improving the interaction experience for users with disabilities.