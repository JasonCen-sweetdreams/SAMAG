Multimodal emotion recognition has become increasingly popular, but the lack of transparency in deep learning models hinders trust and understanding. This paper proposes an adaptive attention mechanism, dubbed 'AEMA', which learns to selectively focus on relevant modalities (e.g., speech, text, or vision) and features to improve emotion recognition. By incorporating attention weights into the training process, AEMA provides interpretable emotion recognition models that outperform state-of-the-art multimodal fusion approaches. We evaluate AEMA on three benchmark datasets, demonstrating improved recognition accuracy and enhanced model explainability.