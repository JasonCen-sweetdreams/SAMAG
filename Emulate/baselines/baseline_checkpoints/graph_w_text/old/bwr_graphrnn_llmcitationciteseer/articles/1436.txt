This paper proposes a decentralized task allocation framework for multi-agent systems, where agents learn to allocate tasks through reinforcement learning. Our approach, called 'DRL-TA', uses a decentralized actor-critic architecture, where each agent learns to optimize its own policy based on local observations and rewards. We evaluate DRL-TA on a variety of task allocation scenarios and demonstrate its effectiveness in improving system efficiency and adaptability compared to traditional centralized approaches. Furthermore, we analyze the impact of agent heterogeneity and communication constraints on the learning process and system performance.