Individuals with motor impairments often face significant challenges when interacting with digital systems. This paper presents EyeTrack, a novel gaze-based interaction framework that leverages machine learning and computer vision techniques to enable accurate and efficient user input. EyeTrack utilizes a combination of eye-tracking sensors and machine learning models to detect and interpret user gaze patterns, allowing users to control digital interfaces without relying on traditional input devices. Our evaluation results demonstrate that EyeTrack achieves high accuracy and user satisfaction in a range of tasks, including text entry, cursor control, and gaming.