Individuals with motor impairments face significant challenges when interacting with traditional computing interfaces. This paper presents a novel gesture-based interface framework, 'Adapti Gesture', which leverages machine learning and computer vision to adapt to the unique abilities and needs of each user. Our approach integrates a probabilistic gesture recognition model with a user-centered design approach, enabling users to customize the interface to their individual motor capabilities. A usability study with 20 participants with motor impairments demonstrates that Adapti Gesture improves interaction speed and accuracy by up to 30% compared to traditional interfaces.