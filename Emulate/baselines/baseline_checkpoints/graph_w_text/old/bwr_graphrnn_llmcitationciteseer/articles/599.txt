Affective state recognition is a crucial aspect of human-computer interaction (HCI), enabling computers to respond empathetically to users' emotional needs. This paper presents EmoTract, a multimodal system that recognizes users' affective states through a combination of facial expression analysis, speech patterns, and physiological signals. EmoTract employs a deep learning framework to fuse features from these modalities, achieving an accuracy of 92.5% on a benchmark dataset. We demonstrate the system's effectiveness in enhancing user experience in a virtual reality environment, opening up new avenues for affective HCI applications.