Emotion recognition is a crucial aspect of human-computer interaction, enabling machines to respond empathetically to users. This paper presents a novel hierarchical attention network (HAN) for multi-modal emotion recognition, leveraging facial expressions, speech, and physiological signals. Our HAN architecture incorporates a modal-agnostic attention mechanism, allowing the model to selectively focus on relevant input modalities and capture complex emotional patterns. Experimental results on a large-scale, multimodal dataset demonstrate the superiority of our approach over state-of-the-art methods, achieving an average F1-score of 0.92 for emotion classification.