Search engine result diversification is crucial for providing users with a comprehensive overview of relevant information. This paper proposes a novel approach to diversification using reinforcement learning. Our method, dubbed 'DiverRL', formulates the diversification problem as a Markov decision process and learns a policy to select diverse documents that maximize user satisfaction. Experimental results on a large-scale web dataset demonstrate that DiverRL outperforms state-of-the-art diversification algorithms in terms of novelty, relevance, and user engagement metrics.