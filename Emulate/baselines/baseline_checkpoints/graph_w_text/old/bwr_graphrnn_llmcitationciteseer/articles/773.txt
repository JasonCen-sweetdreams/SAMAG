This paper presents a novel multi-agent reinforcement learning (MARL) framework for cooperative task allocation in edge computing environments. We formulate the task allocation problem as a decentralized partially observable Markov decision process (DEC-POMDP) and propose a hierarchical MARL algorithm that leverages both local and global information to optimize task assignments. Experimental results on a simulated edge computing platform demonstrate that our approach outperforms existing baseline methods in terms of task completion time, resource utilization, and overall system efficiency.