Neural architecture search (NAS) has revolutionized the design of deep neural networks, but its computational overhead hinders its adoption on resource-constrained devices. This paper presents 'BO-NAS', a novel NAS framework that leverages Bayesian optimization to efficiently explore the architecture search space. By incorporating a probabilistic surrogate model and a customized acquisition function, BO-NAS reduces the number of evaluations required to discover high-performing architectures. Our experiments on various benchmark datasets demonstrate that BO-NAS achieves state-of-the-art results on resource-constrained devices, such as mobile phones and embedded systems, while reducing the search time by up to 5x compared to existing NAS methods.