Multimodal sentiment analysis (MSA) has garnered significant attention in recent years, but existing approaches often lack transparency and interpretability. This paper proposes a novel hierarchical attention network (HAN) for MSA, which leverages both visual and textual features to generate sentiment predictions. Our model incorporates attention mechanisms at multiple levels, enabling the identification of salient features and relationships that contribute to the sentiment classification. Experimental results on three benchmark datasets demonstrate the effectiveness of our approach in achieving state-of-the-art performance while providing explainable insights into the decision-making process.