Cooperative planning in multi-agent systems is crucial for achieving complex tasks in dynamic environments. However, existing approaches often rely on centralized planning, which can be inefficient and inflexible. This paper proposes a decentralized, hierarchical planning framework that enables agents to adapt to changing environmental conditions while coordinating with each other. We introduce a novel, graph-based representation of agent interactions and a distributed planning algorithm that leverages reinforcement learning to improve planning efficiency. Experimental results in a simulated robot soccer domain demonstrate the effectiveness of our approach in achieving cooperative goals while reducing communication overhead.