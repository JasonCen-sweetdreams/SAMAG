Sentiment analysis from multimodal data, such as text, images, and videos, has become increasingly important in various applications. This paper proposes a novel hierarchical attention network (HAN) framework that leverages the strengths of different modalities to improve sentiment analysis. Our HAN model consists of three levels of attention: modality-level, feature-level, and token-level attention. Experimental results on three benchmark datasets demonstrate that our approach outperforms state-of-the-art multimodal sentiment analysis models, achieving an average improvement of 4.2% in terms of accuracy.