Recognizing emotions from multi-modal inputs, such as speech, text, and facial expressions, is a challenging task in human-computer interaction. We propose a novel hierarchical attention network (HAN) that leverages the strengths of each modality to improve emotion recognition accuracy. Our HAN model consists of modality-specific attention modules, followed by a hierarchical fusion mechanism that adaptively weights the importance of each modality. Experimental results on the IEMOCAP dataset show that our model outperforms state-of-the-art methods, achieving an average F1-score improvement of 7.2% while reducing computational complexity by 30%.