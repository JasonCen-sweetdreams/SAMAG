Emotion recognition from multimodal inputs (e.g., speech, text, and vision) is a challenging task due to the complexity and heterogeneity of the data. This paper proposes a novel hierarchical attention network (HAN) that selectively focuses on relevant modalities and features to recognize emotions. We introduce a hierarchical attention mechanism that learns to weight the importance of each modality and feature at multiple levels of abstraction. Experimental results on the CMU-MOSEI dataset demonstrate that our HAN model outperforms state-of-the-art multimodal fusion methods and provides interpretable attention weights, enabling explainable emotion recognition.