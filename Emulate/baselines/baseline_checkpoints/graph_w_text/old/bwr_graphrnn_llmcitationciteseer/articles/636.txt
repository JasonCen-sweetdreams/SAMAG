Emotion recognition is a crucial aspect of human-computer interaction, but current approaches often rely on single-modal inputs, neglecting the rich contextual information available from multiple modalities. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates audio, video, and physiological signals to recognize emotions in a more comprehensive and accurate manner. Our HAN model employs a hierarchical attention mechanism to selectively focus on relevant modalities and time segments, leading to improved emotion recognition performance. Experimental results on a multi-modal dataset demonstrate the effectiveness of our approach, achieving a 12% increase in F1-score compared to state-of-the-art single-modal methods.