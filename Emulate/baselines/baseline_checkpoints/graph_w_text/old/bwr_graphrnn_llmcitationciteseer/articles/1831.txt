Deep neural networks are prone to overfitting when trained on noisy labels, leading to poor generalization performance. This paper presents an adaptive batch normalization (AdaBN) technique that dynamically adjusts the normalization statistics based on the estimated label noise. Our approach leverages a noise-aware loss function and an online estimation method to adapt the batch normalization parameters. Experimental results on several benchmark datasets demonstrate that AdaBN achieves state-of-the-art performance under various label noise scenarios, outperforming existing methods by a significant margin.