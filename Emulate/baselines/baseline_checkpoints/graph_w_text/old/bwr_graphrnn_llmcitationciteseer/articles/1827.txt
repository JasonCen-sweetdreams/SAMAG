Multimodal sentiment analysis in social media is challenging due to the complexity of user-generated content. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that effectively captures contextual relationships between text, image, and video modalities. Our approach integrates modality-specific attention mechanisms, which selectively focus on relevant input features, with a hierarchical fusion strategy that leverages both local and global interactions. Experiments on a large-scale multimodal dataset demonstrate significant improvements in sentiment prediction accuracy over state-of-the-art multimodal fusion methods, particularly in scenarios with noisy or incomplete input data.