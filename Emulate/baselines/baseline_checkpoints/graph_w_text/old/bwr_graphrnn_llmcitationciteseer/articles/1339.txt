In multi-agent systems, task allocation is a crucial problem that affects the overall performance and efficiency of the system. This paper presents a decentralized task allocation framework using reinforcement learning, where each agent learns to allocate tasks based on its local observations and interactions with other agents. We propose a novel algorithm, 'MA-RL-TA', that integrates Q-learning with a graph-based task allocation mechanism. Experimental results on a simulated logistics scenario demonstrate that MA-RL-TA outperforms traditional centralized and decentralized approaches in terms of task completion rate, latency, and robustness to agent failures.