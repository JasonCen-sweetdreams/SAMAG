Visual question answering (VQA) models struggle to effectively integrate multi-modal information from images and text. This paper proposes a novel hierarchical attention network (HAN) that learns to focus on relevant regions in images and words in questions. Our HAN consists of two stages: an intra-modal attention module that captures local dependencies within each modality, and an inter-modal attention module that fuses information across modalities. Experimental results on the VQA-CP v2 dataset demonstrate that our approach outperforms state-of-the-art models while reducing computational costs by 30%.