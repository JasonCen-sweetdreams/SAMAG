Emotion recognition from multi-modal inputs, such as audio, video, and text, is a challenging task due to the varying importance of each modality in different contexts. This paper proposes a novel Hierarchical Attention Network (HAN) that adaptively fuses modality-specific features using a hierarchical structure. The HAN consists of two stages: intra-modality attention, which captures local patterns within each modality, and inter-modality attention, which learns to weight the importance of each modality. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods by 12.3% in terms of weighted F1-score.