Virtual assistants are increasingly integrated into various devices, but their interaction modalities often neglect users' gaze behavior. This paper investigates the user experience and eye movement patterns when interacting with virtual assistants using gaze-based input. We conducted a user study with 30 participants, analyzing their gaze behaviors and self-reported experiences when performing tasks with a gaze-based virtual assistant. Our results show that gaze-based interaction improves user satisfaction and reduces cognitive load compared to traditional voice-based input. We also identified distinct gaze patterns associated with different task types, informing the design of more intuitive and efficient gaze-based interfaces for virtual assistants.