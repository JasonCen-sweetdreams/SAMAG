Deep reinforcement learning (DRL) algorithms are vulnerable to gradient-based attacks, which can significantly degrade their performance. This paper presents a novel adversarial training framework, 'RobustRL', that enhances the robustness of DRL agents against such attacks. By generating gradient-based perturbations on the agent's policy and incorporating them into the training process, RobustRL improves the agent's ability to resist attacks. Our experiments on the Atari 2600 benchmark demonstrate that RobustRL achieves state-of-the-art performance under various attack scenarios, while maintaining a reasonable computational overhead.