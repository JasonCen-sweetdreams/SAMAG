Explainability is crucial in multi-agent reinforcement learning (MARL) to understand agent decision-making. We propose HATMARL, a hierarchical attention network framework that learns to focus on relevant agents, states, and actions to improve explainability. HATMARL incorporates a novel attention mechanism that recursively aggregates attention weights across agents, enabling the model to capture complex multi-agent interactions. Experimental results on a range of MARL benchmarks demonstrate that HATMARL achieves state-of-the-art performance while providing interpretable insights into agent behavior and decision-making processes.