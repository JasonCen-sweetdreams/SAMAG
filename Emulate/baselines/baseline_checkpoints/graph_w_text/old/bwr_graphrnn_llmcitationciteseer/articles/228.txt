Conversational agents have become increasingly prevalent in various applications, but recognizing emotions from multi-modal inputs remains a challenging task. This paper proposes a hierarchical attention network (HAN) that integrates acoustic, linguistic, and visual features to recognize emotions in human-agent interactions. Our approach leverages self-attention mechanisms to capture complex relationships between modalities and utterances. Experimental results on the IEMOCAP dataset demonstrate that HAN outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an average F1-score of 0.83.