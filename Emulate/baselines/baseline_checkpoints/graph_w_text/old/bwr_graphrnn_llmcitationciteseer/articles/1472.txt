Emotion recognition from multi-modal data, such as speech, text, and vision, has numerous applications in human-computer interaction. However, existing approaches often suffer from high computational costs and neglect the inherent hierarchical relationships between modalities. We propose a hierarchical attention network (HAN) that leverages attention mechanisms to selectively focus on relevant modalities and features. Experimental results on the benchmark IEMOCAP dataset demonstrate that our HAN model achieves state-of-the-art performance while reducing computational overhead by 30% compared to existing multi-modal fusion methods.