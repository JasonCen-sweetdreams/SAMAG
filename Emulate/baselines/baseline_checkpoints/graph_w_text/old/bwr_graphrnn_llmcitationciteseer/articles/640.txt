Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a challenging task due to the inherent heterogeneity and complexity of human emotions. This paper introduces a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and temporal segments to recognize emotions. We propose a new multi-task learning objective that jointly optimizes emotion recognition and modality-specific feature extraction. Experimental results on the IEMOCAP and SEMAINE datasets demonstrate that our HAN approach outperforms state-of-the-art methods in terms of emotion recognition accuracy and robustness.