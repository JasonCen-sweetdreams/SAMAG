Emotion recognition from multi-modal inputs (e.g., facial expressions, speech, and text) is a complex task, especially when dealing with large datasets. This paper presents a novel hierarchical attention network (HAN) architecture that selectively focuses on relevant input modalities and regions to improve recognition accuracy. Our HAN model consists of multiple attention layers that learn to weight and combine features from different modalities, enabling efficient processing of high-dimensional input data. Experimental results on the benchmark multimodal emotion recognition dataset (MEREED) demonstrate that our approach outperforms state-of-the-art methods in terms of both accuracy and computational efficiency.