The proliferation of multi-modal documents, containing both text and non-textual elements, has created new challenges for information retrieval systems. This paper proposes a novel hierarchical query expansion approach that leverages both textual and visual features to improve retrieval efficiency. Our method, dubbed 'HQE-MM', exploits the semantic relationships between modalities to generate a set of expanded queries, which are then used to re-rank documents. Experimental results on a large-scale multi-modal dataset demonstrate that HQE-MM achieves significant gains in retrieval performance, particularly for queries with low textual relevance.