Explainability is crucial in multi-agent reinforcement learning (MARL) to ensure trust and accountability in decision-making. This paper proposes a novel hierarchical attention network (HAT) architecture that provides interpretable policies for MARL agents. HAT leverages attention mechanisms to learn hierarchical representations of agent interactions, enabling the identification of key factors influencing agent decisions. We evaluate HAT on a range of MARL benchmarks, demonstrating improved performance and explainability compared to state-of-the-art approaches. Our results have significant implications for real-world applications, such as autonomous vehicles and smart grids.