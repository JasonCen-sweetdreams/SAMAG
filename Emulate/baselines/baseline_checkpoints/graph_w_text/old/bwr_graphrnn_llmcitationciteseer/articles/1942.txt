Multimodal emotion recognition (MER) aims to recognize emotions from diverse modalities, such as speech, text, and vision. Existing MER approaches often rely on early fusion or late fusion strategies, which may not effectively capture the complex interactions between modalities. This paper proposes a novel hierarchical attention network (HAN) architecture, which leverages both intra-modal and inter-modal attention mechanisms to selectively focus on relevant features. Our experiments on the CMU-MOSEI dataset demonstrate that HAN outperforms state-of-the-art MER methods, achieving a significant improvement in emotion recognition accuracy.