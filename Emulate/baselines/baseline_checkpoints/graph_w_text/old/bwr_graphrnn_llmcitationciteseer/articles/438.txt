Sentiment analysis on multi-modal data (e.g., text, image, and video) is crucial for various applications. However, existing methods often fail to capture complex relationships between different modalities. This paper proposes a novel heterogeneous graph neural network (HGNN) framework, 'MM-Sent', which models the interactions between modalities as a heterogeneous graph. MM-Sent leverages graph attention mechanisms to adaptively weight the importance of each modality and node, enabling the network to focus on the most informative features. Experimental results on several benchmark datasets demonstrate that MM-Sent outperforms state-of-the-art methods in multi-modal sentiment analysis tasks.