Task planning for robots in dynamic environments is a challenging problem due to the complexity of real-world scenarios. This paper proposes a hierarchical reinforcement learning (HRL) framework that enables robots to learn efficient task plans while adapting to changing environments. Our approach combines a high-level task planner with a low-level controller, leveraging the strengths of both to optimize task execution. We evaluate our framework on a variety of simulated tasks, demonstrating improved task completion rates and reduced execution times compared to traditional planning methods.