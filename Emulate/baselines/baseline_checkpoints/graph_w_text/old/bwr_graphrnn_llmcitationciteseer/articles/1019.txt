This paper presents a novel gaze-based navigation system for virtual reality (VR) environments using Bayesian inference. Our approach leverages the user's gaze patterns to infer their intended navigation direction and velocity, enabling hands-free navigation. We propose a probabilistic model that integrates gaze data with head tracking information to generate accurate navigation commands. Experimental results show that our approach achieves a significant reduction in navigation errors and improves user experience compared to traditional controller-based navigation methods. The proposed system has implications for improving accessibility and enhancing immersion in VR applications.