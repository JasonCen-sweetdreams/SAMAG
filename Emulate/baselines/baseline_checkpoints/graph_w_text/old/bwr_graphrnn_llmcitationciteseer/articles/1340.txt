Visual question answering (VQA) models often rely on complex neural architectures, making it challenging to interpret their decision-making processes. This paper introduces a hierarchical attention network (HAN) for VQA, which allows for better explainability and improved performance. Our HAN model incorporates a novel attention mechanism that focuses on relevant regions of the input image and question, enabling the identification of key factors influencing the model's predictions. Experimental results on the VQA 2.0 dataset demonstrate the effectiveness of our approach, achieving state-of-the-art performance while providing insights into the model's decision-making process.