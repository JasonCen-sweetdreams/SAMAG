Virtual reality (VR) systems often rely on traditional input modalities, such as controllers or gestures, which can be limiting and unnatural. This paper presents a novel gaze-based interaction framework for VR, which leverages eye-tracking technology to enable more intuitive and immersive user experiences. Our approach incorporates machine learning models to predict user intentions and adapt the interaction paradigm in real-time, based on the user's gaze patterns and behavioral cues. Experimental results demonstrate significant improvements in user performance and satisfaction, particularly for complex VR tasks that require precise manipulation and selection.