Autonomous vehicles operating in uncertain environments, such as those with inclement weather or roadwork, require advanced control systems to ensure safety and efficiency. This paper presents a hierarchical reinforcement learning (HRL) framework, 'Hierarchical Autonomous Control' (HAC), which integrates high-level mission planning with low-level control policy optimization. We demonstrate that HAC outperforms traditional reinforcement learning methods in simulations of complex driving scenarios, achieving improved stability and adaptability in the face of uncertainty. Our approach has significant implications for the development of reliable autonomous vehicles in real-world settings.