Modern search engines face the challenge of adapting to domain shifts in user queries, resulting in suboptimal ranking model performance. This paper proposes a novel contrastive meta-learning approach, 'MetaRank', which enables ranking models to adapt to new domains with limited labeled data. Our method leverages contrastive learning to learn domain-invariant representations and meta-learning to adapt to new domains. Experimental results on a large-scale search engine dataset demonstrate that MetaRank outperforms state-of-the-art baseline methods, achieving a 12.5% improvement in ranking accuracy on unseen domains.