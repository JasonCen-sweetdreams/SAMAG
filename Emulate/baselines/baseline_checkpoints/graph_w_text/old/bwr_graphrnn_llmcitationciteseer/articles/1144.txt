Explainability is crucial in multi-agent systems where human-agent collaboration is involved. This paper presents a novel Hierarchical Attention Network (HAN) architecture that learns to explain the decision-making process of autonomous agents in complex, dynamic environments. Our approach combines attention mechanisms with hierarchical reinforcement learning to generate interpretable rationales for agent actions. Experimental results on a simulated search-and-rescue scenario demonstrate that HAN outperforms state-of-the-art methods in terms of both task performance and explanation quality, providing insights into the decision-making process of individual agents and their interactions.