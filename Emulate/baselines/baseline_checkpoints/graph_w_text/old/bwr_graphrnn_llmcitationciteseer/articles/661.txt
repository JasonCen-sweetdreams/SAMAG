Distributed databases have become increasingly prevalent in modern data-intensive applications. However, optimizing query execution plans remains a challenging task, especially in dynamic environments. This paper proposes a novel approach to query optimization using reinforcement learning (RL). Our method, dubbed 'RL-Optimizer', leverages an RL agent to learn the optimal query plan through trial and error, taking into account various performance metrics such as latency and resource utilization. Experimental results on a real-world dataset demonstrate that RL-Optimizer outperforms traditional rule-based optimizers by up to 30% in terms of query execution time, while adapting seamlessly to changing database workloads.