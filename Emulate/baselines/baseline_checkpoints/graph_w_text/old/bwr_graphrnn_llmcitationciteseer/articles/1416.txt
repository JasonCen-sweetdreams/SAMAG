As Augmented Reality (AR) becomes increasingly pervasive, interaction modalities must adapt to accommodate diverse user needs. We present a novel gaze-based implicit interaction framework for AR interfaces, leveraging machine learning-driven gaze tracking and cognitive load estimation. Our approach enables users to seamlessly transition between explicit and implicit interaction modes, minimizing cognitive overhead and enhancing overall AR experience. We evaluate our system using a user study, demonstrating significant improvements in task completion time and user satisfaction compared to traditional explicit interaction methods.