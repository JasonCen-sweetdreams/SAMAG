This paper presents a novel approach to gesture recognition for individuals with disabilities, leveraging multimodal fusion of computer vision, speech, and electromyography (EMG) signals. We propose an adaptive framework, ' GesturePlus', that dynamically adjusts its recognition model based on the user's abilities and preferences. Our approach incorporates user feedback and personalized thresholds to enhance accuracy and usability. Experimental results with 20 participants demonstrate significant improvements in gesture recognition rates, particularly for users with motor impairments. GesturePlus has the potential to enable more inclusive and accessible human-computer interaction.