Emotion recognition from multi-modal data (e.g., speech, text, and vision) has gained significant attention in human-computer interaction. However, existing approaches often neglect the intricate relationships between modalities. We propose a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and features for emotion recognition. Our HAN model consists of two stages: modality-level attention and feature-level attention. Experimental results on two benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal data, especially in scenarios with noisy or missing modalities.