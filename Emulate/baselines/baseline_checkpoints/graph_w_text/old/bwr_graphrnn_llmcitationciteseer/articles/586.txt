Visual question answering (VQA) models often rely on complex neural architectures, making it challenging to understand their decision-making process. We propose a novel hierarchical attention network (HAN) that incorporates multi-level attention mechanisms to selectively focus on relevant image regions and question tokens. Our approach enables explainable VQA by providing visual and textual attention weights that justify the model's answers. Experimental results on the VQA-X dataset demonstrate that HAN achieves state-of-the-art performance while providing interpretable explanations for its predictions.