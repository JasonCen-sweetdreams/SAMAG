Deep learning models have achieved state-of-the-art performance in sentiment analysis tasks, but their lack of interpretability hinders their adoption in real-world applications. This paper proposes a novel hierarchical attention network (HAN) architecture that incorporates multi-modal inputs (text, images, and audio) to capture complex sentiment expressions. Our approach uses attention mechanisms to highlight relevant input features and generate visual explanations for predicted sentiments. Experimental results on a large-scale multi-modal dataset demonstrate the effectiveness of our HAN model in improving sentiment analysis accuracy and providing insights into the decision-making process.