This paper proposes a novel hierarchical contrastive learning framework for explainable visual reasoning tasks. Our approach, dubbed 'HCLR', leverages a multi-layered representation learning strategy to disentangle visual and semantic features, enabling the model to provide interpretable rationales for its predictions. We demonstrate the effectiveness of HCLR on various visual question answering and visual reasoning benchmarks, achieving state-of-the-art performance while providing insights into the decision-making process. Our method has significant implications for developing trustworthy AI systems that can provide transparent explanations for their decisions.