Deep neural networks (DNNs) are widely used for various machine learning tasks, but their training often relies on differentiable activation functions. This limitation hinders the exploration of non-differentiable activations that can provide better performance or interpretability. We propose a novel gradient-based optimization method, 'NGO', which can effectively train DNNs with non-differentiable activations by approximating the gradient using a probabilistic surrogate function. Experimental results on several benchmark datasets demonstrate the efficacy of NGO in training DNNs with non-differentiable activations, achieving competitive performance to state-of-the-art methods.