Multi-agent cooperation is a crucial aspect of many real-world applications, including autonomous driving, smart grids, and robotic teams. However, the lack of transparency in decision-making processes hinders the deployment of AI-driven agents in such scenarios. This paper proposes a novel hierarchical attention network (HAN) architecture that enables explainable multi-agent cooperation. Our approach learns to selectively focus on relevant agents and their interactions, generating interpretable attention weights that reflect the decision-making process. We evaluate our method on a suite of benchmark tasks, demonstrating improved cooperation outcomes and enhanced explainability compared to state-of-the-art methods.