Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the need for effective fusion of heterogeneous data. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features for emotion recognition. Our approach incorporates explainability techniques to provide insights into the decision-making process, enabling the identification of affective patterns and biases. Experimental results on the IEMOCAP dataset demonstrate the effectiveness of HAN in achieving state-of-the-art performance while offering transparent and interpretable emotion recognition.