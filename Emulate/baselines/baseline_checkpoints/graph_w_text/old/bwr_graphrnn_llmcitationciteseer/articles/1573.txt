Decentralized task allocation is a fundamental problem in multi-agent systems, where agents with limited communication and computation resources need to allocate tasks efficiently. This paper proposes a hierarchical reinforcement learning framework, 'HierMAL', which learns to allocate tasks in a decentralized manner. HierMAL uses a two-level hierarchy, where agents learn to form clusters and then allocate tasks within each cluster. We show that HierMAL outperforms existing decentralized task allocation algorithms in terms of task completion time and resource utilization. Experimental results on a simulated multi-agent system demonstrate the effectiveness of HierMAL in various scenarios.