Deep neural networks have achieved state-of-the-art performance in various applications, but their training often requires massive datasets and computational resources. Online learning provides a promising solution, but existing methods struggle to adapt to changing data distributions and often suffer from overfitting. This paper proposes a novel online learning framework, 'AdaReg', which incorporates adaptive regularization techniques into stochastic gradient descent. We introduce a meta-learning approach to dynamically adjust the regularization strength based on the complexity of the current mini-batch, leading to improved generalization and robustness. Experimental results on several benchmark datasets demonstrate the effectiveness of AdaReg in reducing overfitting and enhancing model performance.