Multimodal sentiment analysis (MSA) aims to identify sentiment polarity from multiple sources, including text, images, and audio. Existing MSA approaches often rely on early fusion, which can lead to modality mismatch and feature redundancy. This paper proposes a hierarchical attention network (HAN) that leverages late fusion to selectively attend to relevant modalities and features. Our HAN framework consists of modality-specific attention layers, a multimodal fusion module, and a sentiment prediction layer. Experimental results on the CMU-MOSI dataset demonstrate that our approach outperforms state-of-the-art methods, achieving an F1-score of 84.2% and a correlation coefficient of 0.83 with human annotations.