Sentiment analysis on multi-modal data, such as videos and images, is a challenging task due to the complexities of modeling interactions between modalities. This paper proposes a novel hierarchical attention network (HAN) that leverages both intra-modal and inter-modal attention mechanisms to capture fine-grained sentiment cues. Experimental results on the MM-IMDB dataset demonstrate that our HAN model outperforms state-of-the-art methods by 3.2% in terms of accuracy, achieving an F1-score of 84.6%. We also conduct ablation studies to analyze the effectiveness of each attention component.