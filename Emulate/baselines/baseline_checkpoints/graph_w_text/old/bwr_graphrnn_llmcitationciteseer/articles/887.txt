Effective human-robot collaboration requires seamless understanding and integration of multimodal cues from humans, robots, and the environment. We propose a hierarchical multimodal fusion framework, 'HMFusion', which leverages graph neural networks to model the complex relationships between vision, language, and proprioception modalities. Our experiments on a human-robot assembly task demonstrate that HMFusion significantly improves collaboration performance and generates interpretable explanations for robot decision-making. We also show that our approach can adapt to new environments and tasks with minimal fine-tuning.