Sentiment analysis has become a crucial task in natural language processing, with the proliferation of multi-modal data on social media and e-commerce platforms. This paper proposes a novel hierarchical attention network (HAN) architecture that effectively integrates visual and textual features for sentiment analysis. Our HAN model consists of modality-specific attention modules and a fusion layer that learns to weigh the importance of each modality. Experimental results on a large-scale, multi-modal sentiment analysis benchmark demonstrate the superiority of our approach over state-of-the-art methods, achieving a 3.5% improvement in accuracy.