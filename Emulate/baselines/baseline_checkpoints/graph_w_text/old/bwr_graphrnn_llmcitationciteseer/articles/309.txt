Multimodal sentiment analysis has gained significant attention in recent years, but it remains a challenging task due to the complexity of integrating heterogeneous data sources. This paper proposes a novel transfer learning approach that leverages graph convolutional networks (GCNs) to learn shared representations across modalities. Our method, 'MultimodalGCN', adaptively weights the importance of each modality and exploits the structural relationships between them. Experimental results on three benchmark datasets demonstrate that MultimodalGCN outperforms state-of-the-art methods, achieving improved sentiment analysis accuracy and reduced training time.