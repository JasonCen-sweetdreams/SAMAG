In multi-agent reinforcement learning, explaining the decision-making process of individual agents is crucial for building trust and understanding complex behaviors. This paper introduces HAN-MARL, a novel hierarchical attention network architecture that enables explainable policy learning in cooperative multi-agent environments. By incorporating attention mechanisms at both local and global levels, HAN-MARL disentangles the contributions of individual agents to the joint policy, thereby providing interpretable explanations for their actions. Experimental results on a variety of multi-agent tasks demonstrate the effectiveness of HAN-MARL in improving both task performance and explainability.