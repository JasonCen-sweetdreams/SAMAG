Emotion recognition is a crucial task in human-computer interaction, but existing approaches often rely on hand-crafted features or shallow fusion of multi-modal data. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages deep learning to jointly model facial expressions, speech, and text. Our HAN framework consists of a feature extractor, a hierarchical attention mechanism, and a multi-modal fusion layer. Experimental results on the IEMOCAP and SEMAINE datasets demonstrate that our approach achieves state-of-the-art performance in recognizing emotions from multi-modal inputs, while reducing computational complexity by 30% compared to existing methods.