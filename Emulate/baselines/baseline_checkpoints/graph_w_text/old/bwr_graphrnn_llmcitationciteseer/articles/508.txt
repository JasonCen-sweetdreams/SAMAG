Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a challenging task in AI. Existing approaches often suffer from high computational complexity and neglect the hierarchical structure of human emotions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that efficiently recognizes emotions from multi-modal inputs. HAN employs a hierarchical attention mechanism to selectively focus on relevant features across modalities and emotional levels. Experimental results on the IEMOCAP dataset demonstrate that HAN outperforms state-of-the-art methods in terms of accuracy and computational efficiency, making it suitable for real-time applications.