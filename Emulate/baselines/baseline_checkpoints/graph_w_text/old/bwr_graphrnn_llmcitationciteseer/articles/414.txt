Current gesture recognition systems in augmented reality (AR) are often biased towards able-bodied individuals, neglecting the needs of people with disabilities. This paper introduces 'IncluGesture', a novel approach that incorporates diversity in gesture recognition by leveraging a large-scale dataset of gestures from diverse populations. Our system utilizes a multi-modal fusion of computer vision, machine learning, and human-computer interaction techniques to recognize and adapt to various abilities and gesture patterns. Experimental results demonstrate that IncluGesture achieves significant improvement in recognition accuracy and user satisfaction for individuals with disabilities, paving the way for more inclusive AR experiences.