Multimodal sentiment analysis is a challenging task that requires effectively integrating and reasoning about heterogeneous data sources, such as text, images, and videos. This paper presents a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and regions within each modality. Our HAN model consists of a modality-agnostic feature extractor, a hierarchical attention mechanism, and a sentiment fusion layer. Experimental results on three benchmark datasets demonstrate that our approach outperforms state-of-the-art methods, achieving an average improvement of 4.5% in sentiment classification accuracy.