Conversational AI systems often struggle to adapt to new tasks or scenarios, requiring extensive retraining or fine-tuning. This paper proposes a meta-learning approach to task-oriented dialogue management, enabling conversational agents to adapt to new tasks with limited data and no additional training. Our method, MetaDia, leverages a task-agnostic dialogue policy and a meta-learner that learns to adapt to new tasks by exploiting the structural similarities between tasks. Experimental results demonstrate that MetaDia achieves significant improvements in dialogue success rates and adaptability compared to state-of-the-art baselines on a diverse range of tasks and datasets.