Multimodal sentiment analysis is a challenging task that requires integrating and interpreting heterogeneous data from text, images, and videos. This paper proposes a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features for sentiment prediction. We introduce a explainability module that generates visual attention maps and textual rationales to provide insights into the decision-making process. Experimental results on multimodal benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in terms of accuracy and interpretability.