Multimodal sentiment analysis in social media is crucial for understanding public opinions and emotions. However, existing approaches often neglect the complex relationships between visual and textual features. This paper proposes a novel Hierarchical Attention Network (HAN) that jointly learns to extract and fuse multimodal features from images and text. Our approach leverages a hierarchical attention mechanism to capture local and global contextual dependencies, enabling the model to focus on sentiment-bearing regions in images and relevant text snippets. Experimental results on a large-scale social media dataset demonstrate that HAN outperforms state-of-the-art methods in terms of sentiment classification accuracy and aspect-based sentiment analysis.