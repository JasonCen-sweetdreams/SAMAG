Sentiment analysis is a crucial task in natural language processing, but most existing approaches struggle to incorporate multi-modal inputs (e.g., text, images, and videos) and provide interpretable results. This paper proposes a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities, features, and instances for sentiment prediction. We introduce a novel attention mechanism that adapts to the input modality and demonstrate its effectiveness on three benchmark datasets. Experimental results show that our approach outperforms state-of-the-art models while providing transparent and trustworthy explanations for its predictions.