Multimodal sentiment analysis (MSA) involves analyzing sentiment from multiple modalities such as text, images, and audio. Existing MSA approaches often struggle to effectively fuse modalities, leading to suboptimal performance. We propose a novel Hierarchical Attention Network (HAN) that adaptively learns to weigh and combine modalities based on their relevance to the sentiment task. Our HAN consists of three stages: modality-specific attention, inter-modality attention, and sentiment prediction. Experiments on three benchmark MSA datasets demonstrate that our approach outperforms state-of-the-art methods, achieving significant improvements in sentiment accuracy and F1-score.