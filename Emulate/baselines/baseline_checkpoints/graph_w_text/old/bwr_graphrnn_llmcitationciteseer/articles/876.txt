Emotion recognition from multi-modal inputs (e.g., text, audio, vision) is crucial for human-computer interaction. However, current models often lack interpretability, hindering trust and reliability. We propose a Hierarchical Attention Network (HAN) that integrates attention mechanisms across modalities and within modalities to learn meaningful representations. Our approach enables emotion attribution to specific input features and modalities, providing explanations for predicted emotions. Experimental results on the CMU-MOSEI dataset demonstrate that HAN outperforms state-of-the-art models in emotion recognition while offering insightful explanations.