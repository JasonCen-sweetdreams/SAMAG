Emotion recognition in human-computer interaction (HCI) has seen significant advancements with the rise of AI-powered systems. However, most existing approaches focus on a single modality, neglecting the richness of multi-modal cues. We propose a novel hierarchical attention network (HAN) that integrates facial expressions, speech, and physiological signals to recognize emotions in HCI. Our HAN model learns to selectively focus on relevant modalities and temporal segments, achieving state-of-the-art performance on the EMOTIC dataset. We also demonstrate the effectiveness of our approach in a real-world HCI scenario, enabling more empathetic and personalized human-computer interactions.