Recognizing emotions from multi-modal inputs (e.g., speech, text, vision) is crucial for human-computer interaction. Existing approaches often suffer from high computational complexity and poor performance on imbalanced datasets. This paper proposes a novel hierarchical attention network (HAN) that leverages both intra- and inter-modal relationships to improve emotion recognition. HAN employs a modular architecture, where each module focuses on a specific modality, and a hierarchical attention mechanism to selectively emphasize relevant features. Experiments on the CMU-MOSEI dataset demonstrate that HAN outperforms state-of-the-art methods in terms of accuracy and computational efficiency, especially on minority classes.