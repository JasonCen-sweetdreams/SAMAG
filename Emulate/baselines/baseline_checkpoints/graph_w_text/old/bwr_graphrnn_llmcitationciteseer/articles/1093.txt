Multi-agent cooperation is crucial in many real-world applications, but the lack of interpretability in current AI models hinders trust and understanding. We propose a hierarchical attention network (HAN) framework that enables explainable cooperation among agents. Our approach combines graph attention mechanisms with hierarchical reasoning to learn task-specific agent interactions. We evaluate HAN on a simulated search-and-rescue scenario, demonstrating improved cooperation and interpretability compared to state-of-the-art methods. Our results show that HAN can facilitate more effective human-AI collaboration and decision-making.