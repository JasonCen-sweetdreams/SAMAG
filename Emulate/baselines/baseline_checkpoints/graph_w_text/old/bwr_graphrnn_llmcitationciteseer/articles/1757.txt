Emotion recognition from multi-modal inputs (e.g., speech, text, vision) remains a challenging task due to the need to model complex relationships between modalities. We propose a novel Hierarchical Attention Network (HAN) that efficiently integrates modalities by learning attention weights at both local and global levels. Our approach outperforms state-of-the-art methods on the CMU-MOSEI dataset, achieving an average F1-score improvement of 7.2% across six emotion categories. We also demonstrate the effectiveness of HAN in real-world applications, including human-computer interaction and affective computing.