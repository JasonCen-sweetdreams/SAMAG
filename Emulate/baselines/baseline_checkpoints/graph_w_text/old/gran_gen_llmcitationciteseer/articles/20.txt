Deep learning models have achieved state-of-the-art performance in multi-modal emotion recognition tasks, but their lack of transparency hinders trust and adoption in real-world applications. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates visual, audio, and text features to recognize emotions in a more interpretable manner. Our approach employs attention mechanisms to highlight salient features and modalities, enabling the generation of explainable emotion recognition outputs. Experimental results on the CMU-MOSEI dataset demonstrate that our HAN model outperforms existing methods in terms of accuracy and provides insights into the decision-making process.