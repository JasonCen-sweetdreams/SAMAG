Recognizing emotions from multi-modal inputs such as speech, text, and facial expressions is a challenging task due to the complexity and variability of human emotions. This paper proposes a hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features to improve emotion recognition accuracy. Our HAN model consists of three stages: modality-specific attention, feature-level attention, and decision-level attention. Experimental results on the IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art methods by 12.5% in terms of weighted F1-score, achieving an accuracy of 85.2%. We also provide insights into the attention weights, revealing that our model is capable of capturing subtle emotional cues from different modalities.