Multimodal sentiment analysis, which involves analyzing sentiment from multiple sources such as text, images, and audio, is a challenging task in natural language processing. This paper proposes a novel hierarchical attention network (HAN) architecture that learns to selectively focus on relevant modalities and features to improve sentiment prediction. Our HAN model consists of three levels of attention: modality-level, feature-level, and token-level attention. We evaluate our model on three benchmark multimodal datasets and demonstrate significant improvements over state-of-the-art methods. Experimental results show that our approach is robust to noisy or missing modalities and can generalize well to unseen datasets.