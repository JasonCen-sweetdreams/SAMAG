Conversational search systems require effective re-ranking models to retrieve relevant documents in response to user queries. This paper presents LTR-Reranking, a neural re-ranking model that leverages query-document interaction features and contextual information from conversation history. Our approach combines a transformer-based encoder with a novel relevance-based attention mechanism, enabling the model to capture nuanced relationships between queries and documents. Experimental results on the TREC Conversational Assistance Track dataset demonstrate that LTR-Reranking outperforms state-of-the-art re-ranking models, achieving significant improvements in mean average precision and normalized discounted cumulative gain.