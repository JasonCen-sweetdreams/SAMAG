Multimodal sentiment analysis has gained significant attention in recent years, but most existing approaches lack transparency and interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates visual, textual, and acoustic features to analyze sentiment in multimodal data. Our model uses attention mechanisms to selectively focus on relevant modalities and features, providing insights into the decision-making process. Experimental results on a large-scale multimodal dataset demonstrate that our approach outperforms state-of-the-art methods while offering improved explainability and robustness to noisy data.