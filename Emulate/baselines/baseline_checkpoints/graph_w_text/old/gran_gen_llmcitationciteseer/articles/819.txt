Explainable AI is crucial for real-world multi-agent dialogue systems, where humans interact with agents to accomplish complex tasks. This paper proposes a Hierarchical Attention Network (HAN) framework that provides interpretable explanations for agent responses. Our HAN model incorporates attention mechanisms at both the utterance and dialogue levels, enabling the identification of key utterances and dialogue contexts that influence agent decisions. Experimental results on a human-agent dialogue dataset demonstrate that HAN outperforms state-of-the-art methods in terms of response accuracy and explanation quality.