Time series forecasting models often struggle to provide interpretable insights into their predictions. This paper introduces a novel hierarchical attention network (HAN) architecture that addresses this issue by learns to focus on relevant temporal and feature-wise dependencies. Our proposed approach, called HATSF, leverages a two-level attention mechanism to identify informative subsequences and features, and then combines them to generate accurate forecasts. We evaluate HATSF on several benchmark datasets and demonstrate its superiority in terms of forecasting performance and interpretability compared to state-of-the-art methods.