Emotion recognition in human-computer interaction (HCI) is a challenging task due to the complexity of human emotional responses. This paper proposes a novel hierarchical attention network (HAN) for multi-modal emotion recognition, which leverages both facial expressions and speech patterns. Our HAN model consists of two stages: the first stage uses convolutional neural networks (CNNs) to extract features from facial images and speech spectrograms, while the second stage employs a hierarchical attention mechanism to selectively focus on relevant features from each modality. Experimental results on a large-scale HCI dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs.