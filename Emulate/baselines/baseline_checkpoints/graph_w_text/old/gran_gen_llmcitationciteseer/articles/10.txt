Multi-modal sentiment analysis (MMSA) tasks require effective fusion of textual, visual, and acoustic cues to capture nuanced sentiment expressions. This paper proposes a novel hierarchical attention network (HAN) architecture that adaptively weighs and integrates multi-modal features. Our HAN model consists of three stages: (1) intra-modal attention for feature refinement, (2) inter-modal attention for modality fusion, and (3) sentiment prediction with a hierarchical graph neural network. Experiments on three benchmark datasets demonstrate that our approach outperforms state-of-the-art MMSA methods, achieving an average improvement of 4.5% in sentiment accuracy.