Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) remains a challenging task due to the complexity and variability of human emotions. We propose a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features to recognize emotions. Our approach leverages attention mechanisms at multiple levels, including modality-level, feature-level, and sample-level, to provide interpretable and accurate emotion recognition. Experimental results on two benchmark datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance on emotion recognition tasks while providing insights into the decision-making process.