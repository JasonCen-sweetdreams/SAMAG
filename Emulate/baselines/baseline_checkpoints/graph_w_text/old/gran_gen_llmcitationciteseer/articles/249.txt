Explainable AI (XAI) has emerged as a crucial aspect of trustworthy AI development. However, existing XAI methods often rely on manual annotation, which is time-consuming and expensive. This paper presents 'HILA', a novel active learning framework that leverages human-in-the-loop feedback to select the most informative instances for labeling. HILA incorporates a Bayesian uncertainty model to quantify annotation uncertainty and adaptively adjust the labeling budget. Experimental results on three real-world datasets demonstrate that HILA reduces labeling effort by up to 50% while maintaining comparable XAI performance to fully-supervised methods.