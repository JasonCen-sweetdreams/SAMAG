Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task, especially when requiring explainability. We propose a hierarchical attention network (HAN) that leverages attention mechanisms to selectively focus on relevant modalities and features. Our approach integrates a novel multi-modal fusion layer, which adaptively weights the contributions of each modality based on their relative importance. Experimental results on the IEMOCAP dataset demonstrate the effectiveness of HAN in achieving state-of-the-art performance while providing interpretable results. We also conduct ablation studies to quantify the impact of individual modalities on emotion recognition.