Deep neural networks (DNNs) are vulnerable to adversarial attacks, which can compromise their performance and reliability. This paper proposes a novel Bayesian neural architecture search (BNAS) method to improve the robustness of DNNs against adversarial attacks. Our approach involves searching for an optimal architecture that minimizes the expected loss under adversarial perturbations. We develop a Bayesian optimization framework that leverages a probabilistic surrogate model to guide the search process. Experimental results on multiple benchmark datasets demonstrate that our method can significantly improve the adversarial robustness of DNNs while maintaining their accuracy on clean data.