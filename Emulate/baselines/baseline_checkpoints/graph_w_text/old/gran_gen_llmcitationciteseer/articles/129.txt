Multimodal sentiment analysis, which involves analyzing user opinions from text, images, and videos, is a crucial task in modern AI systems. However, existing approaches often rely on costly fusion techniques or neglect the hierarchical structure of multimodal data. We propose a novel Hierarchical Attention Network (HAN) that leverages the complementary strengths of different modalities. Our HAN model uses a stacked attention mechanism to selectively focus on relevant regions of the input data, reducing computational overhead while improving sentiment prediction accuracy. Experimental results on three benchmark datasets demonstrate the effectiveness of our approach in achieving state-of-the-art performance with reduced computational resources.