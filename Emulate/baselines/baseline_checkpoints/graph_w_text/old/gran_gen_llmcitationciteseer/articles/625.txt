Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel Hierarchical Attention Network (HAN) that integrates multi-modal features using a hierarchical attention mechanism. Our HAN model learns to selectively focus on relevant modalities and features, achieving state-of-the-art performance on the CMU-MOSEI dataset. We also demonstrate the effectiveness of our approach in real-world applications, such as affective human-robot interaction and mental health monitoring.