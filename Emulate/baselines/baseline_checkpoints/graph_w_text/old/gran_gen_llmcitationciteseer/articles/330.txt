Emotion recognition is crucial for effective human-robot interaction. However, existing approaches often rely on single modalities, neglecting the richness of multi-modal cues. This paper proposes a hierarchical attention network (HAN) that integrates facial expressions, speech, and physiological signals to recognize emotions in human-robot interactions. Our HAN model learns to selectively focus on relevant modalities and temporal segments, achieving state-of-the-art performance on the EMOTIC dataset. We also conduct a user study, demonstrating that robots equipped with our HAN-based emotion recognition system exhibit more empathetic and engaging behavior.