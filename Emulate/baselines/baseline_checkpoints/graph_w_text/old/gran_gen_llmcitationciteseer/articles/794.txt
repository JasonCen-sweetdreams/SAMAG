Multimodal sentiment analysis (MSA) involves analyzing sentiment from heterogeneous data sources, such as text, images, and audio. Existing MSA models often struggle to capture complex interactions between modalities. This paper proposes a novel hierarchical attention network (HAN) architecture, which leverages both intra-modality and inter-modality attention mechanisms to selectively focus on relevant features. Experimental results on three benchmark datasets demonstrate that our HAN model outperforms state-of-the-art MSA approaches, achieving significant improvements in sentiment classification accuracy and F1-score.