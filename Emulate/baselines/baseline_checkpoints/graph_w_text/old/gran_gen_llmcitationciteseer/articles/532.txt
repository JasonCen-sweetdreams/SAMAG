Visual Question Answering (VQA) models have achieved remarkable success, but their vulnerability to adversarial attacks remains a significant concern. We propose a novel graph-based approach to generate targeted attacks on VQA models. Our method, GraphFool, leverages the structural relationships between visual and linguistic features to craft perturbations that mislead the model. Experimental results on the VQA-CP v2 dataset demonstrate that GraphFool outperforms existing attack methods in terms of attack success rate and query efficiency. We also explore the transferability of GraphFool attacks across different VQA models, revealing insights into their robustness.