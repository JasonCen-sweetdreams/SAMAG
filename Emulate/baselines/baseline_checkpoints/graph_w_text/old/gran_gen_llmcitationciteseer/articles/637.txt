Deep neural networks have been shown to be vulnerable to adversarial attacks, which can manipulate the output of the model. In this paper, we propose a Bayesian neural network (BNN) framework to analyze the robustness of deep neural networks against adversarial attacks. Our approach models the uncertainty of the network's weights and uses Bayesian inference to estimate the posterior distribution of the network's output. We evaluate our approach on several benchmark datasets and demonstrate its effectiveness in detecting and mitigating adversarial attacks. The results show that our approach can improve the robustness of deep neural networks against adversarial attacks while providing a uncertainty estimate of the model's output.