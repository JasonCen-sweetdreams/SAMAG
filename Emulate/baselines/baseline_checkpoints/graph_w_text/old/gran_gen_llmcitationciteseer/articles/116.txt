In multi-agent reinforcement learning, agents learn to make decisions by interacting with their environment and each other. However, the lack of transparency in their decision-making processes hinders trust and understanding. This paper proposes a hierarchical attention network (HAN) that enables explainable multi-agent reinforcement learning. Our HAN architecture consists of two attention mechanisms: an inter-agent attention module that captures the relationships between agents, and an intra-agent attention module that highlights the most relevant state features. Experimental results on a cooperative navigation task demonstrate that our approach improves both the agents' performance and the interpretability of their decisions.