Emotion recognition from multi-modal data (e.g., facial expressions, speech, and text) is a challenging task due to the inherent complexity and variability of human emotions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages the strengths of both convolutional and recurrent neural networks to capture spatial and temporal dependencies in multi-modal data. Our approach achieves state-of-the-art performance on the CMU-MOSEI dataset and provides interpretable visualizations of the attention weights, enabling explainability and transparency in emotion recognition. We also demonstrate the effectiveness of our approach in real-world applications, such as affective human-computer interaction and mental health analysis.