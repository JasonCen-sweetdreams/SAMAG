Deep neural networks (DNNs) have achieved state-of-the-art performance in various applications, but their computational complexity and memory requirements hinder deployment on resource-constrained devices. Network pruning is a viable solution, but existing methods often rely on heuristics or simplistic optimization objectives. This paper proposes a novel multi-objective evolutionary optimization framework, 'MOEP', which simultaneously minimizes DNN model size, computational cost, and error rate. MOEP leverages surrogate-assisted evolution and adaptive population sizing to efficiently explore the vast pruning search space. Experimental results on popular image classification benchmarks demonstrate that MOEP prunes DNNs to achieve significant computational savings while preserving accuracy.