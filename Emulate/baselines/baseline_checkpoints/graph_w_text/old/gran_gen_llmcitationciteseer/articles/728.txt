Explainability is crucial in autonomous systems to ensure trust and safety. This paper proposes a hierarchical attention-based reinforcement learning framework, 'HARL', that provides interpretable decision-making processes for autonomous agents. HARL incorporates a novel attention mechanism that selectively focuses on relevant state features and abstracts away irrelevant information, enabling the agent to learn more efficient and explainable policies. We evaluate HARL on a series of autonomous driving scenarios and demonstrate improved explainability and performance compared to state-of-the-art reinforcement learning methods.