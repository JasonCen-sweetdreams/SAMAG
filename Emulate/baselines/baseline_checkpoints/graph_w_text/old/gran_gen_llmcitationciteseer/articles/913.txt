Multi-task learning (MTL) has been widely adopted in various applications, but existing methods often overlook the inherent uncertainty in model predictions. This paper proposes a Bayesian neural network (BNN) framework, 'UncerMTL', which jointly learns multiple tasks while quantifying epistemic uncertainty. We develop a novel hierarchical prior that captures task relationships and adaptively adjusts the uncertainty estimates. Experimental results on the Cityscapes and NYT datasets demonstrate that UncerMTL outperforms state-of-the-art MTL methods in terms of accuracy and uncertainty calibration.