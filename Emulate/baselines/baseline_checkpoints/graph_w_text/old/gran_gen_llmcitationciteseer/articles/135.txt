Sentiment analysis in social media often involves analyzing multimodal data, including text, images, and videos. Traditional approaches rely on task-specific models, which can be computationally expensive and require large datasets. This paper proposes a deep transfer learning framework that leverages pre-trained models to adapt to new sentiment analysis tasks with limited labeled data. We introduce a novel attention mechanism that selectively weights modalities based on their relevance to the target task, leading to improved performance and robustness. Experimental results on several benchmark datasets demonstrate the effectiveness of our approach in multimodal sentiment analysis.