Multimodal sentiment analysis has gained significant attention in recent years, but existing approaches often suffer from a lack of transparency and interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) that leverages the strengths of both vision and language models to analyze sentiment in multimodal data. Our approach uses a hierarchical attention mechanism to selectively focus on relevant regions of the input data, enabling the model to provide explicit explanations for its predictions. Experimental results on a large-scale multimodal dataset demonstrate the effectiveness of HAN in achieving state-of-the-art performance while providing insightful visualizations and textual explanations.