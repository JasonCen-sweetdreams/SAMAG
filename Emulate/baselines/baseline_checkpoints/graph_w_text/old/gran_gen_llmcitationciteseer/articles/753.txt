Emotion recognition from multi-modal inputs, such as speech, text, and vision, remains a challenging task due to the complexity of human emotions and the variability of modalities. This paper introduces Hierarchical Attention Networks (HANs), a novel deep learning architecture that leverages attention mechanisms at multiple levels to selectively focus on salient features from each modality. We evaluate HANs on three benchmark datasets and demonstrate significant improvements in emotion recognition accuracy compared to state-of-the-art methods. Furthermore, we visualize the attention weights to provide insights into the decision-making process, facilitating explainability and trust in AI-driven emotion recognition systems.