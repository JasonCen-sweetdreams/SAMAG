Multimodal sentiment analysis (MSA) involves analyzing sentiment from heterogeneous data sources, such as text, images, and audio. This paper proposes a novel hierarchical attention network (HAN) for MSA, which learns to selectively focus on relevant modalities and features. Our HAN model consists of three attention layers: modality-level attention, feature-level attention, and fusion-level attention. Experimental results on three benchmark datasets demonstrate that our approach outperforms state-of-the-art methods, achieving an average improvement of 4.2% in sentiment accuracy. We also provide insights into the learned attention patterns, revealing interesting correlations between modalities and sentiment.