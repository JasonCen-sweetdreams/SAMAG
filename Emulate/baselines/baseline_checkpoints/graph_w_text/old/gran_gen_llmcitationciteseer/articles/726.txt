Explainability is a crucial aspect of multi-agent reinforcement learning, as it enables humans to understand and trust the decision-making process of autonomous agents. This paper presents a novel hierarchical attention network (HAN) architecture that integrates attention mechanisms at both the agent and team levels. Our approach enables the identification of key factors influencing the agents' decisions and facilitates the interpretation of their behavior. Experimental results on a real-world robotics dataset demonstrate that HAN outperforms state-of-the-art methods in both task performance and explainability, paving the way for more transparent and reliable multi-agent systems.