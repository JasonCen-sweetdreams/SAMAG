This paper presents a novel neural ranking model for document retrieval, leveraging pre-trained language representations to improve the efficiency and effectiveness of search engines. Our proposed approach, called 'NeuRank', incorporates a lightweight neural architecture that fine-tunes pre-trained language models on a large-scale dataset of relevance judgments. We demonstrate that NeuRank significantly outperforms traditional ranking models in terms of retrieval accuracy, while reducing the computational cost of training and inference. Experimental results on several benchmark datasets show that NeuRank achieves state-of-the-art performance, making it a promising solution for large-scale document retrieval applications.