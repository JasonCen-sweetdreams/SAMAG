Neural retrieval models have revolutionized information retrieval by leveraging dense vector representations. However, they often struggle with query formulation and expansion. This paper proposes a novel contrastive learning-based approach, 'ConExp', which learns to generate effective query expansions from a set of relevant and non-relevant documents. Our method outperforms traditional expansion techniques and improves the retrieval performance of several neural models on multiple benchmark datasets. We also analyze the impact of different contrastive objectives on query expansion and provide insights for future research.