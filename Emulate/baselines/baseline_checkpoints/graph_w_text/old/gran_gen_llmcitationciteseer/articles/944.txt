Emotion recognition from multi-modal inputs, such as speech, text, and vision, is a challenging task in affective computing. This paper proposes a novel hierarchical attention network (HAN) that effectively fuses and selectively focuses on relevant features from different modalities. Our HAN model consists of two stages: intra-modal attention, which captures local patterns within each modality, and inter-modal attention, which integrates information across modalities. Experimental results on the CMU-MultiModal dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an average F1-score improvement of 8.2%.