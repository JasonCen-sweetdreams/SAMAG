Sentiment analysis has become a crucial task in natural language processing, but existing approaches struggle to effectively integrate multiple modalities of user-generated content. This paper proposes a novel hierarchical attention network (HAN) architecture that jointly learns to attend to relevant linguistic, visual, and acoustic features in multi-modal data. Our HAN model leverages a meta-learning framework to adapt to diverse datasets and outperforms state-of-the-art methods in sentiment classification tasks on three benchmark datasets. Experiments demonstrate that our approach achieves improved robustness to noisy or missing modalities, enabling more accurate sentiment analysis in real-world applications.