Ad-hoc retrieval tasks often suffer from vocabulary mismatch between the user's query and the relevant documents. Query expansion techniques can mitigate this issue, but previous methods rely on hand-crafted rules or supervised learning. We propose a novel reinforcement learning framework, 'RL-QE', which learns to expand queries by interacting with a reward function that simulates user preferences. Our approach outperforms state-of-the-art methods on several benchmarks, achieving significant improvements in precision and recall. We also demonstrate the robustness of RL-QE to varying query types and domain shifts.