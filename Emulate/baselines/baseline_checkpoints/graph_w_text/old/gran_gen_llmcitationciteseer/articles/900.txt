Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a crucial task in human-computer interaction. However, existing models often suffer from high computational complexity and limited interpretability. This paper proposes a novel hierarchical attention network (HAN) that efficiently integrates modalities and captures complex emotional patterns. Our HAN model utilizes a hierarchical fusion strategy, where intra-modal attention mechanisms are applied before inter-modal fusion, reducing the dimensionality and improving interpretability. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods in terms of both accuracy and computational efficiency.