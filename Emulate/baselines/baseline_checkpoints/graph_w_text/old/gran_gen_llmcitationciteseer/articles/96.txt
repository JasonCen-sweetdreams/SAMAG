Graph neural networks (GNNs) have shown remarkable success in node classification tasks, but their computational complexity can be prohibitive for large graphs. We propose a novel hierarchical graph attention network (H-GAT) that leverages a multi-resolution graph representation to reduce the number of attention heads and improve computational efficiency. Our approach consists of a hierarchical clustering step that groups nodes into clusters, followed by attention-based feature aggregation within and across clusters. Experiments on several benchmark datasets demonstrate that H-GAT achieves state-of-the-art performance while reducing computational cost by up to 50% compared to existing GNN models.