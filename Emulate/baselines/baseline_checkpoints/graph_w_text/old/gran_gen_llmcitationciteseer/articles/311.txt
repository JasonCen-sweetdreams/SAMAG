Distributed database systems have become increasingly prevalent in modern data centers, but optimizing query execution remains a significant challenge. This paper proposes a novel approach to query optimization using reinforcement learning. Our method, 'RL-QO', leverages a Markov decision process to model the query optimization problem and learns an optimal policy through trial and error. We evaluate RL-QO on a real-world distributed database system and demonstrate significant improvements in query execution time and resource utilization compared to traditional optimization techniques.