Voice-based assistants have become ubiquitous, but user frustration remains a significant concern. This paper proposes a multimodal fusion approach to detecting and understanding user frustration in voice-based interactions. We combine acoustic features from speech, linguistic cues from transcripts, and visual features from facial expressions to develop a robust frustration detection model. Our approach outperforms existing unimodal and multimodal fusion methods, achieving an F1-score of 0.92 on a novel dataset of 100 users. We further analyze the relationship between frustration and task complexity, providing insights for designing more empathetic voice-based assistants.