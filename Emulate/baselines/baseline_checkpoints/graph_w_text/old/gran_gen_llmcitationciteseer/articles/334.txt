Emotion recognition from multimodal inputs (e.g., speech, text, vision) is crucial for human-computer interaction. This paper presents a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and features for emotion recognition. Our HAN model consists of three stages: modality-specific attention, feature-level attention, and hierarchical fusion. We evaluate our approach on the Multimodal Emotion Recognition Challenge (MERC) dataset and achieve state-of-the-art performance, outperforming existing multimodal fusion methods by 12.5% in terms of emotion recognition accuracy.