Dialogue state tracking (DST) is a crucial component of task-oriented conversational AI systems. However, existing approaches often struggle with scalability and accuracy, particularly in complex, multi-turn dialogues. This paper proposes a novel hierarchical attention network (HAN) architecture for efficient DST. Our HAN model leverages both local and global attention mechanisms to selectively focus on relevant context and dialogue history, reducing computational costs while improving tracking accuracy. Experimental results on the MultiWOZ 2.1 dataset demonstrate that our approach outperforms state-of-the-art DST models, achieving a 12.5% relative improvement in joint goal accuracy.