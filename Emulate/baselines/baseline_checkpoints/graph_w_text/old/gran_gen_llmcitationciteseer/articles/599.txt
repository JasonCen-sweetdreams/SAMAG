Visual question answering (VQA) systems often struggle to provide interpretable results, limiting their real-world applicability. We propose a novel multi-modal fusion framework, 'ExplainVQA', which leverages attention-based graph neural networks to integrate visual, linguistic, and semantic features. Our approach generates explicit rationales for VQA predictions, enabling users to understand the decision-making process. Experimental results on the VQA-X dataset demonstrate that ExplainVQA outperforms state-of-the-art models in both accuracy and explainability, while reducing computational overhead.