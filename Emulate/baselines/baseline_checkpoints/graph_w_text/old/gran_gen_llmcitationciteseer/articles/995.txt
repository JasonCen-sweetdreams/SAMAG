Deep neural networks (DNNs) are vulnerable to adversarial attacks, which can lead to misclassification. Existing defense mechanisms often rely on empirical evaluations, lacking a theoretical understanding of DNN robustness. This paper proposes a novel framework for analyzing the robustness of DNNs against adversarial attacks using orthogonal projections. We derive a closed-form expression for the adversarial perturbation bound and establish a connection between the Lipschitz constant and the orthogonal projection operator. Our experiments on the CIFAR-10 dataset demonstrate that our approach can effectively detect and mitigate adversarial attacks, outperforming state-of-the-art defense methods.