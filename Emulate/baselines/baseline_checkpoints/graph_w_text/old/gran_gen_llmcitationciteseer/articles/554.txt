Sentiment analysis in multimodal data (e.g., text, images, and videos) is a challenging task due to the complexity of integrating and aligning features from different modalities. This paper proposes a novel hierarchical attention network (HAN) that leverages the strengths of both modalities to improve sentiment prediction. Our HAN model consists of two attention mechanisms: intra-modal attention to capture local patterns within each modality and inter-modal attention to align features across modalities. We evaluate our approach on two multimodal benchmarks and demonstrate significant improvements over state-of-the-art methods.