Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a challenging task, particularly when interpretability is desired. This paper presents a hierarchical attention network (HAN) that leverages the strengths of each modality to recognize emotions while providing explanations for its predictions. Our HAN model uses modality-specific attention mechanisms to identify salient features and subsequently combines them using a hierarchical fusion mechanism. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods in both recognition accuracy and explainability, providing insights into the emotional cues used by the model.