Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the heterogeneity of modalities. This paper proposes a novel Hierarchical Attention Neural Network (HANN) architecture that selectively focuses on relevant modalities and features to improve recognition accuracy. Our HANN model consists of a modality attention module and a feature attention module, which learn to weigh the importance of each modality and feature, respectively. Experimental results on the CMU-MultiModal dataset demonstrate that our approach outperforms state-of-the-art methods, achieving an average F1-score of 0.92 across six emotions.