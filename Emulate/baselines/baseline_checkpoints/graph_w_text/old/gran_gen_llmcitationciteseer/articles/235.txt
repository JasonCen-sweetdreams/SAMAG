Transformer-based models have achieved state-of-the-art performance in question answering tasks, but their lack of transparency and interpretability hinders their adoption in high-stakes applications. We propose a novel method, 'TransExplainer', which generates faithful and relevant explanations for transformer-based question answering models. Our approach leverages attention weights to identify salient input features and generate natural language explanations. We evaluate TransExplainer on several benchmark datasets and demonstrate its effectiveness in improving model interpretability without sacrificing accuracy.