Emotion recognition from multi-modal data (e.g., text, audio, vision) is a challenging task due to the complexity of modal interactions. This paper presents a novel Hierarchical Attention Network (HAN) architecture that captures both intra-modal and inter-modal dependencies. Our HAN model employs a hierarchical attention mechanism to selectively focus on relevant modalities and features, leading to improved emotion recognition performance. Experiments on the CMU-MOSI and IEMOCAP datasets demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal data, while reducing computational costs.