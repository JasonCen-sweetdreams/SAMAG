Graph Neural Networks (GNNs) have shown remarkable performance in node classification tasks. However, their robustness to adversarial attacks remains largely unexplored. This paper investigates the vulnerability of GNNs to structural perturbations and proposes novel defense strategies. We introduce a robustness metric, 'Graph Lipschitzness', to quantify the model's sensitivity to graph modifications. Experimental results on benchmark datasets demonstrate that our adversarial training method, 'GraphShield', significantly improves the robustness of GNNs against targeted attacks while maintaining their classification accuracy.