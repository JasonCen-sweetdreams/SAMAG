Multimodal emotion recognition (MER) faces challenges in capturing complex relationships between audio, video, and text inputs. This paper proposes a novel hierarchical temporal attention (HTA) framework for MER, which leverages both local and global contextual information. HTA comprises a cascaded encoder-decoder architecture with modality-specific attention weights, enabling the model to hierarchically focus on salient features and temporal dependencies. Experimental results on the benchmark IEMOCAP dataset demonstrate that HTA outperforms state-of-the-art MER models, achieving a 12.3% improvement in weighted F1-score and 15.5% in emotion recognition accuracy.