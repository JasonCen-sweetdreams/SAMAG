Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task, particularly when it comes to interpreting model decisions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that not only improves emotion recognition accuracy but also provides explainable results. Our HAN model employs attention mechanisms at multiple levels to selectively focus on relevant input features, modalities, and temporal segments. We evaluate our approach on three benchmark datasets and demonstrate significant improvements over state-of-the-art methods. Furthermore, we provide visualizations and feature importance analysis to illustrate the explainability of our model.