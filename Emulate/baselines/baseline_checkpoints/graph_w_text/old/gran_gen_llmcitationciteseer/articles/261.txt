Real-time analytics in distributed database systems requires efficient query optimization to minimize latency and maximize throughput. This paper presents a novel query optimization framework, 'DynaOpt', which leverages machine learning and graph-based models to predict query execution plans. DynaOpt incorporates a dynamic cost model that adapts to changing system workloads and data distributions, outperforming traditional rule-based and cost-based optimizers. Experimental results on a large-scale distributed database show that DynaOpt reduces query latency by up to 50% and improves system throughput by up to 30%.