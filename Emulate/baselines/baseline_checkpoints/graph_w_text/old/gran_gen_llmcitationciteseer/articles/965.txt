Time-series forecasting models often lack transparency, making it challenging to understand their decisions. This paper proposes a Hierarchical Attention Network (HAN) architecture that incorporates attention mechanisms at multiple scales to identify relevant patterns and features in time-series data. We introduce a novel explainability module that generates feature importance scores, enabling the identification of influential factors driving the forecasting decisions. Experimental results on several benchmark datasets demonstrate that HAN outperforms state-of-the-art models in terms of forecasting accuracy while providing interpretable insights into the forecasting process.