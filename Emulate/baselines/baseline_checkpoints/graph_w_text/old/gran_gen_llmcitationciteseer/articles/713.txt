As data continues to grow in size and complexity, distributed database systems (DDS) have become increasingly prevalent. However, query optimization remains a significant challenge in DDS due to the inherent complexity of data distribution and node heterogeneity. This paper proposes a novel approach to query optimization using machine learning (ML) techniques. We design a ML-based cost model that predicts the execution time of queries and identifies the most efficient query plans. Experimental results on a real-world DDS show that our approach outperforms traditional query optimization methods, achieving an average speedup of 2.5x and reducing query latency by up to 40%.