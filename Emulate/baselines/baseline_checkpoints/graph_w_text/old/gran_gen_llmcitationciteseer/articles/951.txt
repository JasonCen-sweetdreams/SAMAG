Deep reinforcement learning (DRL) has achieved remarkable success in various applications, but its lack of transparency and interpretability hinders its adoption in high-stakes domains. This paper proposes a novel model-agnostic approach to explain DRL decisions using saliency maps, which highlight the most relevant input features contributing to the agent's actions. Our method, 'SaliencyRL', leverages gradient-based feature importance measures and attention mechanisms to generate visually informative and quantitatively accurate explanations. Experimental results on Atari games and a real-world robotics task demonstrate the effectiveness of SaliencyRL in enhancing DRL transparency and facilitating human understanding of agent behavior.