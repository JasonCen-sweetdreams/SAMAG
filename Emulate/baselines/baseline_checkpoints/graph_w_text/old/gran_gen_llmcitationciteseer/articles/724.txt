As multi-agent reinforcement learning (MARL) systems become increasingly complex, explaining their decision-making processes is crucial for trust and accountability. We propose Hierarchical Attention Networks (HANs) to enable explainable MARL by modeling agent interactions and reasoning processes. HANs employ a hierarchical attention mechanism to capture both local and global dependencies among agents, providing insights into their coordination strategies. Experimental results on a multi-robot task allocation problem demonstrate that HANs outperform state-of-the-art MARL methods while generating interpretable attention maps that elucidate agent decision-making.