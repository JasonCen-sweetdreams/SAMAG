Deep neural networks (DNNs) are vulnerable to adversarial attacks, which can lead to misclassification or malfunction. This paper proposes a novel framework for robustness analysis of DNNs against such attacks using Bayesian uncertainty estimation. We introduce a new metric, Bayesian Adversarial Uncertainty (BAU), which quantifies the uncertainty of a DNN's predictions under adversarial perturbations. Our approach leverages the concept of Bayesian neural networks to model the uncertainty of DNNs and estimate the BAU metric. Experimental results on popular image classification datasets demonstrate the effectiveness of our approach in detecting and characterizing adversarial attacks, thereby enabling the development of more robust DNNs.