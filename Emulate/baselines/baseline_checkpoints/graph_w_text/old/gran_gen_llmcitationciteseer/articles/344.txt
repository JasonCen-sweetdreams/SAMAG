As deep learning models become increasingly prevalent in critical applications, their vulnerability to adversarial attacks has become a major concern. This paper proposes a novel framework for detecting adversarial attacks on deep learning models using explainable AI techniques. Our approach leverages saliency maps to identify anomalous input patterns that are likely to be crafted by an attacker. We demonstrate the efficacy of our method on several benchmark datasets and show that it can detect attacks with high accuracy while also providing insights into the attack mechanisms. Our framework has significant implications for designing robust and trustworthy AI systems.