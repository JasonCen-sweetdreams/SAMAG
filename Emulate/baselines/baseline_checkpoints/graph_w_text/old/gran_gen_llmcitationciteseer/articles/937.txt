Autonomous traffic management requires coordinating multiple agents to optimize traffic flow. This paper presents a decentralized multi-agent reinforcement learning framework, 'D-MARL', which enables agents to learn from local observations and communicate with neighboring agents to achieve global optima. We propose a novel graph-based attention mechanism that dynamically weights agent interactions based on their spatial proximity and traffic patterns. Experimental results on a simulated traffic network demonstrate that D-MARL outperforms traditional decentralized control methods and converges to near-optimal solutions in complex traffic scenarios.