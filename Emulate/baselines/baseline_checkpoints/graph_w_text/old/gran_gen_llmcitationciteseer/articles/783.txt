Multi-modal sentiment analysis is a challenging task due to the complexity of fusing heterogeneous data sources. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features for sentiment prediction. Our approach leverages self-attention mechanisms to model intra-modal relationships and inter-modal interactions, generating interpretable attention weights for explainability. Experimental results on the CMU-MOSI and YouTube datasets demonstrate significant performance improvements over state-of-the-art methods, while providing transparent insights into the decision-making process.