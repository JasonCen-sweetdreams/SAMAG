Augmented reality (AR) systems have the potential to revolutionize human-computer interaction. However, current interaction methods, such as handheld controllers or voice commands, can be cumbersome and limiting. This paper proposes a novel gaze-based interaction framework for AR environments, leveraging deep learning techniques to accurately predict user intentions. Our approach uses a convolutional neural network to analyze user gaze patterns, allowing for seamless and intuitive interaction with virtual objects. We evaluate our system in a user study, demonstrating significant improvements in interaction speed and accuracy compared to traditional methods.