Unstructured data, such as images and texts, pose significant challenges for traditional information retrieval systems. This paper presents a novel approach, dubbed 'DeepSim', which leverages contrastive learning to efficiently retrieve deep semantic similarities between unstructured data points. By learning a shared embedding space that captures nuanced relationships, DeepSim enables fast and accurate retrieval of similar data instances. We demonstrate the effectiveness of DeepSim on large-scale image and text datasets, achieving state-of-the-art performance while reducing computational overhead.