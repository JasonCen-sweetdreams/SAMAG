Sentiment analysis on multi-modal data (e.g., text, images, and videos) is a challenging task due to the complexity of capturing cross-modal relationships and the computational costs of processing large datasets. This paper presents a novel Hierarchical Attention Network (HAN) architecture that leverages self-attention mechanisms to selectively focus on relevant modalities and features. We demonstrate the effectiveness of HAN on a benchmark dataset, achieving state-of-the-art performance with significant reductions in computational resources and training time. Our approach has promising applications in real-world scenarios, such as social media analysis and customer feedback systems.