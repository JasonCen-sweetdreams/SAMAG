Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the inherent complexity of human emotions. This paper proposes a novel hierarchical attention network (HAN) architecture that leverages the strengths of each modality to improve emotion recognition accuracy. The HAN model consists of modality-specific attention layers that learn to focus on relevant features, followed by a hierarchical fusion layer that integrates the outputs from each modality. We also introduce an explainability module that provides insights into the decision-making process. Experimental results on the Multimodal Emotion Recognition (MER) dataset demonstrate the effectiveness of our approach, achieving an F1-score of 83.2% compared to 79.5% by the state-of-the-art method.