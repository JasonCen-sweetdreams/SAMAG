Embodied agents operating in real-world environments require the ability to process and integrate multimodal sensory inputs. This paper introduces a self-supervised representation learning framework, 'Multimodal Fusion Network' (MFN), which enables agents to learn effective representations from visual, auditory, and proprioceptive inputs. Our approach leverages a contrastive loss function to align the embeddings of different modalities, facilitating the discovery of shared patterns and structures. We demonstrate the effectiveness of MFN in a series of simulated and real-world robotic experiments, showcasing improved performance in navigation, manipulation, and scene understanding tasks.