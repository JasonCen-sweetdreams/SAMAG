Autonomous exploration in unknown environments is a fundamental challenge in robotics. This paper presents a novel deep reinforcement learning framework, 'DeepExplore', that combines graph-based exploration with deep Q-networks to efficiently explore unknown environments. We introduce a novel reward function that balances exploration and exploitation, and demonstrate its efficacy in various simulated and real-world environments. Experimental results show that DeepExplore outperforms existing methods in terms of exploration efficiency and accuracy.