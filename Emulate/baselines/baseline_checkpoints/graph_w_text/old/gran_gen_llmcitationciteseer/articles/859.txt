Emotion recognition is a crucial aspect of human-computer interaction (HCI), enabling systems to respond empathetically to user needs. This paper proposes a novel hierarchical attention network (HAN) for multi-modal emotion recognition, integrating facial expressions, speech, and physiological signals. Our HAN model leverages attention mechanisms to selectively focus on salient features across modalities, improving recognition accuracy and robustness. Experiments on the SEMAINE database demonstrate that our approach outperforms state-of-the-art methods, achieving an average F1-score of 0.92 for emotion classification.