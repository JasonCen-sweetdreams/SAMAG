Multimodal sentiment analysis, which involves analyzing text, images, and videos to understand public opinion, is a challenging task due to the complexity of human emotions and the varying importance of modalities. This paper proposes a novel hierarchical attention network (HAN) that selectively focuses on relevant modalities and regions within each modality to predict sentiment. We introduce a modality-aware attention mechanism that adaptively weights the importance of each modality based on the input, and a region-aware attention mechanism that highlights salient regions within each modality. Our experiments on a large-scale multimodal dataset demonstrate that our HAN model outperforms state-of-the-art methods in terms of accuracy and provides interpretable explanations for its predictions.