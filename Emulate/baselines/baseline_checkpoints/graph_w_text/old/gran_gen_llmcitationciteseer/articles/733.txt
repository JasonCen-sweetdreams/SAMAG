Emotion recognition from multi-modal data, such as text, audio, and vision, is a challenging task due to the complexity of human emotions and the heterogeneity of the data. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) that leverages the strengths of graph neural networks and attention mechanisms to capture both local and global dependencies in multi-modal data. Our HGAT model consists of two stages: a node-level attention module that learns to weigh the importance of each modality, and a graph-level attention module that captures the relationships between different modalities. Experiments on several benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in multi-modal emotion recognition tasks.