Multi-modal learning has become a crucial aspect of modern AI systems, but existing methods often require large amounts of labeled data and computational resources. This paper proposes a novel hierarchical contrastive learning framework, 'HiCLR', which learns efficient representations by leveraging the intrinsic structures within and across different modalities. HiCLR employs a multi-resolution approach, where modalities are first clustered into semantic groups and then aligned using a contrastive objective. We demonstrate the effectiveness of HiCLR on several benchmark datasets, achieving state-of-the-art results in multi-modal classification and retrieval tasks while reducing computational overhead by up to 40%.