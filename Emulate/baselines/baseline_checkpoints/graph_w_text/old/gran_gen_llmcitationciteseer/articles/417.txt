Deep neural networks (DNNs) are vulnerable to adversarial attacks, which can be mitigated by adversarial training. However, existing methods often rely on heuristics and lack robustness guarantees. This paper proposes a novel approach, 'BayesAdv', which integrates Bayesian uncertainty estimation into adversarial training. By modeling the uncertainty of the DNN's predictions, BayesAdv generates more informative and diverse adversarial examples, leading to improved robustness against various attack types. Experimental results on multiple benchmark datasets demonstrate that BayesAdv outperforms state-of-the-art adversarial training methods, achieving better trade-offs between accuracy and robustness.