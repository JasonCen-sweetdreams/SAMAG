Deep learning-based emotion recognition systems have achieved impressive performance, but their vulnerability to adversarial attacks remains a significant concern. This paper investigates the susceptibility of multimodal fusion models to targeted attacks, focusing on the manipulation of facial expression, speech, and text inputs. We propose a novel attack framework, 'Mimic', which leverages generative adversarial networks to craft imperceptible perturbations that mislead the model's emotion classification. Experimental results on benchmark datasets demonstrate the effectiveness of Mimic in reducing the model's accuracy, highlighting the need for robustness evaluation and defense strategies in multimodal emotion recognition.