Real-time analytics has become a crucial aspect of modern data-driven applications, but query optimization remains a bottleneck in distributed database systems. This paper proposes a novel optimization framework, 'RTOpt', which leverages machine learning-based cost estimation and dynamic query rewriting to minimize query latency. We introduce a probabilistic modeling approach to predict query execution times and adapt to workload changes. Experimental results on a real-world dataset demonstrate that RTOpt achieves up to 3x speedup in query execution compared to existing optimization techniques.