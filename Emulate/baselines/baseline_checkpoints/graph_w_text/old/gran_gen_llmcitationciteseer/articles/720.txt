Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a challenging task, particularly when explaining the underlying decision-making process. We propose a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and features for improved emotion recognition. Our HAN framework integrates a modality-level attention mechanism with a feature-level attention module, enabling the model to provide interpretable explanations for its predictions. Experimental results on the CMU-MOSEI dataset demonstrate the effectiveness of our approach, achieving state-of-the-art performance while providing meaningful insights into the emotional cues that drive the recognition process.