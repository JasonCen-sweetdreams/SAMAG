Graph neural networks (GNNs) have shown remarkable success in node classification tasks, but their computational complexity can be prohibitive for large-scale graphs. This paper proposes a novel hierarchical graph attention network (HiGAT) architecture, which leverages graph coarsening and attention mechanisms to reduce the computational cost of GNNs. HiGAT recursively applies graph attention to coarsened graph representations, enabling efficient feature learning and classification. Our experiments on several benchmark datasets demonstrate that HiGAT achieves state-of-the-art performance while reducing the computational cost by up to 75% compared to existing GNN models.