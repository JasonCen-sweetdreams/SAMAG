Distributed databases have become increasingly prevalent in modern data management systems. However, query optimization remains a significant challenge, particularly in the presence of complex joins. This paper proposes a novel approach to join ordering using machine learning (ML) techniques. We develop a neural network-based model that learns to predict the optimal join order based on query characteristics and database statistics. Experimental results on a real-world distributed database show that our ML-based approach outperforms traditional cost-based optimization methods by up to 30% in terms of query execution time.