Query expansion (QE) is a crucial component in information retrieval systems, aiming to bridge the vocabulary gap between users and documents. This paper proposes a novel reinforcement learning (RL) framework, 'RL-QE', which formulates QE as a Markov decision process. Our approach learns to select expansion terms that maximize the expected retrieval performance, leveraging a reward function that incorporates both relevance and diversity metrics. Experimental results on the TREC ad-hoc retrieval benchmark demonstrate that RL-QE outperforms state-of-the-art QE methods, achieving significant improvements in both precision and recall.