Explainability is crucial in multi-agent systems, where agents make decisions based on complex interactions. This paper proposes a novel hierarchical attention network (HAN) architecture that learns to selectively focus on relevant agents and their interactions. Our approach enables explainable decision-making by identifying influential agents and their relationships. We evaluate HAN on a benchmark multi-agent dataset and demonstrate significant improvements in decision accuracy and interpretability compared to existing methods. Our results have implications for real-world applications, such as autonomous vehicles and smart grids.