Multimodal time series classification tasks, such as human activity recognition, often suffer from limited labeled data and modalities with varying importance. This paper presents a novel self-supervised representation learning framework, 'TimeMIX', which leverages the inherent correlations between modalities to learn robust and generalizable features. TimeMIX applies a combination of contrastive and generative learning objectives to jointly learn modality-invariant and time-aware representations. Experimental results on several benchmark datasets demonstrate that TimeMIX outperforms state-of-the-art methods in terms of classification accuracy and robustness to missing modalities.