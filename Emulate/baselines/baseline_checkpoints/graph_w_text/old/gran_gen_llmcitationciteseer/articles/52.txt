Explainability in multi-agent reinforcement learning (MARL) is crucial for real-world applications, but existing methods often rely on simplistic or centralized representations. We propose Hierarchical Attention Networks (HANs) that learn to identify and prioritize relevant agents, states, and actions in MARL scenarios. HANs consist of multiple attention mechanisms that operate at different hierarchical levels, enabling the model to focus on critical components of the environment and provide interpretable explanations for its decisions. Our experiments on various MARL benchmarks demonstrate that HANs outperform state-of-the-art methods in terms of both performance and explainability.