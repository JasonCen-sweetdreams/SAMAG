Virtual reality (VR) has revolutionized human-computer interaction, but existing gestural interfaces often fail to leverage the embodied cognition of users. This paper presents a novel VR system that incorporates gesture-based interaction grounded in embodied cognition principles. Our approach utilizes machine learning-based gesture recognition and a cognitive model of motor control to predict and adapt to user behavior. In a user study, we demonstrate that our system improves user experience, reduces cognitive load, and enhances overall engagement in VR applications. Our findings have implications for the design of more intuitive and immersive VR interfaces.