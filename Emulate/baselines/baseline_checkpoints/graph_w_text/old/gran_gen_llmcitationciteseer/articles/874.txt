Traditional retrieval models in search engines rely on hand-crafted features, which can be limited and inefficient. This paper proposes a novel hybrid retrieval model that combines the strengths of neural ranking models with the efficiency of traditional sparse retrieval methods. Our approach leverages a contextualized language model to generate dense embeddings for documents and queries, which are then fused with sparse features using a learned gating mechanism. Experimental results on the TREC Deep Learning Track dataset show significant improvements in ranking accuracy and efficiency compared to state-of-the-art neural ranking models.