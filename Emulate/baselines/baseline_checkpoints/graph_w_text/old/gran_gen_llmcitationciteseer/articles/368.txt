Gaze-based interaction has revolutionized human-computer interaction, but existing systems struggle with ambiguous gaze signals and limited contextual understanding. We present EyeGazeLens, a novel framework that integrates multi-modal attention modeling to disambiguate gaze intent. By fusing gaze, speech, and gesture inputs, our approach improves interaction accuracy and efficiency. A user study with 30 participants demonstrates that EyeGazeLens outperforms state-of-the-art gaze-based systems, achieving 92.5% accuracy in a complex task scenario. Our work has significant implications for accessible and natural interaction in various applications, including gaming, education, and healthcare.