Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task, particularly when explainability is desired. This paper introduces a novel Hierarchical Attention Network (HAN) that leverages the strengths of individual modalities to improve overall emotion recognition performance. Our approach uses a hierarchical attention mechanism to selectively focus on relevant modalities and features, providing interpretable results. Experimental evaluations on the CMU-MOSEI dataset demonstrate that HAN outperforms state-of-the-art methods in terms of accuracy and explainability, making it a promising approach for real-world affective computing applications.