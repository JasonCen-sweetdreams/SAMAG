Emotion recognition from multi-modal data, such as speech, text, and vision, has numerous applications in human-computer interaction and affective computing. This paper introduces a novel hierarchical attention network (HAN) architecture that learns to selectively focus on relevant modalities and features for emotion recognition. Our approach exploits both intra-modal and inter-modal relationships to improve recognition accuracy. Experimental results on three benchmark datasets demonstrate the effectiveness of HAN, achieving state-of-the-art performance in recognizing emotions from multi-modal data.