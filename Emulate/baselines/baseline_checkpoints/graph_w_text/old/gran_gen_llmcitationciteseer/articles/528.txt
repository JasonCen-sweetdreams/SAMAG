Node classification is a fundamental task in graph-structured data analysis. While recent advances in graph neural networks (GNNs) have shown promising results, they often suffer from high computational complexity and over-smoothing issues. This paper proposes a novel hierarchical graph attention network (HGAT) architecture that leverages both local and global attention mechanisms to efficiently classify nodes. By recursively applying attention at multiple scales, HGAT captures complex node relationships and adapts to varying graph densities. Experiments on several benchmark datasets demonstrate that HGAT achieves state-of-the-art performance while reducing computational overhead by up to 50% compared to existing GNN models.