Emotion recognition from multi-modal data (e.g., speech, text, and vision) is a challenging task, particularly when faced with noisy or missing inputs. This paper proposes a Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and features to improve emotion recognition. Our model consists of two attention mechanisms: a modality-level attention that weighs the importance of each modality and a feature-level attention that highlights informative features within each modality. We evaluate HAN on three benchmark datasets and demonstrate its superiority over state-of-the-art methods in terms of emotion recognition accuracy and interpretability.