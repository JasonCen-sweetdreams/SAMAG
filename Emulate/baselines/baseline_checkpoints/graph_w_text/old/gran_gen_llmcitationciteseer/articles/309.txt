Multimodal sentiment analysis (MSA) involves analyzing sentiment from heterogeneous data sources, including text, images, and audio. While deep learning models have achieved state-of-the-art performance in MSA, they often lack transparency and interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) that leverages attention mechanisms to selectively focus on relevant modalities and features. Our approach enables explainable MSA by providing visual attention maps and feature importance scores. We evaluate our model on a large-scale multimodal dataset and demonstrate improved performance and interpretability compared to existing methods.