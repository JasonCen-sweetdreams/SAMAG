Emotion recognition from multimodal data, such as speech, text, and vision, remains a challenging task due to the inherent complexity of human emotions and the variability of modalities. This paper proposes a novel hierarchical attention network (HAN) that adaptively integrates information from multiple modalities to recognize emotions. The HAN consists of modality-specific attention modules, a multimodal fusion layer, and a hierarchical attention mechanism that refines attention weights across modalities. Experimental results on the IEMOCAP and CMU-MOSEI datasets demonstrate that our approach outperforms state-of-the-art multimodal fusion methods and achieves improved robustness to modality-specific noise.