Few-shot image classification has seen significant progress with the advent of meta-learning. However, existing approaches often rely on extensive labeled data for pre-training, which can be costly to obtain. This paper proposes a novel hierarchical meta-learning framework that leverages self-supervised pre-training to learn task-agnostic representations. Our approach, 'HMeta', uses a hierarchical structure to adapt to novel tasks, and demonstrates improved performance on several benchmark datasets. We further show that self-supervised pre-training can be effectively combined with meta-learning, achieving state-of-the-art results in few-shot classification tasks.