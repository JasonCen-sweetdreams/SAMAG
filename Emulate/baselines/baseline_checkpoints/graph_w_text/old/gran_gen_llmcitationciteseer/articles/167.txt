Multimodal sentiment analysis has gained significant attention in recent years, but existing methods often struggle to effectively integrate and align visual and textual features. This paper proposes a hierarchical attention network (HAN) that leverages both intra-modal and inter-modal attention mechanisms to learn rich representations of multimodal data. Our approach consists of a visual attention module that captures salient regions in images and a textual attention module that focuses on sentiment-bearing words in text. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our HAN model in improving sentiment analysis performance, particularly in scenarios where modalities exhibit varying degrees of relevance.