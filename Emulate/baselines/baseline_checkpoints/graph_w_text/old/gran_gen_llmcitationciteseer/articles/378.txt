Multi-task learning has been widely adopted in natural language processing (NLP) to leverage shared knowledge across related tasks. However, existing approaches often suffer from high computational costs and limited scalability. This paper proposes a novel hierarchical attention network (HAN) architecture that enables efficient multi-task learning for NLP tasks. By incorporating a hierarchical attention mechanism, HAN selectively focuses on relevant tasks and reduces the computational overhead. Experimental results on several benchmark datasets demonstrate that HAN achieves state-of-the-art performance on multiple NLP tasks while reducing the computational cost by up to 40% compared to existing multi-task learning methods.