Distributed databases are increasingly being used to manage large-scale data storage and processing. However, query optimization in such systems remains a challenging task due to the complexity of data distribution and varying network conditions. This paper proposes a novel approach to query optimization using machine learning techniques. We develop a prediction model that learns from historical query patterns and system metrics to estimate the optimal query execution plan. Our experiments on a real-world distributed database system demonstrate significant improvements in query performance and resource utilization compared to traditional rule-based optimization methods.