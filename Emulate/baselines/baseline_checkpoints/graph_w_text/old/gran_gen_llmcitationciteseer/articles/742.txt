Multi-modal emotion recognition systems often struggle to effectively fuse features from disparate modalities, such as speech, text, and vision. This paper introduces a novel hierarchical attention network (HAN) that adaptively weights and combines modality-specific features to improve emotion recognition accuracy. Our approach leverages self-attention mechanisms to model intra-modality and inter-modality relationships, enabling the network to selectively focus on salient features. Experimental results on the IEMOCAP dataset demonstrate that HAN outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, with significant gains in computational efficiency.