The proliferation of multi-modal data has created opportunities for learning rich representations. This paper introduces a novel approach, Hierarchical Contrastive Clustering (HCC), which discovers hierarchical concepts in multi-modal data without supervision. Our method leverages contrastive learning to align representations across modalities, while a hierarchical clustering algorithm recursively groups similar representations to form concepts. Experiments on a large-scale dataset of images, text, and audio demonstrate that HCC can learn meaningful, hierarchical concepts that improve downstream task performance compared to existing representation learning methods.