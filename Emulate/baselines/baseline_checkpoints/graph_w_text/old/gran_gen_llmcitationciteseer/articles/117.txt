Emotion recognition from multi-modal inputs, such as speech, text, and vision, remains a challenging task due to the complexity of human emotions and the mismatch between modalities. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) that leverages the strengths of graph neural networks and attention mechanisms to model the intricate relationships between modalities. HGAT recursively applies graph attention to capture both local and global dependencies, enabling the network to identify subtle emotional cues. Experimental results on the CMU-MOSI and IEMOCAP datasets demonstrate the superiority of HGAT over state-of-the-art methods, achieving an average improvement of 12.5% in emotion recognition accuracy.