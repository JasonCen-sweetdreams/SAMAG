Conversational agents require accurate emotion recognition to provide empathetic responses. Existing approaches often rely on a single modality, neglecting the rich information present in multi-modal human communication. We propose a Hierarchical Attention Network (HAN) that integrates acoustic, linguistic, and visual features to recognize emotions in human-agent interactions. Our HAN model employs a novel attention mechanism that adaptively weights the importance of each modality, leading to improved recognition accuracy. Experiments on a large-scale, multi-modal emotion recognition dataset demonstrate the effectiveness of our approach, outperforming state-of-the-art methods in recognizing complex emotions.