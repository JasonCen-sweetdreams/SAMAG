Emotion recognition from multimodal inputs (e.g., speech, text, and vision) is a challenging task, particularly when models lack transparency and interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) architecture, which leverages attention mechanisms at multiple levels to selectively focus on relevant modalities and features. Our approach not only achieves state-of-the-art performance on several benchmark datasets but also provides insights into the decision-making process through attention visualization. We demonstrate the effectiveness of HAN in various applications, including human-computer interaction and affective computing.