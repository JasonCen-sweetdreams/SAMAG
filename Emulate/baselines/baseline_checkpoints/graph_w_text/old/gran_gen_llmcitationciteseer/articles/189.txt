Explainability is crucial in multi-agent reinforcement learning (MARL) systems, where decision-making processes are often opaque. This paper presents Hierarchical Attention Networks (HANs), a novel architecture that integrates attention mechanisms and hierarchical reinforcement learning to enhance interpretability in MARL. HANs decompose complex MARL tasks into sub-tasks, enabling the identification of critical agents, states, and actions that influence policy decisions. Experimental results on a real-world robotics scenario demonstrate that HANs improve both task performance and explainability, outperforming state-of-the-art MARL methods.