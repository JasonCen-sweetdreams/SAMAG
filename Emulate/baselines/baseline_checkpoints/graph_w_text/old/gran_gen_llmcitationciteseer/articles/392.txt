Deep neural networks (DNNs) are vulnerable to image-translation attacks, where slight transformations in the input image can mislead the model. This paper proposes an adversarial training framework, 'TransGuard', which incorporates a novel image-translation attack generation module. We demonstrate that TransGuard significantly improves the robustness of DNNs against various image-translation attacks, including rotation, scaling, and flipping. Our evaluation on multiple benchmark datasets shows that TransGuard outperforms state-of-the-art defense methods, achieving an average improvement of 12.5% in accuracy under attack.