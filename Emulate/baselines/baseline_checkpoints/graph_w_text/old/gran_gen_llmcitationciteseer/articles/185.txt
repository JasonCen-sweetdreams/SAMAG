The increasing demand for real-time analytics in distributed database systems poses significant challenges for query optimization. Traditional optimization techniques often fail to meet the performance requirements of low-latency applications. This paper proposes a novel query optimization framework, 'RTOpt', which leverages machine learning models to predict query execution times and optimizes query plans accordingly. We demonstrate that RTOpt outperforms state-of-the-art optimization techniques by up to 3x in terms of query response time and reduces resource utilization by up to 40%.