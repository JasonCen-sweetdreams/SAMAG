Deep neural networks (DNNs) have achieved state-of-the-art performance in various machine learning tasks, but their success heavily relies on careful hyperparameter tuning. This process can be time-consuming and computationally expensive. This paper proposes a novel Bayesian optimization framework, 'HyperBO', which leverages a probabilistic surrogate model to efficiently explore the hyperparameter space. HyperBO adaptively adjusts the exploration-exploitation trade-off based on the uncertainty of the surrogate model, resulting in faster convergence to optimal hyperparameters. Experimental results on several benchmark datasets demonstrate that HyperBO outperforms existing hyperparameter tuning methods in terms of both accuracy and computational efficiency.