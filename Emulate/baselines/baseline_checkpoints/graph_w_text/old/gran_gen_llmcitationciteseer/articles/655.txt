Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task, especially when dealing with ambiguous or conflicting cues. We propose a novel Hierarchical Attention Network (HAN) that integrates attention mechanisms at multiple levels to selectively focus on relevant modalities and features. Our approach achieves state-of-the-art performance on the CMU-MOSEI dataset, outperforming existing methods by 12.3% in terms of emotion recognition accuracy. We also demonstrate the explainability of our model through visualizations of attention weights, providing insights into the decision-making process.