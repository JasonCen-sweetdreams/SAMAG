Few-shot learning has emerged as a crucial aspect of real-world AI applications, where data scarcity and novelty are common. This paper proposes a novel hierarchical graph attention network (HiGAT) architecture for inductive few-shot learning. HiGAT leverages graph neural networks to model complex relationships between objects and incorporates attention mechanisms to focus on relevant features. We demonstrate the efficacy of HiGAT on various benchmark datasets, achieving state-of-the-art performance in 5-way 1-shot and 5-way 5-shot settings. Furthermore, we analyze the attention patterns to provide insights into the learning process, facilitating explainability and transparency in few-shot learning models.