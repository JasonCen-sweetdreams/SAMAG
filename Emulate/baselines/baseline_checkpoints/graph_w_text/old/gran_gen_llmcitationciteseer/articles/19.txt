Existing ad-hoc retrieval models struggle to effectively handle multi-modal queries, leading to subpar performance. We propose a novel deep relevance matching framework, 'DRMM', which leverages a hierarchical attention mechanism to jointly model text, image, and speech modalities. Our approach first extracts modality-specific features using pre-trained encoders, then applies a cross-modal fusion module to generate a unified representation. Experimental results on a large-scale dataset demonstrate that DRMM outperforms state-of-the-art models in both retrieval accuracy and robustness to query variations.