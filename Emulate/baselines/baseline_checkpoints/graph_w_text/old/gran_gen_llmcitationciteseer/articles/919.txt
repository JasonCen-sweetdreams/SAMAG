Multimodal sentiment analysis (MSA) is a challenging task that requires capturing complex relationships between visual, textual, and acoustic cues. We propose a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features, providing interpretable explanations for its predictions. Our HAN model consists of modality-specific attention modules and a hierarchical fusion mechanism, which enables it to adapt to varying input modalities. Experimental results on two benchmark datasets demonstrate that our approach outperforms state-of-the-art MSA methods while offering insights into the decision-making process.