This paper presents a novel framework for designing conversational interfaces that accommodate diverse user abilities and preferences. We introduce a multimodal interaction model that integrates speech, gesture, and gaze input to facilitate more natural and accessible human-computer interaction. Our approach leverages machine learning-based gesture recognition and gaze tracking to enable users to seamlessly switch between input modalities. A user study with 30 participants demonstrates that our approach significantly improves interaction efficiency and user satisfaction for individuals with mobility and visual impairments.