Virtual reality (VR) has immense potential for enhancing user experiences, but existing interfaces often exclude individuals with motor or visual impairments. This paper introduces 'GazeVR', a novel gaze-based interface that leverages machine learning to detect and interpret user gaze patterns in VR environments. Our approach utilizes a deep neural network to classify gaze directions and infer user intentions, enabling hands-free interaction with virtual objects. We conduct a user study with 20 participants, demonstrating significant improvements in accessibility and user satisfaction compared to traditional controller-based interfaces.