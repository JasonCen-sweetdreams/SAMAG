Explainability is crucial in multi-agent reinforcement learning (MARL) to understand the decision-making process of autonomous agents. We propose a novel hierarchical attention network (HAT) to learn interpretable policies in MARL. HAT consists of two attention mechanisms: an intra-agent attention module to focus on relevant agent-specific features, and an inter-agent attention module to model the interactions between agents. We evaluate HAT on a variety of MARL environments, demonstrating improved explainability and competitiveness with state-of-the-art MARL algorithms. We also provide visualizations of the attention weights to illustrate the decision-making process of the agents.