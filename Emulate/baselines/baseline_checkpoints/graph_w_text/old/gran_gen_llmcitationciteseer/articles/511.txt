Emotional expression recognition is crucial in Human-Computer Interaction (HCI) to create empathetic virtual agents. This paper presents a novel multimodal fusion approach, 'EmoVR', to recognize emotional expressions in Virtual Reality (VR) environments. We integrate facial expression analysis, speech recognition, and physiological signal processing to improve recognition accuracy. Our user study with 30 participants demonstrates that EmoVR outperforms single-modality approaches, achieving an average F1-score of 0.92 for recognizing six basic emotions. We discuss the implications of our findings for designing more empathetic and engaging VR experiences.