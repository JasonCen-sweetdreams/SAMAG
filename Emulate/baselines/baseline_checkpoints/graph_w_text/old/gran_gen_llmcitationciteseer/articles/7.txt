Coordinating tasks among multiple agents in dynamic environments is a challenging problem in artificial intelligence. This paper presents a novel hierarchical task planning framework for multi-agent systems, leveraging graph-based reinforcement learning. Our approach represents the task hierarchy as a graph, where nodes correspond to tasks and edges denote dependencies. We introduce a decentralized graph-based Q-learning algorithm, which enables agents to learn coordinated policies for task planning. Experimental results on a simulated robotics scenario demonstrate that our approach outperforms existing methods in terms of task completion efficiency and adaptability to changing environments.