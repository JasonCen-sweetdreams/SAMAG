Emotion recognition in conversational agents is a crucial task, as it enables more empathetic and human-like interactions. This paper proposes a novel hierarchical attention framework, 'HierEmo', which integrates multi-modal features from speech, text, and vision to recognize emotions in conversations. HierEmo employs a hierarchical attention mechanism to selectively focus on relevant modalities and temporal segments, leading to improved emotion recognition accuracy. We evaluate our approach on a large-scale conversational dataset and demonstrate significant performance gains over existing state-of-the-art models.