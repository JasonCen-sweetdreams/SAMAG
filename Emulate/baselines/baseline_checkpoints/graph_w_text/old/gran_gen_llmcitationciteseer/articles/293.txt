In multi-agent systems, exploring the environment efficiently is crucial for achieving optimal collective behavior. We propose a novel deep reinforcement learning framework, 'MADEX', which enables coordinated exploration among agents. MADEX uses a centralized critic to evaluate the joint exploration policy, while each agent learns a decentralized policy to maximize the cumulative reward. We introduce a novel exploration bonus function that encourages agents to visit under-explored regions and avoid redundant exploration. Experimental results in a complex grid-world environment demonstrate that MADEX outperforms state-of-the-art methods in terms of exploration efficiency and collective reward.