Emotion recognition from multi-modal inputs, such as speech, text, and vision, is a crucial task in human-computer interaction. However, existing models often suffer from high computational complexity and ignore the hierarchical relationships between modalities. We propose a novel Hierarchical Attention Network (HAN) that leverages self-attention mechanisms to selectively focus on relevant modalities and features. Our experiments on the CMU-MOSEI dataset demonstrate that HAN outperforms state-of-the-art methods in terms of recognition accuracy and inference speed, making it suitable for real-time applications.