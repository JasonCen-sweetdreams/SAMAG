Virtual reality (VR) systems often rely on explicit user input, such as controllers or voice commands, which can disrupt immersion and hinder user experience. This paper presents 'GazeVR', a novel gaze-based interface that leverages eye-tracking technology to enable intuitive and hands-free interaction in VR environments. We develop a machine learning model that accurately predicts user intentions from gaze patterns and demonstrate its application in a range of VR scenarios, including object manipulation and navigation. Our user study reveals significant improvements in user experience and performance compared to traditional input methods.