Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging AI problem. Existing approaches often rely on black-box models, lacking interpretability. This paper presents a novel hierarchical attention network (HAN) that leverages attention mechanisms to selectively focus on relevant modalities and features, thereby providing explainable emotion recognition. Our HAN model consists of modality-specific attention layers and a hierarchical fusion module, enabling the model to adapt to varying input modalities. Experimental results on a benchmark dataset demonstrate the effectiveness of our approach, achieving state-of-the-art performance while offering insights into the decision-making process.