In recent years, multi-agent systems have become increasingly prevalent in various applications, including autonomous vehicles, smart grids, and robotics. However, the lack of transparency in decision-making processes hinders trust and accountability in these systems. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that enables explainable decision-making in multi-agent settings. Our approach learns to attend to relevant agents, features, and interactions, providing interpretable insights into the decision-making process. Experimental results on a real-world autonomous vehicle dataset demonstrate the effectiveness of HAN in improving decision accuracy and explainability.