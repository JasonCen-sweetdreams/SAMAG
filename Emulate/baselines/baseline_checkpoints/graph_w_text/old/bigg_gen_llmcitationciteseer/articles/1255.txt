This paper addresses the task allocation problem in decentralized multi-agent systems, where agents need to coordinate their actions to achieve a common goal. We propose a novel reinforcement learning framework, 'MA-RL', that enables agents to learn effective task allocation policies in a distributed manner. Our approach leverages a decentralized actor-critic architecture, where each agent maintains its own policy and value functions, and communicates with its neighbors to share information. We evaluate MA-RL in a simulated disaster response scenario and demonstrate improved task allocation efficiency and adaptability compared to traditional, centralized approaches.