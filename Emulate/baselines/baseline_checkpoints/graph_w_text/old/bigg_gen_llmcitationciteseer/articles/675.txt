Emotion recognition from multi-modal data (e.g., speech, text, vision) is a challenging task due to the varying importance of modalities across different emotional states. This paper introduces Hierarchical Attention Networks (HANs), a novel deep learning framework that learns to weigh and combine modality-specific features at multiple levels of abstraction. Our approach leverages self-attention mechanisms to model inter-modality relationships and selectively focus on the most informative features. Experimental results on the IEMOCAP and SEMAINE datasets demonstrate the improved accuracy and efficiency of HANs compared to state-of-the-art multi-modal fusion techniques.