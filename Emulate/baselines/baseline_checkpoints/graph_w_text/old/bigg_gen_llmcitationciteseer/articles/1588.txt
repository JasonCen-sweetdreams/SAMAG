Emotion recognition from multi-modal inputs, such as speech, text, and vision, is a challenging task due to the complexity of human emotions and the variability of modalities. This paper presents a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and time-steps to improve emotion recognition accuracy. Our approach leverages a hierarchical structure to model both intra- and inter-modality relationships, enabling more effective fusion of modalities. Experimental results on the IEMOCAP dataset demonstrate that our HAN outperforms state-of-the-art methods, achieving an F1-score of 84.2% for multi-modal emotion recognition.