Multimodal sentiment analysis (MSA) has become increasingly important in understanding user opinions from social media and online reviews. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that jointly learns text, image, and audio features for MSA. Our approach employs a hierarchical attention mechanism to selectively focus on relevant modalities and regions, resulting in improved sentiment prediction accuracy and reduced computational costs. Experimental results on three benchmark datasets demonstrate the effectiveness of our proposed HAN model, achieving state-of-the-art performance in multimodal sentiment analysis tasks.