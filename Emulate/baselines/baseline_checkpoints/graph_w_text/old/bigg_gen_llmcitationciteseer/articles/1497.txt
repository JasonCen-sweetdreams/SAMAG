Multimodal sentiment analysis has gained increasing attention in recent years, but existing approaches often lack transparency and interpretability. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates visual, textual, and acoustic features for multimodal sentiment analysis. Our HAN model incorporates attention mechanisms at multiple levels, enabling the identification of salient features and their contributions to sentiment predictions. Experiments on a large-scale multimodal dataset demonstrate the effectiveness of our approach in improving sentiment analysis accuracy and providing interpretable explanations. We also conduct a user study to validate the explainability of our model.