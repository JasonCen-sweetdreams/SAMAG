Visual question answering (VQA) models often struggle to provide interpretable explanations for their predictions. This paper introduces 'HierFusion', a novel multimodal fusion framework that leverages hierarchical attention to generate transparent and accurate VQA models. By recursively applying attention mechanisms at different levels of the image and question representations, HierFusion learns to focus on relevant regions and words, resulting in improved performance and explainability. Our experiments on the VQA-X and VQA-CP v2 datasets demonstrate the effectiveness of HierFusion in generating faithful explanations for its predictions, while achieving state-of-the-art results.