Task-oriented dialogue systems often require extensive training data to adapt to new tasks. Zero-shot learning (ZSL) can alleviate this need, but existing methods suffer from high computational complexity. This paper presents 'ZDia', a novel ZSL framework for task-oriented dialogue systems. ZDia leverages a generative model to synthesize task descriptions and a contrastive learning objective to align the dialogue agent's language model with the generated descriptions. Our experiments on the MultiWOZ dataset demonstrate that ZDia achieves state-of-the-art performance in zero-shot settings while reducing computational overhead by 3x compared to existing methods.