Explainability is crucial for real-world adoption of multi-agent reinforcement learning (MARL) systems. This paper proposes a novel hierarchical attention network (HAN) architecture that not only improves the performance of MARL agents but also provides interpretability of their decision-making processes. Our HAN model consists of two levels of attention: the lower level focuses on individual agent-agent interactions, while the higher level captures the global team strategy. We evaluate our approach on a set of cooperative MARL tasks and demonstrate significant improvements in both performance and explainability compared to state-of-the-art baselines.