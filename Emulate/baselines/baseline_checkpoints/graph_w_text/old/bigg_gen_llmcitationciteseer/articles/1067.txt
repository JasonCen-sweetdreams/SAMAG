Emotion recognition from multi-modal inputs, such as speech, text, and facial expressions, is a challenging task due to the complexity and variability of human emotions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features. Our HAN model comprises multiple attention layers, each capturing different levels of abstraction, and a gating mechanism to adaptively combine the outputs. Experimental results on the benchmark multimodal emotion recognition dataset (MERT) demonstrate that our approach outperforms state-of-the-art methods in terms of accuracy and robustness.