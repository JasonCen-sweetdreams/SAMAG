Emotion recognition from multi-modal data (e.g., speech, text, vision) is a challenging task due to the complexities of human emotional expression. This paper presents a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and timestamps. Our HAN model consists of intra-modal attention modules that capture local patterns, followed by inter-modal attention layers that fuse information across modalities. Experimental results on the IEMOCAP and MMEmo datasets show that our approach outperforms state-of-the-art methods, achieving a weighted F1-score of 85.2% and 78.5%, respectively.