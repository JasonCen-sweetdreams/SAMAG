Emotion recognition in conversational agents is a challenging task due to the complexity of human emotions and the variability of modalities such as speech, text, and vision. This paper proposes a novel hierarchical graph attention framework, 'HiGATE', which leverages the complementary strengths of multiple modalities to improve emotion recognition accuracy. HiGATE exploits the hierarchical structure of conversations to model long-range dependencies and attention mechanisms to selectively focus on salient modalities. Experimental results on a large-scale multimodal dataset demonstrate that HiGATE outperforms state-of-the-art methods in recognizing emotions and intensities, paving the way for more empathetic conversational agents.