Emotion recognition from multi-modal inputs, such as speech, text, and vision, remains a challenging task due to the complexity of human emotional expressions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that captures both local and global dependencies across modalities. Our HAN model consists of a feature-level attention mechanism that adaptively weights modality-specific features, followed by a decision-level attention mechanism that integrates the output from each modality. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, with a relative improvement of 12.5% in F1-score.