Emotion recognition from multi-modal data (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotional expressions. This paper proposes a novel Hierarchical Attention Network (HAN) that selectively focuses on relevant modalities and features to improve emotion recognition accuracy. Our HAN model consists of three layers: modality-level attention, feature-level attention, and emotion classification. Experimental results on the IEMOCAP and SEMAINE datasets demonstrate that our proposed HAN outperforms state-of-the-art methods, achieving an average F1-score improvement of 5.1% across seven emotions.