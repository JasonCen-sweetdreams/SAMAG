Virtual reality (VR) applications often rely on controllers or gestures for user interaction, but these methods can be cumbersome and limit immersion. This paper presents an eye-based interaction system utilizing gaze-dependent foveated rendering, which adaptively adjusts rendering resolution based on the user's gaze direction. Our approach leverages a machine learning model to predict gaze points from eye-tracking data and enables intuitive, hands-free interaction in VR environments. We evaluate our system through a user study, demonstrating improved interaction accuracy and reduced visual fatigue compared to traditional VR interaction methods.