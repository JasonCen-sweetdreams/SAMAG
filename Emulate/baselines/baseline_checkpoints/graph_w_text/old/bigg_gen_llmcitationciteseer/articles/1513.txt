As data volumes and velocities continue to increase, real-time analytics has become a critical component of modern data-driven applications. Distributed database systems (DDS) offer a scalable solution, but optimizing query performance remains a significant challenge. This paper presents a novel query optimization framework, 'RealOpt', which leverages machine learning and graph-based techniques to minimize query latency and maximize throughput in DDS. We propose a cost-based optimization approach that considers both data locality and network latency, and demonstrate significant performance improvements over state-of-the-art query optimization techniques using a comprehensive evaluation on a large-scale DDS testbed.