As augmented reality (AR) technology becomes more pervasive, accurate and efficient gesture recognition is crucial for seamless user interaction. This paper presents a novel approach to adaptive gesture recognition using multimodal fusion of computer vision, machine learning, and sensor-based data. Our proposed framework, 'ARgest', incorporates a hierarchical attention mechanism to dynamically weigh and combine the outputs of various modalities, enabling robust recognition of gestures in diverse environments and lighting conditions. Experimental results demonstrate that ARgest outperforms state-of-the-art gesture recognition algorithms in AR settings, achieving an average accuracy improvement of 12.5%.