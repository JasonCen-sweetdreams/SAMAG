Emotion recognition in conversational agents is a challenging task due to the complexity of human emotions and the variability of input modalities. This paper proposes a novel hierarchical attention network (HAN) that integrates audio, video, and textual features to recognize emotions in multi-modal conversations. Our HAN model employs a hierarchical attention mechanism to selectively focus on relevant modalities and time segments, resulting in improved emotion recognition accuracy. Experimental results on the IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions in noisy and ambiguous conversational scenarios.