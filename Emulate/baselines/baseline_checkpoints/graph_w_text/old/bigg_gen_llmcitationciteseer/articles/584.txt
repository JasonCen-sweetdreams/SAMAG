As AI systems become increasingly pervasive in high-stakes domains, it is essential to design interfaces that facilitate trust calibration between humans and AI agents. This paper presents an empirical study on the role of embodied cues in trust calibration, exploring how subtle changes in AI avatar behavior and nonverbal feedback can influence human trust in AI-driven decision-making. Our results show that adaptive trust cues can significantly improve collaboration outcomes by reducing over-trust and under-trust, and we provide design guidelines for incorporating embodied trust calibration into human-AI interfaces.