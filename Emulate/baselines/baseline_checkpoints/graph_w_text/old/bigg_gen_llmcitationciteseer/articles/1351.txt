Emotion recognition from multi-modal data (e.g., speech, text, vision) is crucial for human-computer interaction. This paper presents a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and temporal segments to improve emotion recognition accuracy. Our approach incorporates a hierarchical attention mechanism that captures both intra-modal and inter-modal relationships, enabling more accurate and explainable emotion predictions. Experiments on three benchmark datasets demonstrate the effectiveness of HAN, outperforming state-of-the-art methods by up to 15% in terms of F1-score.