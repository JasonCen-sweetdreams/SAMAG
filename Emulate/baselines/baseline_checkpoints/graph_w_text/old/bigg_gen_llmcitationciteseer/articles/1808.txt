Multi-modal sentiment analysis is a challenging task, requiring the integration of visual, textual, and acoustic features. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages self-attention mechanisms to selectively focus on informative regions in each modality. We introduce a hierarchical fusion strategy that captures both local and global relationships between modalities, enabling more accurate sentiment prediction. Furthermore, our approach generates interpretable attention weights, providing insights into the decision-making process. Experimental results on two benchmark datasets demonstrate the effectiveness of HAN in achieving state-of-the-art performance while offering enhanced transparency and explainability.