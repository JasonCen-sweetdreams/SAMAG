The increasing adoption of AI-driven decision support systems (DSSs) in high-stakes applications has raised concerns about transparency and accountability. This paper proposes a novel hierarchical attention network (HAN) architecture that generates interpretable explanations for AI-driven DSSs. Our approach leverages attention mechanisms to identify relevant features and relationships, enabling the generation of context-specific explanations for AI-driven decisions. Experimental results on a real-world healthcare dataset demonstrate the effectiveness of our approach in improving decision transparency and trustworthiness.