Conversational AI systems, such as chatbots, have transformed human-computer interaction. However, their accessibility for users with disabilities remains a significant concern. This paper presents a novel multimodal interaction framework, 'AccessibleChat', which integrates speech, text, and gesture recognition to facilitate seamless communication. We evaluate the framework through a user study involving participants with visual, auditory, motor, and cognitive impairments, demonstrating significant improvements in task completion rates and user satisfaction. Our work contributes to the development of more inclusive and usable conversational systems.