This paper explores the application of embodied cognition theory in Virtual Reality (VR) environments, focusing on gesture-based interaction. We designed a VR system that uses full-body tracking and machine learning-based gesture recognition to enable users to manipulate virtual objects in a more immersive and natural way. User studies revealed that our approach significantly improved user experience, including enhanced presence, engagement, and learning outcomes. We also found that users' gestures and body language can serve as implicit indicators of their cognitive state, allowing for more effective adaptive feedback. Our results have implications for the design of more intuitive and effective VR interfaces, particularly in domains such as education, training, and therapy.