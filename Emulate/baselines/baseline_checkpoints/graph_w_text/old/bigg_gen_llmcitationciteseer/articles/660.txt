Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) has seen significant progress. However, existing approaches often struggle to effectively fuse and weight the representations from different modalities. This paper introduces Hierarchical Attention Networks (HANs), a novel architecture that leverages hierarchical attention mechanisms to selectively focus on relevant modalities and features. We demonstrate the effectiveness of HANs on several benchmark datasets, achieving state-of-the-art performance in multi-modal emotion recognition tasks. Furthermore, we provide insights into the learned attention patterns, revealing interpretable relationships between modalities and emotions.