Emotion recognition from multi-modal data, such as speech, text, and vision, remains a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) that leverages the strengths of graph neural networks and attention mechanisms to model the intricate relationships between modalities. HGAT learns to adaptively focus on relevant modalities and graph regions, leading to improved emotion recognition performance. Experiments on the CMU-MOSEI dataset demonstrate the effectiveness of HGAT in recognizing emotions from multi-modal data, outperforming state-of-the-art methods by a significant margin.