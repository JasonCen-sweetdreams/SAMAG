Emotion recognition is a crucial aspect of human-robot interaction, but existing approaches often rely on a single modality, neglecting the richness of human emotional expression. This paper introduces HAN-MER, a hierarchical attention network that integrates facial expressions, speech, and physiological signals to recognize emotions in real-time. Our approach employs a novel attention mechanism that adaptively weights the contributions of each modality, achieving state-of-the-art performance on a benchmark dataset. We also explore the impact of cultural background on emotion recognition, demonstrating the importance of culturally-aware AI systems in human-robot collaboration.