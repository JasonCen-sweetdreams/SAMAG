Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is crucial in human-computer interaction. Existing approaches often rely on early fusion, which can be computationally expensive and neglect modality-specific characteristics. We propose a Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and features for emotion recognition. Our experiments on the IEMOCAP dataset show that HAN achieves state-of-the-art performance with a significant reduction in computational cost, making it suitable for real-time applications.