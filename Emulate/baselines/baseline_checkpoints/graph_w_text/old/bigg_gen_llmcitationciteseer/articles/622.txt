Event extraction from multi-modal data sources, such as texts, images, and videos, is a crucial task in various applications, including information retrieval and question answering. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) to tackle this problem. HGAT leverages the strengths of graph attention and hierarchical representations to model the complex relationships between entities, events, and modalities. Experimental results on a large-scale multi-modal dataset demonstrate that HGAT outperforms state-of-the-art methods in event extraction tasks, achieving an F1-score of 83.2% and improving the accuracy by 12.5% compared to the baseline model.