Visual question answering (VQA) tasks require models to learn diverse skills, such as object recognition, scene understanding, and language processing. We propose a novel multi-task learning framework, 'HierMTL', which leverages hierarchical task embeddings to capture complex relationships between VQA tasks. Our approach learns a shared representation across tasks and adaptively adjusts the task embeddings based on their semantic similarity. Experiments on the VQA 2.0 dataset demonstrate that HierMTL outperforms existing state-of-the-art methods, achieving a 5.2% absolute improvement in overall accuracy while reducing computational overhead by 30%.