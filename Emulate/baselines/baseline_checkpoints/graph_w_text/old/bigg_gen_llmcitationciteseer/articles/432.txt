Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task, particularly when considering the need for model interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages attention mechanisms at multiple levels to identify salient features across modalities. Our approach achieves state-of-the-art performance on the CMU-MOSEI dataset, outperforming existing methods by 12.3% in terms of weighted F1-score. Furthermore, we demonstrate the explainability of our model through visualizations of attention weights, providing insights into the decision-making process.