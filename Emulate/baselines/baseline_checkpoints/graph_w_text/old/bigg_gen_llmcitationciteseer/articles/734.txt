Gestural interfaces have become ubiquitous in modern computing, but individuals with motor impairments often struggle to interact with these systems. This paper presents a novel gestural interface framework, 'EaseGest', which incorporates machine learning-based gesture recognition and adaptive feedback mechanisms to accommodate varying levels of motor ability. We conducted a user study with 20 participants having motor impairments, demonstrating that EaseGest improves gesture recognition accuracy by 35% and reduces fatigue by 40% compared to existing gestural interfaces. Our findings inform the design of more inclusive and accessible HCI systems.