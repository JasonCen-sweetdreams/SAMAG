Emotion recognition from multiple modalities, such as speech, text, and vision, is a challenging task due to the complexity of human emotions and the varying relevance of each modality. This paper proposes a hierarchical attention network (HAN) that adaptively weighs the importance of each modality and captures both local and global contextual information. We evaluate our approach on the IEMOCAP and SEMAINE datasets, achieving state-of-the-art performance in multi-modal emotion recognition tasks. Our results demonstrate the effectiveness of HAN in modeling the hierarchical structure of emotional expressions and its potential applications in affective computing and human-computer interaction.