Effective human-robot collaboration requires seamless integration of various sensing modalities, including vision, speech, and gestural inputs. This paper presents a novel hierarchical attention network (HAN) that learns to fuse and prioritize these modalities in real-time. Our HAN architecture consists of multiple attention blocks that recursively refine the feature representations, enabling the robot to adapt to changing user behaviors and preferences. Experimental results on a human-robot collaborative assembly task demonstrate significant improvements in task completion time and user satisfaction compared to state-of-the-art multi-modal fusion methods.