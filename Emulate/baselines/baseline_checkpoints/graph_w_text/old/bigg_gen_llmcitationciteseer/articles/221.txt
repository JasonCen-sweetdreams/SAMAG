Explainability in computer vision is crucial for building trustworthy AI systems. This paper introduces a novel hierarchical attention-based framework, 'HA-XAI', which generates interpretable explanations for CNN-based image classification models. Our approach leverages a hierarchical attention mechanism to identify relevant regions-of-interest and attribute feature importance scores. We demonstrate the effectiveness of HA-XAI on various benchmark datasets, showcasing its ability to provide more accurate and informative explanations compared to existing XAI methods. Our framework has significant implications for applications in healthcare, autonomous driving, and surveillance.