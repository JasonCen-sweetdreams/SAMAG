Recent advances in dialogue systems rely on knowledge graphs (KGs) to provide contextual understanding. However, existing KG embedding methods are vulnerable to noisy or incomplete data, leading to suboptimal performance. This paper introduces 'RKM', a novel robust KG embedding approach that leverages multi-modal data (text, images, and audio) to learn more accurate and robust representations. We propose a hierarchical attention mechanism to fuse information from different modalities and a graph autoencoder to denoise the KG. Experimental results on a large-scale dialogue dataset demonstrate that RKM outperforms state-of-the-art KG embedding methods in terms of dialogue response generation and question-answering tasks.