Deep reinforcement learning (DRL) has achieved remarkable success in various applications, but the lack of transparency in learned policies hinders their trustworthiness. This paper presents a novel explainability framework, 'PolicyLens', which provides insights into the decision-making process of DRL agents. We propose a model-agnostic, gradient-based technique to generate attributions for state features, enabling the identification of critical factors influencing policy decisions. Experiments on Atari games and a real-world robotics task demonstrate the effectiveness of PolicyLens in improving policy understanding and facilitating the detection of biases.