Visual Question Answering (VQA) tasks require models to integrate information from multiple sources, including images, questions, and context. This paper introduces a novel Hierarchical Attention Network (HAN) for multimodal fusion in VQA, which learns to selectively focus on relevant regions in the image, question, and context. Our approach outperforms state-of-the-art methods on the VQA 2.0 dataset, achieving an overall accuracy of 73.4%. We also provide an extensive ablation study, demonstrating the effectiveness of our hierarchical attention mechanism in capturing complex relationships between modalities.