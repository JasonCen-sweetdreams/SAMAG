Multi-agent systems often require decision-making mechanisms that balance individual and collective goals. This paper presents a novel Hierarchical Attention Network (HAN) architecture for explainable decision-making in multi-agent settings. HAN learns to focus on relevant agents, intents, and context features, generating attention weights that provide insight into the decision-making process. We evaluate HAN on a real-world autonomous vehicle dataset, demonstrating improved decision quality and interpretability compared to state-of-the-art methods. Our approach has implications for trustworthy AI in various multi-agent domains, such as smart cities and industrial control systems.