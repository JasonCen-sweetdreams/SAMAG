Coordinating autonomous agents in complex, dynamic environments is a challenging problem. This paper proposes a novel approach using multi-objective reinforcement learning (MORL) to train agents that can effectively coordinate their actions to achieve multiple, conflicting objectives. We introduce a new MORL algorithm, 'CoorMORL', which leverages a hierarchical decomposition of the coordination problem and uses a novel reward function that balances individual and team objectives. Experimental results on a simulated robotic search and rescue scenario demonstrate that CoorMORL outperforms existing methods in achieving efficient coordination and adapting to changing environments.