Multi-agent reinforcement learning (MARL) is a challenging problem due to the exponential growth of the joint action space. This paper presents a novel hierarchical attention network (HAN) architecture that addresses this issue by learning to focus on relevant agents and their interactions. Our approach consists of two stages: a graph attention module that captures agent relationships and a hierarchical reinforcement learning module that learns to optimize joint policies. Experimental results on several MARL benchmarks demonstrate that HAN achieves state-of-the-art performance while reducing computational complexity.