Deep reinforcement learning (DRL) has achieved remarkable success in various domains, but its application to complex tasks is often hindered by the exploration-exploitation trade-off. This paper introduces 'HiExplor', a hierarchical exploration framework that leverages task decomposition and abstraction to accelerate DRL. By learning a hierarchy of exploration strategies, HiExplor adapts to changing task requirements and reduces the number of interactions needed to achieve optimal policies. Experimental results on Atari games and robotic control tasks demonstrate that HiExplor outperforms state-of-the-art exploration methods in terms of sample efficiency and final performance.