Graph neural networks (GNNs) have achieved state-of-the-art performance in node classification tasks, but their computational complexity and over-smoothing issues hinder their adoption in large-scale graphs. This paper proposes a novel hierarchical attention mechanism for GNNs, dubbed HierGAT, which adaptively selects informative nodes and their corresponding feature dimensions. We theoretically analyze the proposed method's expressive power and provide empirical evidence on several benchmark datasets, demonstrating significant improvements in classification accuracy and computational efficiency compared to existing GNN models.