This paper presents a novel multimodal emotion recognition framework that leverages hierarchical attention networks to fuse features from visual, acoustic, and linguistic cues in human-robot interaction. Our approach incorporates a hierarchical attention mechanism that selectively focuses on relevant features at multiple levels of abstraction, leading to improved emotion recognition accuracy. We evaluate our model on a large-scale dataset of human-robot interactions and demonstrate significant performance gains over state-of-the-art methods. The proposed framework has important implications for developing more empathetic and socially aware robots that can effectively recognize and respond to human emotions.