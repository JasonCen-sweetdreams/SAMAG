As augmented reality (AR) technology becomes increasingly pervasive, there is a growing need for more intuitive and natural human-computer interaction methods. This paper presents a novel multimodal interaction framework that leverages eye-gaze information to facilitate seamless user engagement in AR environments. Our approach combines computer vision, machine learning, and human-computer interaction techniques to enable users to manipulate virtual objects using gaze-based selection and gesture recognition. Evaluation results with 20 participants demonstrate the effectiveness of our approach in reducing interaction time and improving user satisfaction in AR-based tasks.