In multi-agent systems, decision-making processes often involve complex interactions and negotiations among agents. While deep neural networks can improve decision-making outcomes, their lack of transparency hinders trust and understanding. This paper proposes a novel hierarchical attention network (HAN) architecture that enables explainable decision-making in multi-agent environments. Our HAN model learns to selectively focus on relevant agents and their interactions, generating interpretable attention weights that reveal the decision-making process. Experimental results on a real-world autonomous vehicle dataset demonstrate improved decision-making performance and enhanced explainability compared to baseline methods.