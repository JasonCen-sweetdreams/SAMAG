Object detection and segmentation are crucial tasks in autonomous vehicles, but existing approaches typically address these problems separately. This paper proposes a novel multi-task learning framework, 'MT-ODS', which jointly optimizes object detection and segmentation using a shared encoder and task-specific decoders. We introduce a new loss function that balances the object detection loss with a soft segmentation mask loss, enabling the model to learn from weakly annotated data. Experimental results on the KITTI benchmark show that MT-ODS outperforms state-of-the-art single-task models while reducing computational overhead.