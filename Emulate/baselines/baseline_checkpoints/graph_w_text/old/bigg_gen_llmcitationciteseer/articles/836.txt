Emotion recognition from multi-modal data (e.g., speech, text, vision) is a challenging task due to the complexity of human emotions and the heterogeneity of modalities. We propose a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and temporal segments to improve recognition accuracy. Our approach outperforms state-of-the-art methods on the IEMOCAP and SEMAINE datasets, achieving an average F1-score improvement of 12.3% and 8.5%, respectively. We also demonstrate the robustness of HAN to noisy or missing data, making it a promising approach for real-world affective computing applications.