Question answering (QA) over knowledge graphs (KGs) has seen significant progress with the advent of graph neural networks. However, existing methods often fail to capture complex relationships between entities and suffer from over-smoothing issues. We propose HierAttKG, a novel KG embedding framework that leverages hierarchical attention mechanisms to selectively focus on relevant entities and relationships. Our approach incorporates both entity-level and relation-level attention, enabling the model to learn contextualized representations that better capture the semantics of KGs. Experimental results on several benchmark QA datasets demonstrate that HierAttKG outperforms state-of-the-art methods by a significant margin, achieving new SOTA results on the popular MetaQA dataset.