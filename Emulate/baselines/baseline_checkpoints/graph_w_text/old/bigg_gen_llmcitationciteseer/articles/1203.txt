Real-time analytics in distributed database systems demands efficient query execution to support timely decision-making. This paper presents a novel query optimization framework, 'RTQuest', which leverages machine learning models to predict query execution times and dynamically adjust resource allocation. RTQuest incorporates a probabilistic cost model that accounts for data skew, node heterogeneity, and concurrent query workloads. Experimental results on a distributed relational database show that RTQuest reduces average query latency by up to 35% compared to state-of-the-art optimizers, while maintaining high throughput and resource utilization.