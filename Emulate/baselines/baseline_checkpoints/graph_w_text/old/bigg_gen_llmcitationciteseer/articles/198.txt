Autonomous vehicles rely on reinforcement learning (RL) to make decisions, but the lack of interpretability hinders trust and safety. We introduce 'HERO', a hierarchical RL framework that incorporates explainability into the decision-making process. HERO uses a novel attention mechanism to identify relevant features and provide visual explanations for the agent's actions. We evaluate HERO on a realistic autonomous driving simulator and demonstrate improved performance and interpretability compared to state-of-the-art RL methods. Our approach enables more transparent and accountable autonomous vehicle control.