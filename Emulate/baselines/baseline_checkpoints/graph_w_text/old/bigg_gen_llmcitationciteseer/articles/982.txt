Emotion recognition from multimodal inputs (e.g., speech, text, and vision) has seen significant progress with deep learning techniques. However, the lack of explainability in these models hinders their adoption in real-world applications. This paper presents a novel hierarchical attention network (HAN) that leverages attention mechanisms to selectively focus on relevant input modalities and features. Our approach achieves state-of-the-art performance on three benchmark datasets while providing interpretable emotion recognition results. We demonstrate the effectiveness of HAN through extensive experiments and visualizations, showcasing its potential for real-world applications in affective computing and human-computer interaction.