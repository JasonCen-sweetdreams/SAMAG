Multimodal sentiment analysis (MSA) involves analyzing sentiment from multiple sources, such as text, images, and audio. Existing MSA methods rely on early fusion or late fusion strategies, which have limitations in capturing complex interactions between modalities. We propose a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and regions within each modality. Our HAN model consists of a modality-level attention module and a region-level attention module, which are stacked hierarchically to capture fine-grained sentiment information. Experiments on three benchmark MSA datasets demonstrate that our approach outperforms state-of-the-art methods in terms of sentiment accuracy and correlation with human annotations.