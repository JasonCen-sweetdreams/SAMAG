Emotion recognition from multi-modal data, such as speech, text, and facial expressions, remains a challenging task due to the complex interactions between modalities. This paper introduces a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features to improve recognition accuracy. Our approach outperforms state-of-the-art methods on the CMU-MOSEI dataset, achieving a 12% relative improvement in F1-score. Furthermore, we provide insights into the decision-making process of our model through attention visualization, enabling explainability and transparency in emotion recognition.