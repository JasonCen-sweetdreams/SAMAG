Virtual reality (VR) systems often rely on explicit user input, which can be cumbersome and limiting. This paper proposes EyeGazePath, a novel machine learning framework that predicts user intent from gaze patterns in VR. Our approach leverages a combination of convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to learn gaze patterns and contextualize them within the VR environment. We evaluate EyeGazePath on a dataset of 50 users performing various tasks in a VR simulation, achieving an average accuracy of 87.2% in predicting user intent. The results demonstrate the potential of gaze-based interfaces for enhancing user experience and interaction in VR applications.