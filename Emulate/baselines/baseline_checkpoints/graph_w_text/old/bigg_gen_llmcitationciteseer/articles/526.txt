Explainability is crucial in multi-agent reinforcement learning (MARL) to build trust in decision-making systems. This paper proposes a novel hierarchical attention network (HAN) architecture that learns to identify and highlight critical agents, actions, and states contributing to the collective reward. Our approach consists of two stages: first, a graph attention network (GAT) is employed to model agent interactions, and then, a hierarchical attention mechanism is applied to identify key components of the MARL process. Experimental results on a variety of MARL benchmarks demonstrate the effectiveness of HAN in providing interpretable explanations while maintaining competitive performance.