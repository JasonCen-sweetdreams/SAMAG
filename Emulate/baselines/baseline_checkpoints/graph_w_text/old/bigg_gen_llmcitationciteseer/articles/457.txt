Multimodal emotion recognition is a challenging task due to the complexity of human emotions and the varying quality of multimodal data. This paper proposes a novel hierarchical attention network (HAN) that integrates audio, video, and text features to recognize emotions. Our HAN model employs a multi-level attention mechanism to selectively focus on relevant modalities, timestamps, and features. Experimental results on the IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art multimodal fusion methods, achieving an average F1-score of 0.83 for emotion classification. We also conduct ablation studies to analyze the contributions of each modality and attention level.