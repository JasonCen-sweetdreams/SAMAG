This paper presents a novel decentralized multi-agent reinforcement learning (MARL) framework that leverages graph attention networks (GATs) to learn effective cooperation strategies in complex, partially observable environments. Our approach, dubbed 'GraphMA', enables agents to selectively focus on relevant neighboring agents and communicate via sparse attention weights, reducing the curse of dimensionality and improving scalability. We demonstrate the efficacy of GraphMA in a variety of multi-agent tasks, including robot soccer and traffic management, and show that it outperforms existing decentralized MARL methods in terms of convergence speed and policy quality.