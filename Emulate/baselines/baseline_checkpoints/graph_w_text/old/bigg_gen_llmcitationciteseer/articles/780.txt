Dialogue state tracking (DST) is a crucial component in task-oriented dialogue systems, but existing approaches often lack interpretability. This paper proposes a hierarchical attention network (HAN) framework for DST, which learns to focus on relevant context and slot information simultaneously. Our model consists of two stacked layers: a slot-level attention layer and a context-level attention layer. We evaluate our approach on the MultiWOZ 2.1 dataset and demonstrate improved performance over state-of-the-art baselines. Moreover, our attention weights provide insights into the decision-making process, enhancing model transparency and trustworthiness.