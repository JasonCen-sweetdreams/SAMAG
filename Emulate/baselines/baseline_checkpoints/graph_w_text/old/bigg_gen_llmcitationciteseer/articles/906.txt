Explainability is crucial in multi-agent reinforcement learning (MARL) to understand the decision-making process of agents. This paper proposes a novel hierarchical attention network (HAN) architecture that learns to attend to relevant agents and their interactions in MARL environments. Our approach enables the generation of interpretable explanations for agent policies, improving transparency and trust in autonomous systems. We evaluate our method on a range of MARL benchmarks, demonstrating improved explainability and policy performance compared to state-of-the-art methods.