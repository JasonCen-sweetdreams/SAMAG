Multimodal sentiment analysis has gained significant attention in recent years, but existing approaches often struggle to provide explainable results. This paper proposes a novel hierarchical attention network (HAN) architecture that jointly learns to represent and attend to relevant modalities, such as text, images, and audio. Our approach leverages self-attention mechanisms to capture fine-grained relationships between modalities, enabling interpretable sentiment predictions. We evaluate our approach on three benchmark datasets and demonstrate significant improvements over state-of-the-art multimodal fusion techniques, while providing insightful visualizations of attention patterns.