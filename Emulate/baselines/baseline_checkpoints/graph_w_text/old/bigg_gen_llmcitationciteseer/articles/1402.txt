Multi-modal emotion recognition is a challenging task due to the complexity of human emotions and the heterogeneity of data sources. This paper introduces Hierarchical Attention Networks (HANs), a novel deep learning framework that leverages attention mechanisms to fuse and selectively focus on relevant modalities. Our approach consists of a hierarchical structure that combines modality-specific attention weights with a global attention layer, enabling the model to capture both local and global patterns in the data. Experimental results on the IEMOCAP and SEMAINE datasets demonstrate the effectiveness of HANs in improving emotion recognition accuracy and reducing computational costs.