Recent advances in question answering (QA) have leveraged knowledge graph embeddings to reason over complex relationships between entities. However, existing approaches often rely on computationally expensive attention mechanisms, hindering their deployment in real-time applications. This paper proposes a novel attention mechanism, 'KG-Attentive', which leverages the structural properties of knowledge graphs to efficiently compute attention weights. We evaluate KG-Attentive on multiple QA benchmarks, demonstrating significant improvements in inference speed while maintaining state-of-the-art accuracy. Additionally, we provide an in-depth analysis of the computational complexity and memory usage of our approach.