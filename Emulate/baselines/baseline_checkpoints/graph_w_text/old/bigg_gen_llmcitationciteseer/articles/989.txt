Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the inherent complexity of human emotions and the variability of modalities. This paper proposes a novel Hierarchical Attention Network (HAN) that integrates attention mechanisms at both the intra-modal and inter-modal levels to selectively focus on relevant features. We demonstrate the effectiveness of HAN on three benchmark datasets, achieving state-of-the-art performance in emotion recognition tasks. Moreover, our approach provides interpretable results, enabling the identification of salient features contributing to the recognized emotions.