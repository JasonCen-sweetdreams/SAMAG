Multi-label classification is a fundamental task in many applications, but existing models often struggle with scalability and computational efficiency. This paper proposes a novel hierarchical attention network, 'HiAtt', which leverages a tree-based structure to model label dependencies and correlations. We introduce a sparse attention mechanism that adaptively selects relevant labels and reduces computational overhead. Experiments on several benchmark datasets demonstrate that HiAtt achieves state-of-the-art performance while requiring significantly fewer parameters and computations compared to existing models.