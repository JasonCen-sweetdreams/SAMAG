Emotion recognition is a crucial aspect of human-computer interaction (HCI), enabling systems to respond empathetically to users. This paper presents a novel hierarchical attention network (HAN) architecture that integrates multi-modal data from speech, text, and facial expressions to recognize emotions. Our approach utilizes attention mechanisms to selectively focus on relevant features within each modality and across modalities, leading to improved recognition accuracy. Experimental results demonstrate the effectiveness of our HAN model in recognizing emotions in a real-world HCI setting, outperforming state-of-the-art methods that rely on single-modal or fusion-based approaches.