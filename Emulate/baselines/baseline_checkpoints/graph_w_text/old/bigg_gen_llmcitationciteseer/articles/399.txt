Emotion recognition from multi-modal inputs, such as speech, text, and vision, is a challenging task in affective computing. This paper presents a novel hierarchical attention network (HAN) architecture that selectively focuses on relevant modalities and features to improve recognition accuracy and provide explanations for the predicted emotions. Our HAN model consists of modality-specific attention modules and a hierarchical fusion mechanism, allowing it to adapt to varying input quality and correlations. Experimental results on benchmark datasets demonstrate the effectiveness of our approach in recognizing emotions and providing interpretable explanations, outperforming state-of-the-art methods.