Knowledge graph embedding (KGE) has become a crucial component in various AI applications, but existing methods often struggle with handling complex graph structures and noisy data. To address these limitations, we propose a novel KGE framework, HAT-KGE, which leverages hierarchical attention networks to selectively focus on relevant subgraphs and entities. Our approach achieves state-of-the-art performance on several benchmark datasets, including FB15k-237 and YAGO3-10, and demonstrates improved robustness to noisy data. We further provide an in-depth analysis of the attention mechanism, revealing insights into the learned representations and their correlation with graph properties.