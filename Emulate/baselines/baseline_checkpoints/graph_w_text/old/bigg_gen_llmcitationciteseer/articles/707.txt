Emotion recognition from multi-modal data (e.g., speech, text, and vision) is a challenging task, particularly when dealing with heterogeneous input formats and noisy data. This paper presents a novel hierarchical attention network (HAN) architecture that leverages both local and global attention mechanisms to selectively focus on relevant features from different modalities. Our approach enables explainable emotion recognition by generating visual and textual attributions that highlight the most influential input features. Experimental results on several benchmark datasets demonstrate the effectiveness of HAN in improving emotion recognition accuracy and providing interpretable results.