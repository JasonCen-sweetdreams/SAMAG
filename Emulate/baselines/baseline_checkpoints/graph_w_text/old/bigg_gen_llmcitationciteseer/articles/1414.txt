Autonomous vehicles (AVs) have the potential to revolutionize urban transportation, but their efficient coordination remains a significant challenge. This paper presents a novel multi-agent reinforcement learning (MARL) framework, 'CoopAV', which enables decentralized decision-making among AVs to minimize travel time and reduce congestion. We propose a hierarchical, graph-based architecture that incorporates both local and global rewards to promote cooperation among agents. Our experimental results demonstrate that CoopAV outperforms state-of-the-art MARL methods in simulated urban scenarios, achieving up to 25% reduction in travel time and 30% decrease in congestion.