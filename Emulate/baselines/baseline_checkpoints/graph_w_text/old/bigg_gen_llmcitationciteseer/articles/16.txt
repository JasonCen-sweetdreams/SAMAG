Emotion recognition from multi-modal inputs, such as speech, text, and vision, remains a challenging task in affective computing. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features for emotion inference. Our HAN model consists of three hierarchical layers: modality attention, feature attention, and emotion attention. Experimental results on the IEMOCAP and CMU-MOSEI datasets demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an improvement of 8.5% in F1-score.