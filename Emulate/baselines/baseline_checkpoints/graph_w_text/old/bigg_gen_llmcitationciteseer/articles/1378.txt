Multimodal sentiment analysis (MSA) has gained popularity in recent years, but existing approaches often struggle with scalability and modality-agnostic feature fusion. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that efficiently integrates visual, textual, and acoustic features for MSA. Our approach leverages a modality-aware attention mechanism to selectively focus on relevant features and reduce dimensionality. Experimental results on three benchmark datasets demonstrate that HAN outperforms state-of-the-art methods in terms of accuracy and computational efficiency, making it a promising solution for large-scale MSA applications.