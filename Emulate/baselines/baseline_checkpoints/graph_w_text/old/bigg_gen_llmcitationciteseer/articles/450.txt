This paper presents a novel approach to decentralized task allocation for multi-agent systems using hierarchical reinforcement learning. Our method, called HRL-TA, combines a high-level planning agent with low-level execution agents to efficiently allocate tasks in dynamic environments. We evaluate HRL-TA in a simulated disaster response scenario, demonstrating improved task completion rates and reduced communication overhead compared to traditional centralized and distributed approaches. Our results show that HRL-TA can effectively scale to large numbers of agents and tasks, making it a promising solution for real-world multi-agent systems.