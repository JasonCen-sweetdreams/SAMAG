Question answering (QA) tasks require models to process complex contextual information, often resulting in computationally expensive and memory-intensive models. This paper proposes a novel hierarchical multi-task learning (HMTL) framework that leverages task hierarchies to share knowledge across related QA tasks. Our approach uses a hierarchical attention mechanism to selectively focus on relevant tasks and contextual information, reducing the need for specialized models and improving overall efficiency. Experimental results on several benchmark datasets demonstrate that HMTL achieves comparable or better performance than state-of-the-art models while requiring significantly fewer parameters and computations.