Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the complex relationships between modalities. This paper proposes a novel hierarchical graph attention network (HGAT) that learns to selectively focus on relevant modalities and their interactions. Our approach leverages graph attention mechanisms to model both intra- and inter-modality relationships, enabling the network to capture subtle emotional cues. Experimental results on the CMU-MOSEI dataset demonstrate that HGAT outperforms state-of-the-art methods, achieving a 12.5% improvement in emotion recognition accuracy.