Emotion recognition from multi-modal data (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the variability of individual responses. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) that integrates graph neural networks and attention mechanisms to effectively capture the hierarchical dependencies and interactions between different modalities. Experimental results on the IEMOCAP dataset show that HGAT outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an average F1-score improvement of 12.5%.