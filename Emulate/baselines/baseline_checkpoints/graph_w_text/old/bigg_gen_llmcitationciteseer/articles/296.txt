Emotion recognition from multi-modal data (e.g., speech, text, and vision) has gained increasing attention in human-computer interaction. However, existing models often lack transparency and interpretability, hindering trust and adoption in real-world applications. This paper proposes a novel hierarchical attention network (HAN) that leverages the strengths of each modality to recognize emotions while providing explicit explanations for its predictions. Our HAN model employs a hierarchical structure to capture both intra-modal and cross-modal relationships, and incorporates attention mechanisms to highlight relevant features and modalities contributing to the predicted emotions. Experimental results on the IEMOCAP dataset demonstrate the effectiveness of our approach in achieving state-of-the-art performance while offering useful insights into the decision-making process.