Effective disaster response requires coordinating multiple agents, such as robots, drones, and emergency responders. This paper presents a novel hierarchical reinforcement learning framework for multi-agent coordination in dynamic, partially observable environments. Our approach, 'HIERARCHY', leverages a two-level hierarchy of agents, where high-level agents allocate tasks to low-level agents based on their capabilities and availability. We evaluate HIERARCHY in a simulated disaster response scenario, demonstrating improved response times and reduced casualties compared to decentralized and centralized baselines. Our results have significant implications for real-world disaster response applications.