Multimodal sentiment analysis has gained increasing attention in recent years, but existing approaches often lack interpretability. This paper presents a novel hierarchical attention network (HAN) that integrates visual, acoustic, and textual features to predict sentiment. We propose a multimodal fusion mechanism that adaptively weights modality-specific attention weights, enabling the model to focus on relevant features for sentiment prediction. Experimental results on three benchmark datasets demonstrate that our approach achieves state-of-the-art performance and provides insightful explanations for sentiment predictions.