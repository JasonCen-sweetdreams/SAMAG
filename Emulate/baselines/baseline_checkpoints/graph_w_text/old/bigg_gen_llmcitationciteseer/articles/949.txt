Virtual reality (VR) systems often rely on cumbersome controllers or simplistic gesture recognition algorithms, limiting the immersion and interactivity of the user experience. We present GESTAR, a multimodal gesture recognition framework that combines computer vision, machine learning, and sensor fusion to accurately detect and interpret complex hand and finger gestures in real-time. GESTAR's modular architecture enables seamless integration with various VR platforms and devices, and our user study demonstrates significant improvements in user satisfaction and task performance compared to traditional input methods.