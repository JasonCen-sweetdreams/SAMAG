Multi-modal emotion recognition from facial expressions, speech, and text has seen significant progress with deep learning approaches. However, existing methods often rely on complex fusion strategies or exhaustive feature engineering. This paper proposes a novel Hierarchical Attention Network (HAN) that leverages the strengths of each modality while reducing computational overhead. Our HAN architecture adaptively weights and combines modality-specific features, enabling efficient and accurate emotion recognition. Experiments on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods while requiring fewer parameters and computations.