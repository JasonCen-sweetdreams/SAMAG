Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the complexity and diversity of human emotions. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates modality-specific and cross-modal attention mechanisms to selectively focus on relevant features and modalities. Experiments on three benchmark datasets demonstrate that our approach achieves state-of-the-art performance in emotion recognition tasks, while reducing computational costs by up to 30% compared to existing methods.