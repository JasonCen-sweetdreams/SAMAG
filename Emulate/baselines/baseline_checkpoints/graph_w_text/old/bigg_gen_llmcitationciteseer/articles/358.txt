Individuals with motor impairments often rely on assistive interfaces, but these systems can be cumbersome and require explicit input. We propose a novel gaze-based intention detection framework that leverages machine learning and computer vision to infer user intent from eye movements. Our approach incorporates a probabilistic graphical model that combines gaze patterns, facial expressions, and contextual information to predict the user's desired action. Experimental results demonstrate that our system achieves high accuracy (>90%) in detecting user intentions, enabling more efficient and intuitive interactions with assistive interfaces.