Accurate affective state recognition is crucial for creating empathetic and personalized human-computer interfaces. This paper presents EmoTract, a novel multimodal framework that integrates computer vision, speech processing, and physiological signal analysis to recognize users' emotional states. EmoTract employs a hierarchical attention mechanism to fuse features from various modalities, enabling robust and context-aware affect recognition. We evaluate EmoTract on a large, publicly available dataset and demonstrate improved performance over state-of-the-art unimodal and multimodal approaches. The proposed framework has implications for developing more empathetic and engaging HCI systems.