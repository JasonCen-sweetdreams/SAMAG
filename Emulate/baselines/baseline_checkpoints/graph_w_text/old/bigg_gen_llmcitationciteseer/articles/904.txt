As multi-agent systems become increasingly prevalent in real-world applications, the need for transparency and interpretability in their decision-making processes grows. This paper presents a novel hierarchical attention network architecture, 'ExplainAgent', designed to provide insights into the decision-making processes of multiple agents interacting in complex environments. ExplainAgent leverages hierarchical attention mechanisms to selectively focus on relevant state and action features, generating attention weights that can be visualized to explain agent behavior. Experimental results on a range of multi-agent benchmarks demonstrate improved performance and interpretability compared to existing state-of-the-art methods.