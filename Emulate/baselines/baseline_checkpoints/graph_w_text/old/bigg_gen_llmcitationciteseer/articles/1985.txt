Emotion recognition from multi-modal data (e.g., text, audio, and video) is a challenging task due to the complexity of human emotions and the heterogeneity of modalities. This paper presents a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and features to improve emotion recognition. Our HAN model consists of two stages: modality-level attention and feature-level attention. We evaluate our approach on the popular CMU-MOSI dataset and achieve state-of-the-art performance, outperforming existing methods by a significant margin. We also provide insights into the learned attention patterns, which can inform the development of more effective emotion recognition systems.