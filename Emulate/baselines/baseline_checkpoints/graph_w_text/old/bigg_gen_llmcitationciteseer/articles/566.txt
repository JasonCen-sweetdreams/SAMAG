Multimodal emotion recognition (MER) has gained significant attention in recent years due to its potential applications in human-computer interaction. However, existing MER models often suffer from high computational complexity and neglect the hierarchical relationships between different modalities. We propose a novel hierarchical attention network (HAN) that leverages attention mechanisms to selectively focus on relevant modalities and features. Our experiments on the CMU-MOSEI dataset demonstrate that HAN achieves state-of-the-art performance in MER while reducing computational costs by up to 40%. We also provide an in-depth analysis of the attention patterns learned by HAN, providing insights into the importance of each modality in MER.