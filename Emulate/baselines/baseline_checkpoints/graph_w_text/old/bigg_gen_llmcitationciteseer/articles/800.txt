Deep reinforcement learning (DRL) has achieved remarkable success in complex decision-making tasks. However, its vulnerability to adversarial attacks raises concerns about its reliability. This paper investigates the robustness of DRL agents against policy-based attacks, which target the agent's decision-making process. We propose a novel framework that leverages policy-augmented latent space to enhance the agent's robustness. Our approach, 'PARL', incorporates adversarial policy augmentation into the latent space, enabling the agent to anticipate and respond to potential attacks. Experimental results on various Atari games demonstrate that PARL significantly improves the robustness of DRL agents against policy-based attacks, outperforming state-of-the-art defenses.