Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the variability and complexity of human emotional expressions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates modality-specific attention mechanisms to selectively focus on salient features. We demonstrate the efficacy of HAN on three benchmark datasets, showcasing significant improvements in emotion recognition accuracy and reduced computational complexity compared to existing state-of-the-art approaches.