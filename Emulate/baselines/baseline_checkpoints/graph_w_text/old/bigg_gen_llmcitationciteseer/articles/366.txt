Emotion recognition in human-computer interaction (HCI) is a crucial task for developing empathetic machines. This paper proposes a novel hierarchical attention network (HAN) architecture that leverages multi-modal input (facial expressions, speech, and physiological signals) to recognize emotions in real-time. Our HAN model consists of modality-specific attention modules and a hierarchical fusion mechanism that adaptively weights and combines the outputs. Experimental results on the RECOLA dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing complex emotions, such as anxiety and boredom, with an average F1-score improvement of 12.5%