Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) has seen significant advancements with deep learning. However, the lack of interpretability hinders trust and adoption in real-world applications. This paper proposes a novel hierarchical attention network (HAN) architecture that leverages intra- and inter-modal attention to selectively focus on relevant features. We demonstrate that HAN outperforms state-of-the-art models on three benchmark datasets, while providing visual and quantitative explanations for its predictions. A user study further validates the effectiveness of HAN's explanations in improving human trust and decision-making.