In multi-agent systems, decision-making processes often involve complex interactions and uncertain outcomes. This paper proposes a novel hierarchical attention network (HAN) architecture that enhances explainability and interpretability in multi-agent decision making. Our approach learns to identify influential agents, extract relevant features, and visualize decision-making processes. We evaluate HAN on a real-world autonomous vehicle dataset, demonstrating improved decision accuracy and transparency compared to existing methods. Our results have implications for the development of trustworthy and accountable AI systems in complex, dynamic environments.