Deep neural networks (DNNs) are vulnerable to adversarial attacks, which can lead to catastrophic consequences in safety-critical applications. This paper proposes a novel Bayesian neural symbolic learning (BNSL) framework to evaluate the robustness of DNNs against adversarial attacks. We integrate Bayesian neural networks with symbolic reasoning to learn probabilistic representations of adversarial examples. Our approach enables the identification of vulnerable regions in the input space and quantifies the uncertainty of the DNN's predictions. Experimental results on benchmark datasets demonstrate the effectiveness of our approach in detecting and mitigating adversarial attacks, outperforming traditional robustness evaluation methods.