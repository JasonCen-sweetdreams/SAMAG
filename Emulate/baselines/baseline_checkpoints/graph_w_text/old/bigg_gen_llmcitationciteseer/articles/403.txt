Multi-agent systems often require coordination among agents to achieve a common goal. In partially observable environments, agents must balance exploration and exploitation to learn an optimal policy. We propose a novel framework, Bayesian Social Learning (BSL), which enables agents to learn from each other's experiences and adapt to changing environments. BSL uses a Bayesian inference mechanism to update agents' beliefs about the environment and selects actions based on a social learning criterion. Experimental results in a distributed robotics scenario demonstrate that BSL outperforms traditional reinforcement learning approaches in terms of both exploration efficiency and task completion rate.