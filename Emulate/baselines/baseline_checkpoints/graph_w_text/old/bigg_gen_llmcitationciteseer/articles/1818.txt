Emotion recognition from multi-modal inputs, such as speech, text, and facial expressions, is a challenging task due to the heterogeneity and complexity of the data. We propose a novel Hierarchical Graph Attention Network (HGAT) architecture that captures both intra-modal and inter-modal relationships between different input modalities. Our approach leverages graph attention mechanisms to selectively focus on relevant modalities and features, leading to improved emotion recognition performance. Experimental results on the CMU-MOSEI dataset demonstrate the effectiveness of HGAT, achieving state-of-the-art performance on multi-modal emotion recognition tasks.