Inclusive design requires understanding diverse user needs and preferences. This paper presents an embodied conversational agent (ECA) framework that elicits user preferences through natural language interactions and nonverbal cues. Our ECA, 'PrefBot', uses multimodal fusion to analyze user input and infer preferences, which are then used to generate personalized design recommendations. We conducted a user study with 30 participants, demonstrating that PrefBot improves the accuracy of preference elicitation by 37% compared to traditional survey-based methods. Our approach has implications for designing more accessible and user-centered products.