Multimodal sentiment analysis has gained increasing attention in recent years, but existing methods often struggle to effectively fuse and weigh the contributions of different modalities. This paper proposes a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and features at multiple scales. Our approach leverages a stacked architecture of attention layers to capture both local and global dependencies between modalities, and demonstrates state-of-the-art performance on several benchmark datasets. We also provide an in-depth analysis of the learned attention patterns, revealing insights into the complex relationships between modalities in multimodal sentiment analysis.