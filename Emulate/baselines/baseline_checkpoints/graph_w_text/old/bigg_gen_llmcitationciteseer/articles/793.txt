Autonomous vehicles rely on efficient decision-making algorithms to navigate complex environments. This paper presents a novel hierarchical reinforcement learning (HRL) framework for autonomous vehicle decision-making, which leverages a hierarchical state representation to efficiently explore the action space. Our approach, dubbed 'Hierarchical Q-Networks' (HQN), exploits the temporal and spatial correlations in the environment to reduce the exploration complexity. Experimental results on a simulated autonomous driving platform demonstrate that HQN outperforms state-of-the-art reinforcement learning methods in terms of sample efficiency and decision-making speed, while maintaining safety and stability guarantees.