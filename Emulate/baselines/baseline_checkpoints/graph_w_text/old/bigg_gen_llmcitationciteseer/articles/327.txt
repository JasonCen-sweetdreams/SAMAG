Emotion recognition is a crucial aspect of human-computer interaction, with applications in fields like affective computing, healthcare, and education. This paper presents a novel multimodal approach that combines deep learning-based facial feature extraction with speech emotion recognition using convolutional recurrent neural networks. Our proposed framework, 'EmoFusion', leverages a weighted fusion mechanism to integrate the outputs from both modalities, resulting in improved recognition accuracy and robustness to noise. Extensive experiments on a large-scale emotion dataset demonstrate the effectiveness of EmoFusion in real-time human emotion recognition, outperforming state-of-the-art unimodal and multimodal approaches.