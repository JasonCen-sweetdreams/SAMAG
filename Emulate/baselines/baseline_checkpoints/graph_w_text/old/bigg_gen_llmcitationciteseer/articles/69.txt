Autonomous vehicles require efficient control systems to navigate complex scenarios. This paper introduces a hierarchical reinforcement learning framework, 'HierRL', which leverages a two-level hierarchy to balance exploration and exploitation in autonomous vehicle control. The upper level optimizes high-level navigation strategies, while the lower level refines low-level control actions. We demonstrate HierRL's effectiveness in a simulated urban driving environment, achieving improved fuel efficiency and reduced computational overhead compared to flat reinforcement learning approaches.