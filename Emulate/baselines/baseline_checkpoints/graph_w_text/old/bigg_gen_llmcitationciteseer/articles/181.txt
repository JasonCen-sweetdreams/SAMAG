The proliferation of multimedia data has led to a surge in multi-modal document retrieval, where the goal is to retrieve relevant documents containing diverse media types such as text, images, and videos. This paper proposes a novel deep neural network-based approach, 'MMRNet', which leverages the strengths of both CNNs and transformers to efficiently retrieve multi-modal documents. MMRNet utilizes a hierarchical attention mechanism to fuse the representations of different modalities, allowing it to capture complex contextual relationships between them. Experimental results on a large-scale multi-modal dataset demonstrate the effectiveness of MMRNet in improving retrieval accuracy and efficiency compared to state-of-the-art methods.