Time-series anomaly detection is crucial in various domains, but existing methods often lack interpretability. This paper proposes a novel hierarchical attention network (HAN) architecture that detects anomalies while providing explanations. Our approach consists of a two-level attention mechanism: (1) a local attention module that captures short-term dependencies and (2) a global attention module that incorporates long-term dependencies and contextual information. We evaluate our method on three real-world datasets and demonstrate its superiority over state-of-the-art methods in terms of detection accuracy and explainability.