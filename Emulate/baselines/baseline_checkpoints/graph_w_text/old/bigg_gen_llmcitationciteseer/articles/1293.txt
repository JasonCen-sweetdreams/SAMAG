Neural architecture search (NAS) has emerged as a promising technique for automating the design of deep neural networks. However, existing NAS methods often suffer from high computational costs and limited exploration capabilities. This paper proposes a novel graph-based reinforcement learning approach, 'GraphNAS', which leverages graph theory to efficiently explore the architecture search space. We introduce a hierarchical graph representation that captures complex relationships between neural network components and a reinforcement learning objective that encourages the discovery of novel and effective architectures. Experimental results on several benchmark datasets demonstrate that GraphNAS outperforms state-of-the-art NAS methods in terms of search efficiency and model performance.