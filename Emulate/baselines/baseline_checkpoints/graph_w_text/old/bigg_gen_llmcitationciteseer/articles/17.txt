Multimodal sentiment analysis (MSA) involves analyzing sentiment from text, image, and video data. While existing methods achieve high accuracy, they often lack interpretability. We propose Hierarchical Attention Networks (HAN) for Explainable MSA, which leverages attention mechanisms to weigh the importance of different modalities and features. HAN consists of a multimodal encoder, a sentiment predictor, and an explainer module. The explainer generates visualizations and feature importance scores, enabling users to understand the sentiment prediction. Experiments on three benchmark datasets demonstrate that HAN outperforms state-of-the-art methods while providing insightful explanations.