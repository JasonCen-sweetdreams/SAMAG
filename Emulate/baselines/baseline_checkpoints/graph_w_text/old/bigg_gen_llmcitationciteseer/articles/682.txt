Deep neural networks have achieved state-of-the-art performance in various machine learning tasks, but their success heavily relies on the careful tuning of hyperparameters. While Bayesian optimization has shown promise in hyperparameter tuning, its computational cost can be prohibitively high for large models. This paper proposes a novel Bayesian optimization algorithm, 'BOHB+', which leverages a hierarchical surrogate model and an efficient exploration strategy to reduce the number of function evaluations. We evaluate BOHB+ on several benchmark datasets and demonstrate its ability to find better hyperparameters with reduced computational overhead compared to existing Bayesian optimization methods.