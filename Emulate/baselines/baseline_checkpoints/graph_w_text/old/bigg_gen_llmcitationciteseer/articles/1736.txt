Reinforcement learning (RL) has shown great promise in real-world robotics, but its application is often hindered by the need for manual design of neural network architectures. This paper proposes a novel neural architecture search (NAS) method, 'RL-NAS', which leverages evolutionary algorithms to discover efficient RL policies for robotics tasks. Our approach incorporates a novel reward function that balances exploration and exploitation, allowing RL-NAS to adapt to changing environments. We evaluate RL-NAS on a range of robotics tasks, demonstrating improved performance and sample efficiency compared to manually designed architectures.