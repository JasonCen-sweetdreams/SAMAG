This paper presents a novel approach to decentralized task allocation in dynamic environments using multi-agent reinforcement learning. We propose a hierarchical framework, 'MA-RL-TA', which integrates a high-level task allocator with low-level agent controllers. The allocator learns to assign tasks to agents based on their capabilities and environmental constraints, while the controllers adapt to changing task requirements using reinforcement learning. We evaluate MA-RL-TA in a simulated disaster response scenario, demonstrating improved task completion rates and reduced communication overhead compared to traditional, centralized allocation methods.