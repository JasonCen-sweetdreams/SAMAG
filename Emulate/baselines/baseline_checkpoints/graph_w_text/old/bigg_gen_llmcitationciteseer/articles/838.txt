Multimodal emotion recognition systems often rely on complex fusion strategies, making it difficult to understand the underlying decision-making process. This paper introduces a hierarchical attention network (HAN) that learns to selectively focus on relevant input modalities and features for emotion recognition. Our approach leverages a novel self-attention mechanism to model inter-modal interactions and provides interpretable visualizations of the attention weights. Experimental results on the CMU-MOSEI dataset demonstrate that HAN outperforms state-of-the-art multimodal fusion methods while offering a transparent and explainable emotion recognition framework.