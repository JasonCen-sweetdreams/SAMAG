Deep neural networks (DNNs) are prone to errors when faced with out-of-distribution (OOD) inputs. Existing detection methods often rely on ad-hoc heuristics or require significant modifications to the underlying model. We propose a novel approach, Hierarchical Gaussian Mixture (HGM), which models the output distribution of a DNN as a hierarchical mixture of Gaussians. HGM enables efficient OOD detection by identifying inputs that fall outside the learned distribution. Our experiments on various image classification benchmarks demonstrate that HGM outperforms state-of-the-art methods in terms of detection accuracy and computational efficiency.