Emotion recognition in human-robot interaction (HRI) is a crucial aspect of creating socially intelligent robots. This paper proposes a novel hierarchical attention mechanism that integrates multi-modal features from speech, text, and facial expressions to recognize emotions in HRI. Our approach leverages a hierarchical graph neural network to model the complex relationships between different modalities and capture contextual information. Experimental results on a large-scale HRI dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions, achieving an accuracy of 92.1% and a F1-score of 0.913.