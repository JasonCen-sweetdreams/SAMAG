Multimodal emotion recognition (MER) has gained increasing attention in human-computer interaction. However, existing approaches often rely on computationally expensive fusion techniques or neglect the hierarchical structure of emotional cues. This paper proposes a novel hierarchical attention network (HAN) for MER, which leverages attention mechanisms to selectively focus on relevant modalities and features at multiple scales. Experimental results on three benchmark datasets show that HAN outperforms state-of-the-art methods in terms of accuracy and computational efficiency, demonstrating its potential for real-world affective computing applications.