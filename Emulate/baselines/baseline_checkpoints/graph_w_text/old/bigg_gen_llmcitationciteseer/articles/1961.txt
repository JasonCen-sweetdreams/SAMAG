Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task, particularly when explainability is desired. This paper presents a novel Hierarchical Attention Network (HAN) architecture that integrates audio, textual, and visual features to recognize emotions in human-computer interactions. Our HAN model employs a hierarchical attention mechanism to selectively focus on relevant modalities and features, providing interpretable results. Experimental results on the IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art methods in terms of emotion recognition accuracy and interpretability.