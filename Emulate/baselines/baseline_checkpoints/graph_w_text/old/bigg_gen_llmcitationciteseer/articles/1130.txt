Emotion recognition is a vital aspect of human-computer interaction, but existing approaches often rely on individual modalities, such as speech or facial expressions. We propose a novel hierarchical attention network (HAN) that integrates multimodal features from speech, text, and vision to recognize emotions more accurately. Our HAN model employs a hierarchical structure to capture both local and global dependencies across modalities, enabling more effective emotion inference. Experimental results on the CMU-MOSI dataset demonstrate that our approach outperforms state-of-the-art methods, achieving a significant improvement in emotion recognition accuracy and F1-score.