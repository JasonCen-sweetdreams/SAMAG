Cooperative reinforcement learning in multi-agent systems is challenging due to the exponential growth of the joint action space. This paper proposes a novel hierarchical graph attention network (HGAT) architecture that leverages both local and global dependencies to learn effective cooperation strategies. HGAT consists of two stages: (1) intra-agent graph attention to model local interactions and (2) inter-agent graph attention to capture global coordination. We evaluate HGAT on several cooperative tasks, including robotic soccer and autonomous driving, demonstrating improved performance and stability compared to existing methods.