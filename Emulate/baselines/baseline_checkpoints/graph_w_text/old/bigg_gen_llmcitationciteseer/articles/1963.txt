Multimodal fusion is a crucial component of visual question answering (VQA) systems, as it enables the integration of linguistic and visual features to generate accurate answers. This paper proposes a novel hierarchical attention network (HAN) architecture that learns to selectively focus on relevant regions of the input image and question embedding spaces. Our HAN model consists of two stacked attention layers: a visual attention module that identifies salient image regions, and a linguistic attention module that weights question words based on their relevance to the visual context. Experimental results on the VQA 2.0 dataset demonstrate that our approach achieves state-of-the-art performance, outperforming existing multimodal fusion techniques.