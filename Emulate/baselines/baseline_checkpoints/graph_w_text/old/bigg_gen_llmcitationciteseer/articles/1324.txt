Vision-and-language models have achieved state-of-the-art performance in various multimodal tasks, but their decision-making processes remain opaque. This paper proposes a novel explainability framework that leverages neural architecture search to identify interpretable sub-networks within these models. Our approach, 'MME-XAI', generates modular explanations for both visual and linguistic inputs, providing insights into how the model leverages cross-modal interactions. We evaluate MME-XAI on the VQA-X and Visual Grounding datasets, demonstrating significant improvements in explanation quality and fidelity compared to existing methods.