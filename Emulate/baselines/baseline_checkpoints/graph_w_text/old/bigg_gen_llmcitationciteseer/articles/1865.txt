Few-shot learning in natural language processing (NLP) remains a challenging task due to the limited availability of labeled data. This paper introduces a novel hierarchical attention network (HAN) architecture that leverages both word-level and sentence-level attention to capture complex contextual relationships in text data. Our approach enables the model to adapt to new classes with only a few examples, achieving state-of-the-art performance on several benchmark datasets. We also provide an in-depth analysis of the attention mechanisms, demonstrating their effectiveness in capturing relevant semantic information.