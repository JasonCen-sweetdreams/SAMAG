Multimodal sentiment analysis on social media platforms is a challenging task due to the complexity of integrating and processing heterogeneous data from different modalities. This paper presents a novel hierarchical attention network (HAN) that leverages the strengths of both visual and textual features to improve sentiment analysis performance. Our proposed HAN model employs a hierarchical attention mechanism to selectively focus on relevant regions in images and words in text, enabling the model to capture nuanced sentiment expressions. Experimental results on a large-scale multimodal dataset demonstrate the superiority of our approach over state-of-the-art methods, achieving a significant improvement in sentiment classification accuracy.