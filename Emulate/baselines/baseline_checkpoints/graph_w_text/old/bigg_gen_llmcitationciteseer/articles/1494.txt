This paper introduces Hierarchical Attention Networks (HAN) for explainable dialogue systems. HAN leverages multi-hop attention mechanisms to selectively focus on relevant context and intent information, enabling more accurate and interpretable dialogue response generation. We propose a novel hierarchical attention framework that combines token-level and utterance-level attention to capture long-range dependencies and contextual relationships. Experimental results on the DSTC7 dataset demonstrate that HAN outperforms state-of-the-art baselines in both response accuracy and explainability, offering insights into the decision-making process of the dialogue system.