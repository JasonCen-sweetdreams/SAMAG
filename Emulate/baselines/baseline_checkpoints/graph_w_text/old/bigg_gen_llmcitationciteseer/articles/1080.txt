Time-series forecasting is a crucial task in many applications, but the lack of interpretability in traditional deep learning models hinders their adoption in high-stakes domains. This paper introduces HATSF, a hierarchical attention network that incorporates explainable components to improve both accuracy and transparency. Our approach leverages self-attention mechanisms to identify relevant input features and temporal dependencies, while also providing feature importance scores and visualizations to facilitate model understanding. Experimental results on six real-world datasets demonstrate that HATSF outperforms state-of-the-art methods in terms of forecasting performance and interpretability.