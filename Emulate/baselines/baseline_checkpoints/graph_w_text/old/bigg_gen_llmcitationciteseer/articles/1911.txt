Multimodal sentiment analysis (MSA) has become increasingly important for understanding user opinions and behaviors. However, existing MSA models often lack interpretability, making it difficult to understand the decision-making process. This paper proposes a novel Hierarchical Attention Network (HAN) for MSA, which leverages attention mechanisms to selectively focus on relevant modalities and features. Our approach integrates a feature-level attention module to extract sentiment-carrying features from text, image, and audio inputs, and a modality-level attention module to weigh the importance of each modality. Experimental results on the CMU-MOSI dataset demonstrate that our HAN model outperforms state-of-the-art MSA methods while providing interpretable insights into the sentiment analysis process.