Multi-modal search systems, which retrieve documents based on both text and image features, are becoming increasingly popular. However, existing approaches suffer from low recall and high computational costs. This paper proposes a novel deep learning-based query expansion method, 'DQE-MM', which leverages both text and image encoders to generate a rich set of expansion terms. Our approach is particularly effective in capturing complex semantic relationships between query terms and documents. Experimental results on a large-scale multi-modal dataset show that DQE-MM achieves significant improvements in retrieval accuracy and efficiency compared to state-of-the-art methods.