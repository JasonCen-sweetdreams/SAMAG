Deep neural networks are prone to errors when encountering out-of-distribution (OOD) inputs. Existing OOD detection methods often rely on auxiliary datasets, additional annotations, or complex architectures. In this paper, we propose a self-supervised learning approach, 'SSDOD', that leverages the network's own activations to detect OOD samples. By training a simple binary classifier on the difference between in-distribution and OOD activation statistics, we achieve state-of-the-art OOD detection performance on various datasets. Our method is model-agnostic, scalable, and requires no additional data or annotations.