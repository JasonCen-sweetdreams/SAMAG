Virtual reality (VR) technology has the potential to revolutionize the way people with disabilities interact with digital information. However, existing VR interfaces often rely on manual input methods that can be challenging or impossible for individuals with motor impairments. This paper presents a novel gaze-based interface that leverages eye-tracking technology to enable users to navigate and interact with virtual objects in a more natural and intuitive way. Our system uses machine learning algorithms to predict user intentions from gaze patterns and achieves an average accuracy of 92.5% in a user study with participants with and without disabilities. The results demonstrate the potential of gaze-based interfaces to enhance accessibility in VR environments.