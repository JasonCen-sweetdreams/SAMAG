Explainability is a critical component of multi-agent reinforcement learning (MARL) systems, as they operate in complex, dynamic environments. This paper presents HAN-MARL, a novel hierarchical attention network framework that learns to identify and explain the interactions between agents. HAN-MARL employs a hierarchical attention mechanism to model the relationships between agents, allowing it to selectively focus on the most relevant interactions. We evaluate HAN-MARL on a range of MARL benchmarks, demonstrating improved performance and interpretability compared to state-of-the-art methods.