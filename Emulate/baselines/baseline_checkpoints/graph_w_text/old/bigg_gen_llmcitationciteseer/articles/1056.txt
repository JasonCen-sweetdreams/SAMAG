Explainability in multi-agent reinforcement learning (MARL) is crucial for real-world applications. However, existing methods focus on individual agents, ignoring the complex interactions between them. We propose HAN-MARL, a hierarchical attention network that models agent relationships and provides interpretable explanations for joint policy decisions. HAN-MARL learns to focus on critical agents and their interactions, improving cooperation and overall team performance. We demonstrate the effectiveness of HAN-MARL in a variety of MARL environments, including a real-world autonomous driving scenario.