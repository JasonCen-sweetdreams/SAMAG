Deep neural networks (DNNs) are vulnerable to adversarial attacks, which can have severe consequences in safety-critical applications. This paper proposes a Bayesian neural network (BNN) framework to analyze the robustness of DNNs against such attacks. We develop a novel method to approximate the posterior distribution of the DNN's weights using a BNN, enabling the quantification of uncertainty in the model's predictions. Our approach allows for the detection of adversarial examples and the estimation of the model's robustness to different types of attacks. Experimental results on several benchmark datasets demonstrate the effectiveness of our method in identifying vulnerable regions of the input space and improving the robustness of DNNs.