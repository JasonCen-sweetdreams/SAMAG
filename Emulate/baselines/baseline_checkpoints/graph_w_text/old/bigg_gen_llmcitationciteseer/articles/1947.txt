Query expansion is a crucial step in ad-hoc retrieval, but traditional methods often struggle with vocabulary mismatch and semantic ambiguity. This paper proposes a novel deep learning approach, 'NeuroQE', which leverages pre-trained language models to generate context-aware query expansions. By fine-tuning a BERT-based encoder-decoder architecture on a large-scale retrieval dataset, NeuroQE learns to capture subtle semantic relationships between query terms and relevant documents. Experimental results on several benchmark collections demonstrate significant improvements in retrieval effectiveness over state-of-the-art query expansion techniques.