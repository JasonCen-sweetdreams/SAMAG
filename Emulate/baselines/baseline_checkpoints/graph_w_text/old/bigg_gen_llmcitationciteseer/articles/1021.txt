Multimodal emotion recognition has garnered significant attention in recent years, but existing approaches often lack interpretability. This paper presents a novel Hierarchical Attention Network (HAN) architecture that leverages attention mechanisms to selectively focus on relevant modalities and features. Our approach achieves state-of-the-art performance on the CMU-MOSEI dataset, while providing visual and textual explanations for the predicted emotions. We demonstrate the efficacy of HAN through extensive experiments and visualizations, showcasing its potential for real-world affective computing applications.