Emotion recognition from multimodal inputs, such as speech, text, and facial expressions, remains a challenging task. We propose a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and regions of interest. Our architecture consists of modality-specific attention layers followed by a hierarchical fusion mechanism, enabling the model to capture complex cross-modal interactions. Experimental results on the Multimodal Emotion Recognition dataset demonstrate the effectiveness of our approach, achieving state-of-the-art performance with improved robustness to noisy or missing modalities.