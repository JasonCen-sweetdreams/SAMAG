Multimodal databases, comprising images, videos, and text, pose significant challenges for efficient information retrieval. This paper presents a novel hierarchical query expansion (HQE) approach that leverages the complementary strengths of visual and textual features to improve search performance. HQE iteratively refines the query representation by incorporating relevance feedback from both modalities, adaptively adjusting the weights of visual and textual features to optimize retrieval accuracy. Experiments on a large-scale multimodal dataset demonstrate that HQE outperforms state-of-the-art query expansion methods, achieving a 25% increase in mean average precision and a 30% reduction in query latency.