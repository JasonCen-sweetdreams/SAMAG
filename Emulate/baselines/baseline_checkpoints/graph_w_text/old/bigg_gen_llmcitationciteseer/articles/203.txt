Conversational agents are increasingly used in various applications, but their ability to recognize and respond to emotions is limited. This paper proposes a novel multi-modal attention fusion framework, 'EMA', which leverages textual, acoustic, and visual cues to improve emotion recognition in conversational agents. EMA uses a hierarchical attention mechanism to selectively focus on relevant modalities and features, providing explainable emotion recognition. Our experiments on a large-scale conversational dataset show that EMA outperforms state-of-the-art methods in emotion recognition accuracy and provides insightful visualizations of the attention weights.