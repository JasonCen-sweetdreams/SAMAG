Knowledge graph embedding (KGE) has emerged as a crucial technique for link prediction tasks. However, most existing KGE methods struggle to capture complex relationships between entities. This paper presents a novel hierarchical attention-based KGE approach, 'HATKE', which leverages the hierarchical structure of knowledge graphs to model multi-relational dependencies. HATKE learns entity and relation embeddings by jointly optimizing a hierarchical attention mechanism and a translational distance metric. Experimental results on multiple benchmark datasets demonstrate that HATKE outperforms state-of-the-art KGE methods in terms of link prediction accuracy and computational efficiency.