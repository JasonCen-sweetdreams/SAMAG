Emotion recognition from multi-modal inputs, such as audio, video, and text, is a challenging task due to the diversity of emotional cues and the complexity of human emotions. This paper proposes a novel hierarchical attention network (HAN) architecture that effectively fuses and weights features from different modalities. Our HAN model consists of a hierarchical structure of attention layers, which learn to selectively focus on relevant modalities and features. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods in terms of recognition accuracy and computational efficiency, making it suitable for real-world applications.