This paper presents a novel multimodal framework for recognizing users' affective states in human-computer interaction. Our approach combines computer vision, speech recognition, and physiological signal processing to detect and classify emotional responses in real-time. We introduce a new dataset, 'EmoHCI', which includes annotated multimodal recordings of users interacting with a virtual agent. Experimental results demonstrate that our approach outperforms state-of-the-art methods in recognizing emotional states, with an average F1-score of 0.92. The proposed framework has implications for designing more empathetic and personalized interfaces in various applications, including virtual assistants, gaming, and healthcare.