Dense retrieval has emerged as a powerful paradigm for document ranking, but its computational overhead hinders its adoption in real-world search systems. This paper presents a novel approach, 'QADR', which adaptively selects a subset of relevant documents for dense retrieval based on the query context. We propose a lightweight query encoder that predicts the relevance of documents and prunes the retrieval set, reducing the computational cost by up to 75%. Our experiments on the MS MARCO dataset demonstrate that QADR achieves comparable ranking performance to state-of-the-art dense retrieval models while significantly improving query latency.