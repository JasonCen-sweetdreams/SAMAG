Neural search engines have gained popularity due to their ability to capture semantic relationships between queries and documents. However, they often struggle with lexical gaps and limited query context. This paper proposes a novel approach to query expansion using contrastive learning, which learns to differentiate between relevant and irrelevant documents in a self-supervised manner. Our method, dubbed 'ConEx', leverages a siamese neural network to learn a dense representation of queries and documents, allowing for more effective query expansion. Experimental results on several benchmark datasets demonstrate that ConEx outperforms traditional query expansion techniques, especially in scenarios with limited training data.