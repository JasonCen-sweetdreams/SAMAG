Deep reinforcement learning (DRL) has achieved remarkable success in autonomous systems, but its lack of transparency hinders trust and understanding. This paper addresses the explainability challenge in DRL by introducing a novel technique, 'RL-Explain', which generates model-agnostic explanations for DRL agents. RL-Explain leverages a combination of feature importance and saliency maps to provide insights into the decision-making process of the agent. We evaluate RL-Explain on a suite of autonomous driving tasks and demonstrate its effectiveness in improving the interpretability of DRL policies, while maintaining their performance.