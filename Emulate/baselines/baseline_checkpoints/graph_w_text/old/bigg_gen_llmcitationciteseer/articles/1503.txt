Multimodal retrieval systems often struggle to capture the nuances of user queries, leading to suboptimal performance. This paper proposes a novel latent semantic analysis (LSA) approach to enhance query expansion for multimodal retrieval. We develop a neural network-based LSA model that jointly learns textual and visual representations of the query, and demonstrate its effectiveness in improving retrieval accuracy. Experimental results on a large-scale multimodal dataset show significant improvements over state-of-the-art query expansion techniques, particularly for queries with ambiguous or polysemous terms.