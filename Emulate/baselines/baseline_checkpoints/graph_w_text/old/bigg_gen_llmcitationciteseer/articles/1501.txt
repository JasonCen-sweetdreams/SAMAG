Emotion recognition is a crucial aspect of human-computer interaction (HCI), but existing approaches often rely on single modalities, neglecting the complexity of human emotions. This paper proposes a novel hierarchical attention network (HAN) that integrates multi-modal inputs from facial expressions, speech, and physiological signals. Our HAN model learns to selectively focus on salient modalities and features, achieving state-of-the-art performance on the EmoReact dataset. We demonstrate that our approach outperforms baseline models in recognizing nuanced emotions and handling noisy or missing data.