In decentralized multi-agent systems, task allocation is a challenging problem due to the lack of centralized control and communication constraints. This paper proposes a novel reinforcement learning framework, 'DAGMA', which enables agents to learn task allocation policies in a decentralized manner. DAGMA uses a distributed actor-critic architecture, where each agent learns to allocate tasks based on its local observations and communicates with neighboring agents to achieve global optimality. We demonstrate the effectiveness of DAGMA in a simulated disaster response scenario, where agents must allocate tasks to rescue victims in a timely manner.