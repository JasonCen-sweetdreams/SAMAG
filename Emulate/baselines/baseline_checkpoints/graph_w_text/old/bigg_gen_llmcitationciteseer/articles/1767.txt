Multimodal sentiment analysis has gained significant attention in recent years, but existing approaches often suffer from high computational costs and limited scalability. This paper proposes a novel hierarchical attention network (HAN) architecture that efficiently integrates visual, textual, and acoustic features for multimodal sentiment analysis. Our HAN model leverages a hierarchical attention mechanism to selectively focus on relevant modalities and features, reducing the dimensionality of the input space and improving computational efficiency. Experimental results on the CMU-MOSI dataset demonstrate that our approach outperforms state-of-the-art methods in terms of accuracy and speed, achieving a 2x speedup while maintaining a 95% accuracy rate.