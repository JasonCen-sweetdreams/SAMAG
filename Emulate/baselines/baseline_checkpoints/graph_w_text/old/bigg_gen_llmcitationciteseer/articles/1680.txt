This paper proposes a novel distributed multi-agent reinforcement learning (MARL) framework that leverages asynchronous experience sharing to accelerate learning in complex, decentralized environments. Our approach, dubbed 'AsyncMAx', enables agents to independently explore and learn from their local experiences while periodically sharing knowledge with neighboring agents. We derive a novel asynchronous update rule that ensures convergence to the optimal policy in the presence of communication delays and partial observability. Experimental results on a range of cooperative and competitive MARL benchmarks demonstrate that AsyncMAx outperforms existing synchronous and asynchronous MARL methods in terms of learning speed and overall performance.