Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a challenging task, particularly when it comes to providing interpretable results. This paper introduces HAN-EMO, a novel hierarchical attention network that leverages the complementary information from different modalities to recognize emotions. HAN-EMO uses a hierarchical fusion mechanism that adaptively weights the importance of each modality based on the input context, and incorporates an attention module to highlight the most informative regions in each modality. Experimental results on the IEMOCAP dataset demonstrate that HAN-EMO outperforms existing state-of-the-art methods in terms of recognition accuracy and provides meaningful visualizations of the emotion recognition process.