Cooperation among multiple agents is crucial in many real-world applications, such as autonomous driving, smart grids, and surveillance systems. However, the lack of transparency in existing multi-agent cooperation models hinders their adoption in high-stakes scenarios. This paper introduces Hierarchical Attention Networks (HANs), a novel framework that enables explainable cooperation among agents. HANs employ a hierarchical attention mechanism to selectively focus on relevant agents and their interactions, allowing for interpretable decision-making. We evaluate HANs on a suite of benchmark problems, demonstrating improved cooperation outcomes and enhanced explainability compared to state-of-the-art methods.