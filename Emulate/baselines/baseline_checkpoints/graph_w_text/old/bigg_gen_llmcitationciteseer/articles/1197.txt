Visual Question Answering (VQA) models often rely on complex multimodal fusion strategies to integrate visual and textual features. However, these models lack transparency, making it challenging to understand their decision-making processes. This paper proposes a novel Multi-Modal Attention Fusion (MMAF) approach that leverages modular attention mechanisms to selectively weigh and combine visual and textual features. Our method enables explainable VQA by generating visual attention maps and textual rationales that provide insights into the model's reasoning process. Experimental results on the VQA 2.0 dataset demonstrate that MMAF achieves state-of-the-art performance while providing interpretable outputs.