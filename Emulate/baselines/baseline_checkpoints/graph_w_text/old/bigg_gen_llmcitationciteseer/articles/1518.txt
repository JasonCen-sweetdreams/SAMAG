Cloud computing platforms face the challenge of efficiently allocating resources to meet dynamic workload demands. This paper presents a Deep Hierarchical Reinforcement Learning (DHRL) framework that learns to optimize resource allocation in cloud computing environments. Our approach employs a hierarchical structure of deep Q-networks to model the complex interactions between resource allocation and workload dynamics. Experimental results on a real-world cloud dataset demonstrate that DHRL outperforms state-of-the-art methods in terms of resource utilization, response time, and energy efficiency, while adapting to changing workload patterns.