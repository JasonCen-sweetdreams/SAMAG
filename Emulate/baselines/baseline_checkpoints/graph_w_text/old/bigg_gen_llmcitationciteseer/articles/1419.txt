Cross-lingual information retrieval (CLIR) is an essential task in modern search engines, enabling users to retrieve relevant documents across language boundaries. This paper proposes a novel neural ranking model, 'MonoLingua', that leverages multilingual BERT-based embeddings to learn a shared representation space for documents and queries across languages. We experiment with a large-scale CLIR dataset and demonstrate that MonoLingua outperforms state-of-the-art machine translation-based approaches, achieving a 12% improvement in mean average precision. Furthermore, we show that MonoLingua can be easily adapted to new languages with minimal additional training data.