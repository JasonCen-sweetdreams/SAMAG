As data volumes and velocities continue to increase, real-time analytics in distributed database systems become increasingly challenging. This paper proposes an adaptive query optimization framework, 'AQOR', that leverages machine learning to dynamically adjust query plans based on changing data distributions and system workloads. AQOR incorporates a novel cost model that captures the trade-offs between query latency, data freshness, and resource utilization. Experimental results on a real-world IoT dataset demonstrate that AQOR outperforms state-of-the-art query optimizers by up to 30% in terms of query latency and 25% in terms of resource efficiency.