Emotion recognition from multi-modal data, such as speech, text, and vision, remains a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel hierarchical attention network (HAN) framework that learns to selectively focus on relevant modalities and temporal segments to improve emotion recognition accuracy. Our HAN model consists of a multi-modal fusion layer, followed by a hierarchical attention mechanism that captures both local and global contextual information. Experimental results on three benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal data.