Multimodal sentiment analysis aims to capture sentiment from multiple sources, such as text, images, and audio. Existing approaches often rely on early or late fusion, which may not effectively capture complex interactions between modalities. This paper proposes a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and their interactions. Our HAN model achieves state-of-the-art performance on two benchmark datasets, outperforming existing multimodal fusion methods. We also provide insights into the attention weights, revealing interesting patterns of modality interactions that can inform future research.