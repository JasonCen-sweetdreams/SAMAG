Voice assistants have become ubiquitous, but their interactions can perpetuate cognitive biases, exacerbating social inequalities. This paper presents a multimodal approach to designing inclusive voice assistants that mitigate biases. We propose a framework that integrates natural language processing, computer vision, and machine learning to detect and adapt to users' cognitive biases. Our approach involves training voice assistants to recognize and respond to subtle cues, such as tone, facial expressions, and linguistic patterns. A user study with 100 participants demonstrates significant reductions in biased responses and improved user satisfaction. Our findings have implications for designing more inclusive and socially responsible AI systems.