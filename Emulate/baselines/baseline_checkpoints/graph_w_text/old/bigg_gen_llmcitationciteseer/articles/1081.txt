Virtual reality (VR) systems rely heavily on user input, but traditional controllers can be cumbersome and detract from the immersive experience. This paper explores the potential of gaze-based interaction as a natural and intuitive alternative. We conducted a user study to analyze eye movement patterns in VR and developed a machine learning model to predict user intentions from gaze data. Our results show that our approach can accurately detect and respond to user gestures, such as selections and navigation, with a mean accuracy of 92.5%. We discuss the implications of our findings for the design of more intuitive and accessible VR interfaces.