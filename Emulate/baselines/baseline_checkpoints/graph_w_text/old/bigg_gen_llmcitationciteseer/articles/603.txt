Multimodal search systems, which retrieve relevant images and text, are becoming increasingly popular. However, query expansion remains a significant challenge in these systems. This paper proposes a novel deep reinforcement learning framework, 'MRL-Expander', which learns to select the most informative expansion terms for a given multimodal query. MRL-Expander leverages a combination of visual and textual features to model the context and relevance of potential expansion terms. Experimental results on a large-scale multimodal dataset show that MRL-Expander outperforms state-of-the-art query expansion methods, achieving a 15% relative improvement in mean average precision.