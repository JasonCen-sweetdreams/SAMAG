Emotion recognition from multi-modal data (e.g., speech, text, vision) remains a challenging task, particularly in real-world settings where data is noisy and incomplete. We propose a hierarchical attention network (HAN) that adaptively weighs and combines modalities to improve recognition accuracy and efficiency. Our HAN model consists of three stages: intra-modality attention, inter-modality attention, and decision-level fusion. Experimental results on the IEMOCAP dataset show that our approach outperforms state-of-the-art methods in terms of recognition accuracy and computational efficiency, making it suitable for real-time emotion recognition applications.