Emotion recognition in human-robot interaction (HRI) is crucial for developing empathetic robots. However, existing approaches struggle to integrate multiple modalities (e.g., speech, vision, and physiological signals) effectively. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that fuses modalities at multiple levels, capturing both intra- and inter-modality relationships. Our experiments on a large HRI dataset demonstrate that HAN outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an F1-score of 0.92. We also analyze the attention weights to provide insights into the importance of each modality in emotion recognition.