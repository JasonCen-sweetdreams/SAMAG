Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a challenging task due to the complexity of human emotions and the varying importance of different modalities. This paper proposes a novel hierarchical attention network (HAN) that leverages both intra-modal and inter-modal attention mechanisms to selectively focus on the most relevant features and modalities. Experimental results on the CMU-MOSEI dataset demonstrate that our HAN model outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an average F1-score of 0.83 across six emotions.