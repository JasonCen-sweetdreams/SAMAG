Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) remains a challenging task due to the complexity of human emotions and the variability of input data. This paper proposes a novel Hierarchical Attention Network (HAN) that effectively fuses information from different modalities and captures subtle emotional cues. Our HAN model consists of multiple attention layers that selectively focus on relevant features at different levels of abstraction, enabling efficient and accurate emotion recognition. Experimental results on the IEMOCAP and SEMAINE datasets demonstrate the superiority of our approach over state-of-the-art methods, achieving improved recognition rates and reduced computational costs.