Conversational agents struggle to balance task-oriented goals with engaging user interactions. We propose a hierarchical reinforcement learning framework, 'HierTask', that leverages a two-level policy to optimize both task completion and dialogue coherence. The upper level policy selects a task-oriented action, while the lower level policy generates a natural language response. We demonstrate that HierTask outperforms existing methods in task success rate and user satisfaction on a popular conversational dataset. Moreover, our approach enables agents to adapt to diverse user personas and dialogue styles.