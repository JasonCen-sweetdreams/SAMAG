Explainability is crucial in multi-agent systems, where autonomous decision-making can have significant consequences. This paper proposes a novel hierarchical attention network (HAN) framework for interpretable decision-making in multi-agent settings. Our approach aggregates agent-level features using a hierarchical attention mechanism, enabling the identification of influential agents and their contributions to the collective decision. We evaluate our HAN model on a real-world autonomous vehicle dataset, demonstrating improved explainability and decision accuracy compared to baseline methods. The proposed framework has significant implications for real-world applications, such as autonomous transportation and smart cities.