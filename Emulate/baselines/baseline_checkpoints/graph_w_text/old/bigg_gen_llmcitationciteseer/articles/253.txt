Emotional intelligence (EI) is a crucial aspect of human-AI interaction. We propose a novel hierarchical graph attention network (HGAT) for multi-modal EI, which integrates facial expression, speech, and text features to accurately recognize emotions. HGAT employs a hierarchical graph structure to model the complex relationships between modalities, and attention mechanisms to selectively focus on relevant features. Experimental results on a large-scale emotional intelligence dataset demonstrate that HGAT outperforms state-of-the-art methods, achieving an average F1-score of 0.92 across six emotions. Our approach has potential applications in affective computing, human-computer interaction, and AI-driven mental health diagnosis.