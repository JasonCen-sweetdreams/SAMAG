Graph neural networks (GNNs) have achieved state-of-the-art performance in node classification tasks. However, their computational complexity and memory requirements grow rapidly with the size of the input graph. This paper introduces HiGAT, a novel hierarchical graph attention network that leverages a multi-resolution graph representation to reduce computational costs while maintaining accuracy. HiGAT uses a hierarchical clustering approach to group nodes and apply attention mechanisms at multiple scales, resulting in significant speedups and memory savings. Experimental results on several benchmark datasets demonstrate the effectiveness of HiGAT in classifying nodes in large-scale graphs.