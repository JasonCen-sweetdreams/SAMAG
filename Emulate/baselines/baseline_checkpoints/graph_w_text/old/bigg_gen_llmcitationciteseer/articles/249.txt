Sentiment analysis in human-computer dialogue systems remains a challenging task, particularly when dealing with multimodal inputs (e.g., text, speech, and vision). This paper proposes a novel Hierarchical Attention Network (HAN) architecture that captures nuanced emotional cues from diverse modalities. Our HAN model employs modality-specific attention mechanisms to selectively focus on relevant features, followed by a hierarchical fusion strategy to integrate multimodal representations. Experimental results on a benchmark emotion-centric dialogue dataset demonstrate improved sentiment analysis performance compared to state-of-the-art approaches, showcasing the potential of our HAN architecture for enhancing affective computing in dialogue systems.