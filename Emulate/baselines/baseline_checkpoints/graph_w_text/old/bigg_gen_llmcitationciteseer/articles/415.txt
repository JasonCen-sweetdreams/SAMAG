Explainability is a critical concern in multi-agent reinforcement learning (MARL), where complex interactions between agents can lead to opaque decision-making processes. This paper proposes a novel hierarchical attention network (HAN) architecture that enables interpretable MARL policies. Our approach integrates attention mechanisms at both the intra-agent and inter-agent levels, allowing the model to selectively focus on relevant features and agent interactions. We evaluate HAN on a series of cooperative and competitive MARL benchmarks, demonstrating improved interpretability and robustness compared to state-of-the-art methods.