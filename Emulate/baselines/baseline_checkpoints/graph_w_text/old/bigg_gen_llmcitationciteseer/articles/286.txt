Multi-agent systems are increasingly deployed in real-world applications, but their decision-making processes often lack transparency. This paper proposes a Hierarchical Attention Network (HAN) framework for explainable multi-agent decision-making. HANs learn to selectively focus on relevant agents and their interactions, generating interpretable attention weights that reveal the reasoning behind collective decisions. We evaluate our approach using a simulated robotic soccer environment, demonstrating improved performance and explainability over baseline methods. Our results have implications for the development of transparent and accountable multi-agent systems in various domains.