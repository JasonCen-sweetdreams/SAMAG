Virtual reality (VR) systems often rely on manual controllers or voice commands, which can be cumbersome and limiting. This paper presents EyeGaze, a novel multimodal interface that leverages eye-tracking and machine learning to enable hands-free interaction in VR. Our approach combines convolutional neural networks (CNNs) with Gaussian mixture models to detect and classify gaze patterns, allowing users to select virtual objects and navigate 3D environments with precision and ease. A user study demonstrates that EyeGaze outperforms traditional interfaces in terms of task completion time and user satisfaction, opening up new possibilities for accessible and immersive VR experiences.