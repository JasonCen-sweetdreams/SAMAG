Multimodal sentiment analysis is a crucial task in HCI, enabling computers to understand human emotions and behaviors. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that jointly models textual, visual, and acoustic modalities. Our HAN framework consists of modality-specific attention modules and a hierarchical fusion mechanism, which adaptively weights the contributions of each modality to sentiment prediction. Experimental results on two benchmark datasets demonstrate that our approach outperforms state-of-the-art multimodal fusion techniques, achieving significant improvements in sentiment accuracy and F1-score.