Cooperative task allocation is a challenging problem in multi-agent systems, where agents must learn to allocate tasks efficiently while considering the actions of other agents. This paper proposes a decentralized multi-agent reinforcement learning framework, 'Coop-MARL', which enables agents to learn cooperative policies without relying on centralized controllers. We introduce a novel communication protocol that allows agents to share information and coordinate their actions in real-time. Experimental results on a simulated robotic warehouse scenario demonstrate that Coop-MARL outperforms traditional centralized approaches in terms of task completion time and resource utilization.