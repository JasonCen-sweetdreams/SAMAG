Emotion recognition from multimodal inputs, such as facial expressions, speech, and text, is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates modality-specific attention mechanisms to selectively focus on relevant features from each modality. Our approach achieves state-of-the-art performance on the benchmark Multimodal Emotion Recognition (MER) dataset, outperforming existing fusion-based methods by 12.3% in terms of weighted F1-score. We also provide insights into the learned attention patterns, highlighting the importance of cross-modal interactions in emotion recognition.