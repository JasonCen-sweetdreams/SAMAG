Gesture recognition systems have become ubiquitous in human-computer interaction, but their performance often degrades for users with motor impairments. This paper presents a novel framework for designing inclusive gesture recognition systems that can accommodate users with varying levels of motor abilities. We propose a multimodal approach that combines computer vision, machine learning, and human-centered design principles to recognize gestures from users with motor impairments. Our system is evaluated through a user study with 20 participants, demonstrating an average recognition accuracy of 92.5% and high user satisfaction. The results provide insights into the design of accessible HCI systems that can improve the quality of life for individuals with motor impairments.