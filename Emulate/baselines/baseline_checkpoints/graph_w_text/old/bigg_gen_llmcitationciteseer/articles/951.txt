Emotion recognition is a crucial aspect of human-computer interaction, enabling more empathetic and personalized systems. This paper proposes a novel hierarchical attention network (HAN) architecture for multi-modal emotion recognition, fusing facial expressions, speech, and physiological signals. Our HAN model learns to selectively focus on relevant modalities and temporal segments, improving robustness to noisy or incomplete data. Experimental results on a large-scale multimodal dataset demonstrate state-of-the-art performance, with significant improvements in recognizing emotions in the wild.