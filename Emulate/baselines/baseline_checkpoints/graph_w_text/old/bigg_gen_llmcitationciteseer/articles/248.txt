Emotion recognition from multi-modal data (e.g., speech, text, and vision) is a crucial AI application. However, existing approaches often lack transparency and interpretability. This paper presents a novel Hierarchical Attention Network (HAN) architecture that not only improves recognition accuracy but also provides explainable results. Our HAN model learns to selectively focus on relevant modalities, features, and time-steps, generating attention weights that reveal the importance of each input component. We conduct extensive experiments on three benchmark datasets, demonstrating the effectiveness of our approach and its potential for real-world affective computing applications.