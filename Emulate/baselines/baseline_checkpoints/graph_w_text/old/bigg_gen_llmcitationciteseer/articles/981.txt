Multi-agent reinforcement learning (MARL) has shown great promise in solving complex problems, but suffers from the curse of dimensionality as the number of agents grows. This paper proposes a novel approach that leverages graph attention networks (GATs) to model inter-agent relationships and facilitate efficient learning. Our method, 'GAT-MARL', learns a sparse attention mask that focuses on relevant agents, reducing the computational complexity of value function estimation. Experimental results on a variety of MARL benchmarks demonstrate that GAT-MARL outperforms state-of-the-art methods in terms of learning speed and convergence rate.