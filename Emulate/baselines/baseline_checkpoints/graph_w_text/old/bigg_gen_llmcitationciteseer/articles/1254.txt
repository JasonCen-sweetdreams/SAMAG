Multimodal neural machine translation (MNMT) has shown promising results in incorporating visual context into machine translation. However, existing MNMT models are vulnerable to adversarial attacks, which can significantly degrade their performance. This paper proposes a novel adversarial training framework for MNMT, which incorporates visual context into the adversarial perturbation generation process. Our approach, called VisAdv, leverages the visual features extracted from the source image to generate targeted perturbations that attack the model's visual attention mechanism. Experimental results on the Multi30K dataset show that VisAdv improves the robustness of MNMT models against adversarial attacks, achieving state-of-the-art results in terms of BLEU score and visual attention accuracy.