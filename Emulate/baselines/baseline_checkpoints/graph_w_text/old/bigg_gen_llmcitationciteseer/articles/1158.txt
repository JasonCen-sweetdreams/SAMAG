Multi-modal sentiment analysis has become increasingly important for businesses to understand customer opinions from various sources. While deep learning models have achieved high accuracy, they often lack interpretability. This paper proposes a novel hierarchical attention network (HAN) that integrates visual and textual features to analyze sentiment from images and text. We introduce a modular attention mechanism that learns to focus on relevant regions of the image and corresponding words in the text. Our experiments on a large-scale dataset demonstrate that HAN outperforms state-of-the-art models in terms of accuracy and provides insightful visualizations for explainability.