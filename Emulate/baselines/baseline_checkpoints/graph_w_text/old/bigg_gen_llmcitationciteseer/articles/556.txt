Deep reinforcement learning (DRL) has achieved remarkable success in various applications, but its vulnerability to adversarial attacks remains a significant concern. This paper presents a novel graph-based anomaly analysis approach to detect and mitigate adversarial attacks in DRL systems. By modeling the agent's behavior as a graph, we identify subtle patterns indicative of adversarial perturbations. Our method, called GRAAD, leverages graph-based features and a one-class SVM to detect anomalies in real-time. Experimental results on Atari games and robotic control tasks demonstrate GRAAD's effectiveness in detecting a wide range of attacks while maintaining the agent's performance.