Explainability is a crucial aspect in multi-agent reinforcement learning (MARL) as it enables understanding agent behaviors and decision-making processes. However, existing MARL methods often lack interpretability, hindering their adoption in real-world applications. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to explain agent policies in MARL. HAN employs a hierarchical structure to model agent relationships and attention mechanisms to focus on relevant interactions. Experimental results on a variety of MARL benchmarks demonstrate that HAN achieves state-of-the-art performance while providing insights into agent decision-making processes.