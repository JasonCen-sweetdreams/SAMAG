Emotion recognition from multi-modal data (e.g., speech, text, vision) remains a challenging task, particularly when seeking to provide interpretable results. We introduce Hierarchical Attention Networks (HANs), a novel deep learning architecture that leverages attention mechanisms to selectively focus on relevant modalities and features. HANs outperform state-of-the-art methods on the CMU-MOSEI dataset, achieving a 12.5% improvement in emotion recognition accuracy. Furthermore, our model provides visual explanations of the attention weights, enabling the identification of salient features driving emotional intensity predictions.