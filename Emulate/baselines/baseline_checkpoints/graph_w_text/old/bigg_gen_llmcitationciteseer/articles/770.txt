With the proliferation of multimodal data, traditional indexing and retrieval methods struggle to efficiently handle the complexities of heterogeneous data. This paper proposes a novel approach, 'HGN-Index', which leverages hierarchical graph neural networks to learn a unified representation of multimodal data. Our method exploits the structural relationships within and across modalities, enabling fast and accurate querying. Experimental results on a large-scale dataset demonstrate that HGN-Index outperforms state-of-the-art methods in terms of retrieval efficiency and effectiveness, while reducing the storage requirements by up to 30%.