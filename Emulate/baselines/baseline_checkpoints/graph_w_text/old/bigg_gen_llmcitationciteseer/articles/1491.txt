Multimodal sentiment analysis, which involves analyzing sentiment from text, images, and videos, is a challenging task due to the inherent complexities of each modality. This paper proposes a novel hierarchical attention network (HAN) that learns to selectively focus on relevant multimodal features and their interactions. Our approach leverages a hierarchical structure to model the relationships between modalities and incorporates an attention mechanism to generate interpretable explanations for the predicted sentiment labels. Experimental results on two benchmark datasets demonstrate the effectiveness of our HAN model in improving sentiment analysis performance and providing insightful explanations.