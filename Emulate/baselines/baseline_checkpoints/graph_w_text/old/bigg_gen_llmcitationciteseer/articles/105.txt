Query expansion is a crucial component in ad-hoc retrieval systems, as it helps to disambiguate user queries and improve retrieval accuracy. This paper proposes a novel reinforcement learning-based approach, 'RL-QE', to efficiently expand queries. RL-QE models the query expansion process as a Markov decision process, where an agent learns to select expansion terms that maximize the expected retrieval performance. We introduce a reward function that combines relevance feedback with semantic similarity measures, enabling the agent to adapt to diverse query topics. Experimental results on the TREC Ad-Hoc retrieval benchmark demonstrate that RL-QE outperforms state-of-the-art query expansion methods, achieving a significant improvement in mean average precision.