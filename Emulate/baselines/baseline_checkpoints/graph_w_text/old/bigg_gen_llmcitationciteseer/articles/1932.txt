Few-shot learning has gained significant attention in recent years, but it remains a challenging problem, especially when labeled data is scarce. This paper introduces a hierarchical attention network (HAN) that leverages both local and global feature representations to improve few-shot learning performance. Our approach uses a novel attention mechanism that adaptively weights the importance of different features and instances, allowing the model to focus on the most informative parts of the input data. Experimental results on benchmark datasets demonstrate that HAN outperforms state-of-the-art methods, achieving higher accuracy with limited labeled data.