This paper introduces HAN-MMD, a novel hierarchical attention network architecture for multi-modal dialogue systems that incorporates visual, textual, and acoustic cues. Unlike existing approaches that rely on black-box models, HAN-MMD provides explainable dialogue responses by learning to attend to relevant input modalities and generating attention-based visualizations. We evaluate HAN-MMD on a benchmark dataset and demonstrate significant improvements in response accuracy, fluency, and user satisfaction compared to state-of-the-art models. Our approach has potential applications in human-robot interaction, customer service chatbots, and virtual assistants.