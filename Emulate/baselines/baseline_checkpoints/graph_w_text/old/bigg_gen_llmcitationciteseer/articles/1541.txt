This paper introduces a novel hierarchical attention network (HAN) architecture for multi-modal sentiment analysis, capable of integrating and exploiting the complementary information present in text, images, and videos. Our approach employs a stacked attention mechanism to selectively focus on relevant modalities and features, leading to improved sentiment classification performance. We conduct extensive experiments on three benchmark datasets, demonstrating the effectiveness of our proposed HAN model in capturing nuanced sentiment expressions and outperforming state-of-the-art uni-modal and early-fusion approaches.