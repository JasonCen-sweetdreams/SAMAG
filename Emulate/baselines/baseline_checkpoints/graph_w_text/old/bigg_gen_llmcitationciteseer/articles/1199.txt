E-commerce search engines rely heavily on accurate retrieval models to facilitate user purchases. While text-based retrieval models have shown promising results, they often neglect the significance of visual and audio features in product search. This paper proposes a novel multimodal ranking model, 'Multimodal Transformer Ranking' (MTR), which leverages deep neural networks to fuse textual, visual, and audio features for product retrieval. We introduce a novel attention mechanism that adaptively weighs modalities based on their relevance to the query. Experimental results on a large-scale e-commerce dataset demonstrate MTR's superiority over state-of-the-art text-based ranking models, achieving a 15% improvement in retrieval accuracy.