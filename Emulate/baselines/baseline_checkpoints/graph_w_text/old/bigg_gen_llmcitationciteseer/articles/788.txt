Multivariate time series forecasting is a crucial task in many real-world applications. Modeling temporal dependencies between variables remains a significant challenge, particularly when dealing with long-range dependencies and high-dimensional data. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to capture complex temporal dependencies by recursively applying attention mechanisms at multiple scales. Our approach reduces the computational complexity of traditional attention-based models while improving forecasting accuracy. Experimental results on several benchmark datasets demonstrate the effectiveness of HAN in capturing long-range dependencies and outperforming state-of-the-art methods.