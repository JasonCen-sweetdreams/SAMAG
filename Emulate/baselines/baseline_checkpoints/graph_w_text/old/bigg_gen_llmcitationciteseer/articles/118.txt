Virtual reality (VR) systems require intuitive and efficient interaction mechanisms to provide an immersive user experience. This paper presents a novel gaze-based interaction framework, 'GazeVR', which leverages deep learning to accurately predict user intentions from eye movement data. Our approach employs a convolutional neural network (CNN) to classify gaze patterns into distinct interaction commands, such as selection, manipulation, and navigation. We evaluate GazeVR on a custom-built VR platform and demonstrate significant improvements in interaction accuracy and user satisfaction compared to traditional gaze-based interfaces.