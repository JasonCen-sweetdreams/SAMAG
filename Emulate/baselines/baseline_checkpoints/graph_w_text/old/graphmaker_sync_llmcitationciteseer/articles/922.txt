Emotion recognition from multi-modal inputs, such as speech, text, and vision, remains a challenging problem due to the complexity of human emotions and the heterogeneity of modalities. This paper presents a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features for emotion recognition. Our HAN model consists of modality-specific attention modules and a hierarchical fusion mechanism, which adaptively combines the outputs from each modality. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods, achieving an average F1-score improvement of 12.3% across six emotions.