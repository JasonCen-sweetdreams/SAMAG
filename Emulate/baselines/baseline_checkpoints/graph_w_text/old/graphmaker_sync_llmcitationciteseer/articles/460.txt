Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the inherent heterogeneity and complexity of the data. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on the most informative modalities and features. Our approach consists of two stages: modality-level attention and feature-level attention. Experimental results on the CMU-MOSEI dataset demonstrate that HAN outperforms state-of-the-art methods in recognizing emotions from multi-modal data, achieving an F1-score of 83.2%.