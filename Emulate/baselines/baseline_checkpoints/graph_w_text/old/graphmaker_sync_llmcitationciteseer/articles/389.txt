Sentiment analysis has become a crucial task in natural language processing. However, existing approaches often struggle to effectively fuse information from multiple modalities (e.g., text, images, and audio) and provide interpretable results. This paper proposes a hierarchical attention network (HAN) for multi-modal sentiment analysis, which learns to selectively focus on relevant input features and modalities. Our approach incorporates a novel explainability module that generates visualizations of the attention weights, enabling insight into the decision-making process. Experiments on a large-scale dataset demonstrate the efficacy of our HAN model in achieving state-of-the-art performance and providing meaningful explanations for its predictions.