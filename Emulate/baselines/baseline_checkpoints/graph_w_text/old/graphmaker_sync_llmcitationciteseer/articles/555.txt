In multi-agent systems, making decisions that are transparent and interpretable is crucial for trust and accountability. This paper presents a novel hierarchical attention network (HAN) architecture that enables explainable decision-making in multi-agent settings. Our approach leverages attention mechanisms to identify relevant interactions between agents and extract interpretable features from high-dimensional observation spaces. We evaluate the proposed HAN model on a variety of multi-agent scenarios and demonstrate its effectiveness in improving decision-making accuracy while providing insightful explanations of the decision-making process.