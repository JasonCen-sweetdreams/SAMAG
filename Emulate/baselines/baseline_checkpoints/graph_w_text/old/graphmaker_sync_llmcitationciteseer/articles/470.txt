Gaze-based interaction in virtual reality (VR) is limited by the accuracy and latency of eye-tracking systems. This paper introduces EyeGazeLens, a machine learning framework that leverages multimodal fusion of eye-tracking, head-tracking, and pupil dilation data to enhance the precision and responsiveness of gaze-based interaction. Our approach involves training a deep neural network to predict the user's intended gaze target, taking into account individual differences in eye movement patterns and VR environment complexity. Evaluations with 20 participants show that EyeGazeLens achieves a 35% reduction in gaze error and a 20% increase in interaction speed compared to state-of-the-art methods.