Deep reinforcement learning (DRL) policies are vulnerable to adversarial attacks, which can significantly impact their performance in safety-critical applications. This paper proposes a novel constrained policy optimization framework, 'AdvRO', which enhances the robustness of DRL policies against adversarial perturbations. AdvRO formulates the policy optimization problem as a constrained Markov decision process, where the constraints encode the adversarial robustness requirements. We demonstrate the effectiveness of AdvRO on several benchmark environments, showing improved robustness against both white-box and black-box attacks compared to state-of-the-art DRL algorithms.