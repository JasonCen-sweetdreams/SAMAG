Deep neural networks (DNNs) are vulnerable to adversarial attacks, which can compromise their performance and security. Existing detection methods often rely on statistical analysis of input data or model outputs, but these approaches can be evaded by sophisticated attacks. We propose a novel graph-based anomaly detection framework, 'GAD', that models the internal neural network activations as a graph and identifies anomalous patterns indicative of adversarial attacks. Experimental results on benchmark datasets demonstrate the effectiveness of GAD in detecting diverse attack types, including poisoning and evasion attacks, with high accuracy and low false positives.