Autonomous vehicles (AVs) rely on reinforcement learning (RL) to navigate complex scenarios. However, the lack of transparency in RL decision-making hinders trust and reliability. We propose a novel graph attention network (GAT) based approach, 'ExplainRL', to provide interpretable policies for AV control. By incorporating graph-based attention mechanisms, our model can selectively focus on relevant sensory inputs, resulting in improved explainability and robustness. Experimental results on a simulated AV dataset demonstrate the effectiveness of ExplainRL in balancing trade-offs between reward, safety, and interpretability.