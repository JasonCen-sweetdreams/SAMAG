Virtual reality (VR) systems often struggle to provide effective feedback to users, leading to frustration and decreased immersion. This paper presents 'GazeGuide', a novel framework that leverages eye-tracking data to adaptively adjust feedback mechanisms in real-time. Our approach integrates machine learning-based gaze prediction with a probabilistic user model to dynamically adjust the type, timing, and intensity of feedback. We demonstrate the efficacy of GazeGuide through a user study, showing significant improvements in user satisfaction, task performance, and overall VR experience.