Explainability is a crucial aspect of multi-agent decision-making, as it enables humans to understand and trust the decisions made by autonomous agents. This paper introduces a novel hierarchical attention network (HAN) architecture that generates interpretable explanations for multi-agent decision-making. Our approach integrates attention mechanisms at both the agent and system levels, allowing the model to focus on relevant information and provide contextual explanations. We evaluate our approach on a real-world autonomous driving dataset, demonstrating improved explainability and decision-making performance compared to existing methods.