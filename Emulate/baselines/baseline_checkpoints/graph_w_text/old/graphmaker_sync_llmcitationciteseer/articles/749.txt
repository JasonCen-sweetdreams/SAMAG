Explainable AI is crucial for trustworthy decision-making in multi-agent systems. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that enables transparent and interpretable decision-making in complex, dynamic environments. Our approach combines attention mechanisms with hierarchical graph neural networks to capture intricate relationships between agents and their contexts. Experimental results on a real-world autonomous vehicle dataset demonstrate that HAN outperforms state-of-the-art methods in terms of decision accuracy and interpretability, while providing insights into the decision-making process.