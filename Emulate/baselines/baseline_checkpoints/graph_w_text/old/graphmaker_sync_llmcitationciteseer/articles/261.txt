State-of-the-art question answering (QA) models rely on retrieval-based approaches, which involve selecting relevant documents from a large corpus. However, these methods often suffer from inefficient document representation and ineffective attention mechanisms. This paper proposes a novel hierarchical attention framework, 'HieraDoc', which learns to represent documents in a multi-granular manner, capturing both local and global contextual information. Our experiments on the SQuAD dataset demonstrate that HieraDoc achieves significant improvements in QA performance, outperforming existing retriever-reader architectures. We also show that our approach is more computationally efficient, making it suitable for large-scale QA applications.