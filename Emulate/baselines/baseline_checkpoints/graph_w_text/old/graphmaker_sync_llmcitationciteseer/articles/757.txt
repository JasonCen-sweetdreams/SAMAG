Multi-modal emotion recognition (MMER) has gained significant attention in recent years, but existing approaches often rely on heavy feature engineering and suffer from high computational overhead. This paper presents a novel hierarchical attention network (HAN) architecture that leverages multi-modal fusion and attention mechanisms to efficiently recognize emotions from speech, text, and visual cues. Our HAN model learns to selectively focus on relevant modalities and timestamps, reducing the need for manual feature crafting and improving recognition accuracy. Experimental results on benchmark datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance with reduced computational complexity.