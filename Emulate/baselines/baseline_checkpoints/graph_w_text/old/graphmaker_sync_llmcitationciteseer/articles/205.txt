Graph neural networks (GNNs) have achieved state-of-the-art performance in various graph-related tasks. However, their hyperparameter tuning process is often computationally expensive and requires significant domain expertise. This paper proposes a Bayesian optimization framework, 'GNN-Tune', which leverages a novel acquisition function that incorporates both the predictive uncertainty and the graph structure. Our approach significantly reduces the number of evaluations required to find optimal hyperparameters, resulting in improved performance and faster convergence. We evaluate GNN-Tune on several benchmark datasets and demonstrate its effectiveness in various GNN architectures.