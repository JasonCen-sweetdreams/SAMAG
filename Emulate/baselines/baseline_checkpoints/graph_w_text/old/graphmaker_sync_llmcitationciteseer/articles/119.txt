Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the complex relationships between modalities. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) that captures both intra-modality and inter-modality interactions. The HGAT model consists of a hierarchical graph structure, where local attention mechanisms are applied at each level to selectively focus on relevant modalities and features. We evaluate our approach on three benchmark datasets and demonstrate state-of-the-art performance in recognizing emotions from multi-modal data.