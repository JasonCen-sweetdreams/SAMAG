Graph neural networks (GNNs) have achieved state-of-the-art performance in node classification tasks, but often require large amounts of labeled data. This paper proposes a novel attention-based GNN architecture, 'AttentiveLabel', which leverages limited label information to improve model performance. We introduce a self-supervised attention mechanism that adapts to the graph structure and node features, enabling the model to focus on the most informative nodes and labels. Experimental results on several benchmark datasets demonstrate that AttentiveLabel outperforms existing GNNs in low-label regimes, achieving competitive performance with significantly fewer labeled examples.