Deep neural networks have revolutionized various applications, but their performance heavily relies on hyperparameter tuning. Bayesian optimization (BO) has emerged as a promising approach for hyperparameter tuning, but its computational cost can be prohibitively high. This paper proposes a novel BO variant, 'HyperBO+', which leverages a probabilistic memory-based approach to reduce the number of expensive function evaluations. We demonstrate the efficacy of HyperBO+ on several benchmark datasets, achieving state-of-the-art results while reducing the tuning time by up to 50% compared to existing BO methods.