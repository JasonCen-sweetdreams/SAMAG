Graph neural networks (GNNs) have shown remarkable success in various graph-based tasks, but their vulnerability to adversarial attacks raises concerns. This paper presents a comprehensive study of defense mechanisms against adversarial attacks on GNNs. We investigate the effectiveness of six state-of-the-art defense methods, including graph purification, adversarial training, and input preprocessing, on three benchmark datasets. Our experiments reveal that a combination of adversarial training and graph purification yields the most robust performance, outperforming individual defense strategies. We also analyze the trade-off between model robustness and accuracy, providing insights for practitioners.