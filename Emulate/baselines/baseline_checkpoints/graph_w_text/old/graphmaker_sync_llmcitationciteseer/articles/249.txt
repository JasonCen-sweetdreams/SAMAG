Graph neural networks (GNNs) have achieved state-of-the-art performance in node classification tasks, but suffer from scalability issues and over-smoothing problems. This paper proposes a novel hierarchical graph attention network (HGAT) that leverages attention mechanisms to selectively aggregate features from neighboring nodes at multiple scales. We demonstrate that HGAT outperforms existing GNNs on several benchmark datasets, especially in inductive settings where the model is required to generalize to unseen graphs. Furthermore, we provide theoretical insights into the expressive power of HGAT and its robustness to graph perturbations.