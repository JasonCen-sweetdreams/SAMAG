Graph neural networks (GNNs) have achieved state-of-the-art performance in node classification tasks. However, existing methods often neglect the hierarchical structure of real-world graphs, leading to suboptimal performance. This paper proposes a novel hierarchical attention-based GNN (HAGNN) that captures both local and global contextual information. Our approach introduces a recursive attention mechanism that adaptively selects relevant neighbors at each hierarchical level, allowing the model to focus on the most informative nodes. Experimental results on benchmark datasets demonstrate that HAGNN outperforms existing GNN variants by a significant margin, especially on graphs with complex hierarchical structures.