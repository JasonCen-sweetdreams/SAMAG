Explainability is crucial in multi-agent reinforcement learning (MARL) to understand the decision-making process of autonomous agents. This paper proposes a novel hierarchical attention network (HAN) framework, 'ExplainMARL', which integrates attention mechanisms at both the agent and team levels to provide transparent policy explanations. We demonstrate the effectiveness of ExplainMARL in a variety of MARL scenarios, showcasing improved explainability and interpretability without compromising policy performance. Furthermore, we provide a theoretical analysis of the attention weights, enabling the identification of critical agents and states that influence team decision-making.