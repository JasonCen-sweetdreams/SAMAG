Multimodal emotion recognition is a challenging task due to the complex relationships between different modalities such as speech, text, and vision. This paper proposes a novel hierarchical attention network (HAN) architecture that leverages the strengths of each modality to recognize emotions. Our HAN model consists of modality-specific attention modules that adaptively weight the importance of each modality, followed by a fusion layer that combines the outputs. We evaluate our approach on the Multimodal Emotion Recognition Challenge (MERC) dataset and demonstrate significant improvements over state-of-the-art multimodal fusion techniques.