Multimodal sentiment analysis, which involves analyzing text, images, and videos to understand user sentiments, is a challenging task due to the complexity of human emotions and the diversity of multimedia data. This paper proposes a novel hierarchical attention network (HAN) that effectively captures cross-modal interactions and identifies salient features contributing to sentiment decisions. Our HAN model leverages self-attention mechanisms to weigh the importance of different modalities and features, enabling explainable sentiment analysis. Experimental results on a large-scale multimodal dataset demonstrate the superiority of our approach over state-of-the-art methods in terms of accuracy and interpretability.