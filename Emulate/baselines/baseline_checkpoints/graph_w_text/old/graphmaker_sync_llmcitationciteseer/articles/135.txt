Graph Neural Networks (GNNs) have achieved state-of-the-art performance in node classification tasks. However, their vulnerability to adversarial attacks remains largely unexplored. In this paper, we propose a novel attack framework, 'GraphFool', which leverages the graph structure to perturb node features and edges. We demonstrate that GraphFool can significantly degrade the performance of popular GNN models, including Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs), on several benchmark datasets. We also investigate the robustness of GNNs against these attacks and provide insights into the design of more resilient models.