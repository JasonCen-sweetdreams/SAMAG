Deep neural networks have achieved state-of-the-art performance in various applications, but their success heavily relies on careful hyperparameter tuning. However, traditional grid search and random search methods are computationally expensive and often fail to identify optimal hyperparameters. This paper proposes a Bayesian optimization approach, namely 'BO-HT', which leverages a probabilistic surrogate model to search for optimal hyperparameters in a more efficient and effective manner. We demonstrate the superiority of BO-HT over popular hyperparameter tuning methods on several benchmark datasets, achieving improved model performance and reduced computational cost.