Virtual reality (VR) has the potential to revolutionize human-computer interaction, but current VR interfaces often lack inclusivity. This paper explores the application of embodied cognition principles to gesture-based interfaces in VR, focusing on users with motor impairments. We designed and evaluated a novel VR system that uses machine learning-based gesture recognition and adaptive feedback to facilitate intuitive interaction. Our user study with 20 participants demonstrated significant improvements in task completion time and user satisfaction compared to traditional controller-based interfaces. The results have implications for the development of more accessible and inclusive VR experiences.