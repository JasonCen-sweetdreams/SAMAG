Multi-modal sentiment analysis is a challenging task that requires effective fusion of text, image, and audio cues. We propose a novel Hierarchical Attention Graph Neural Network (HAGNN) architecture that leverages graph attention mechanisms to model relationships between modalities. Our approach adaptively weights the importance of each modality and captures complex dependencies between them. Experiments on two benchmark datasets demonstrate that HAGNN outperforms state-of-the-art methods in sentiment classification and regression tasks, particularly in the presence of noisy or incomplete data.