Deep reinforcement learning (DRL) has achieved remarkable success in various applications, but its vulnerability to adversarial attacks poses a significant threat. This paper proposes a novel approach to detect adversarial attacks in DRL systems using explainable AI (XAI) techniques. We develop a method to generate saliency maps for DRL policies, which enables the identification of perturbed states that lead to policy manipulation. Our experiments on Atari games demonstrate that the proposed approach can detect adversarial attacks with high accuracy, outperforming existing detection methods. We further analyze the interpretability of our approach, providing insights into the decision-making process of DRL agents under attack.