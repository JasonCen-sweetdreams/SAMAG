Multi-agent reinforcement learning (MARL) has shown great promise in complex decision-making scenarios, but interpretability remains a significant challenge. This paper proposes a novel hierarchical attention network (HAT) architecture that enables explainable MARL. HAT learns to selectively focus on relevant agents and their interactions, generating attention weights that provide insights into the decision-making process. We evaluate HAT on a range of MARL benchmarks, demonstrating improved performance and interpretability compared to existing methods. Our approach has the potential to facilitate trust and understanding in AI decision-making systems.