Knowledge graph embeddings have become a crucial component in various AI applications. However, existing methods often suffer from high computational complexity and limited scalability. This paper proposes a novel hierarchical graph attention network (HGAN) architecture for efficient knowledge graph embeddings. HGAN leverages a hierarchical attention mechanism to selectively focus on relevant entities and relationships, reducing the computational overhead. We evaluate HGAN on several benchmark datasets and demonstrate significant improvements in both model efficiency and embedding quality. Furthermore, we show that HGAN can be seamlessly integrated with downstream AI applications, such as question answering and recommender systems.