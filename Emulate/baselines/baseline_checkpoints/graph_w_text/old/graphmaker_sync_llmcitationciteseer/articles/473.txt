Deep learning models have shown remarkable performance in sentiment analysis tasks, but their lack of transparency hinders their adoption in high-stakes applications. This paper proposes a novel hierarchical attention network (HAN) that integrates visual, textual, and acoustic features for multi-modal sentiment analysis. We introduce a novel attention mechanism that selectively weighs modalities based on their relative importance, enabling explainable sentiment predictions. Experimental results on a large-scale multi-modal dataset demonstrate that our HAN outperforms state-of-the-art models while providing interpretable insights into the decision-making process.