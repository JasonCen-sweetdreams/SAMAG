Multimodal sentiment analysis (MSA) aims to predict sentiment from heterogeneous data, such as text, images, and audio. Existing methods often rely on early fusion or simple concatenation of features, neglecting complex relationships between modalities. This paper introduces Hierarchical Graph Attention Networks (HGAT), a novel framework that models modality interactions using a hierarchical graph structure. HGAT employs graph attention mechanisms to selectively focus on relevant modalities and their relationships, improving MSA performance. Experiments on the CMU-MOSI dataset demonstrate that HGAT outperforms state-of-the-art methods by 3.5% in terms of accuracy and 2.1% in terms of F1-score.