Gaze-based interaction has become a crucial modality in virtual reality (VR) systems, but existing methods often suffer from limitations in accuracy, robustness, and user experience. This paper presents EyeGaze, a novel multimodal fusion framework that combines eye-tracking, head-tracking, and machine learning techniques to enhance gaze-based interaction in VR. Our approach leverages a probabilistic graphical model to integrate cues from multiple sensors and modalities, providing more accurate and robust gaze estimation. A comprehensive evaluation with 30 participants demonstrates the superiority of EyeGaze over state-of-the-art methods, with significant improvements in interaction speed, accuracy, and user satisfaction.