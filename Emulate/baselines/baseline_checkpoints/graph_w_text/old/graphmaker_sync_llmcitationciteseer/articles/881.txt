Despite the success of deep learning models in medical diagnosis, their lack of transparency and interpretability hinders trust in AI-driven decision-making. This paper proposes a novel hierarchical attention network (HAN) architecture that incorporates explainability into the diagnosis process. Our HAN model utilizes a hierarchical structure to focus on relevant patient features and medical imaging regions, generating attention weights that provide insights into the diagnosis rationale. Experimental results on a large-scale disease diagnosis dataset demonstrate that our approach achieves state-of-the-art performance while providing interpretable explanations for AI-driven medical diagnosis.