Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a challenging task in affective computing. This paper presents a novel hierarchical attention network (HAN) architecture that integrates and selectively focuses on relevant modalities to improve recognition accuracy. Our HAN model consists of three stages: modality-specific attention, cross-modal attention, and hierarchical fusion. Experimental results on the IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from combined speech, text, and facial expression inputs, with an average F1-score improvement of 7.2%.