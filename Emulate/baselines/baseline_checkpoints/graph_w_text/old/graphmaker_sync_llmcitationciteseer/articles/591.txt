Recommender systems have increasingly incorporated heterogeneous data sources, including text, images, and graph structures. This paper proposes a novel graph neural network architecture, 'MultiModalGNN', that leverages the strengths of each modality to generate personalized recommendations. We introduce a novel attention mechanism that adaptively weights the contributions of different modalities based on the user's interaction history. Experimental results on a large-scale e-commerce dataset demonstrate that MultiModalGNN outperforms state-of-the-art models in terms of precision, recall, and rating prediction accuracy.