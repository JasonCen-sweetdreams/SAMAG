Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task in affective computing. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages the strengths of each modality to improve emotion recognition accuracy. Our approach uses a hierarchical attention mechanism to selectively focus on relevant modalities and features, allowing for interpretability and explainability of the model's decisions. Experimental results on the IEMOCAP dataset demonstrate that HAN outperforms state-of-the-art methods in multi-modal emotion recognition, achieving a 10% improvement in F1-score.