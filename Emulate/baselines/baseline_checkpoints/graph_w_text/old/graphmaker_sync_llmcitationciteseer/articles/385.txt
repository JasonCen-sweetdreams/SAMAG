Multimodal retrieval systems, which support queries comprising both text and images, have gained popularity in recent years. However, the performance of these systems often suffers from the vocabulary mismatch problem, where the query terms do not appear in the indexed documents. In this paper, we propose a novel query expansion approach that leverages deep neural networks to learn a multimodal representation of the query. Our approach, called MQR, uses a convolutional neural network to extract visual features from the query image and a recurrent neural network to model the textual query. Experimental results on a large-scale multimodal dataset demonstrate that MQR outperforms state-of-the-art query expansion methods, achieving a significant improvement in retrieval accuracy.