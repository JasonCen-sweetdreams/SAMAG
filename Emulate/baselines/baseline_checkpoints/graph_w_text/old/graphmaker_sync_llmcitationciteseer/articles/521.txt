Cooperative task allocation in multi-agent systems is a challenging problem, particularly in decentralized settings where agents have limited communication and partial observability. This paper proposes a novel decentralized multi-agent reinforcement learning (MARL) framework, 'CoopTask', which enables agents to learn cooperative policies for task allocation. We introduce a graph-based communication protocol that allows agents to share information and coordinate their actions, and a hierarchical reinforcement learning architecture that integrates task allocation and execution. Experimental results on a simulated search-and-rescue scenario demonstrate that CoopTask outperforms traditional decentralized MARL approaches in terms of task completion efficiency and robustness to communication failures.