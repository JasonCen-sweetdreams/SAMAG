Multi-modal sentiment analysis has gained increasing attention in recent years, but existing approaches often suffer from high computational complexity and limited scalability. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates visual, textual, and acoustic features to jointly model sentiment across different modalities. Our approach leverages a hierarchical attention mechanism to selectively focus on relevant features and modalities, leading to improved sentiment prediction accuracy and reduced computational overhead. Experimental results on three benchmark datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance with significantly reduced inference time.