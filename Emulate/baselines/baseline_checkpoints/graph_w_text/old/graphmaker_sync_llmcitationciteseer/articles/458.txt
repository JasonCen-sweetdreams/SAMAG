Deep neural networks (DNNs) have achieved state-of-the-art performance in various machine learning tasks, but their training process can be computationally expensive and time-consuming. This paper proposes a novel batch-incremental learning approach, 'BILA', which adaptively adjusts the batch size and learning rate during training to improve efficiency. BILA leverages the idea of incremental learning to reduce the number of model updates, and incorporates a dynamic batch sizing strategy to optimize memory usage and computation time. Experimental results on several benchmark datasets demonstrate that BILA achieves comparable model accuracy to traditional stochastic gradient descent while reducing training time by up to 40%.