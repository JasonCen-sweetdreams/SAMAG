Emotion recognition from multi-modal inputs has gained significant attention in human-computer interaction. However, existing approaches often rely on black-box models, hindering their adoption in high-stakes applications. We propose a novel hierarchical attention network (HAN) that leverages visual, acoustic, and linguistic cues to recognize emotions in a explainable manner. Our HAN model incorporates a hierarchical attention mechanism to selectively focus on relevant modalities and features, providing transparent insights into the decision-making process. Experimental results on the CMU-MOSEI dataset demonstrate the effectiveness of our approach, achieving state-of-the-art performance while providing interpretable emotional intelligence.