Emotion recognition from multimodal data (e.g., text, audio, and video) is a challenging task due to the complexity of human emotions and the varying importance of different modalities. This paper proposes a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and time segments to improve emotion recognition accuracy. Our approach consists of three stacked attention layers: modality-level attention, segment-level attention, and feature-level attention. Experiments on the benchmark Multimodal Emotion Recognition (MER) dataset demonstrate that our HAN model outperforms state-of-the-art methods by 5.6% in terms of weighted F1-score.