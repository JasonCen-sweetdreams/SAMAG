Multimodal sentiment analysis is a challenging task that requires effective fusion of features from heterogeneous data sources. This paper presents a Hierarchical Attention Network (HAN) that leverages the strengths of both visual and textual modalities. Our approach employs a novel attention mechanism that adaptively weights modality-specific features based on their relevance to the sentiment task. Experimental results on the Multimodal Sentiment Analysis (MuSe) benchmark demonstrate that HAN outperforms state-of-the-art methods in terms of accuracy and computational efficiency. We also provide insights into the performance gains achieved by our approach through detailed ablation studies and visualizations.