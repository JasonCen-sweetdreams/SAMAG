This paper presents a decentralized task allocation framework for multi-agent systems, where agents learn to allocate tasks efficiently using reinforcement learning. We propose a novel algorithm, 'MA-RL', which combines graph neural networks with Q-learning to enable agents to adapt to dynamic task environments. Experimental results on a variety of task allocation scenarios demonstrate that MA-RL outperforms existing decentralized methods in terms of task completion rate and agent utility. We also provide theoretical guarantees on the convergence of MA-RL and analyze its scalability in large-scale multi-agent systems.