Multimodal sentiment analysis (MSA) involves analyzing sentiment from multiple sources, such as text, images, and audio. However, most existing MSA models lack transparency, making it challenging to understand their decision-making processes. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that incorporates explainability mechanisms to provide insights into the sentiment classification process. Our HAN model uses a hierarchical attention mechanism to weigh the importance of different modalities and features, enabling the identification of influential factors contributing to the sentiment prediction. Experiments on a multimodal dataset demonstrate that our approach achieves state-of-the-art performance while providing interpretable results.