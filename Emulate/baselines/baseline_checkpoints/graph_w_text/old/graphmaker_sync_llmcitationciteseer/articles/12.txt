As virtual reality (VR) technology advances, the need for intuitive and natural interaction methods becomes increasingly important. This paper explores the concept of embodied cognition in VR, focusing on the role of gestures in enhancing user experience and task performance. We designed and evaluated a gesture-based interaction system that utilizes machine learning to recognize and adapt to users' gestures. Our user study (N=30) reveals significant improvements in user engagement, task efficiency, and cognitive load reduction compared to traditional controller-based interaction. The findings have implications for the design of more immersive and effective VR interfaces.