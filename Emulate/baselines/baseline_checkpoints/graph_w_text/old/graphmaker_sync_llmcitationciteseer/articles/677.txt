Explainability is a crucial aspect in multi-agent reinforcement learning, as it allows for better understanding and trust in complex decision-making processes. This paper introduces HAN-MARL, a novel hierarchical attention network architecture that enables explainable multi-agent reinforcement learning. HAN-MARL learns to selectively focus on relevant agents and their interactions, generating attention weights that can be interpreted as importance scores. We evaluate HAN-MARL on a cooperative navigation task and demonstrate improved performance and interpretability compared to state-of-the-art methods. Our approach has potential applications in real-world domains such as autonomous vehicles and smart cities.