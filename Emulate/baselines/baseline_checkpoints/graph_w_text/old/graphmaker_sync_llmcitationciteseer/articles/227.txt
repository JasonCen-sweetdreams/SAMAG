Emotion recognition from multimodal data, such as speech, text, and vision, is a challenging task due to the inherent heterogeneity and complexity of the data. This paper proposes a novel heterogeneous graph attention network (HGAT) that integrates multiple modalities into a unified graph structure. We design a modality-aware attention mechanism that adaptively weighs the importance of each modality and their interactions. Experimental results on the challenging CMU-MOSEI dataset demonstrate that HGAT outperforms state-of-the-art multimodal fusion methods, achieving an improvement of 12.5% in emotion recognition accuracy.