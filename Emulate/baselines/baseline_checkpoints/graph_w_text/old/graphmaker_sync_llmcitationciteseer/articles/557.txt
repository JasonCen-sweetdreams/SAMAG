Emotion recognition from multi-modal data (e.g., speech, text, and vision) has numerous applications in human-computer interaction. However, existing approaches often lack transparency and interpretability. This paper proposes a novel hierarchical attention network (HAN) that leverages the strengths of each modality to recognize emotions while providing explainable results. Our HAN model incorporates modality-specific attention mechanisms to focus on salient features, and a hierarchical fusion strategy to combine the outputs. Experimental results on the CMU-MOSEI dataset demonstrate the effectiveness of our approach, achieving state-of-the-art performance and providing insightful visualizations of the attention weights.