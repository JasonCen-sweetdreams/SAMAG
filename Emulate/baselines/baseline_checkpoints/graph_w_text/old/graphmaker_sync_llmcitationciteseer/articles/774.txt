Mental health support systems often struggle to engage users due to a lack of empathy and personalization. This paper presents a novel multimodal interaction framework for empathic conversational agents, which leverages computer vision, natural language processing, and affective computing to recognize and respond to users' emotional cues. Our framework integrates a deep learning-based emotion detection module with a dialogue management system, enabling agents to adapt their tone, language, and nonverbal behavior to users' emotional states. We evaluate our approach through a user study, demonstrating improved user satisfaction and emotional comfort in mental health support conversations.