Emotion recognition is a crucial aspect of human-robot interaction, enabling robots to respond empathetically to users. This paper presents a novel hierarchical attention network (HAN) architecture that integrates facial expression, speech, and body language cues to recognize emotions in real-time. Our HAN model consists of modality-specific attention modules that learn to weigh the importance of each feature type, followed by a fusion layer that combines the outputs. Experimental results on the EMOTIC dataset demonstrate that our approach outperforms state-of-the-art multimodal fusion methods, achieving an average F1-score of 0.85 across six emotions.