Emotion recognition from multi-modal data remains a challenging task, particularly when explaining the underlying decision-making process. We propose a novel hierarchical attention network (HAN) that leverages self-attention mechanisms to selectively focus on relevant features from input modalities. Our approach enables explainable emotion recognition by visualizing attention weights and identifying salient features. Experimental results on a benchmark dataset show that HAN outperforms state-of-the-art methods in terms of recognition accuracy and provides insightful explanations for the predicted emotions.