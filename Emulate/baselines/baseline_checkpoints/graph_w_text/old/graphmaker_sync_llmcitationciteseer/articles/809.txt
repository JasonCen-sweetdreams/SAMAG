Knowledge graph embedding (KGE) has gained popularity in various AI applications, but existing methods often suffer from scalability issues and limited expressiveness. We propose a novel approach, Hierarchical Graph Attention Networks (HGAT), which leverages the hierarchical structure of knowledge graphs to learn more effective embeddings. HGAT employs a recursive attention mechanism to selectively focus on relevant graph regions, thereby reducing computational complexity and improving embedding quality. Experimental results on benchmark datasets demonstrate that HGAT outperforms state-of-the-art KGE methods in terms of link prediction and node classification tasks.