Multimodal information retrieval (MMIR) involves retrieving relevant information from diverse data sources, including text, images, and videos. In this paper, we propose a novel context-aware retrieval model, 'CAMR', which incorporates multimodal fusion and contextual information to improve retrieval accuracy. CAMR employs a hierarchical attention mechanism to capture both local and global contextual features, and integrates a multimodal embedding framework to jointly learn textual and visual representations. Experimental results on the Wikimedia dataset demonstrate that CAMR outperforms state-of-the-art MMIR models, achieving a 15% improvement in mean average precision.