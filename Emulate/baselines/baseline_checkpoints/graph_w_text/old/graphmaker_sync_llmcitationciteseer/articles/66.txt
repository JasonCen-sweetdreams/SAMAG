Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the variability of modalities. We propose a novel Hierarchical Graph Attention Network (HGAT) that captures both intra- and inter-modality relationships. HGAT employs a hierarchical graph structure to model the interactions between modalities and a graph attention mechanism to selectively focus on relevant features. Experimental results on the CMU-MOSEI dataset demonstrate that HGAT outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving a significant improvement in F1-score and correlation coefficient.