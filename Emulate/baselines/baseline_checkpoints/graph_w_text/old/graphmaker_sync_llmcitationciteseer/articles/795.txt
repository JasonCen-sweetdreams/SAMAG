Multi-task learning has gained popularity in recent years, but it often suffers from conflicting objectives and increased computational costs. This paper proposes a novel hierarchical attention network (HAN) that efficiently handles multiple tasks by selectively focusing on relevant features and tasks. Our HAN architecture consists of task-specific attention modules and a hierarchical fusion mechanism that adaptively weighs task importance. Experimental results on several benchmark datasets demonstrate that our approach outperforms existing state-of-the-art methods in terms of accuracy, convergence speed, and computational efficiency.