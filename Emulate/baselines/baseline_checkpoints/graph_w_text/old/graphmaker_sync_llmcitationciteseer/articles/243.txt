Emotion recognition from multi-modal data (e.g., audio, video, and text) is a challenging task due to the heterogeneity of features and modalities. This paper proposes a novel heterogeneous graph attention network (HGAT) to model the complex relationships between modalities and features. HGAT incorporates a hierarchical graph structure to capture both intra- and inter-modality interactions, and employs attention mechanisms to selectively focus on salient features. Experimental results on the CMU-MOSEI dataset demonstrate that HGAT outperforms state-of-the-art methods in multi-modal emotion recognition, achieving a 12.5% improvement in F1-score.