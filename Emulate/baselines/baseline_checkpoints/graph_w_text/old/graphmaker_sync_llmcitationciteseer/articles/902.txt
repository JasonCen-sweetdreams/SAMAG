Continual learning poses a significant challenge for deep neural networks, which often require retraining or fine-tuning to adapt to new tasks. We propose SelfMod, a novel framework that enables neural networks to modify their own architecture and weights in response to changing task distributions. By incorporating a meta-learner that predicts optimal network modifications, SelfMod achieves state-of-the-art performance on several benchmark continual learning datasets while reducing computational overhead by up to 75%. We demonstrate the efficacy of SelfMod in various scenarios, including incremental class learning, task-free learning, and online learning.