Multimodal emotion recognition has gained significant attention in AI research, but most existing approaches lack interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates visual, audio, and textual features to recognize emotions in videos. Our HAN model employs attention mechanisms at multiple levels to selectively focus on salient features, enabling explainability of emotion recognition decisions. Experimental results on the Multimodal Emotion Recognition dataset demonstrate that our approach outperforms state-of-the-art methods in terms of accuracy and provides insightful visualizations of attention weights.