Emotion recognition from multi-modal data, such as speech, text, and facial expressions, is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a Hierarchical Graph Attention Network (HGAT) that leverages the graph structure of multi-modal data to learn rich representations of emotions. HGAT employs a hierarchical attention mechanism to selectively focus on relevant modalities and graph nodes, enabling the model to capture both local and global dependencies. Experimental results on the IEMOCAP and CMU-MOSEI datasets demonstrate that HGAT outperforms state-of-the-art methods in recognizing emotions from multi-modal data.