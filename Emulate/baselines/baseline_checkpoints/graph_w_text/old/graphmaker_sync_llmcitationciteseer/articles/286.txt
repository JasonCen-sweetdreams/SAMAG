Sentiment analysis from multimodal data, such as text, images, and videos, is a challenging task due to the inherent complexity of fusion and alignment. This paper proposes a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and features. Our model consists of a modality-level attention module, which weighs the importance of each modality, and a feature-level attention module, which refines the feature representations. Experimental results on three benchmark datasets demonstrate the effectiveness of HAN, outperforming state-of-the-art multimodal fusion methods by 3.5% to 5.2% in terms of sentiment classification accuracy.