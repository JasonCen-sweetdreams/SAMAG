This paper presents a novel hierarchical attention network (HAN) architecture for multi-modal dialogue systems, enabling explainable and context-aware conversation flow. Our approach integrates visual, acoustic, and linguistic features to generate coherent and engaging responses. We introduce a new attention mechanism that selectively focuses on relevant modalities, and incorporate a hierarchical reasoning module to capture long-range dependencies. Experimental results on a large-scale dialogue dataset demonstrate significant improvements in response quality, coherence, and user satisfaction, while providing interpretable insights into the decision-making process.