Time series forecasting is a crucial task in various domains, including finance, healthcare, and energy. While deep learning models have achieved state-of-the-art performance, their lack of interpretability hinders their adoption in high-stakes applications. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that incorporates self-attention mechanisms to select relevant input features and time steps. We introduce a Lipschitz continuity-based regularization term to encourage model explainability. Experimental results on real-world datasets demonstrate that HAN outperforms existing benchmarks while providing meaningful insights into its decision-making process.