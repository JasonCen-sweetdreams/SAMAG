Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a challenging task due to the heterogeneity of modalities and the complexity of human emotions. This paper introduces a novel Hierarchical Attention Network (HAN) that effectively integrates and selectively focuses on relevant modalities and features. Our HAN model consists of a multi-modal feature extractor, a hierarchical attention mechanism, and an emotion classification module. Experimental results on the benchmark IEMOCAP dataset demonstrate that our approach significantly outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an average F1-score of 0.854.