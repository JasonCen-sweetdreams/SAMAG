Emotion recognition from multi-modal data (e.g., speech, text, and vision) is a challenging task, particularly when dealing with complex and ambiguous emotional expressions. This paper introduces HATERN, a novel hierarchical attention network that selectively focuses on relevant modalities and temporal segments to improve explainability and accuracy. Our approach leverages self-organizing maps to discover latent emotional patterns and generates visualizations to facilitate human interpretability. Experimental results on the OMG-Emotion dataset demonstrate HATERN's superior performance and ability to provide insightful emotional state representations.