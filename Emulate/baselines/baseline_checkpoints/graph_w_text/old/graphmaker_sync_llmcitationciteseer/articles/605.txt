Multimodal sentiment analysis (MSA) involves analyzing sentiment from multiple sources such as text, images, and audio. Graph-based neural networks have shown promising results in MSA, but they often fail to capture complex relationships between modalities. This paper proposes a hierarchical graph attention network (HGAT) that integrates graph attention mechanisms with multimodal fusion. HGAT learns to attend to relevant nodes and modalities at multiple scales, enabling it to capture hierarchical dependencies and improve sentiment prediction. Experiments on three benchmark datasets demonstrate that HGAT outperforms state-of-the-art models in MSA tasks, achieving a relative improvement of 12.3% in accuracy.