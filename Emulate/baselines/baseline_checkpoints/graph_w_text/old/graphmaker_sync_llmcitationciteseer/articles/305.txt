Robot navigation in dynamic environments poses significant challenges, particularly in scenarios where the robot must adapt to changing obstacles and goals. This paper introduces a hierarchical reinforcement learning (HRL) framework that leverages a high-level planning module to select tasks and a low-level control module to execute them. We propose a novel task representation that incorporates both spatial and temporal information, enabling the robot to efficiently navigate complex environments. Experimental results on a robotic platform demonstrate improved navigation performance and reduced computational overhead compared to flat reinforcement learning approaches.