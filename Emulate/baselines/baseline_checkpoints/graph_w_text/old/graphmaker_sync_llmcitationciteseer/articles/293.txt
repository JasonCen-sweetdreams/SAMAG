Multimodal sentiment analysis (MSA) has become increasingly important for understanding user opinions and emotions in online content. However, existing approaches often rely on expensive, compute-intensive models or require large amounts of annotated data. This paper proposes a novel hierarchical attention network (HAN) for MSA, which leverages both visual and textual features to capture complex sentiment expressions. By recursively applying attention mechanisms at the word, sentence, and modalities levels, our HAN model achieves state-of-the-art performance on two benchmark datasets while reducing computational costs by up to 40%. We demonstrate the effectiveness of our approach in real-world applications, such as product review analysis and social media monitoring.