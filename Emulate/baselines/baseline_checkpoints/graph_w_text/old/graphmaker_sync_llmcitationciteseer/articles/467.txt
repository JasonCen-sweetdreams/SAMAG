Emotion recognition in human-robot interaction is crucial for building trust and rapport between humans and robots. This paper proposes a novel hierarchical attention network (HAN) framework that integrates multimodal cues from speech, text, and facial expressions to recognize emotions. Our HAN model consists of modality-specific attention layers that learn to focus on relevant features and a fusion layer that combines the outputs to predict emotions. Experimental results on a large multimodal dataset show that our approach outperforms state-of-the-art methods in recognizing complex emotions like surprise and excitement.