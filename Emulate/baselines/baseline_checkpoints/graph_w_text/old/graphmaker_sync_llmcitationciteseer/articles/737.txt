Graph Neural Networks (GNNs) have achieved state-of-the-art performance in various graph-based applications. However, recent studies have shown that GNNs are vulnerable to adversarial attacks, which can manipulate the graph structure or node features to mislead the model. This paper presents a comprehensive taxonomy of adversarial attacks on GNNs, categorizing them based on the attack strategy, scope, and target. We also propose a novel defense mechanism, 'GraphShield', which combines graph anomaly detection with adversarial training to improve the robustness of GNNs. Experimental results on several benchmark datasets demonstrate the effectiveness of GraphShield against various attack types.