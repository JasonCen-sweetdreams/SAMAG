In this paper, we investigate the problem of coordinating multiple agents in a decentralized setting with incomplete information. We propose a distributed reinforcement learning framework that enables agents to learn effective coordination strategies despite limited knowledge of the environment and other agents' actions. Our approach combines decentralized Q-learning with a novel communication protocol that allows agents to share information and adapt to changing circumstances. Experimental results on a range of multi-agent scenarios demonstrate that our approach outperforms existing methods in terms of coordination efficiency and robustness to uncertainty.