Neural architecture search (NAS) has emerged as a promising approach to automate the design of deep neural networks. However, the computational cost of evaluating candidate architectures remains a significant bottleneck. We propose a novel NAS framework that leverages Bayesian optimization and graph-based encoding to efficiently explore the architecture space. Our approach, dubbed 'BOGS', utilizes a probabilistic graph model to represent neural architectures and performs Bayesian optimization to identify high-performing candidates. Experiments on several benchmark datasets demonstrate that BOGS achieves competitive performance to state-of-the-art NAS methods while reducing the search time by up to 5x.