Coordinating multiple agents to achieve complex tasks requires effective communication and cooperation strategies. This paper presents a novel hierarchical attention network (HAN) architecture that enables explainable multi-agent cooperation in decentralized settings. Our approach leverages attention mechanisms to selectively focus on relevant information from neighboring agents, while a hierarchical structure facilitates the integration of local and global knowledge. We evaluate our method on a series of simulated tasks, demonstrating improved cooperation and reduced communication overhead compared to existing decentralized reinforcement learning approaches.