Cloud computing platforms face the challenge of efficiently allocating resources to diverse workloads, leading to suboptimal performance and high energy consumption. This paper presents a novel hierarchical reinforcement learning (HRL) framework, 'CloudOpt', which tackles this problem by learning to allocate resources at multiple levels of abstraction. CloudOpt consists of a high-level policy that decides on resource allocation strategies and a low-level policy that adjusts resource provisioning to match the allocated strategy. We evaluate CloudOpt on a real-world cloud dataset and demonstrate significant improvements in resource utilization, job completion times, and energy efficiency compared to state-of-the-art scheduling algorithms.