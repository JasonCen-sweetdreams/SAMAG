Traditional document representation methods in information retrieval (IR) systems often fail to capture nuanced semantic relationships between words and documents. This paper proposes a novel hierarchical attention-based document representation model, HiAttDR, which leverages both local and global contextual information. Our approach first applies self-attention mechanisms to model word-word relationships within documents, and then uses a hierarchical attention framework to aggregate document-level representations. Experimental results on several benchmark datasets demonstrate that HiAttDR outperforms state-of-the-art methods in ad-hoc retrieval tasks, while reducing computational overhead by up to 30%.