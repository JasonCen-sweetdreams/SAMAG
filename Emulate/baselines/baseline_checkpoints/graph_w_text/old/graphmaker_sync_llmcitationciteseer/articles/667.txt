Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the complexities of human emotional expression. This paper presents a novel Hierarchical Attention Network (HAN) architecture that learns to fuse and selectively focus on relevant modalities and features to improve emotion recognition. Our HAN model consists of multiple attention layers that capture both intra- and inter-modal relationships, enabling effective emotion recognition in real-world scenarios. Experimental results on the IEMOCAP and CMU-MOSEI datasets demonstrate the superiority of our approach over state-of-the-art methods, achieving improved accuracy and robustness in noisy or ambiguous emotional contexts.