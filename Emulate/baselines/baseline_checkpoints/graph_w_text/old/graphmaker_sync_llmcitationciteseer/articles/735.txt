Multimodal emotion recognition has gained significant attention in recent years, but existing approaches often struggle with the complexity of multimodal data. In this paper, we propose a novel hierarchical attention network (HAN) that leverages the complementary information from different modalities. Our HAN model consists of multiple layers of attention mechanisms, enabling the model to focus on the most relevant features and modalities. We evaluate our approach on the CMU Multimodal Opinion Sentiment and Emotion (CMU-MOSE) dataset and achieve state-of-the-art performance in emotion recognition. Furthermore, our approach reduces the computational complexity by 30% compared to existing multimodal fusion methods.