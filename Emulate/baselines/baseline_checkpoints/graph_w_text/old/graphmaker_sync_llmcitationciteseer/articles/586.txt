Ad-hoc search systems rely on accurate query representation to retrieve relevant documents. This paper explores the application of neural retrieval models to improve query representation in ad-hoc search. We propose a novel neural architecture, 'QRN', which leverages transformer-based encoders to learn query-document interactions. QRN incorporates a query-aware attention mechanism that adaptively weights query terms based on their importance in the document context. Experimental results on the TREC-CAR dataset demonstrate that QRN outperforms traditional query representation methods, achieving a 12.5% improvement in mean average precision.