Graph neural networks (GNNs) have achieved state-of-the-art performance in node classification tasks, but their scalability is limited by the need to compute attention weights for all node pairs. This paper proposes Hierarchical Graph Attention Networks (HGAT), which leverages a hierarchical graph representation to reduce attention computation costs. HGAT recursively applies graph attention mechanisms at multiple scales, capturing both local and global node relationships. We demonstrate the efficacy of HGAT on large-scale graph datasets, achieving improved node classification accuracy while reducing computational overhead by up to 75% compared to existing GNN models.