Graph neural networks (GNNs) have achieved state-of-the-art performance in node classification tasks, but their computational complexity and memory requirements can be prohibitive for large graphs. This paper proposes a novel hierarchical graph attention network (HGAN) architecture that leverages multi-scale graph representations to reduce computational overhead while maintaining accuracy. Our approach learns to selectively focus on relevant nodes and edges at each hierarchy level, leading to significant speedups and improved scalability. Experimental results on several benchmark datasets demonstrate the effectiveness of HGAN in node classification tasks, particularly in the presence of noisy or incomplete graph data.