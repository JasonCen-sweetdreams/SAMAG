Emotion recognition in human-robot interaction (HRI) is crucial for developing robots that can empathize with humans. This paper presents a novel hierarchical attention network (HAN) for multi-modal emotion recognition in HRI. Our approach leverages visual, audio, and textual cues to recognize emotions and identifies the most informative modalities using attention mechanisms. Experimental results on a large-scale HRI dataset demonstrate that our HAN outperforms state-of-the-art methods in recognizing emotions, especially in cases where humans exhibit subtle or ambiguous emotional expressions.