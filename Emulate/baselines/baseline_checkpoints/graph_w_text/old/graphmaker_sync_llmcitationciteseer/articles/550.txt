While deep learning models have achieved state-of-the-art performance in medical image analysis, their lack of transparency and explainability hinders their adoption in high-stakes clinical applications. This paper proposes a novel meta-learning framework, 'MetaXplain', which adapts to diverse medical imaging datasets and generates task-specific explanations for model predictions. By leveraging a few-shot learning paradigm, MetaXplain learns to synthesize attention maps and saliency features that provide insights into the decision-making process of the model. Experimental results on multiple benchmark datasets demonstrate the effectiveness of MetaXplain in improving model interpretability and trustworthiness in medical image analysis.