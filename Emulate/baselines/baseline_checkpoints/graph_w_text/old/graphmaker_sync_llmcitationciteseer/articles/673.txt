Emotion recognition is a crucial aspect of human-computer interaction (HCI), enabling systems to respond empathetically to users. This paper presents EmoTract, a novel multimodal emotion recognition framework that fuses facial expression, speech, and physiological signals. EmoTract employs a deep neural network to learn a shared representation across modalities, improving emotion recognition accuracy and robustness. We evaluate EmoTract on a large, diverse dataset, achieving state-of-the-art performance in recognizing emotions in real-world HCI scenarios. The framework's potential applications include affective computing, mental health monitoring, and personalized human-computer interaction.