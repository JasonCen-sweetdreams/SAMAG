Multimodal sentiment analysis (MSA) aims to identify sentiment polarity from heterogeneous data sources, such as text, images, and audio. Existing methods often rely on early fusion or late fusion strategies, which may not fully capture the complex interactions between modalities. We propose a Hierarchical Attention Network (HAN) that jointly models intra-modality and inter-modality relationships. Our approach utilizes a stacked attention mechanism to selectively focus on relevant regions within and across modalities, leading to improved sentiment classification accuracy. Experimental results on the CMU-MOSI dataset demonstrate the effectiveness of HAN in MSA tasks, outperforming state-of-the-art methods by a significant margin.