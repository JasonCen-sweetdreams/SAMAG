In multi-agent reinforcement learning, understanding the decision-making process of individual agents is crucial for effective collaboration. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that enables explainable policy learning in multi-agent settings. By incorporating attention mechanisms at both agent and joint action levels, HAN learns to identify relevant features and interactions that influence agent decisions. Experimental results on a suite of multi-agent environments demonstrate improved performance and interpretability of HAN compared to state-of-the-art methods.