Cloud computing has revolutionized the way resources are allocated and utilized. However, the increasing complexity of cloud infrastructure necessitates the development of novel resource allocation strategies. This paper proposes a hierarchical reinforcement learning (HRL) framework that leverages the benefits of both model-free and model-based reinforcement learning to optimize resource allocation in cloud computing. Our approach consists of a high-level controller that learns to allocate resources at the cluster level and a low-level controller that fine-tunes resource allocation at the instance level. Experimental results demonstrate that our HRL framework outperforms state-of-the-art baselines in terms of resource utilization, latency, and energy efficiency.