Deep reinforcement learning (DRL) has shown promise in autonomous vehicle control, but its vulnerability to adversarial attacks poses a significant safety risk. This paper investigates the susceptibility of DRL policies to targeted perturbations in sensor inputs, demonstrating that even minor manipulations can cause catastrophic failures. We propose a novel attack framework, 'AV-Attack', which leverages the problem's inherent uncertainty to generate highly effective perturbations. Our results on a simulated autonomous driving environment reveal the need for robustness measures in DRL-based control systems, and we outline potential defenses against such attacks.