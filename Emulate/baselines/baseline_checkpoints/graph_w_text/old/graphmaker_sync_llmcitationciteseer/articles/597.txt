This paper presents a novel decentralized task allocation framework for multi-agent systems, leveraging deep reinforcement learning. Our approach, called 'MARL-TA', enables agents to learn cooperative policies for task assignment and execution in a distributed manner, without relying on a centralized controller. We introduce a new reward function that incorporates both individual agent performance and global system efficiency, and demonstrate improved task completion rates and reduced communication overhead compared to existing techniques. Experiments on a simulated disaster response scenario show the scalability and adaptability of MARL-TA in dynamic, uncertain environments.