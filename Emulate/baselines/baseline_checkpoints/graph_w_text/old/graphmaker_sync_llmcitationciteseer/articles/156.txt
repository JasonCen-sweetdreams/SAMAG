Graph neural networks (GNNs) have shown promising results in node classification tasks, but often struggle with handling complex graph structures and noisy node features. This paper proposes a novel hierarchical attention-based GNN framework, 'HAT-GNN', which leverages attention mechanisms at both node and graph levels to selectively focus on relevant neighbors and features. We demonstrate the effectiveness of HAT-GNN on several benchmark datasets, achieving state-of-the-art results and outperforming existing GNN models by up to 10% in terms of micro-F1 score. Our ablation studies further highlight the importance of hierarchical attention in handling noisy graph data.