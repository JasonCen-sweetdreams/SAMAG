Emotion recognition from multi-modal data, such as audio, video, and text, is a challenging task due to the complexity of human emotions and the varying quality of modalities. This paper proposes a novel hierarchical attention network (HAN) that adaptively fuses information from multiple modalities to recognize emotions. The HAN consists of modality-specific attention layers that capture local patterns and a hierarchical fusion layer that integrates information across modalities. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal data, achieving an accuracy of 85.2%.