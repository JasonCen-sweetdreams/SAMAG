Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is crucial for human-computer interaction. However, existing approaches often lack interpretability, making it challenging to understand the decision-making process. This paper introduces HAN-EMO, a hierarchical attention network that incorporates attention mechanisms at multiple levels to selectively focus on relevant modalities and features. We demonstrate the effectiveness of HAN-EMO on the IEMOCAP dataset, achieving state-of-the-art performance while providing visual explanations for its predictions. Our proposed model has implications for affective computing and human-robot interaction applications.