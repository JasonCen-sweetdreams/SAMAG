Reinforcement learning (RL) agents have achieved remarkable success in complex tasks, but their decision-making processes often remain opaque. This paper introduces Hierarchical Graph Attention Networks (HGAT), a novel architecture that enhances the explainability of RL policies. By modeling the environment as a graph and applying hierarchical attention mechanisms, HGAT disentangles the agent's decision-making process into interpretable components. We demonstrate the effectiveness of HGAT on several Atari games and a real-world robotics task, showcasing improved transparency and performance over state-of-the-art RL methods.