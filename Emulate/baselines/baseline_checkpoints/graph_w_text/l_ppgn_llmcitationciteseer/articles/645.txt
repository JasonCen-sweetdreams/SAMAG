Recognizing emotions in conversational dialogue is a challenging task due to the complexity of human emotions and the multimodal nature of human communication. This paper proposes a Hierarchical Attention Network (HAN) that integrates visual, acoustic, and linguistic features to recognize emotions in conversational dialogue. Our HAN model uses a hierarchical attention mechanism to selectively focus on relevant features and context, outperforming state-of-the-art models in emotion recognition tasks. We evaluate our model on the IEMOCAP dataset and achieve significant improvements in F1-score and accuracy.