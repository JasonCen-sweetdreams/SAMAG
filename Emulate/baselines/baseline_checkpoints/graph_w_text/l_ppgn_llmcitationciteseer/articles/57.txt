Neural retrieval models have shown promising results in information retrieval tasks, but often struggle to effectively incorporate external knowledge for query expansion. This paper proposes a novel approach, 'CtxQE', which leverages contextualized embeddings to generate informative expansion terms. We introduce a hierarchical attention mechanism that captures both local and global contextual relationships between query terms and documents. Experimental results on several benchmark datasets demonstrate that CtxQE outperforms state-of-the-art query expansion methods, achieving significant improvements in retrieval effectiveness and efficiency.