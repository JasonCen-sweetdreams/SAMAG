Knowledge graph embeddings have become a crucial component in various AI applications. However, existing methods often lack interpretability, making it challenging to understand the learned representations. This paper presents a novel hierarchical graph attention network (HGAT) that learns explainable knowledge graph embeddings. HGAT incorporates multi-hop attention mechanisms to capture complex relationships between entities and their attributes. We introduce a sparsity-inducing regularization term to promote interpretable attention weights, enabling the identification of influential entities and relationships. Experimental results on several benchmark datasets demonstrate the effectiveness of HGAT in link prediction and entity classification tasks, while providing insights into the learned representations.