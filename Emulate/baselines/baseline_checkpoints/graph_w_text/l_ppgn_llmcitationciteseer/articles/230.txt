Virtual reality (VR) interfaces can greatly benefit from accurate gesture recognition, enabling more immersive and interactive experiences. This paper presents a novel multi-modal fusion approach that combines computer vision, electromyography (EMG), and inertial measurement unit (IMU) data to recognize hand gestures in VR. Our proposed framework, 'FusionVR', leverages a deep learning architecture to integrate the strengths of each modality, achieving a 25% increase in recognition accuracy compared to state-of-the-art vision-only methods. We evaluate FusionVR on a dataset of 20 participants performing 10 distinct gestures, demonstrating its potential for enhancing VR interactions.