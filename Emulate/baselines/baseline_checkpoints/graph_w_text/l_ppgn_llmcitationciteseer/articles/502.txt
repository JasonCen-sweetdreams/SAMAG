Anomaly detection in time-series data is a crucial task in various domains. Existing methods often rely on labeled datasets, which are scarce and expensive to obtain. This paper introduces a self-supervised approach based on contrastive predictive coding (CPC) to learn representations for time-series anomaly detection. Our method, dubbed 'TS-CPC', leverages the temporal dependencies in the data to learn a predictive model that distinguishes between normal and anomalous patterns. We demonstrate the effectiveness of TS-CPC on several benchmark datasets, achieving state-of-the-art performance while requiring no labeled data.