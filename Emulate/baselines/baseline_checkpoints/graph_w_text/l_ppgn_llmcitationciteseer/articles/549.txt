Dense passage retrieval has shown great promise in various information retrieval tasks, but its computational cost and memory requirements limit its scalability. This paper addresses this issue by proposing an efficient index pruning method, which reduces the number of passages to be ranked without sacrificing retrieval accuracy. Our approach leverages the clustering structure of the passage representations and the locality-sensitive hashing to identify and remove redundant passages in the index. Experimental results on several benchmark datasets demonstrate that our method achieves significant speedup and memory reduction while maintaining the retrieval performance of the original dense passage retrieval model.