Emotion recognition from multimodal data, such as speech, text, and vision, is a challenging task due to the heterogeneity and complexity of the input features. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features to improve emotion recognition accuracy. Our HAN model consists of multiple layers of attention mechanisms that capture hierarchical dependencies between modalities and facilitate the fusion of complementary information. Experimental results on the IEMOCAP and CMU-MOSEI datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance in multimodal emotion recognition.