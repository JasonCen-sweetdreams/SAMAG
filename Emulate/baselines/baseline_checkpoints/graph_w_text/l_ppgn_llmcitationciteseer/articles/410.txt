Neural architecture search (NAS) has emerged as a promising approach to automate the design of deep neural networks. However, the computational cost of evaluating candidate architectures hinders the adoption of NAS in resource-constrained settings. This paper presents a novel graph-based Bayesian optimization (BO) framework for efficient hyperparameter tuning in NAS. By encoding the neural architecture as a graph and leveraging graph kernels, our approach reduces the dimensionality of the search space and enables faster convergence to optimal hyperparameters. Experimental results on popular NAS benchmarks demonstrate that our method achieves state-of-the-art performance while reducing the computation time by up to 3x compared to existing BO methods.