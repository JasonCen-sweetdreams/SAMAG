Multimodal emotion recognition (MER) systems struggle to provide interpretable insights into their decision-making processes. We propose a novel hierarchical attention framework, 'HierAttnMER', which leverages visual, acoustic, and linguistic cues to recognize emotions in videos. Our approach incorporates a hierarchical attention mechanism that selectively focuses on relevant features and modalities, enabling more accurate and explainable MER. Experiments on the IEMOCAP dataset demonstrate that HierAttnMER outperforms state-of-the-art MER models while providing saliency maps that highlight the most influential features for emotion recognition.