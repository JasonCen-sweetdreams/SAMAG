This paper presents a novel Hierarchical Attention Network (HAN) architecture for multi-modal emotion recognition, leveraging facial expressions, speech, and text inputs. Our HAN model incorporates a hierarchical fusion mechanism that attends to relevant modalities at different levels, capturing both local and global context. Experiments on the IEMOCAP dataset demonstrate that our approach achieves state-of-the-art performance in recognizing emotions, outperforming existing multi-modal fusion methods. We also show that our model is computationally efficient, making it suitable for real-world applications.