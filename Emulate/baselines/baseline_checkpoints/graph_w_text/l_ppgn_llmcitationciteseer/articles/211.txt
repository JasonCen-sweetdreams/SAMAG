Emotion recognition systems have significant implications for human-computer interaction, but often rely on a single modality, leading to biased or inaccurate results. This paper presents a novel multimodal fusion approach that integrates facial expressions, speech, and physiological signals to recognize emotions in real-time. Our framework leverages a hierarchical attention mechanism to weight and combine modalities, achieving improved recognition rates and robustness to individual differences. We demonstrate the effectiveness of our approach in a user study with 50 participants, highlighting its potential for enhancing emotional intelligence in HCI applications.