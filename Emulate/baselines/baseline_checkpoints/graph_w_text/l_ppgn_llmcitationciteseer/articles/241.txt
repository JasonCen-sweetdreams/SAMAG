Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the heterogeneity of modalities. This paper proposes a novel Hierarchical Attention Neural Network (HANN) architecture that leverages the strengths of each modality and captures intricate relationships between them. Our model uses a hierarchical attention mechanism to selectively focus on relevant features and modalities, outperforming state-of-the-art methods on the CMU-MOSEI and IEMOCAP datasets. Experiments demonstrate the effectiveness of HANN in recognizing emotions from diverse modalities, with potential applications in affective computing and human-computer interaction.