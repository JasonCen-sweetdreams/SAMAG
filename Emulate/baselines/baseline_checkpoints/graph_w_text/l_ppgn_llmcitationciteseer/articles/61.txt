Robot navigation in dynamic environments is a challenging problem that requires efficient and adaptive decision-making. This paper presents a hierarchical reinforcement learning framework that integrates a high-level task planner with a low-level motion controller. The proposed approach leverages a graph-based representation of the environment to learn a hierarchical policy that balances exploration and exploitation. Experimental results on a simulated robot navigation task demonstrate that our approach outperforms state-of-the-art methods in terms of navigation efficiency and adaptability to changing environmental conditions.