Cooperative multi-agent reinforcement learning (MARL) is a challenging problem due to the exponential increase in state and action spaces with the number of agents. This paper proposes a novel hierarchical attention network (HAN) architecture that learns to selectively focus on relevant agents and their interactions. Our approach consists of two stages: intra-agent attention for learning individual policies and inter-agent attention for coordinating actions. We evaluate HAN on several MARL benchmarks and demonstrate significant improvements in task completion rates and overall rewards compared to state-of-the-art methods.