The proliferation of edge devices has created a need for efficient neural networks that can operate within stringent resource constraints. This paper proposes a novel neural architecture search (NAS) framework, 'BayesNAS', which leverages Bayesian HyperNetworks to generate accurate and efficient models. By incorporating a probabilistic prior over the architecture space, BayesNAS can effectively explore the vast search space and identify optimal architectures under resource constraints. Our experiments on various image classification benchmarks demonstrate that BayesNAS can discover models that achieve state-of-the-art performance while reducing computational requirements by up to 50%.