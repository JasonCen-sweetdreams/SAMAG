Deep neural networks have achieved state-of-the-art performance in various machine learning tasks, but their success heavily relies on careful hyperparameter tuning. This process is often time-consuming and computationally expensive. We propose a novel approach, BOHyp, which leverages Bayesian optimization to efficiently search the hyperparameter space. BOHyp models the hyperparameter-performance relationship using a probabilistic surrogate function and adaptively selects the most informative hyperparameter configurations to evaluate. Experimental results on several benchmark datasets demonstrate that BOHyp can find near-optimal hyperparameters with significantly fewer evaluations than traditional grid search and random search methods.