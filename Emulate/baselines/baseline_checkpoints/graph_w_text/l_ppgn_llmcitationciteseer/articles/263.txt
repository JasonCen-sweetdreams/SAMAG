Visual Question Answering (VQA) models have achieved impressive performance, but their robustness to adversarial attacks remains a concern. This paper proposes a novel attention-based input preprocessing technique, 'AttentionShield', to improve the adversarial robustness of VQA models. We introduce a learnable attention module that adaptively weights input features based on their importance for answering the question, thereby reducing the impact of adversarial perturbations. Our experiments on the VQA-CP dataset demonstrate that AttentionShield improves the robustness of state-of-the-art VQA models by up to 15% against strong adversarial attacks, while maintaining their accuracy on clean data.