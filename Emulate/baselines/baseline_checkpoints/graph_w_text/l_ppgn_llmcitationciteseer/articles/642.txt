Deep neural networks (DNNs) are vulnerable to adversarial attacks, which can severely compromise their performance. This paper proposes a novel framework, 'BayesGuard', that leverages Bayesian uncertainty estimation to analyze the robustness of DNNs against adversarial attacks. We introduce a hierarchical Bayesian model that captures the uncertainty of DNN weights and outputs, enabling the detection of potential vulnerabilities. Our approach is shown to outperform existing methods in identifying adversarial examples on benchmark datasets, and provides a more comprehensive understanding of DNN robustness.