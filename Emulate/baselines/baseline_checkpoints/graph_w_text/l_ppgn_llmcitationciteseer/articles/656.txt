Accurate recognition of human affective states is crucial for developing empathetic human-computer interfaces. This paper presents EmoReact, a multimodal framework that combines facial expression, speech, and physiological signal processing to recognize emotions in real-time. We propose a novel deep learning architecture that fuses the outputs of multiple modalities using a hierarchical attention mechanism, achieving improved recognition accuracy and robustness. Our evaluation on a publicly available dataset demonstrates EmoReact's superiority over state-of-the-art unimodal and multimodal approaches, with an average F1-score of 0.92 across seven emotions.