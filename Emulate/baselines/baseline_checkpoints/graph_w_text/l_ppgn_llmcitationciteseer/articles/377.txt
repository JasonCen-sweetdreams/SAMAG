Sentiment analysis is a crucial task in natural language processing, but existing approaches often struggle to capture complex relationships between text and visual modalities. This paper presents a novel hierarchical attention network (HAN) architecture that integrates multi-modal inputs and provides interpretable explanations for sentiment predictions. Our HAN model employs a coarse-to-fine attention mechanism to selectively focus on relevant regions of images and sentences, allowing for more accurate sentiment classification. Experimental results on the Multi-Modal Sentiment Analysis (MMSA) benchmark demonstrate the efficacy of our approach, with significant improvements in both performance and explainability.