Virtual Reality (VR) has the potential to revolutionize assistive technologies, but existing systems often fail to account for individual differences in user abilities and needs. This paper introduces GAUMER, a novel gaze-informed adaptive user model that leverages machine learning and eye-tracking data to personalize VR interactions for users with disabilities. Our approach dynamically adjusts interface elements, navigation, and feedback mechanisms to accommodate users' gaze patterns, cognitive load, and motor abilities. A user study with 30 participants demonstrates that GAUMER significantly improves task completion times, accuracy, and overall user experience for individuals with motor impairments.