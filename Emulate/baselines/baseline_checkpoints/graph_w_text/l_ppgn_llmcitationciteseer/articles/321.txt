Emotion recognition in human-robot interaction (HRI) is crucial for creating empathetic and personalized robots. This paper proposes a novel hierarchical attention network (HAN) that integrates multi-modal features from facial expressions, speech, and body language to recognize emotions in real-time. Our HAN architecture consists of three levels of attention: modality-level, feature-level, and decision-level attention. We evaluate our approach on a large-scale HRI dataset and demonstrate significant improvements in emotion recognition accuracy compared to state-of-the-art methods. The proposed framework has potential applications in developing more empathetic robots for healthcare, education, and customer service.