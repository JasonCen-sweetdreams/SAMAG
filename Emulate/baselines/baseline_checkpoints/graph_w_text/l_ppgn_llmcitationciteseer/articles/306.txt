Dialogue state tracking (DST) is a crucial component of task-oriented dialogue systems, but existing approaches often lack interpretability. We propose a Hierarchical Attention Network (HAN) architecture for DST, which incorporates both turn-level and dialogue-level attention mechanisms to selectively focus on relevant context. Our model, HAT-DST, learns to extract relevant information from the dialogue history and generate explicit state representations, enabling explainability and improved tracking accuracy. Experimental results on the MultiWOZ dataset demonstrate the effectiveness of HAT-DST in achieving state-of-the-art performance while providing transparent and interpretable DST.