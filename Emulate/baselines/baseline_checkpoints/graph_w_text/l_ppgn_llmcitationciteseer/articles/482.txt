Query expansion is a crucial component of ad-hoc retrieval systems, yet its effectiveness is often limited by the quality of the expansion terms. This paper proposes a novel deep reinforcement learning framework, 'RL-QE', which learns to select optimal expansion terms by maximizing a reward function that balances retrieval effectiveness and query drift. We employ a hierarchical attention mechanism to model the complex relationships between the query, documents, and candidate terms. Experimental results on the TREC Robust04 dataset demonstrate that RL-QE significantly outperforms state-of-the-art query expansion techniques, achieving a 15% improvement in mean average precision.