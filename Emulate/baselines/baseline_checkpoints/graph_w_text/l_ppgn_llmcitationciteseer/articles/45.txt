Social media platforms generate vast amounts of multimodal data, including text, images, and videos. Sentiment analysis of this data is crucial for businesses and policymakers. This paper proposes a hierarchical attention network (HAN) that integrates multimodal features to improve sentiment analysis. Our HAN model consists of three stages: intra-modal attention, inter-modal attention, and fusion. We evaluate our approach on a large-scale social media dataset and demonstrate significant improvements in sentiment analysis accuracy compared to state-of-the-art unimodal and multimodal baselines.