Robot arm control in uncertain environments is a challenging problem in robotics. This paper proposes a hierarchical reinforcement learning (HRL) framework, 'HierRL', which decomposes the task into high-level goal-directed planning and low-level motion control. We introduce a novel option-learning mechanism that enables the agent to adapt to changing environmental conditions by selecting appropriate sub-policies. Experimental results on a real-world robot arm platform demonstrate that HierRL outperforms state-of-the-art model-free RL methods in terms of task completion efficiency and adaptability to uncertainty.