Emotion recognition is a crucial component of human-computer interaction (HCI) systems. With the proliferation of multimodal interfaces, it is essential to develop models that can effectively integrate and process heterogeneous data from various sources. This paper proposes a novel hierarchical attention network (HAN) architecture for multimodal emotion recognition. Our approach leverages attention mechanisms to selectively focus on salient features from different modalities, such as speech, text, and facial expressions. Experimental results on the Multimodal Emotion Recognition dataset demonstrate that our HAN model outperforms state-of-the-art methods, achieving an F1-score of 0.92 in recognizing six basic emotions.