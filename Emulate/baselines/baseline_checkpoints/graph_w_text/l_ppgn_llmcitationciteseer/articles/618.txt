Emotion recognition plays a crucial role in creating empathetic human-computer interfaces. This paper introduces EmoTact, a novel multimodal framework that leverages facial expressions, speech cues, and physiological signals to recognize emotions in real-time. Our approach utilizes a deep learning architecture that fuses features from multiple modalities, achieving state-of-the-art performance on the benchmark EmoReact dataset. We also present a user study demonstrating the effectiveness of EmoTact in enhancing user experience in a virtual reality application.