Gesture-based interfaces in virtual reality (VR) often suffer from low accuracy and user frustration due to inadequate visual feedback. This paper presents a novel approach to designing adaptive visual feedback for gesture recognition in VR. We propose a machine learning-based framework that dynamically adjusts the visual feedback based on the user's performance, gesture complexity, and cognitive load. Our user study with 30 participants shows that the adaptive feedback significantly improves gesture recognition accuracy and reduces user frustration, enabling more intuitive and immersive VR experiences.