Deep reinforcement learning (DRL) has achieved impressive successes in various domains, but its vulnerability to adversarial attacks raises concerns about its reliability in safety-critical applications. This paper proposes an adversarial experience replay (AER) framework that enhances the robustness of DRL agents against perturbed state observations. We introduce a novel algorithm that generates adversarial transitions using a learned perturbation model and incorporates them into the experience replay buffer. Our experiments on Atari games and robotic control tasks demonstrate that AER significantly improves the resilience of DRL agents to adversarial attacks, while maintaining their performance in nominal environments.