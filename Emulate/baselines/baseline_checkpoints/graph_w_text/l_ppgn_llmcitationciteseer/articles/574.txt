This paper proposes a novel neural ranking model, MultiModalRank, designed for efficient multi-modal retrieval in information retrieval systems. By leveraging a hierarchical attention mechanism and a knowledge distillation approach, MultiModalRank effectively fuses textual and visual features to improve retrieval accuracy. Experiment results on a large-scale dataset demonstrate that MultiModalRank outperforms state-of-the-art models in terms of both retrieval effectiveness and computational efficiency.