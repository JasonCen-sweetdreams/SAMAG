Multimodal dialogue systems have shown promising results in human-computer interaction, but their lack of transparency and interpretability hinders user trust. This paper presents a novel hierarchical attention framework, 'HAT', which enables explainable multimodal dialogue understanding. HAT incorporates visual and linguistic attention mechanisms to generate contextualized embeddings, which are then used to predict dialogue responses. Experimental results on the Multimodal Dialogue Dataset demonstrate that HAT outperforms state-of-the-art models in terms of response accuracy and dialogue coherence, while providing visualized attention maps for enhanced interpretability.