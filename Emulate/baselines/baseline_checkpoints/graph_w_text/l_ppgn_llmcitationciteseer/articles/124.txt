Dense passage retrieval (DPR) has emerged as a prominent paradigm for open-domain question answering. However, DPR models often struggle to generalize to unseen queries, leading to suboptimal retrieval performance. This paper proposes a neural query expansion (NQE) framework that leverages a transformer-based encoder to generate semantically enriched query representations. We introduce a novel training objective that encourages the NQE model to generate queries that are both diverse and relevant to the original query. Experimental results on several benchmark datasets demonstrate that NQE significantly improves the retrieval effectiveness of DPR models, especially for rare or out-of-vocabulary queries.