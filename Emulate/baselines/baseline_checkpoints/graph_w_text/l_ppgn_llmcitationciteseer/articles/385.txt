Deep reinforcement learning (DRL) has shown remarkable success in various domains, but its vulnerability to adversarial attacks raises concerns about its reliability in real-world applications. This paper proposes a novel adversarial training framework, 'AdvRL', which incorporates carefully crafted perturbations into the training process to enhance the robustness of DRL policies. We introduce a dynamic perturbation schedule that adapts to the policy's sensitivity to attacks, thereby improving its resilience to varying adversarial strengths. Experimental results on popular DRL benchmarks demonstrate that AdvRL significantly improves the robustness of DRL policies while maintaining their performance on original tasks.