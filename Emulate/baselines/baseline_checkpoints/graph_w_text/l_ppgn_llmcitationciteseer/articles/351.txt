Sentiment analysis has become a crucial task in natural language processing, but existing approaches often struggle with multi-modal data. This paper proposes a novel Hierarchical Attention Network (HAN) that effectively integrates textual, visual, and acoustic features for sentiment analysis. The proposed HAN utilizes a hierarchical attention mechanism to selectively focus on salient features across modalities, leading to improved sentiment prediction accuracy. Experimental results on a large-scale multi-modal dataset demonstrate the superiority of HAN over state-of-the-art approaches, highlighting its potential for real-world applications.