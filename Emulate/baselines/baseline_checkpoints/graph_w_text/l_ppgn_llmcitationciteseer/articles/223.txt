Effective disaster response relies on the coordination of multiple agents, including emergency responders, drones, and autonomous vehicles. This paper proposes a hierarchical reinforcement learning framework to optimize agent coordination in dynamic, partially-observable environments. Our approach combines a high-level planner with low-level controllers, enabling agents to adapt to changing circumstances while minimizing communication overhead. We demonstrate the efficacy of our approach in a simulated disaster response scenario, achieving significant reductions in response time and resource utilization compared to decentralized or scripted approaches.