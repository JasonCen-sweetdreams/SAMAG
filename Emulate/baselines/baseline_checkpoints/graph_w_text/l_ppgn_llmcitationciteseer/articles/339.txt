Accurate affective state recognition is crucial for developing empathetic human-computer interfaces. This paper presents EmoTract, a novel multimodal framework that integrates computer vision, speech processing, and physiological signal analysis to recognize users' emotional states. We propose a hierarchical attention mechanism that captures complex relationships between modalities and contextual cues. Experimental results on a large-scale HCI dataset demonstrate EmoTract's superiority in recognizing subtle emotional shifts, outperforming state-of-the-art unimodal and multimodal approaches. We discuss implications for affective computing and HCI applications.