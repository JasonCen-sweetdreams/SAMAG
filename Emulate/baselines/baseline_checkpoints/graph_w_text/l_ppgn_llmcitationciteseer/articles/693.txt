Emotion recognition from multi-modal data, such as speech, text, and vision, remains a challenging problem due to the complexity of human emotions and the variability of modalities. We propose a novel Hierarchical Graph Attention Network (HGAT) that learns to fuse and align features from different modalities in a hierarchical manner. Our model consists of modality-specific graph attention layers that capture intra-modal relationships, followed by a cross-modal fusion layer that leverages attention to weigh the importance of each modality. Experimental results on the CMU-MultiModal dataset demonstrate that HGAT outperforms state-of-the-art methods in recognizing emotions across different modalities.