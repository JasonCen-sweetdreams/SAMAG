Multi-modal sentiment analysis has become increasingly important in various applications, such as customer feedback analysis and social media monitoring. However, existing approaches often suffer from high computational costs and limited scalability. This paper presents a novel hierarchical attention network (HAN) architecture that effectively integrates visual, textual, and acoustic features for efficient multi-modal sentiment analysis. The proposed HAN model leverages attention mechanisms at both local and global levels to selectively focus on salient features and modalities, reducing the computational overhead while improving sentiment prediction accuracy. Experimental results on benchmark datasets demonstrate the superiority of our approach over state-of-the-art methods.