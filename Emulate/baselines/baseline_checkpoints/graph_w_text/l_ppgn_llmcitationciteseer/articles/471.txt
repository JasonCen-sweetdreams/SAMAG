Virtual assistants are increasingly prevalent in smart home and IoT environments, but their voice-based interaction modalities can be limiting. This paper explores the potential of gaze-based interaction with virtual assistants, leveraging eye-tracking technology to enable more natural and intuitive user experiences. We conduct a user study to investigate the impact of gaze-based interaction on user satisfaction, error rates, and task completion times. Our results show that gaze-based interaction can significantly reduce error rates and improve user experience, especially for users with disabilities. We also propose a novel error mitigation strategy that leverages machine learning-based gaze prediction to detect and correct user errors.