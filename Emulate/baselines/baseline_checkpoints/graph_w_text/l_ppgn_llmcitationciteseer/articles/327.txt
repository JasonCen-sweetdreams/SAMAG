Dialogue systems have made significant progress with the advent of deep learning, but their opaqueness hinders trust and understanding. We propose a novel Hierarchical Attention Network (HAN) architecture that leverages multi-modal inputs (text, speech, and vision) to generate interpretable responses. HAN uses a hierarchical attention mechanism to focus on relevant input modalities, contextualize user queries, and provide explicit explanations for its responses. Our experiments on a large-scale multi-modal dialogue dataset demonstrate improved response accuracy, fluency, and human-evaluated explainability compared to state-of-the-art models.