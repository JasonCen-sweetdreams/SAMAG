In dynamic environments, task allocation is a challenging problem that requires adapting to changing conditions and coordinating among multiple agents. This paper proposes a multi-agent reinforcement learning (MARL) framework for distributed task allocation. We introduce a novel communication protocol that enables agents to share information and learn from each other's experiences. Our approach, called 'Dyna Alloc', is evaluated in a simulated disaster response scenario, where agents must allocate tasks to rescue teams in response to uncertain and changing environmental conditions. Results show that Dyna Alloc outperforms traditional centralized allocation methods and achieves near-optimal performance in dynamic environments.