Autonomous vehicles rely on reinforcement learning (RL) to navigate complex scenarios. However, the lack of transparency in RL decision-making hinders trust and safety. This paper proposes 'CLEAR', an explainable RL framework that integrates model-based and model-free approaches. CLEAR leverages attention mechanisms to identify relevant environment features and generate interpretable explanations for policy decisions. We evaluate CLEAR on a simulated autonomous driving benchmark, demonstrating improved interpretability without compromising control performance. Our approach has implications for real-world autonomous systems, enabling human-AI collaboration and increasing public trust.