Emotion recognition from multi-modal data (e.g., speech, text, and vision) has gained significant attention in human-computer interaction. This paper presents a novel hierarchical attention network (HAN) architecture that learns to selectively focus on relevant modalities and features for emotion recognition. Our approach integrates attention mechanisms at multiple levels, enabling the model to provide explainable results by highlighting the most informative modalities and features. Experimental results on the IEMOCAP dataset demonstrate that our HAN outperforms state-of-the-art methods in terms of recognition accuracy and provides interpretable results.