Deep neural networks (DNNs) have been shown to be vulnerable to backdoor attacks, where an attacker injects malicious data to manipulate the model's behavior. We propose a novel adversarial training framework, 'BackdoorGuard', to improve the robustness of DNNs against backdoor attacks. Our approach leverages a two-player game formulation, where a generator network crafts backdoor patterns, and a discriminator network detects and rejects them. We demonstrate the effectiveness of BackdoorGuard on various benchmark datasets, achieving significant improvements in model robustness compared to existing defense methods.