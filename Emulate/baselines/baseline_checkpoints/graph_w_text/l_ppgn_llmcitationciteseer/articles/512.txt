Event detection from multi-modal data (e.g., text, images, videos) is a crucial task in various applications. However, existing methods often rely on complex neural architectures, making it challenging to interpret the decision-making process. This paper proposes a novel hierarchical attention network (HAN) that leverages the strengths of both self-attention and graph attention mechanisms. Our HAN model detects events by learning to focus on relevant modalities, entities, and relationships, providing explainable insights into the detection process. Experimental results on a real-world dataset demonstrate the effectiveness of our approach in improving event detection accuracy while enhancing model interpretability.