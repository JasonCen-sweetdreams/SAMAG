Virtual reality (VR) systems often rely on multimodal interaction, combining speech, gesture, and gaze inputs. However, incorrect recognition of user intent can lead to frustration and errors. This paper proposes a novel gaze-based error detection approach, leveraging machine learning algorithms to identify inconsistencies between gaze patterns and intended actions. We evaluate our approach on a VR-based puzzle game, demonstrating a significant reduction in error rates and improved user experience. Our results have implications for the design of more intuitive and robust multimodal interfaces in VR and beyond.