Multimodal emotion recognition is a challenging task that requires understanding complex interactions between visual, auditory, and textual cues. This paper proposes a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features to improve emotion recognition accuracy. Our HAN architecture consists of modality-specific attention modules that learn to weight features based on their importance, followed by a fusion layer that integrates information across modalities. We evaluate our approach on the CMU-MOSEI dataset and demonstrate state-of-the-art performance on multimodal emotion recognition tasks. Furthermore, we provide visualizations of attention weights to facilitate explainability and interpretability of our model's predictions.