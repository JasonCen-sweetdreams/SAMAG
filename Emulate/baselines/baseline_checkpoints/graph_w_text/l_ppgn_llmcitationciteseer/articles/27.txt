Passage retrieval is a critical component of many question answering and information retrieval systems. However, existing methods often rely on computationally expensive neural networks or sparse keyword-based representations. This paper proposes a novel approach, 'QODE', which learns query-oriented document embeddings that capture the semantic relationships between passages and queries. Our experiments on several benchmark datasets demonstrate that QODE achieves state-of-the-art passage retrieval performance while reducing computational overhead by up to 75%. We further analyze the effectiveness of QODE in various IR applications, including open-domain question answering and dialog systems.