Few-shot learning has gained significant attention in recent years, but existing methods often struggle with scalability and complexity. This paper proposes a novel hierarchical graph attention network (HGAT) that addresses these limitations by learning hierarchical representations of graph-structured data. Our approach leverages attention mechanisms to selectively focus on relevant nodes and edges, enabling efficient few-shot learning on large-scale graphs. We demonstrate the efficacy of HGAT on several benchmark datasets, achieving state-of-the-art performance with reduced computational overhead.