Multi-modal emotion recognition tasks often require processing large amounts of data from various sources, such as audio, video, and text. This paper presents a novel approach to reducing the computational overhead of such tasks by introducing sparse attention networks. Our proposed architecture, 'SparseEmoNet', learns to selectively focus on the most relevant modalities and features, thereby reducing the number of parameters and computations required. Experimental results on the IEMOCAP and CMU-MOSEI datasets demonstrate that SparseEmoNet achieves state-of-the-art performance while reducing inference time by up to 40% compared to existing methods.