Virtual reality (VR) has the potential to revolutionize human-computer interaction, but understanding user emotions in VR remains a significant challenge. We propose a novel multimodal fusion approach that combines facial expression, speech, and physiological signals to recognize emotions in VR. Our framework utilizes a deep learning architecture that integrates convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to extract and fuse features from each modality. Experimental results with a custom-built VR dataset demonstrate that our approach achieves state-of-the-art emotion recognition accuracy, outperforming existing unimodal and multimodal approaches.