Relevance feedback is a crucial component in content-based image retrieval (CBIR) systems, as it enables users to refine their queries based on visual similarity. This paper presents a novel multi-modal relevance feedback approach that leverages both visual and textual features to improve the retrieval efficiency of CBIR systems. Our proposed framework, 'MMRF', incorporates a deep neural network to learn a unified representation of images and text, allowing for more effective user feedback. Experimental results on two benchmark datasets demonstrate that MMRF outperforms state-of-the-art methods in terms of retrieval accuracy and query latency.