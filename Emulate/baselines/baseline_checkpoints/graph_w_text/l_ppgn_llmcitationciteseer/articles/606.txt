Virtual reality (VR) interfaces often suffer from cluttered menus, leading to user frustration and decreased immersion. This paper presents 'GazeMenu', a novel gaze-based adaptive menu system that dynamically reorganizes menu items based on user attention and interaction patterns. We introduce a machine learning model that predicts user intent from gaze data and adapts menu layouts to reduce visual search time and improve selection accuracy. A user study with 30 participants demonstrates that GazeMenu significantly reduces menu interaction time and improves overall VR experience compared to traditional menu systems.