E-commerce search engines rely heavily on textual queries, but users often have multimodal preferences (e.g., images, videos) that are not effectively captured by traditional keyword-based search. This paper proposes a novel context-aware query expansion framework that incorporates visual and textual features to improve multimodal retrieval. Our approach utilizes a graph-based attention mechanism to weigh the importance of different modalities based on the user's query context. Experimental results on a large e-commerce dataset show that our method significantly outperforms state-of-the-art baselines in terms of retrieval accuracy and user satisfaction.