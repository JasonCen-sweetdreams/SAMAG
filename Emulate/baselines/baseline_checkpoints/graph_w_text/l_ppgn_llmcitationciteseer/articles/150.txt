In dynamic environments, task-oriented agents must adapt to changing resource availability and task priorities. This paper presents a hierarchical reinforcement learning framework, 'HRL-Coor', that enables agents to coordinate their actions and optimize resource allocation. Our approach combines a high-level task scheduler with low-level resource allocators, each trained using a variant of Q-learning. We demonstrate the effectiveness of HRL-Coor in a simulated smart factory scenario, showing improved task completion rates and reduced resource waste compared to decentralized or static allocation strategies.