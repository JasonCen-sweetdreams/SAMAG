As robots increasingly interact with humans in various settings, recognizing emotions from multi-modal cues (e.g., speech, facial expressions, and body language) becomes crucial for effective human-robot collaboration. This paper proposes a hierarchical attention network (HAN) that integrates and weighs the contributions of different modalities to improve emotion recognition accuracy. Our HAN model consists of modality-specific attention modules and a hierarchical fusion mechanism that captures complex relationships between modalities. Experimental results on the MPI-EMO dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions in human-robot interaction scenarios, achieving an F1-score of 0.92.