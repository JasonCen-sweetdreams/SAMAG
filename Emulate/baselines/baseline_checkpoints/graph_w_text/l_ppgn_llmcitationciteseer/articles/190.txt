Deep neural networks have been shown to be vulnerable to adversarial attacks, which can be detrimental to their deployment in real-world applications. This paper proposes a novel approach to robustify deep neural networks against adversarial attacks by leveraging Bayesian uncertainty. We develop a Bayesian neural network framework that incorporates uncertainty estimates into the model's predictions, allowing for the detection of adversarial samples. Our approach is shown to be effective in improving the robustness of deep neural networks against various types of adversarial attacks, including FGSM and PGD. Experimental results demonstrate the efficacy of our approach on several benchmark datasets.