Autonomous driving has made significant progress in recent years, but real-world deployment remains a challenge due to the complexity and variability of urban environments. This paper proposes a hierarchical reinforcement learning (HRL) framework, 'Hierarchical Autonomous Vehicle' (HAV), which leverages a scalable and modular architecture to tackle long-horizon decision-making tasks in autonomous driving. HAV decomposes the driving task into high-level goals and low-level actions, allowing for more efficient exploration and adaptation to new scenarios. We demonstrate the efficacy of HAV on a large-scale simulated driving dataset and a real-world autonomous vehicle platform, achieving improved performance and safety compared to state-of-the-art flat RL approaches.