Conventional document retrieval systems rely on handcrafted ranking models or supervised learning approaches, which often struggle to generalize to diverse query types and document collections. This paper proposes a hierarchical reinforcement learning (HRL) framework, 'DocHRL', that learns to optimize document retrieval by modeling the search process as a Markov decision process. Our approach leverages a hierarchical policy that adaptively selects the most relevant documents at each level of the retrieval process, resulting in significant improvements in retrieval efficiency and effectiveness. Experimental results on several benchmark datasets demonstrate the superiority of DocHRL over state-of-the-art retrieval models.