Transformers have become the de facto standard for natural language processing tasks, but their vulnerability to adversarial attacks raises concerns about their reliability. This paper presents a novel approach to improving the robustness of transformers by regularizing their hierarchical attention mechanism. We introduce a new penalty term that encourages the model to focus on relevant input regions, reducing the impact of perturbations. Experiments on several benchmark datasets demonstrate that our method, 'HAR-Transformer', achieves state-of-the-art performance under various attack scenarios, while maintaining competitive results on clean data.