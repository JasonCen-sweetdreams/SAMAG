Deep neural networks (DNNs) are vulnerable to adversarial attacks, which can compromise their performance and reliability. This paper presents a novel approach to detect adversarial attacks on DNNs using graph-based anomaly analysis. We model the DNN's intermediate representations as a graph and identify anomalous patterns indicative of adversarial attacks. Our method, dubbed 'GraphGuard', leverages graph convolutional networks and attention mechanisms to detect subtle changes in the graph structure. Experimental results on benchmark datasets demonstrate the effectiveness of GraphGuard in detecting various types of adversarial attacks, including input manipulation and poisoning attacks.