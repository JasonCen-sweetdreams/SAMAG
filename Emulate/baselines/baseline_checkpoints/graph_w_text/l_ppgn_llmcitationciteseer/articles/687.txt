Emotion recognition from multi-modal data (e.g., audio, video, and text) has seen significant progress, but most approaches lack interpretability. This paper proposes a novel Hierarchical Attention-based Multi-Modal Fusion (HAMMF) framework, which leverages attention mechanisms to selectively focus on relevant modalities and features. We introduce a novel Explainable Emotion Recognition (EER) metric to quantify the model's interpretability. Experimental results on three benchmark datasets demonstrate that HAMMF outperforms state-of-the-art methods in both recognition accuracy and EER, providing insights into the emotional cues exploited by the model.