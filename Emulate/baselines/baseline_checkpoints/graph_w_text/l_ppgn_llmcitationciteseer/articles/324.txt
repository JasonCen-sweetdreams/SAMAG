Emotion recognition from multi-modal inputs, such as speech, text, and vision, is crucial for human-computer interaction. However, existing approaches often suffer from high computational complexity and limited interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) that efficiently integrates and selectively focuses on relevant modalities. Our HAN model employs a hierarchical structure to model intra- and inter-modal relationships, resulting in improved recognition accuracy and reduced computational cost. Extensive experiments on benchmark datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance while reducing inference time by up to 30%