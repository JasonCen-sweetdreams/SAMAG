Zero-shot learning (ZSL) has gained popularity in recent years, but most existing methods rely on expensive annotation and suffer from poor scalability. This paper proposes a hierarchical attention network (HAN) for efficient ZSL, which leverages semantic relationships between classes to transfer knowledge from seen to unseen categories. Our HAN model consists of a class-attention module that adaptively weights class embeddings and a feature-attention module that selectively focuses on relevant input features. Experimental results on benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in terms of classification accuracy and computational efficiency.