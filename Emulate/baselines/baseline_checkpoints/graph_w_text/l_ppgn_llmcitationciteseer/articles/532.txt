Sentiment analysis has become increasingly important in natural language processing, but existing methods struggle to effectively fuse multimodal features from text, image, and audio inputs. We propose a novel Hierarchical Graph Attention Network (HGAT) that learns to attend to relevant features across modalities, capturing complex relationships between them. Our approach leverages graph-based representation learning to model the intricate structures present in multimodal data. Experimental results on benchmark datasets demonstrate that HGAT outperforms state-of-the-art methods, achieving higher accuracy and robustness in sentiment classification tasks.