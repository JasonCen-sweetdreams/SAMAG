Affective state recognition is crucial in human-computer interaction, but existing approaches rely on either facial expressions or physiological signals, neglecting the complementary information they provide. We introduce EmoTract, a novel multimodal fusion framework that leverages facial action units and physiological signals (heart rate, skin conductance, and electroencephalography) to recognize users' emotional states. Our approach employs a graph convolutional network to model the relationships between facial features and physiological signals, achieving an improvement of 12.5% in F1-score over baseline methods. EmoTract enables more accurate and robust affective state recognition, with potential applications in mental health monitoring, personalized recommendation systems, and human-centered AI.