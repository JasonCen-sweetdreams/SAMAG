Graph Neural Networks (GNNs) have shown remarkable success in various applications, but they are vulnerable to adversarial attacks. This survey provides a comprehensive overview of existing attack methods on GNNs, including node injection, edge perturbation, and feature manipulation. We also propose a novel defense mechanism, 'GraphShield', which leverages graph autoencoders and adversarial training to improve the robustness of GNNs. Experimental results demonstrate that GraphShield achieves state-of-the-art performance in defending against various types of attacks on benchmark datasets.