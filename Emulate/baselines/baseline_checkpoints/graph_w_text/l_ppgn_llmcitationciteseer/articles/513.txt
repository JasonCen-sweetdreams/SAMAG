Deep reinforcement learning (DRL) has achieved remarkable success in various applications, but its performance heavily relies on careful hyperparameter tuning. This process is often time-consuming and computationally expensive, particularly when dealing with high-dimensional search spaces. We propose a novel Bayesian optimization method, 'BO-DRL', which leverages a probabilistic surrogate model to efficiently search for optimal hyperparameters. Our approach incorporates a customized acquisition function that balances exploration and exploitation, and we demonstrate its effectiveness on several benchmark DRL tasks. Experimental results show that BO-DRL significantly outperforms traditional grid search and random search methods, achieving comparable performance to state-of-the-art techniques at a fraction of the computational cost.