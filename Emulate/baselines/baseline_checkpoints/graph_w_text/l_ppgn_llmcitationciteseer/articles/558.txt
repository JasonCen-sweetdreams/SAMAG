Emotion recognition from multi-modal inputs (e.g., speech, vision, text) is a challenging task that requires robust fusion and interpretation of heterogeneous features. This paper presents a novel Hierarchical Attention Network (HAN) architecture that leverages self-attention mechanisms to selectively focus on relevant modalities and features. We demonstrate that HAN outperforms state-of-the-art multi-modal fusion methods on two benchmark datasets, achieving improved emotion recognition accuracy and providing interpretable attention weights that reveal the most salient input features. Our approach has implications for affective computing, human-computer interaction, and AI-driven decision-making.