Emotion recognition is a crucial aspect of human-computer interaction, but existing approaches often rely on uni-modal features, failing to capture the complexity of human emotions. This paper proposes a novel deep hierarchical clustering framework, 'EmoClus', which integrates multimodal features from speech, text, and facial expressions. We introduce a hierarchical clustering algorithm that iteratively refines emotion clusters, resulting in more accurate and nuanced recognition of emotions. Experiments on a large-scale multimodal dataset demonstrate the superiority of EmoClus over state-of-the-art approaches, achieving an average emotion recognition accuracy of 85.2%.