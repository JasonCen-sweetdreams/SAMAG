Graph neural networks have achieved state-of-the-art performance in node classification tasks, but their computational complexity grows quadratically with the number of nodes. This paper introduces Hierarchical Graph Attention Networks (HGAT), a novel architecture that leverages hierarchical graph representations to reduce computational costs while maintaining performance. HGAT applies attention mechanisms at multiple scales, allowing the model to focus on relevant nodes and subgraphs. We evaluate HGAT on several benchmark datasets, demonstrating improved scalability and competitive performance compared to existing graph neural network models.