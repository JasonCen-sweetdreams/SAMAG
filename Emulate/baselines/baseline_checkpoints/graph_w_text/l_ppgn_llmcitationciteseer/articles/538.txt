Knowledge graph embedding (KGE) has become a crucial step in various AI applications, but existing methods struggle to capture complex relation patterns. This paper proposes a novel KGE approach, 'ARAE', which incorporates adaptive relation attention to selectively focus on relevant relation triples. We introduce a neural attention mechanism that learns to weigh relation importance based on entity embeddings and graph structure. Experimental results on benchmark datasets show that ARAE outperforms state-of-the-art KGE methods in link prediction and entity classification tasks, while reducing computational overhead.