Deep neural networks are vulnerable to data poisoning attacks, where an adversary manipulates the training data to compromise the model's performance. This paper proposes a novel adversarial training framework, 'PoisonGuard', which proactively defends against data poisoning attacks by perturbing the training data in a way that simulates potential attacks. We demonstrate that PoisonGuard improves the robustness of deep neural networks against various data poisoning scenarios, including label flipping and backdoor attacks. Our experiments on multiple benchmark datasets show that PoisonGuard achieves state-of-the-art performance in detecting and mitigating data poisoning attacks, while maintaining the model's accuracy on clean data.