In complex multi-agent systems, decision-making often relies on opaque AI models, hindering trust and explainability. We propose a novel hierarchical attention network (HAN) architecture that enables transparent and interpretable decision-making in multi-agent settings. By integrating attention mechanisms across agent hierarchies, our approach captures subtle dependencies and relationships between agents, improving the overall decision-making process. Experimental results on a real-world autonomous vehicle dataset demonstrate that HAN outperforms state-of-the-art methods in terms of decision quality and explainability, while reducing the need for explicit feature engineering.