Multimodal dialogue systems have become increasingly popular, but their lack of transparency hinders trust and understanding. This paper proposes a hierarchical attention network (HAN) architecture that incorporates visual, acoustic, and textual features to generate more accurate and explainable responses. Our approach uses a novel multimodal fusion mechanism to capture cross-modal relationships and a hierarchical attention module to selectively focus on relevant input features. Experimental results on two benchmark datasets demonstrate that our HAN model outperforms state-of-the-art baselines in response accuracy and generating more interpretable explanations.