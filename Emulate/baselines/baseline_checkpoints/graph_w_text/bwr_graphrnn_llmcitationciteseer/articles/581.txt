Dialogue systems have become increasingly prevalent in human-computer interaction, but their underlying decision-making processes remain opaque. This paper presents a novel hierarchical attention network (HAN) architecture for multi-modal dialogue systems, capable of generating interpretable explanations for its responses. Our approach incorporates visual and textual context through multi-modal fusion, and applies attention mechanisms at both utterance and token levels to identify salient features. Experimental results on a benchmark dataset demonstrate that our HAN model achieves state-of-the-art performance in response generation while providing transparent and meaningful explanations.