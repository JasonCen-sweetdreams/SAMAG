This paper presents a novel gesture-based interface framework, 'AccessibleMotion', designed to improve the usability and accessibility of touchscreen devices for visually impaired users. We introduce a machine learning-driven gesture recognition engine that adapts to individual users' motor abilities and preferences, enabling more accurate and efficient interaction. Our user study with 20 visually impaired participants demonstrates significant improvements in navigation speed and accuracy compared to existing assistive technologies.