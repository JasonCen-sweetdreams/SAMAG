Neural retrieval models have revolutionized information retrieval by leveraging dense vector representations. However, these models often struggle with query expansion, leading to suboptimal retrieval performance. This paper proposes an adaptive re-ranking framework, 'AdaQE', which dynamically adjusts the query expansion process based on the retrieval context. By incorporating a reinforcement learning-based controller, AdaQE learns to optimize the trade-off between query representation and document similarity. Experimental results on the TREC Deep Learning Track dataset demonstrate significant improvements in retrieval effectiveness and efficiency compared to state-of-the-art neural retrieval models.