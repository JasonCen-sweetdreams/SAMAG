Autonomous vehicles require real-time decision-making to navigate complex scenarios. We propose a novel hierarchical reinforcement learning (HRL) framework that leverages a two-level abstraction to balance exploration and exploitation. The high-level policy focuses on strategic decision-making, while the low-level policy executes tactical control actions. We introduce a novel experience replay mechanism that adaptively samples transitions based on their relevance to the current state, reducing the need for large-scale exploration. Experimental results demonstrate that our approach achieves superior performance and faster adaptation to changing environments compared to flat reinforcement learning methods.