Coordinating multi-agent systems in complex environments remains a challenging problem. This paper introduces a novel distributed reinforcement learning framework that leverages graph attention to facilitate cooperation among agents. Our approach, dubbed 'GraphCoop', enables agents to learn coordinated policies by attending to relevant neighbors in a graph-structured state space. We demonstrate the effectiveness of GraphCoop in a variety of simulated environments, including traffic control and robotic swarm navigation, and show that it outperforms existing decentralized learning methods in terms of convergence speed and policy quality.