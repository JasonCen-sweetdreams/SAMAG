Deep reinforcement learning (DRL) has achieved remarkable success in various domains, but its vulnerability to adversarial attacks raises concerns about its reliability. This paper presents a novel adversarial training framework, 'AdvRL', which leverages a min-max optimization strategy to enhance the robustness of DRL policies. We demonstrate that AdvRL can effectively defend against both white-box and black-box attacks, while maintaining competitive performance on standard benchmarks. Our extensive experiments on Atari games and robotic control tasks show that AdvRL can significantly improve the robustness of DRL agents, highlighting its potential for real-world applications.