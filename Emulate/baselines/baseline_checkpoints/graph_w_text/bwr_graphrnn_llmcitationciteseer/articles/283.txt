Multimodal sentiment analysis has gained significant attention in recent years, but existing methods often lack interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) that integrates textual, visual, and acoustic features for sentiment analysis. Our HAN model employs a hierarchical attention mechanism that selectively focuses on salient features and modalities, providing explicit explanations for sentiment predictions. We conduct extensive experiments on three benchmark datasets, demonstrating that our approach outperforms state-of-the-art methods while providing insightful explanations for multimodal sentiment analysis.