This paper presents a novel approach to coordinating multi-agent systems using reinforcement learning and graph attention. We propose a decentralized framework, 'GraphMARL', which enables agents to learn cooperative policies by attending to relevant neighboring agents and their interactions. Our approach integrates a graph neural network with a multi-agent reinforcement learning algorithm, allowing agents to adapt to dynamic environments and improve overall system performance. Experimental results on a simulated traffic control scenario demonstrate that GraphMARL outperforms existing methods in terms of global reward and scalability.