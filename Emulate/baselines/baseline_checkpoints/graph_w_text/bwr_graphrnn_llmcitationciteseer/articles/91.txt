Emotion recognition from multimodal inputs (e.g., speech, text, vision) is a crucial task in human-computer interaction. However, existing methods often lack interpretability, making it challenging to understand their decision-making processes. This paper proposes a Hierarchical Attention Network (HAN) that incorporates explainable attention mechanisms to recognize emotions from multimodal data. We introduce a novel hierarchical fusion strategy that selectively weights and combines attention weights from different modalities. Experimental results on the IEMOCAP dataset demonstrate that our approach achieves state-of-the-art performance while providing insightful attention visualizations that elucidate the emotional cues exploited by the model.