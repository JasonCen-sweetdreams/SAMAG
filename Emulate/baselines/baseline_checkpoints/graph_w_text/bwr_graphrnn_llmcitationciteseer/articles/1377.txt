Voice assistants have become ubiquitous, but their adoption by older adults is hindered by difficulties in speech recognition, navigation, and comprehension. This paper presents a multimodal interaction framework, 'EasySpeak', which integrates visual, auditory, and tactile cues to facilitate more accessible and engaging interactions. We conducted a user study with 30 older adults, demonstrating significant improvements in task completion rates, user satisfaction, and cognitive load reduction. Our findings inform the design of more inclusive and supportive voice assistants, enabling older adults to fully benefit from smart home technologies.