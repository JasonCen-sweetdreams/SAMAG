Gaze estimation is a crucial component in human-computer interaction (HCI), enabling applications like gaze-based interfaces and visual attention analysis. We present EyeGaze+, a deep learning-based gaze estimation system that leverages a novel multi-modal fusion approach combining eye images and 3D head pose information. Our system achieves state-of-the-art performance on the MPIIGaze dataset, with a mean absolute error of 4.21Â°. We also demonstrate the effectiveness of EyeGaze+ in a real-world HCI scenario, where users interacted with a gaze-based interface to select targets on a screen with high accuracy.