Recommender systems have become increasingly prevalent in online platforms, yet their opacity often raises concerns about bias and fairness. This paper introduces 'HierGAT', a hierarchical graph attention network that incorporates both item and user relationships to provide interpretable recommendations. We demonstrate that HierGAT outperforms state-of-the-art models on three benchmark datasets, while offering visual explanations for its suggestions through attention weights. Our approach enables users to understand the reasoning behind recommendations, fostering trust and transparency in AI-driven systems.