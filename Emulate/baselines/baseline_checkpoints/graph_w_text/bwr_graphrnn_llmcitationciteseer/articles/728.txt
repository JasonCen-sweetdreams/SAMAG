Knowledge graph embeddings have proven effective in various AI applications, but their scalability remains a major bottleneck. This paper presents HGRAN, a novel hierarchical graph attention network architecture that learns multi-relational knowledge graph embeddings at scale. HGRAN employs a hierarchical attention mechanism that captures complex relationships between entities and their context, while reducing computational complexity through a novel sparse attention mechanism. Our experiments on large-scale knowledge graphs demonstrate that HGRAN achieves state-of-the-art performance in link prediction and entity classification tasks, while being orders of magnitude faster than existing methods.