Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the inherent complexity of human emotions and the variability of modalities. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) framework that leverages the strengths of graph neural networks and attention mechanisms to model the intricate relationships between modalities. Our approach learns to weigh the importance of each modality and captures contextual dependencies through hierarchical graph attention. Experimental results on the CMU-MOSEI dataset demonstrate that HGAT outperforms state-of-the-art methods in recognizing emotions from multi-modal data.