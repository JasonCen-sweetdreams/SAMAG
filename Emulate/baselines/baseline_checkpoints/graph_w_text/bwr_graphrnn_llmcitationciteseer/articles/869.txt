Neural architecture search (NAS) has emerged as a promising technique for automating the design of deep neural networks. However, existing NAS methods are often computationally expensive and require significant resources. This paper proposes a novel gradient-based meta-learning approach, 'MetaNAS', which leverages the concept of meta-learning to efficiently search for optimal neural architectures. By learning a meta-model that adapts to various architecture search tasks, MetaNAS reduces the computational cost of NAS by an order of magnitude while achieving competitive performance on several benchmark datasets.