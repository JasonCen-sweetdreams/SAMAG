Deep reinforcement learning (DRL) has achieved impressive results in various domains, but its brittleness to adversarial attacks hinders its deployment in real-world scenarios. This paper proposes a novel constrained policy optimization framework, 'ARPO', which incorporates adversarial robustness into the policy learning process. By formulating the policy optimization problem as a constrained Markov decision process, ARPO ensures that the learned policy is not only optimal but also resilient to a wide range of adversarial perturbations. We demonstrate the effectiveness of ARPO on several benchmark environments, showcasing its ability to withstand attacks while maintaining competitive performance.