Emotion recognition systems have been increasingly employed in various human-computer interaction applications. However, existing approaches often rely on visual or auditory cues, overlooking the potential of haptic feedback. This paper proposes a novel multimodal emotion recognition framework that leverages deep learning techniques to integrate facial expressions, speech patterns, and vibrotactile feedback. Our system, 'EmoTouch', uses a wearable device to provide subtle vibrations that enhance user experience and improve emotion recognition accuracy. Experimental results demonstrate that EmoTouch outperforms state-of-the-art unimodal and multimodal approaches, achieving an F1-score of 0.92 in a real-world affective computing dataset.