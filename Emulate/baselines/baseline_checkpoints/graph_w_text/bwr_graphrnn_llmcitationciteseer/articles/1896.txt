Autonomous vehicles require efficient and safe control strategies to navigate complex scenarios. This paper presents a hierarchical reinforcement learning framework that integrates uncertainty-aware exploration with a probabilistic motion forecasting module. Our approach, 'HRL-UAE', leverages a high-level policy to select optimal goal-directed actions while a low-level controller adapts to uncertain environment dynamics. We demonstrate the efficacy of HRL-UAE in simulated urban driving scenarios, showcasing improved navigation performance and reduced uncertainty compared to state-of-the-art model-free RL methods.