Augmented reality (AR) systems require intuitive and efficient user interfaces. We propose a novel gaze-based interaction method that leverages deep reinforcement learning to predict user intent from eye movement data. Our approach employs a hierarchical policy framework, combining a high-level goal-directed policy with a low-level motor control policy. The system is trained using a large-scale dataset of human gaze patterns and AR interactions. Experimental results demonstrate significant improvements in interaction speed and accuracy compared to traditional manual input methods, paving the way for more immersive and accessible AR experiences.