Multimodal human-computer interaction (HCI) systems that incorporate gaze-based input can enhance user experience and accessibility. This paper presents a novel intention recognition framework that leverages machine learning and computer vision techniques to decode users' gaze patterns and infer their intentions in real-time. Our approach integrates a probabilistic gaze estimation model with a neural network-based intention classifier, achieving a recognition accuracy of 92.5% on a diverse dataset of users. We demonstrate the feasibility of our approach through a proof-of-concept HCI system that enables gaze-controlled navigation and selection in a virtual environment.