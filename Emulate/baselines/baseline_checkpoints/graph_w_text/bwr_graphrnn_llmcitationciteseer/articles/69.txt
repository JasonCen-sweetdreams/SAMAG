Recent advancements in knowledge graph embedding have shown promise in question answering tasks. However, existing methods often disregard the hierarchical structure inherent in knowledge graphs, leading to suboptimal performance. This paper proposes a novel hierarchical graph attention mechanism, 'HGA-QA', which learns to focus on relevant entities and relations at different levels of abstraction. We evaluate HGA-QA on several benchmark datasets, demonstrating significant improvements in question answering accuracy and reduced computational overhead compared to state-of-the-art methods.