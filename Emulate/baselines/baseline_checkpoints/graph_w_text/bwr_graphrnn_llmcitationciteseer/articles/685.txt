Passage retrieval is a crucial component in many information retrieval systems. However, the effectiveness of traditional query expansion methods is limited by their reliance on term frequency and co-occurrence statistics. This paper presents a novel approach that leverages neural embeddings to capture semantic relationships between query terms and passage contents. Our proposed method, 'EmbedExpan', uses a transformer-based architecture to learn dense representations of query terms and passages, and then performs query expansion by identifying the most similar terms in the embedding space. Experimental results on several benchmark datasets demonstrate that EmbedExpan outperforms state-of-the-art query expansion methods, achieving significant improvements in retrieval accuracy and efficiency.