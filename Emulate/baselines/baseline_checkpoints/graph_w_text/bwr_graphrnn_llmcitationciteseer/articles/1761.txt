This paper addresses the challenging problem of multi-modal fusion in computer vision tasks, where the optimal combination of modalities and neural architectures is critical. We propose a novel neural architecture search (NAS) method, 'MMF-NAS', which leverages a hierarchical search space and a differentiable neural predictor to efficiently search for the best fusion strategy. Our experiments on several benchmark datasets demonstrate that MMF-NAS outperforms state-of-the-art methods in terms of accuracy and computational efficiency, while also providing interpretable insights into the importance of different modalities.