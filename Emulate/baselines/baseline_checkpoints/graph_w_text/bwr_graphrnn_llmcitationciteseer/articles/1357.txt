Multimodal fusion in Visual Question Answering (VQA) remains a challenging task due to the complexity of aligning and integrating visual and textual features. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant regions of the image and words in the question, enabling more accurate and interpretable answers. Our approach leverages a hierarchical structure to model the relationships between modalities, and incorporates a novel attention mechanism that adaptively weights the importance of each modality. Experimental results on the VQA 2.0 dataset demonstrate significant improvements in accuracy and explanation quality compared to state-of-the-art methods.