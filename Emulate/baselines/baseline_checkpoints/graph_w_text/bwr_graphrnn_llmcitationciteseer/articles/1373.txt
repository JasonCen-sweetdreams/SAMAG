Graph Neural Networks (GNNs) have achieved state-of-the-art performance in node classification tasks. However, their vulnerability to adversarial attacks remains largely unexplored. This paper investigates the susceptibility of GNNs to node-level perturbations and proposes a novel attack strategy, GraphAttacker. We develop a reinforcement learning framework to learn perturbation strategies that maximize the misclassification rate. Our experiments on several benchmark datasets demonstrate that GraphAttacker can significantly degrade the performance of GNNs, highlighting the need for robustness measures in graph-based AI applications.