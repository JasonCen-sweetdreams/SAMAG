Deep learning models have achieved state-of-the-art performance in medical image analysis tasks, but their lack of transparency hinders trust in AI-driven decision-making. This paper proposes a novel hierarchical attention network (HAN) architecture that generates interpretable explanations for AI-driven diagnoses. Our HAN model recursively applies attention mechanisms to identify relevant regions and features in medical images, enabling the generation of explainable heatmaps and feature importance scores. Experimental results on a large MRI dataset demonstrate that our approach improves model interpretability without sacrificing diagnostic accuracy.