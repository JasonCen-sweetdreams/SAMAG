Virtual reality (VR) has the potential to revolutionize accessibility, but existing control mechanisms can be limiting for users with disabilities. This paper presents 'GazeGest', a novel gaze-based gesture recognition system that enables intuitive interaction with VR environments. Our approach leverages machine learning models to classify gaze patterns and identify corresponding gestures, achieving an accuracy of 92.5% in user studies. We also introduce a novel gaze-based gesture vocabulary, designed in collaboration with accessibility experts, to facilitate seamless interaction with VR applications.