Deep reinforcement learning (DRL) has achieved remarkable success in various domains, but its lack of interpretability hinders its adoption in high-stakes applications. This paper proposes a novel explainability framework, 'RL-EX', which leverages model-based reinforcement learning to generate human-interpretable explanations for DRL policies. RL-EX integrates a model-based RL component that learns a dynamics model of the environment and a generative model that produces explanations in the form of natural language and visualizations. Our experiments on Atari games and real-world robotics tasks demonstrate that RL-EX provides faithful and informative explanations without compromising policy performance.