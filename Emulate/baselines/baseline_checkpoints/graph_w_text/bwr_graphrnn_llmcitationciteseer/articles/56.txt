Accurate disease diagnosis relies on the effective integration of multi-modal data from various sources, including medical images, electronic health records, and genomic data. We propose a novel Hierarchical Graph Attention Network (HiGAT) framework that leverages the strengths of graph neural networks and attention mechanisms to model complex relationships between multi-modal data. HiGAT learns to selectively focus on relevant features and interactions, leading to improved diagnostic performance. Experimental results on a large-scale dataset demonstrate the superiority of HiGAT over state-of-the-art methods, with significant improvements in diagnosis accuracy and interpretability.