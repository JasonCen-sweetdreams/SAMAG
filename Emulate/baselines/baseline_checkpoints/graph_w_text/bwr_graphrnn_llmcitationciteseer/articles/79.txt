Affective computing has numerous applications in human-computer interaction, but recognizing emotional states from multimodal input remains a challenging task. This paper presents an approach that leverages transfer learning to improve emotional state recognition from facial expressions, speech, and physiological signals. We fine-tune a pre-trained convolutional neural network (CNN) on a large affective dataset and adapt it to our target multimodal dataset using a novel domain adaptation technique. Experimental results show that our approach outperforms state-of-the-art methods in recognizing emotional states, achieving an accuracy of 87.2% on the benchmark dataset.