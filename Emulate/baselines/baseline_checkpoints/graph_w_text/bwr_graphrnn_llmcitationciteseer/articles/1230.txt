Visual Question Answering (VQA) models often rely on complex neural networks, making it challenging to interpret their decision-making processes. We present HANA, a Hierarchical Attention Network architecture that enables explainable VQA by identifying relevant regions in the image and questions. Our model consists of a question-guided attention mechanism that selects relevant objects and a hierarchical reasoning module that contextualizes object interactions. Experimental results on the VQA 2.0 dataset demonstrate that HANA outperforms state-of-the-art models while providing visualizations that illustrate the reasoning process.