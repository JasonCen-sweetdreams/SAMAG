Multimodal sentiment analysis (MSA) aims to predict sentiment from heterogeneous data sources such as text, images, and videos. Existing approaches often rely on early fusion or late fusion strategies, which may not effectively capture complex interactions between modalities. This paper proposes a hierarchical attention neural network (HANN) for MSA, which recursively applies attention mechanisms to different modalities and fusion levels. Our experiments on the CMU-MOSI and YouTube datasets show that HANN outperforms state-of-the-art methods by 3.2% and 2.5% in terms of accuracy, respectively. We also provide an in-depth analysis of the learned attention patterns, revealing insights into the importance of modality interactions in MSA.