Accurate disease diagnosis is often hindered by the complexity of integrating multiple data modalities. This paper presents a novel Hierarchical Graph Attention Network (HGAT) framework that leverages graph neural networks and attention mechanisms to integrate multi-modal data from electronic health records, medical imaging, and genomic data. HGAT learns to focus on relevant features and relationships across modalities, enabling accurate diagnosis and identification of disease subtypes. Experiments on a large-scale cancer diagnosis dataset demonstrate that HGAT outperforms state-of-the-art methods, achieving a 12% improvement in diagnosis accuracy.