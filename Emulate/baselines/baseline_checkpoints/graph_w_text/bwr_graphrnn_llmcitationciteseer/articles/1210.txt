Visual Question Answering (VQA) models have achieved impressive performance, but their lack of transparency hinders trust and understanding. We propose a novel attention-based framework, 'ExplainVQA', which generates explanations for its reasoning process. Our approach leverages a multi-modal attention mechanism to selectively focus on relevant image regions and question words, enabling the model to provide interpretable answers. Experimental results on VQA 2.0 demonstrate that ExplainVQA achieves state-of-the-art performance while providing insightful explanations for its predictions.