Deep reinforcement learning (DRL) agents have achieved impressive results, but their decision-making processes remain opaque. To address this, we introduce a novel approach, ContraX, which incorporates contrastive regularization into the policy optimization process. ContraX encourages the agent to learn attributions that distinguish between successful and failed trajectories, thereby providing human-interpretable explanations for its actions. We evaluate ContraX on a range of Atari games and demonstrate significant improvements in both policy performance and attribution quality. Our results have important implications for the development of transparent and trustworthy AI systems.