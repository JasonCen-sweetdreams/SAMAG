This paper presents a novel approach to cooperative exploration in multi-agent systems using deep reinforcement learning. We propose a decentralized, asynchronous framework that enables agents to learn from their experiences and adapt to changing environments. Our algorithm, 'CoEx', integrates Graph Attention Networks with Deep Q-Networks to facilitate efficient communication and coordination among agents. Experimental results on a series of multi-agent exploration tasks demonstrate that CoEx outperforms state-of-the-art methods in terms of exploration efficiency, coordination, and task completion rate.