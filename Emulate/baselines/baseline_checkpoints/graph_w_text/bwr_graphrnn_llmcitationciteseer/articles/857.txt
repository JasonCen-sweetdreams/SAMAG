Embodied conversational agents (ECAs) in virtual reality (VR) have the potential to revolutionize human-computer interaction. However, creating ECAs that can engage users in a natural and intuitive manner remains a significant challenge. This paper presents a novel multimodal fusion framework that combines speech, gesture, and gaze cues to enable more expressive and responsive ECAs. Our approach leverages a deep neural network to integrate these modalities and generate context-dependent responses. We evaluate our framework in a VR-based conversational scenario and demonstrate improved user engagement and perceived agent intelligence compared to unimodal or naive multimodal approaches.