Affective computing has revolutionized human-computer interaction (HCI) by enabling machines to recognize and respond to human emotions. However, existing approaches rely on a single modality, such as facial expressions or speech, and fail to capture the complexity of human emotions. This paper presents EmotionAware, a novel multimodal affective computing framework that integrates computer vision, natural language processing, and physiological signal processing to recognize and classify emotions. Our approach leverages a graph-based neural network to fuse features from multiple modalities, achieving a 25% improvement in emotion recognition accuracy compared to state-of-the-art unimodal approaches. We demonstrate EmotionAware's potential in enhancing user experience in HCI applications, such as personalized recommendations and emotional support systems.