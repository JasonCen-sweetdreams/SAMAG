Emotion recognition in human-robot interaction (HRI) is crucial for developing socially intelligent robots. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates facial, vocal, and linguistic cues to recognize emotions in HRI. Our HAN model utilizes a hierarchical attention mechanism to selectively focus on relevant modalities and features, outperforming state-of-the-art multi-modal fusion approaches. Experimental results on the SEMAINE database demonstrate improved emotion recognition accuracy and robustness to noisy or missing modalities.