Cooperative multi-agent reinforcement learning (MARL) has shown great promise in various real-world applications, but the curse of dimensionality in large state and action spaces hinders its efficiency. We propose a novel hierarchical attention network (HAN) framework, which leverages attention mechanisms to selectively focus on relevant agents and features in the environment. Our approach reduces the complexity of MARL problems by learning a hierarchical representation of the agents' states and actions. Experimental results on a range of cooperative MARL benchmarks demonstrate that HAN achieves significant improvements in learning speed and final performance compared to state-of-the-art methods.