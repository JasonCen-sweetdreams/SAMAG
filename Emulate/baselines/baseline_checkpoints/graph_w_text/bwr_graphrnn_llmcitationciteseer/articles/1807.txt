Emotion recognition from multimodal data, such as speech, text, and vision, is a challenging task due to the heterogeneity and complexity of the input data. This paper presents a novel hierarchical attention-based transformer architecture, 'MERT', which learns to fuse and align features from different modalities. MERT employs a stacked transformer encoder to model intra-modal and inter-modal relationships, and introduces a hierarchical attention mechanism to selectively focus on relevant modalities and features. Experimental results on the CMU-MOSEI dataset demonstrate that MERT outperforms state-of-the-art methods in multimodal emotion recognition tasks, achieving an average F1-score improvement of 12.5%.