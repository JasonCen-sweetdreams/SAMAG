Autonomous driving has made significant progress, but the lack of transparency in deep reinforcement learning (DRL) models hinders their adoption. This paper presents 'ExplaDrive', a novel framework that generates interpretable explanations for DRL policies in autonomous driving. We employ a generative adversarial network to learn a disentangled representation of driving scenarios, which is then used to generate visual and textual explanations for the agent's actions. Experiments on the CARLA simulator demonstrate that ExplaDrive improves the understanding of DRL models, leading to more trustworthy and safer autonomous driving systems.