Deep reinforcement learning (DRL) agents have achieved remarkable success in complex tasks, but their lack of transparency hinders their deployment in high-stakes applications. This paper proposes a novel, model-agnostic approach to explainability in DRL, leveraging saliency maps to identify the most relevant state features contributing to an agent's decisions. Our method, 'Saliency-RL', is applicable to various DRL architectures and outperforms existing attribution methods in terms of fidelity and computational efficiency. We demonstrate the effectiveness of Saliency-RL in a range of Atari games and a real-world autonomous driving scenario, providing insights into the decision-making processes of DRL agents.