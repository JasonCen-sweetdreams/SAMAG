Autonomous vehicles require efficient and robust control systems to navigate complex environments. This paper presents a novel deep reinforcement learning (DRL) framework that integrates real-time sensor data from cameras, lidars, and GPS to optimize vehicle control. Our method, 'SensorFusionRL', uses a multi-modal fusion approach to combine sensor inputs and improve the agent's decision-making process. We demonstrate the effectiveness of SensorFusionRL in simulations and real-world experiments, showcasing improved stability, adaptability, and safety compared to traditional model-based control methods.