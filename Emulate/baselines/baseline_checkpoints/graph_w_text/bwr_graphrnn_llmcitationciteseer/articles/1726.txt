This paper presents a novel multi-agent reinforcement learning framework, 'MARL-TA', designed to efficiently allocate tasks in dynamic environments. MARL-TA integrates a hierarchical decomposition of the task allocation problem with a decentralized, actor-critic architecture. We introduce a novel exploration strategy that adapts to changing environment conditions, allowing agents to learn effective task allocation policies in real-time. Experimental results in a simulated robotic warehouse scenario demonstrate that MARL-TA outperforms state-of-the-art methods in terms of task completion time and overall system efficiency.