The increasing adoption of AI systems in high-stakes applications has raised concerns about their interpretability and trustworthiness. This paper proposes a novel multi-task learning framework, 'HATMI', that integrates hierarchical attention mechanisms to jointly learn predictive models and provide explanations for AI decision-making. Our approach leverages task-specific attention weights to identify relevant features and generate contextualized explanations. Experimental results on several benchmark datasets demonstrate that HATMI outperforms state-of-the-art methods in both predictive performance and explanation quality, enabling more transparent and accountable AI systems.