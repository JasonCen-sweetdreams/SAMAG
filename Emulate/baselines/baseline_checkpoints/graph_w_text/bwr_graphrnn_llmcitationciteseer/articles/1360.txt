Virtual reality (VR) has the potential to revolutionize accessibility for individuals with disabilities. However, current VR systems often rely on manual input methods, which can be challenging or impossible for users with mobility impairments. This paper presents 'EyeQuest', an eye-gaze driven adaptive interaction framework for VR. EyeQuest utilizes machine learning-based gaze estimation and adaptive difficulty adjustment to enable seamless interaction with virtual objects. Our user study with 20 participants demonstrates significant improvements in task completion time and user satisfaction compared to traditional input methods.