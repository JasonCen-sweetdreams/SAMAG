Deep neural networks are increasingly vulnerable to adversarial attacks, which can compromise their performance and reliability. This paper proposes a novel approach to detect adversarial attacks using explainable AI techniques. We develop a method to generate interpretable heatmaps that highlight the regions of the input data most influential in the model's predictions. By analyzing these heatmaps, we can identify anomalous patterns indicative of adversarial attacks. Our experiments on several benchmark datasets demonstrate the effectiveness of our approach in detecting attacks with high accuracy and robustness.