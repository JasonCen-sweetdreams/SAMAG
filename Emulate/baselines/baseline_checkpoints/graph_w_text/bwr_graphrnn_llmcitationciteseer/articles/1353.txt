Recent advances in autonomous driving have focused on deep reinforcement learning (RL) methods, but these often suffer from high sample complexity and limited scalability. We propose a novel hierarchical RL framework, 'HRL-AD', which leverages a multi-level abstraction of driving tasks to improve efficiency and adaptability. By decomposing the driving policy into high-level goals and low-level control primitives, HRL-AD reduces the exploration space and achieves faster convergence. We evaluate HRL-AD on a realistic simulation platform, demonstrating improved driving performance and robustness to diverse scenarios and weather conditions.