In multi-agent reinforcement learning, coordinating exploration is crucial to achieve optimal policies. Existing methods often rely on heuristic-based strategies or assume prior knowledge of the environment. This paper proposes a novel approach, 'GraphAttention-Explore', which leverages graph attention networks to learn coordinated exploration policies. Our method represents the environment as a graph, where agents are nodes, and edges encode their relationships. We demonstrate the efficacy of GraphAttention-Explore in several complex multi-agent tasks, showcasing improved exploration efficiency and policy performance compared to state-of-the-art baselines.