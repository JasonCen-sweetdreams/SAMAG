Traditional information retrieval systems struggle to effectively handle multi-modal queries, which combine text, images, and other media. This paper proposes a novel approach, Hierarchical Embeddings for Multi-Modal Retrieval (HEMMR), which leverages hierarchical representations to jointly model and embed different modalities. Our method enables efficient retrieval of relevant documents by projecting multi-modal queries into a unified semantic space. Experiments on a large-scale dataset demonstrate that HEMMR outperforms state-of-the-art methods in terms of retrieval accuracy and computational efficiency.