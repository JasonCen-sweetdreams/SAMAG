Emotion recognition is a crucial aspect of human-robot interaction (HRI), enabling robots to respond empathetically to users. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates multi-modal sensory inputs from speech, facial expressions, and physiological signals. Our HAN model learns to selectively focus on relevant modalities and temporal segments, improving emotion recognition accuracy in HRI scenarios. Experiments on the RECOLA dataset demonstrate that our approach outperforms state-of-the-art methods, achieving an F1-score of 0.842 in recognizing six emotions. We discuss the implications of our work for developing more empathetic and socially aware robots.