Accurate and real-time traffic forecasting is crucial for intelligent transportation systems. While Graph Neural Networks (GNNs) have shown promise in this domain, they often suffer from high computational complexity and neglect the importance of node attention. This paper introduces 'GraphAttentionFlow', a novel GNN architecture that incorporates attention mechanisms to focus on relevant nodes and edges in the traffic graph. We propose a parallelizable attention scheme that reduces computational overhead by 30% compared to existing GNN models. Experiments on a large-scale traffic dataset demonstrate that GraphAttentionFlow outperforms state-of-the-art methods in terms of forecasting accuracy and inference speed.