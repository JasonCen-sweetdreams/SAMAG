Query expansion is a crucial step in information retrieval (IR) systems, aimed at reformulating user queries to improve retrieval accuracy. This paper presents a novel approach, 'RL-QE', that leverages reinforcement learning to optimize query expansion. RL-QE treats query expansion as a Markov decision process, where the agent learns to select relevant expansion terms based on user feedback. We propose a reward function that incorporates both relevance and novelty metrics, and demonstrate significant improvements over traditional query expansion techniques on several benchmark datasets. Our experiments show that RL-QE achieves higher precision, recall, and F1-score compared to state-of-the-art methods, while reducing the computational overhead.