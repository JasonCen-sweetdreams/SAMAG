Coordinated exploration in multi-agent systems is a challenging problem due to the exponential growth of the joint action space. This paper proposes a novel deep graph reinforcement learning (DGRL) framework that leverages graph neural networks to model the interactions between agents and their environment. Our approach, called CoEx, learns a distributed policy that maximizes the cumulative reward while encouraging agents to explore diverse regions of the state space. We evaluate CoEx on several benchmarks, including a robot navigation task, and demonstrate significant improvements in exploration efficiency and coordination compared to existing methods.