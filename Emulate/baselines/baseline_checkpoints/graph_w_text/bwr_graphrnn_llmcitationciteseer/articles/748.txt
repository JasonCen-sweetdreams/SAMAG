Multi-hop question answering (MHQA) requires models to reason over multiple sentences and identify relevant information. While recent advances in transformer-based architectures have improved MHQA performance, they often lack transparency and interpretability. We propose a novel hierarchical attention-based model, HARE, which incorporates explicit reasoning modules to generate explanations for its predictions. HARE uses a top-down attention mechanism to identify relevant sentences and a bottom-up attention mechanism to identify relevant entities, enabling the model to generate faithful explanations. Experimental results on the HotPotQA dataset demonstrate that HARE achieves state-of-the-art performance while providing insightful explanations for its predictions.