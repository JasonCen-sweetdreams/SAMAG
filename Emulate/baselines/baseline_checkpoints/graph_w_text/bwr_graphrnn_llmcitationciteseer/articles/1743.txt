Autonomous drone navigation in dynamic environments is a challenging problem due to the need to adapt to changing obstacles, weather conditions, and mission objectives. This paper proposes a hierarchical reinforcement learning framework, 'HierDrone', which integrates a high-level mission planner with a low-level motion controller. The mission planner uses a graph-based representation to optimize the drone's trajectory, while the motion controller employs a deep Q-network to adapt to local environment changes. We evaluate HierDrone in a simulated urban environment and demonstrate improved navigation efficiency and robustness compared to flat reinforcement learning baselines.