Emotion recognition in human-computer interaction (HCI) has traditionally relied on single-modal inputs, such as facial expressions or speech. However, humans communicate emotions through multiple modalities, including verbal and non-verbal cues. This paper proposes a novel hierarchical attention network (HAN) that integrates and selectively focuses on relevant modalities to improve emotion recognition accuracy. Our HAN model consists of three stages: intra-modal attention, inter-modal attention, and fusion. Experimental results on a multi-modal emotion dataset demonstrate significant improvements over state-of-the-art single-modal and early-fusion approaches, achieving an F1-score of 0.83.