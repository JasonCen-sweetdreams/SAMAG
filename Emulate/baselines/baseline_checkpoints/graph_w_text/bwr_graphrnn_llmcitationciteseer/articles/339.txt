Graph neural networks (GNNs) have achieved state-of-the-art performance in various graph-based applications. However, they are vulnerable to adversarial attacks, which can manipulate node features or graph structures to mislead the model. This paper proposes a novel topological denoising framework, 'GraphShield', to enhance the robustness of GNNs against such attacks. By learning to identify and remove noisy nodes and edges, GraphShield improves the model's resistance to perturbations while preserving its performance on clean data. Experimental results on several benchmark datasets demonstrate the effectiveness of our approach in defending against various types of attacks.