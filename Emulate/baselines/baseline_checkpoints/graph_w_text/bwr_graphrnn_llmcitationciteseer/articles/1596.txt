People with motor disabilities face significant barriers when interacting with virtual reality (VR) systems. This paper presents a novel gaze-based interaction framework that enables users to navigate and manipulate virtual objects using only their eye movements. We developed a machine learning model that accurately predicts user intent from eye-tracking data and integrates it with a VR interface. Our user study with 15 participants with motor disabilities shows that our approach significantly improves task completion time and user satisfaction compared to traditional controller-based interfaces.