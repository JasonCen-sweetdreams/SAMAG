Deep neural networks (DNNs) have achieved state-of-the-art performance in various applications, but their large size and computational requirements hinder their deployment on resource-constrained devices. This paper proposes a novel multi-task learning framework for efficient DNN pruning, dubbed 'MTLP'. MTLP jointly optimizes multiple tasks, including the primary task and auxiliary tasks that promote sparsity and regularization. We demonstrate that MTLP outperforms existing pruning methods in terms of accuracy and computational efficiency on several benchmark datasets. Additionally, we provide theoretical insights into the convergence properties of MTLP.