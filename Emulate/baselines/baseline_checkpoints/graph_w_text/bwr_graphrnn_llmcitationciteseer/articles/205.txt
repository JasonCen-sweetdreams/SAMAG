Deep learning models have been shown to be vulnerable to adversarial attacks, which can have significant consequences in safety-critical applications. This paper proposes a novel approach to robustness analysis using Bayesian neural networks (BNNs). We leverage the probabilistic nature of BNNs to quantify the uncertainty of the model's predictions and detect adversarial examples. Our method, 'BayesDefend', is shown to outperform state-of-the-art defenses against a range of attacks, including PGD and CW attacks, on multiple benchmark datasets. We also provide a theoretical analysis of the robustness guarantees provided by BayesDefend, demonstrating its effectiveness in improving the reliability of deep learning models.