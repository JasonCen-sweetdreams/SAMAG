Autonomous vehicles rely on reinforcement learning (RL) to navigate complex scenarios. However, existing RL methods often neglect uncertainty in sensor readings and environment dynamics, leading to suboptimal or unsafe decisions. This paper presents Hierarchical Uncertainty-Aware Reinforcement Learning (HUAL), which integrates probabilistic modeling and hierarchical RL to address uncertainty in autonomous vehicle control. We demonstrate HUAL's efficacy in simulated urban driving scenarios, showcasing improved safety and adaptability in the face of sensor noise and uncertain environment dynamics.