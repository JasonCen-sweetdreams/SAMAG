As humans interact with computers using diverse modalities (e.g., voice, gesture, and gaze), effective visualization of system responses is crucial for efficient communication. This paper presents an adaptive visualization framework, 'MVis', which dynamically adjusts visual representation based on user preferences, task complexity, and contextual factors. We introduce a novel multi-modal attention mechanism that leverages machine learning to identify relevant input modalities and adapt visualization strategies accordingly. A user study demonstrates that MVis significantly improves task completion time and user satisfaction in multi-modal human-computer interaction scenarios.