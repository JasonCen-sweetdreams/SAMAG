Sentiment analysis is a crucial task in natural language processing, but it becomes increasingly challenging when dealing with multi-modal data (e.g., text, images, and audio). This paper proposes a novel hierarchical attention network (HAN) architecture that effectively integrates and weighs the contributions of different modalities. Our HAN model consists of modality-specific attention modules and a hierarchical fusion layer, enabling it to capture both local and global relationships across modalities. Experimental results on a large-scale multi-modal dataset demonstrate the superiority of our approach over state-of-the-art methods, achieving an average improvement of 12.3% in sentiment accuracy.