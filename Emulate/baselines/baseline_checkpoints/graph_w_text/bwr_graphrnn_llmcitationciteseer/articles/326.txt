Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) poses significant challenges due to the inherent complexity of human emotions and the variability of modalities. This paper presents a novel Hierarchical Attention Network (HAN) that leverages both intra-modal and inter-modal relationships to improve emotion recognition. Our approach employs a bottom-up attention mechanism to selectively focus on salient features within each modality and a top-down attention scheme to integrate information across modalities. Experimental results on the IEMOCAP and SEMAINE datasets demonstrate the effectiveness of our HAN model, achieving state-of-the-art performance in recognizing emotions from multi-modal inputs.