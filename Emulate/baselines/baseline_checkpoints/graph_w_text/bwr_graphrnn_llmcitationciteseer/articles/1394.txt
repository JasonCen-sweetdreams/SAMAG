Multi-hop question answering (MQA) requires models to perform complex reasoning over multiple sentences. While state-of-the-art models have achieved impressive results, their black-box nature hinders interpretability. This paper proposes a novel hierarchical attention-based explainable reasoning framework, 'HERO', which disentangles reasoning steps and provides transparent attention weights. We introduce a multi-task objective that jointly optimizes answer accuracy and explainability, enabling the model to generate faithful explanations for its predictions. Experimental results on three MQA benchmarks demonstrate that HERO improves both answer accuracy and explanation quality compared to existing explainable models.