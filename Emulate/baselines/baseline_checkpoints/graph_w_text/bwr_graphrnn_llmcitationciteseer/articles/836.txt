The increasing adoption of AI-driven decision support systems in healthcare has raised concerns about the transparency and explainability of these models. This paper proposes a novel hierarchical attention network (HAN) architecture that provides interpretable explanations for AI-driven decision making in healthcare. Our HAN model leverages attention mechanisms to identify critical patient features and clinical concepts that influence the prediction. We evaluate our approach on a real-world Electronic Health Record (EHR) dataset and demonstrate improved model interpretability and accuracy compared to traditional machine learning models.