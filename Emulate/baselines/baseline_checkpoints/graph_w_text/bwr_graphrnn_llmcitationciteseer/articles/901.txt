Image captioning models often struggle to provide interpretable results, limiting their applicability in real-world scenarios. This paper introduces a novel hierarchical attention network (HAN) architecture that generates captions while providing explicit attention weights for visual and linguistic components. We propose a multi-task learning objective that jointly optimizes caption generation and attention weights, enabling the model to focus on salient image regions and relevant contextual information. Experimental results on the COCO dataset demonstrate that our HAN model achieves state-of-the-art performance while offering improved explainability and interpretability.