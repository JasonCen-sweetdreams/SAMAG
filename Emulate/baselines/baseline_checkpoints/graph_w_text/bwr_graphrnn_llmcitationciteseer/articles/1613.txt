Dialogue state tracking (DST) is a crucial component in task-oriented dialogue systems. Existing approaches struggle to effectively incorporate multi-modal inputs (e.g., text, speech, and vision) and often rely on complex, manually-designed feature engineering. We propose a novel hierarchical attention framework, 'HierarTrack', which learns to selectively focus on relevant input modalities and contextual information. Our approach leverages a hierarchical encoder to capture both local and global dependencies, enabling more accurate DST. Experimental results on the MultiWOZ dataset demonstrate significant improvements over state-of-the-art methods in terms of joint goal accuracy and overall dialogue success rate.