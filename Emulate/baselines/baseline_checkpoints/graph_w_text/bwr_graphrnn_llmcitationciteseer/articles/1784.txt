This paper presents a novel hierarchical attention network (HAN) architecture for generating contextually relevant and interpretable responses in multi-modal dialogue systems. Our approach leverages both textual and visual features to model user intent and sentiment. We propose a dual-attention mechanism that captures both intra-modal and inter-modal relationships between input modalities. Experimental results on a large-scale dialogue dataset demonstrate the effectiveness of our approach in generating coherent and diverse responses, while providing insights into the reasoning process through attention visualization.