Recent advances in AI-powered emotion recognition systems have relied heavily on deep learning models, which often suffer from a lack of interpretability. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates multi-modal input from facial expressions, speech, and text. Our approach leverages attention mechanisms to selectively focus on relevant features and modalities, enabling more accurate and interpretable emotion recognition. Experimental results on a benchmark dataset demonstrate the effectiveness of our approach, achieving state-of-the-art performance with improved Explainability and Transparency.