Deep neural networks have achieved state-of-the-art performance in various applications, but their lack of transparency and interpretability hinders their adoption in high-stakes domains. This paper presents a novel hierarchical attention mechanism, 'HierAttn', that improves the explainability of deep neural networks. HierAttn disentangles the feature importance across multiple levels of abstraction, providing a more comprehensive understanding of the decision-making process. We evaluate HierAttn on several benchmark datasets and demonstrate its effectiveness in improving the accuracy of feature importance attribution and model interpretability.