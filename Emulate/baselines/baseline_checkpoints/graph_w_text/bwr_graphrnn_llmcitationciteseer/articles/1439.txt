Emotion recognition in conversational systems is a crucial aspect of human-computer interaction. However, existing approaches often rely on single-modal inputs (e.g., text or speech) and neglect the complexities of human emotions. This paper proposes a novel hierarchical attention network (HAN) that integrates multi-modal inputs (text, speech, and facial expressions) to recognize emotions in conversations. The HAN model employs a hierarchical structure to capture both local and global dependencies across modalities, and an attention mechanism to selectively focus on informative regions. Experiments on a large-scale conversational dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions, especially in cases of ambiguity or conflicting cues.