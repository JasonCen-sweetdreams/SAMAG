Multimodal sentiment analysis has garnered significant attention in recent years due to the proliferation of multimodal data on social media platforms. However, most existing approaches rely on early fusion or simple concatenation of modalities, neglecting the complex interactions between them. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that explicitly models the inter-modal relationships and captures fine-grained sentiment cues from text, image, and audio modalities. Experimental results on the CMU-MOSI dataset demonstrate the effectiveness of HAN in improving sentiment analysis performance, especially in scenarios where modalities exhibit conflicting sentiments.