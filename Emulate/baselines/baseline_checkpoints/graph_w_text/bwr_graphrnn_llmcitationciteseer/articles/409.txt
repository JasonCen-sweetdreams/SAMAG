Intelligent assistive systems (IAS) have the potential to significantly improve the quality of life for individuals with disabilities. However, current IAS interfaces often rely on explicit user input, which can be cumbersome and limiting. This paper presents a novel gaze-based intention recognition framework that leverages machine learning and computer vision techniques to infer user intentions from gaze patterns. Our approach achieves an average accuracy of 92.5% in recognizing user intentions in a controlled laboratory setting, outperforming existing state-of-the-art methods. We discuss the implications of our framework for the development of more intuitive and user-friendly IAS interfaces.