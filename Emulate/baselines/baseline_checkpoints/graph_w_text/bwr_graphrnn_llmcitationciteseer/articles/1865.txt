Graph Neural Networks (GNNs) have achieved state-of-the-art performance in various node classification tasks. However, their robustness against adversarial attacks remains largely unexplored. In this paper, we propose a novel attack framework, 'GraphFool', which generates perturbations on the graph structure and node features to mislead GNNs. We demonstrate that GraphFool can significantly degrade the performance of popular GNN architectures, even when they are trained with robustness-enhancing techniques. Furthermore, we develop a defense mechanism that incorporates adversarial training and graph purification to improve the resilience of GNNs against such attacks.