In multi-agent systems, dynamic task allocation is crucial for efficient resource utilization and adaptability to changing environments. We propose a reinforcement learning framework, 'HARA', that enables heterogeneous agents to coordinate and optimize task allocation in real-time. HARA leverages a decentralized actor-critic architecture, where each agent learns to balance exploration and exploitation of tasks based on its capabilities and environmental feedback. We evaluate HARA on a simulated disaster response scenario, demonstrating improved task completion rates and reduced coordination overhead compared to traditional planning-based approaches.