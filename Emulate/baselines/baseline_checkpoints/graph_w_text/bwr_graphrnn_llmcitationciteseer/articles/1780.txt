Multimodal reasoning tasks, such as visual question answering, require models to integrate and reason about information from different modalities. While deep neural networks have achieved state-of-the-art performance, they often lack transparency and interpretability. This paper presents a Hierarchical Attention Network (HAN) architecture that leverages attention mechanisms to selectively focus on relevant modalities and regions. We demonstrate the effectiveness of HAN on several benchmark datasets, achieving improved performance and providing insights into the decision-making process through visualized attention maps.