Cooperative task allocation is a crucial problem in multi-agent systems, where agents must work together to achieve a common goal. This paper proposes a decentralized multi-agent reinforcement learning framework, 'CoopMARL', which enables agents to learn cooperative strategies for task allocation in dynamic environments. We introduce a novel communication mechanism that allows agents to share task-related information and adapt to changing circumstances. Experimental results on a simulated logistics scenario demonstrate that CoopMARL outperforms traditional centralized approaches in terms of task completion efficiency and robustness to agent failures.