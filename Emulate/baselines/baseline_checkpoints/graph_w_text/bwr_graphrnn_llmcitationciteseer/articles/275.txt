Virtual reality (VR) has revolutionized human-computer interaction, but users often struggle to navigate virtual environments due to limited haptic feedback. This paper presents a novel gaze-based adaptive haptic feedback system, 'GazeHaptics', which leverages eye-tracking data to dynamically adjust tactile feedback in real-time. Our approach utilizes machine learning algorithms to predict user intentions and subtly modulate haptic cues, thereby enhancing user immersion and reducing disorientation. User studies demonstrate significant improvements in task performance and subjective experience, highlighting the potential of GazeHaptics to transform VR interactions.