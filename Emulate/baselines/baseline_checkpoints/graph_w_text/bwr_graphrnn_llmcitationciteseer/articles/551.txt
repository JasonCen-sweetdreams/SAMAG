Multi-task learning has shown promise in improving model performance and efficiency. However, as the number of tasks grows, the complexity of task relationships and interactions increases, leading to scalability issues. This paper presents a novel hierarchical task embedding framework, 'HiTE', which learns to represent tasks as hierarchical clusters of embeddings. We demonstrate that HiTE enables efficient knowledge sharing across tasks, improves overall model performance, and reduces training time. Experiments on a large-scale multi-task dataset show that HiTE outperforms state-of-the-art methods, achieving a 25% improvement in average task accuracy.