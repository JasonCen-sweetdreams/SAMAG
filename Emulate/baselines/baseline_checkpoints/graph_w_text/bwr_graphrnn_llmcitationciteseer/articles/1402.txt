Emotion recognition is a crucial aspect of human-computer interaction, enabling more empathetic and personalized systems. This paper presents a novel hierarchical attention framework, 'HIER', for multi-modal emotion recognition. HIER leverages both visual and auditory cues from facial expressions and speech, incorporating attention mechanisms to selectively focus on salient features. Our experiments on the RECOLA dataset demonstrate that HIER outperforms state-of-the-art approaches, achieving an average F1-score of 0.83 across six emotions. We further analyze the learned attention patterns, revealing insights into the complex relationships between modalities and emotions.