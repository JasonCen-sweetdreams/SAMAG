Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the varying importance of each modality in different contexts. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features at multiple levels of abstraction. Our approach achieves state-of-the-art performance on three benchmark datasets, outperforming existing multi-modal fusion methods by up to 12% in terms of emotion recognition accuracy. We also demonstrate the interpretability of our model through attention visualization, providing insights into the emotional cues extracted from each modality.