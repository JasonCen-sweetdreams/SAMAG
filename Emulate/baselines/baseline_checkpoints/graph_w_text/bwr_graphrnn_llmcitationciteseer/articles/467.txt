Autonomous agents operating in stochastic environments must balance exploration and exploitation to achieve their objectives. This paper presents a novel decision-theoretic planning framework that integrates planning-as-satisficing and model-based reinforcement learning. Our approach, called 'StoPAS', leverages a probabilistic model of the environment to compute a satisficing plan that maximizes the expected utility of the agent. We demonstrate the effectiveness of StoPAS in a series of experiments involving a robotic agent navigating a partially observable maze, and show that it outperforms state-of-the-art planning and reinforcement learning methods in terms of cumulative reward and planning efficiency.