Multi-label classification is a fundamental problem in machine learning, where an instance can be associated with multiple labels simultaneously. Existing approaches often suffer from high computational complexity and ignore the label dependencies. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that efficiently captures label correlations and improves classification performance. Our HAN model employs a hierarchical attention mechanism to selectively focus on relevant labels and features, reducing the computational overhead. Experimental results on benchmark datasets demonstrate that HAN outperforms state-of-the-art methods in terms of F1-score and inference time.