Conversational systems increasingly rely on multimodal input, such as text, speech, and facial expressions, to understand user sentiment. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that jointly models these modalities to improve sentiment analysis. Our approach leverages attention mechanisms to selectively focus on relevant input features, and a hierarchical fusion strategy to integrate modality-specific representations. Experimental results on a large conversational dataset demonstrate that our HAN model outperforms state-of-the-art multimodal fusion methods, achieving a 12% improvement in sentiment classification accuracy.