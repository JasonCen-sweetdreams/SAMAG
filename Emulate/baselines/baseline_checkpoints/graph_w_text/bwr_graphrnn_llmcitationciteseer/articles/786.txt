Multimodal information retrieval (MMIR) tasks, such as image-text retrieval, are becoming increasingly popular. However, existing approaches struggle to effectively model the complex relationships between different modalities. This paper proposes a novel hybrid retrieval model, 'HybridGNN', which integrates graph neural networks (GNNs) with traditional retrieval models. HybridGNN learns to represent multimodal data as a graph, capturing both intra-modal and inter-modal relationships. We demonstrate the effectiveness of our approach on two MMIR benchmarks, achieving significant improvements over state-of-the-art methods.