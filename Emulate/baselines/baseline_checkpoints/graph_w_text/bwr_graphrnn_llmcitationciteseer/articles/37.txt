Visual question answering (VQA) tasks require models to fuse information from both visual and textual inputs. However, existing multimodal fusion methods often incur high computational costs or neglect the asymmetric relationships between modalities. We propose a novel attention mechanism, 'ModalCross', which adaptively weighs the importance of each modality based on the question context. Our approach leverages a hierarchical graph-based structure to model interactions between objects, regions, and questions. Experimental results on the VQA 2.0 benchmark demonstrate that ModalCross outperforms state-of-the-art methods while reducing computational overhead by 30%