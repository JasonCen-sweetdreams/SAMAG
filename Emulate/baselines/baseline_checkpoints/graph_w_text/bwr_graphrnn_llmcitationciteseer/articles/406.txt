As Augmented Reality (AR) systems become increasingly prevalent, there is a growing need for interfaces that can adapt to user behavior and preferences. This paper presents a novel gaze-aware adaptive interface framework that leverages machine learning and computer vision techniques to dynamically adjust AR content and layout based on user gaze patterns. Our approach incorporates a probabilistic model of user attention to predict gaze fixations and saccades, enabling the system to proactively optimize content rendering and reduce visual clutter. User studies demonstrate significant improvements in user engagement, task completion time, and overall AR experience.