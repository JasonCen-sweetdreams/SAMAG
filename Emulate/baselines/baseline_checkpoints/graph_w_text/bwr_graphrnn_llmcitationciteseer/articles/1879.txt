Entity disambiguation is a crucial step in knowledge graph construction, but existing approaches struggle with scalability and accuracy. We propose a novel method, 'GraphAttentiveED', which leverages graph embeddings and attention-based neural networks to disambiguate entities in large-scale knowledge graphs. Our approach learns to represent entities as dense vectors, capturing their context and relationships, and employs an attention mechanism to focus on relevant information during disambiguation. Experimental results on the Wikidata5M dataset demonstrate that GraphAttentiveED outperforms state-of-the-art methods in terms of precision, recall, and F1-score, while reducing computational time by an order of magnitude.