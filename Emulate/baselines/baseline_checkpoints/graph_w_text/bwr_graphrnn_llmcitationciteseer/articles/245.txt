Deep reinforcement learning (DRL) has achieved remarkable successes in various applications, but its vulnerability to adversarial attacks raises concerns. This paper investigates attacks on DRL policies with temporal logic specifications, which specify constraints on the behavior of the agent. We propose a novel attack framework, 'TL-Attack', that crafts perturbations to the environment such that the policy violates the temporal logic specifications. Our experiments on several DRL benchmarks demonstrate the effectiveness of TL-Attack in inducing undesired behavior. We also propose a defense mechanism based on adversarial training with temporal logic regularization, which significantly improves the robustness of DRL policies.