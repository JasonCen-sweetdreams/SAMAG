Virtual reality (VR) technology has the potential to revolutionize accessibility, but current interactions often rely on manual inputs, which can be limiting for users with motor impairments. This paper presents a novel gaze-based adaptive interaction system, 'GAZE+', that leverages eye-tracking data to enable more intuitive and accessible interactions in VR environments. Our approach incorporates machine learning-based gaze prediction and adaptive difficulty adjustment, allowing users to seamlessly navigate and interact with virtual objects. A user study with 20 participants demonstrates significant improvements in task completion time and user satisfaction compared to traditional manual input methods.