Multimodal emotion recognition in conversational agents is a challenging task due to the complexity of human emotions and the variability of input modalities. This paper proposes a novel hierarchical attention network (HAN) that integrates visual, acoustic, and linguistic features to recognize emotions in human-agent interactions. Our HAN model employs a bottom-up attention mechanism to selectively focus on salient features at each modality, and a top-down attention mechanism to weigh the importance of each modality. Experimental results on a benchmark dataset show that our HAN model outperforms state-of-the-art multimodal fusion approaches, achieving a 12.5% improvement in emotion recognition accuracy.