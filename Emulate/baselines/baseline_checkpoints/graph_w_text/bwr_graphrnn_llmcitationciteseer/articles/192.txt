E-commerce search engines often struggle to effectively retrieve relevant results when users query with multi-modal inputs, such as images and text. This paper introduces a novel neural ranking model, 'MMR-NRM', which jointly learns to represent and rank multi-modal queries and documents. Our approach leverages a transformer-based architecture to capture complex relationships between modalities and incorporates a listwise ranking loss function to optimize retrieval performance. Experimental results on a large-scale e-commerce dataset demonstrate that MMR-NRM significantly outperforms state-of-the-art methods in terms of recall and normalized discounted cumulative gain.