Individuals with motor disabilities face significant barriers when interacting with virtual assistants. This paper presents a novel gaze-based interaction system, 'EyeVA', which enables users to control virtual assistants using only their gaze. Our approach leverages a combination of machine learning-based gaze tracking and natural language processing to infer user intent. We conducted a user study with 20 participants with motor disabilities and demonstrated significant improvements in interaction speed and accuracy compared to traditional speech-based interfaces. The results have important implications for improving accessibility and independence in daily life.