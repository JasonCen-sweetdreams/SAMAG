Multi-modal emotion recognition systems have gained popularity in recent years, leveraging advances in computer vision, natural language processing, and audio analysis. However, their robustness to adversarial attacks remains largely unexplored. This paper presents a comprehensive study on the vulnerability of multi-modal emotion recognition systems to adversarial attacks. We propose a novel attack framework, 'EmoAttack', which generates targeted perturbations to manipulate the emotional state classification of a given input. Our experiments on three benchmark datasets demonstrate that EmoAttack can successfully deceive state-of-the-art models, highlighting the need for robustness evaluation and defense strategies in this domain.