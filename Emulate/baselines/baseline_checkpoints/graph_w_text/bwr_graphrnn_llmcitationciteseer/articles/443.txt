Neural retrieval models have shown promising results in information retrieval tasks, but often struggle with query representation and expansion. This paper proposes a novel approach to query expansion using BERT-based document embeddings. We first fine-tune a BERT model on a large corpus of documents to generate dense vector representations. Then, we develop an efficient algorithm to expand the query by identifying the most relevant terms from the document embeddings. Our experiments on several benchmark datasets demonstrate significant improvements in retrieval accuracy and efficiency compared to traditional query expansion methods.