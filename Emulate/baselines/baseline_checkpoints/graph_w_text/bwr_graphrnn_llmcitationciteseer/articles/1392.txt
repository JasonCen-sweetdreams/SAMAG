Few-shot image classification remains a challenging problem in machine learning, especially when dealing with limited labeled data. This paper introduces a novel meta-learning approach that leverages self-supervised contrastive learning to improve the adaptability of few-shot learners. Our method, MetaCon, uses a contrastive loss to learn a generic feature extractor that can be fine-tuned with a few labeled examples for a new task. We demonstrate the effectiveness of MetaCon on several benchmark datasets, achieving state-of-the-art performance in few-shot image classification tasks. Furthermore, our approach shows improved robustness to domain shifts and outperforms existing methods in low-data regimes.