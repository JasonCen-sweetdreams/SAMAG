Real-time traffic signal control is a complex problem that requires coordinating multiple agents to minimize congestion and reduce travel times. This paper presents a novel multi-agent reinforcement learning framework, 'MARL-TSC', that leverages decentralized partially observable Markov decision processes (DEC-POMDPs) to model the traffic network. MARL-TSC employs a hierarchical attention mechanism to selectively focus on relevant agents and states, reducing the computational complexity and improving the scalability of the approach. Experimental results on a real-world traffic dataset demonstrate that MARL-TSC outperforms state-of-the-art methods in reducing traffic congestion and travel times.