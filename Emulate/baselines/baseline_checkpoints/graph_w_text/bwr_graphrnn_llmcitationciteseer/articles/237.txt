Autonomous vehicles require efficient coordination to ensure safe and efficient traffic flow. This paper proposes a decentralized multi-agent reinforcement learning framework, 'MARL-CAR', which enables autonomous vehicles to learn cooperative policies in a distributed manner. We introduce a novel communication protocol that facilitates the exchange of local observations and intentions among agents, thereby improving coordination and reducing collisions. Experimental results on a simulated traffic scenario demonstrate that MARL-CAR outperforms traditional centralized approaches in terms of throughput and safety.