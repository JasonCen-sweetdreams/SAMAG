Distributed resource allocation is a complex problem in multi-agent systems, where agents must learn to coordinate their actions to optimize global rewards. This paper presents a novel framework for scalable multi-agent reinforcement learning, called 'MA-RL-DA', which leverages decentralized actor-critic methods and communication protocols to facilitate cooperation among agents. We demonstrate the effectiveness of MA-RL-DA in simulated and real-world scenarios, showcasing improved resource allocation efficiency and adaptability to dynamic environments.