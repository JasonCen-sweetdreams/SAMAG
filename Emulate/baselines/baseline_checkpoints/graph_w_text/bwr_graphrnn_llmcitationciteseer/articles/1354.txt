Deep learning models for multi-label classification often struggle to provide interpretable results. This paper proposes a Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant input features and labels. Our approach leverages self-attention mechanisms to model label correlations and generate label-wise attention weights. We demonstrate the effectiveness of HAN on several benchmark datasets, achieving state-of-the-art performance while providing insight into the decision-making process through attention visualization.