Dialogue systems often struggle to balance multiple tasks, such as intent recognition, sentiment analysis, and response generation. This paper proposes a hierarchical attention network (HAN) architecture that learns to selectively focus on relevant context, intent, and sentiment features across tasks. Our HAN model is designed to be explainable, providing visualizations of attention weights to facilitate understanding of the decision-making process. Experimental results on a large-scale multi-task dialogue dataset demonstrate improved performance and interpretability compared to state-of-the-art models, highlighting the potential of HAN for real-world applications.