Visual question answering (VQA) tasks require models to integrate information from both visual and linguistic inputs. This paper presents a novel hierarchical attention network (HAN) architecture that efficiently fuses multimodal features for improved VQA performance. Our approach utilizes a hierarchical structure to model complex relationships between visual regions and question tokens, and introduces a novel attention mechanism that adaptively weighs the importance of each modality. Experimental results on the VQA 2.0 benchmark demonstrate significant improvements over state-of-the-art models, particularly in scenarios with limited training data.