Deep reinforcement learning (DRL) has achieved remarkable success in various applications, including robotics, finance, and healthcare. However, recent studies have shown that DRL policies are vulnerable to adversarial attacks, which can compromise their performance and safety. This paper presents a comprehensive taxonomy of adversarial attacks on DRL policies, including poisoning, evasion, and exploration attacks. We also propose a set of defense strategies, including input preprocessing, reward shaping, and policy regularization, to mitigate the effects of these attacks. Our experiments on several benchmark environments demonstrate the effectiveness of our defense strategies in improving the robustness of DRL policies.