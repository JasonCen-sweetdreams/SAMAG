Augmented reality (AR) interfaces often rely on manual input methods, which can be cumbersome and limiting. This paper presents a novel gaze-based adaptive navigation system, 'GazeNav', that leverages eye-tracking technology to enable more intuitive and efficient user interaction. We introduce a machine learning model that predicts user intent from gaze patterns and adapts the navigation scheme accordingly. Our user study demonstrates that GazeNav significantly reduces task completion time and improves user satisfaction compared to traditional input methods. The system's flexibility and adaptability make it suitable for a wide range of AR applications.