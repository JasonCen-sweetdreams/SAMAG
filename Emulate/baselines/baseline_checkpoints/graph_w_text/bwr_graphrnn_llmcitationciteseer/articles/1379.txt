The increasing adoption of AI-driven recommendation systems has raised concerns about their transparency and accountability. This paper proposes a novel hierarchical graph attention network (HGAT) framework that enables explainable recommendations. HGAT leverages graph neural networks to model user-item relationships and attention mechanisms to identify influential nodes. We introduce a novel explanation module that generates interpretable feature importance scores, enabling users to understand the reasoning behind recommended items. Experimental results on multiple benchmark datasets demonstrate the effectiveness of HGAT in improving recommendation accuracy while providing meaningful explanations.