We propose a novel decentralized reinforcement learning framework for coordinating heterogeneous multi-agent systems. Our approach, dubbed 'HetMAS', leverages a graph neural network to learn a shared representation of the agents' dynamics and objectives. By incorporating a decentralized actor-critic architecture, HetMAS enables agents to learn cooperative policies in a self-organizing manner, without relying on a centralized controller or explicit communication. We demonstrate the effectiveness of HetMAS in a simulated robotic swarm scenario, where agents learn to adapt to changing environmental conditions and achieve improved task performance.