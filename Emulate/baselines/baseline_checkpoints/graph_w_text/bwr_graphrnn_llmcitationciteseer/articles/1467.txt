We consider the problem of decentralized multi-agent planning under incomplete information, where agents must make decisions based on partial observations of the environment. This paper proposes a novel approach that combines deep reinforcement learning with decentralized partially observable Markov decision processes (DEC-POMDPs). Our method, called DRL-DEC, uses a neural network to learn a policy for each agent that maximizes the collective reward while adapting to the uncertainty of the environment. Experimental results on a variety of benchmark domains demonstrate that DRL-DEC outperforms existing methods in terms of scalability and solution quality.