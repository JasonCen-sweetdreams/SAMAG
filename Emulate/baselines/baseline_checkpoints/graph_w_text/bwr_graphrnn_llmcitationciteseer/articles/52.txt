Time series anomaly detection is a crucial task in various applications, but it often suffers from limited labelled data. This paper proposes a self-supervised representation learning approach, 'TSRL-AD', that leverages the inherent structures of time series data to learn effective representations. We design a contrastive learning framework that jointly learns a feature extractor and an anomaly detector, without requiring labelled anomalies. Experimental results on several benchmark datasets demonstrate that TSRL-AD outperforms state-of-the-art methods in terms of detection accuracy and robustness, while requiring significantly fewer labelled samples.