Cooperative navigation in multi-agent systems is a challenging problem due to the complexity of agent interactions and the need for adaptive decision-making. This paper presents a hierarchical reinforcement learning framework, 'HRL-CoNav', which enables agents to learn both low-level motion control and high-level cooperative strategies. Our approach leverages a novel hierarchical State-Action-Critic (SAC) architecture to handle the Curse of Dimensionality and improve learning efficiency. Experimental results on a variety of simulation scenarios demonstrate the effectiveness of HRL-CoNav in achieving efficient and collision-free navigation in dense environments.