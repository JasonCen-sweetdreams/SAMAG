Deep reinforcement learning (DRL) has achieved remarkable success in complex tasks, but its vulnerability to adversarial attacks raises concerns about its reliability. This paper proposes a novel hierarchical policy distillation framework, 'Hierarchical Robust Policy Distillation' (HRPD), to improve the robustness of DRL policies against adversarial perturbations. HRPD leverages a teacher-student architecture, where a robust teacher policy guides the learning of a student policy through a hierarchical distillation process. Experimental results on multiple Atari games demonstrate that HRPD significantly enhances the robustness of DRL policies against various types of adversarial attacks.