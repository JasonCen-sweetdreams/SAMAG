Deep neural networks (DNNs) have achieved state-of-the-art performances in various AI applications, but their increasing complexity and computational requirements hinder their deployment on resource-constrained devices. This paper proposes a novel hierarchical structured pruning (HSP) method that reduces the computational cost and memory footprint of DNNs while preserving their accuracy. HSP leverages a nested pruning strategy, where redundant filters and channels are eliminated at multiple scales, resulting in a more efficient and compact network architecture. Experimental results on ImageNet and CIFAR-10 datasets demonstrate that HSP outperforms existing pruning methods, achieving up to 5.2x reduction in FLOPS and 3.5x reduction in model size with minimal accuracy loss.