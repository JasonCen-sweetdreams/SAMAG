Deep neural networks have demonstrated impressive performance on graph-structured data, but their robustness to adversarial attacks remains a concern. This paper presents a comprehensive evaluation of the robustness of graph neural networks (GNNs) to various types of adversarial attacks, including node injection, edge perturbation, and feature manipulation. We propose a novel attack method, Graph-FGSM, which combines the Fast Gradient Sign Method with graph-based optimization techniques to generate highly effective adversarial examples. Our experiments on several benchmark datasets demonstrate the vulnerability of GNNs to these attacks and provide insights into the design of more robust models.