Sentiment analysis on multimodal data, such as images and text, is a challenging task due to the complexity of modeling interactions between modalities. This paper presents a novel hierarchical attention-based neural network, 'HMN', which learns to selectively focus on relevant regions of the input data. HMN consists of two stages: (1) modality-specific attention modules that extract features from each modality, and (2) a hierarchical fusion module that adaptively combines features across modalities. Experimental results on three benchmark datasets demonstrate that HMN outperforms state-of-the-art methods in multimodal sentiment analysis, achieving an average improvement of 3.2% in accuracy.