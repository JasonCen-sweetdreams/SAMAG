Deep learning models have achieved state-of-the-art performance in various applications, but their hyperparameters require careful tuning for optimal results. Bayesian optimization has emerged as a promising approach for hyperparameter tuning, but its computational cost can be prohibitive for large models and datasets. This paper presents a novel Bayesian optimization algorithm, 'HyperBO', which leverages a combination of Gaussian processes and random forest surrogates to efficiently explore the hyperparameter space. We demonstrate the effectiveness of HyperBO on several benchmarks, achieving significant speedups over existing methods while maintaining competitive model performance.