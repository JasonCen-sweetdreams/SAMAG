Multimodal sentiment analysis has become increasingly important with the proliferation of multimedia data on social media platforms. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that effectively integrates visual, acoustic, and textual features to improve sentiment analysis performance. Our HAN model employs a hierarchical attention mechanism to selectively focus on relevant multimodal features, and a sentiment-aware fusion strategy to combine the outputs from each modality. Experimental results on the CMU-MOSI dataset demonstrate that our approach outperforms state-of-the-art methods in both single-modal and multimodal settings, achieving an average F1-score improvement of 5.2%.