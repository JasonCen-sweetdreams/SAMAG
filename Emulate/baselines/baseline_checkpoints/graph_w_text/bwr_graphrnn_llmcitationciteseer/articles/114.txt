Deep neural networks are vulnerable to adversarial attacks, which can compromise their performance and reliability. This paper proposes a novel approach to detect such attacks by analyzing the gradient patterns of the model's loss function. We introduce a gradient-based anomaly score, which captures the deviation of input samples from the expected gradient distribution. Our experiments on various image classification datasets demonstrate that our method can accurately identify adversarial examples, even in the presence of strong attacks and limited training data. The proposed approach can be seamlessly integrated into existing deep learning frameworks, providing an additional layer of security and robustness.