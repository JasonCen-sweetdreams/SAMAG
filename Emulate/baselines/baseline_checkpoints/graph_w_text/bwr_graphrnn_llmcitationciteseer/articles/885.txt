Multimodal emotion recognition is a challenging task that involves fusing information from multiple sources such as speech, text, and vision. This paper proposes a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features for emotion recognition. Our HAN model consists of two stages: modality attention and feature attention. The modality attention module learns to weight the importance of each modality, while the feature attention module focuses on the most informative features within each modality. Experimental results on the CMU-MOSI dataset demonstrate that our HAN model outperforms state-of-the-art multimodal fusion methods, achieving an average F1-score of 0.83.