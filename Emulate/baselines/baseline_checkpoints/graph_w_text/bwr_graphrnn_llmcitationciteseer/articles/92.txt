Few-shot learning has gained significant attention in recent years due to its ability to learn from limited labeled data. However, existing methods often struggle to generalize to unseen classes and require a large amount of labeled data for training. This paper proposes a novel hierarchical attention network (HAN) that leverages both labeled and unlabeled data to improve few-shot learning performance. Our approach uses a hierarchical attention mechanism to selectively focus on relevant features and instances, and a consistency regularization term to encourage the model to produce consistent predictions across different augmentations of the input data. Experiments on several benchmark datasets demonstrate that HAN outperforms state-of-the-art few-shot learning methods, especially in scenarios with limited labeled data.