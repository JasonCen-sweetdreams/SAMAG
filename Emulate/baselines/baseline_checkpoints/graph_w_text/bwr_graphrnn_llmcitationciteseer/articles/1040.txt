Deep reinforcement learning (DRL) has revolutionized autonomous systems, but the lack of transparency in decision-making processes hinders trust and adoption. This paper addresses the challenge of explaining DRL agents' behavior in real-world scenarios. We propose a novel model-agnostic explainability technique, 'RL-X', which leverages attention mechanisms and attribution methods to provide interpretable insights into the agent's policy. Our approach is shown to improve the understanding of DRL agents' decisions in complex environments, such as autonomous driving and robotics, without compromising their performance.