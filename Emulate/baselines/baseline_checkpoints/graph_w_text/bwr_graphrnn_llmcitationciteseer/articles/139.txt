This paper presents an innovative deep learning framework for recognizing user affective states in human-computer interaction. Our approach leverages multimodal fusion of facial expressions, speech patterns, and physiological signals to accurately detect emotions such as joy, sadness, and frustration. We propose a novel attention-based neural network architecture that adaptively weighs the importance of different modalities based on the user's context. Experimental results show that our approach outperforms state-of-the-art methods in emotion recognition, with a mean accuracy of 92.1% across six emotions. Our work has significant implications for developing emotionally intelligent interfaces that can provide personalized support and enhance user experience.