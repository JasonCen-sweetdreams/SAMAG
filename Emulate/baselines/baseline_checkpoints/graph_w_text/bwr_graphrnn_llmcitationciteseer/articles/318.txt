Individuals with motor impairments face significant barriers to participating in virtual reality (VR) experiences. This paper presents Eyebit, a novel gaze-driven interface that enables users to interact with VR environments using only their eye movements. Our approach leverages a machine learning-based gaze tracking system and a contextual understanding of the VR scene to accurately infer user intent. We conducted a user study with 20 participants, demonstrating that Eyebit achieves a 3.5x increase in interaction speed and a 2.2x reduction in error rate compared to traditional gaze-based interfaces. Our results have significant implications for expanding access to VR experiences for individuals with motor impairments.