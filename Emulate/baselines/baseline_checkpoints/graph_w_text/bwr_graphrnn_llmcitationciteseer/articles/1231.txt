Contrastive learning has emerged as a powerful paradigm for self-supervised learning, but its application to multi-modal data remains challenging. This paper proposes a novel graph-based hierarchical clustering approach, 'GraphCon', which integrates graph neural networks with contrastive learning to learn robust representations from multi-modal data. We demonstrate that GraphCon can effectively capture complex relationships between modalities, leading to improved performance on downstream tasks such as image-text retrieval and zero-shot classification. Experiments on several benchmark datasets show that GraphCon outperforms existing state-of-the-art methods in multi-modal contrastive learning.