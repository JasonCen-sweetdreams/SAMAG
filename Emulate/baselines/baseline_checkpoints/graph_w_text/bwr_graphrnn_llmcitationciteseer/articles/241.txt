This paper presents a novel decentralized coordination framework for autonomous agents using reinforcement learning. Our approach, dubbed 'Decentralized-CoRL', enables agents to learn coordination strategies in complex, dynamic environments without relying on a centralized controller. We introduce a multi-agent Q-network that incorporates communication and observation graphs to facilitate cooperation and conflict resolution. Experimental results in a realistic simulated environment demonstrate the efficacy of Decentralized-CoRL in achieving efficient task allocation and resource utilization compared to traditional, non-learning based approaches.