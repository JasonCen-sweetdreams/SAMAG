Emotion recognition from multi-modal data (e.g., speech, text, vision) is crucial in human-computer interaction. Existing methods often suffer from scalability issues due to the complexity of fusing and processing large amounts of heterogeneous data. We propose Hierarchical Graph Attention Networks (HGAT), a novel framework that leverages graph attention mechanisms to selectively focus on relevant modalities and features. HGAT adopts a hierarchical architecture to efficiently process multi-modal data, enabling it to handle large datasets and varying input lengths. Experimental results on the CMU-MOSEI dataset demonstrate the superior performance and efficiency of HGAT compared to state-of-the-art methods.