Multimodal sentiment analysis (MSA) aims to analyze sentiment from various modalities, such as text, images, and audio. Despite recent advances, existing MSA models lack interpretability, making it challenging to understand the decision-making process. This paper proposes a hierarchical attention network (HAN) that incorporates explainable multimodal fusion. Our HAN model employs a novel attention mechanism that assigns importance scores to each modality, enabling the identification of sentiment-bearing regions. Experimental results on benchmark datasets demonstrate that our approach outperforms state-of-the-art MSA models while providing insightful visualizations of sentiment-driven modality interactions.