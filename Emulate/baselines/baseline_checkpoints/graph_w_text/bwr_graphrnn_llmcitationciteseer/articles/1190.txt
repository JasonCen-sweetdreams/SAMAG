This paper presents a novel approach to adversarial video generation using spatio-temporal Generative Adversarial Networks (GANs) with an attention mechanism. Our proposed model, called STAGAN, incorporates a spatial attention module to focus on salient regions in the input frames and a temporal attention module to capture long-range dependencies. We evaluate STAGAN on multiple benchmark datasets and demonstrate its ability to generate realistic and diverse videos that can deceive state-of-the-art video classification models. Experimental results show that STAGAN outperforms existing video generation models in terms of visual quality and adversarial effectiveness.