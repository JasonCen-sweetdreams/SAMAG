Graph Neural Networks (GNNs) have achieved state-of-the-art performance in various graph-based applications. However, their training process is often computationally expensive and memory-intensive, limiting their scalability. This paper proposes a novel Hierarchical Attention Network (HAN) framework that accelerates GNN training by selectively aggregating node features at multiple scales. Our approach reduces the computational complexity of GNN training while maintaining its expressive power. Experimental results on several benchmark datasets demonstrate that HAN achieves significant speedups over traditional GNN training methods, making it a promising solution for large-scale graph learning tasks.