Autonomous vehicles rely on deep reinforcement learning (DRL) policies to navigate complex environments. However, these policies are vulnerable to adversarial attacks, which can compromise safety and reliability. We investigate the susceptibility of DRL policies to targeted attacks, crafting perturbations that manipulate the agent's perception of the environment. Our experiments demonstrate that even small perturbations can induce catastrophic failures in state-of-the-art DRL models. We propose a novel defense mechanism, 'RobustRL', which incorporates adversarial training and uncertainty quantification to improve resilience against attacks. Experimental results show that RobustRL significantly reduces the attack success rate, enhancing the safety of autonomous vehicles in adversarial scenarios.