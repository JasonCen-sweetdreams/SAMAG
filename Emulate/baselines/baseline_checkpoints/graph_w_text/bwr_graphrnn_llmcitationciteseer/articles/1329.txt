Deep reinforcement learning (DRL) has achieved remarkable success in various applications, but its vulnerability to adversarial attacks poses significant security risks. This paper proposes a novel graph-based anomaly detection framework, 'GARD', to detect adversarial attacks in DRL. GARD leverages graph neural networks to model the complex relationships between state-action pairs and identifies anomalies based on graph structural properties. We evaluate GARD on several DRL benchmarks and demonstrate its effectiveness in detecting various types of adversarial attacks, including poisoning and evasion attacks.