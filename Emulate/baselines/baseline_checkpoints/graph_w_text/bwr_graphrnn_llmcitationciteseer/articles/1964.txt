Augmented Reality (AR) interfaces often rely on manual input methods, which can be cumbersome and limiting. This paper presents EyeTrackingLens, a novel framework that leverages gaze-based interaction to enable more natural and efficient user experiences in AR. We develop a machine learning model that accurately predicts user intent from eye movement patterns, and integrate it with a real-time gaze tracking system. Our user study demonstrates that EyeTrackingLens significantly reduces interaction time and improves user satisfaction in AR-based tasks. We also discuss the implications of our approach for users with disabilities and its potential applications in various domains.