Neural retrieval models have achieved state-of-the-art performance in information retrieval tasks, but often rely on dense vector representations that incur high computational costs. This paper proposes a novel query expansion approach, 'SparseQE', which leverages sparse embeddings to reduce the dimensionality of query representations while preserving their semantic meaning. We introduce a sparse autoencoder-based method to learn compact query embeddings that are optimized for retrieval tasks. Experiments on several benchmark datasets demonstrate that SparseQE outperforms traditional query expansion techniques while reducing the computational overhead of neural retrieval models.