Virtual reality (VR) systems often rely on visual cues to communicate feedback to users, but this can lead to cognitive overload and decreased immersion. This paper explores the use of auditory cues to enhance gesture recognition in VR. We propose a novel approach that leverages psychoacoustic modeling to design informative and non-distracting sound effects. In a user study, we demonstrate that our approach improves gesture recognition accuracy by 22% compared to a baseline visual-only system, while also reducing cognitive workload. Our findings have implications for the design of more accessible and engaging VR experiences.