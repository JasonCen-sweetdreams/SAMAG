Emotion recognition from multi-modal inputs, such as speech, text, and vision, remains a challenging task in affective computing. This paper introduces a novel hierarchical attention network (HAN) that leverages the strengths of each modality to improve emotion recognition accuracy. Our HAN model consists of modality-specific attention modules that adaptively weight features from each modality, followed by a fusion layer that combines the outputs. Experiments on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods, achieving an average F1-score of 0.83 across six emotions.