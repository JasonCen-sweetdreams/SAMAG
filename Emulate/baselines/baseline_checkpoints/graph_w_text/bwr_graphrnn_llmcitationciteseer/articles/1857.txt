Deep neural networks have been shown to be vulnerable to adversarial attacks, which can have severe consequences in safety-critical applications. This paper proposes a novel framework for robustness analysis of deep neural networks using Bayesian neural symbolic learning. Our approach combines the strengths of symbolic methods and Bayesian neural networks to identify vulnerabilities in the decision-making process of deep models. We demonstrate the effectiveness of our approach on several benchmark datasets, including ImageNet and CIFAR-10, and show that it can detect adversarial attacks with high accuracy. Our framework provides a promising direction for developing more robust and trustworthy AI systems.