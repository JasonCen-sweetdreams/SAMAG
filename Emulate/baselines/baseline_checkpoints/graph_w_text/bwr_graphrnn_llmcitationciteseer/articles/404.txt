Emotion recognition in human-robot interaction (HRI) is crucial for robots to understand and respond to users' emotional states. This paper proposes a novel hierarchical attention network (HAN) for multimodal emotion recognition, incorporating facial expressions, speech, and physiological signals. Our HAN model learns to selectively focus on relevant modalities and features, improving emotion recognition accuracy compared to existing multimodal fusion methods. We evaluate our approach on a large HRI dataset, demonstrating improved recognition of emotions such as joy, sadness, and frustration, and explore its potential applications in HRI scenarios.