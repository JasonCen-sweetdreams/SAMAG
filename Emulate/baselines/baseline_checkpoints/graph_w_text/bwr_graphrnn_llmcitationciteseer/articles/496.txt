Voice assistants have become ubiquitous, but their interaction modalities often exclude users with disabilities. This paper presents a multimodal input framework for voice assistants that incorporates gesture, gaze, and facial expression recognition to enable more inclusive interaction. We conducted a user study with participants with varying abilities and found that our framework improved task completion rates by 30% compared to traditional voice-only input. We discuss the design implications and trade-offs of our approach and provide guidelines for future research on accessible voice assistants.