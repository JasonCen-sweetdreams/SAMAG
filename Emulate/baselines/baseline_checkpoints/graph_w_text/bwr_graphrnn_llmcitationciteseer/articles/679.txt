Multimodal sentiment analysis has gained significant attention in recent years, but existing approaches often lack interpretability. This paper proposes a novel hierarchical attention network (HAN) that integrates visual, textual, and acoustic features to predict sentiment scores. Our HAN model consists of three stages: modality-specific attention, cross-modal interaction, and sentiment fusion. We introduce a novel attention mechanism that learns to focus on relevant regions in images and utterances, enabling explainable multimodal sentiment analysis. Experimental results on the CMU-MOSEI dataset demonstrate the effectiveness of our approach, achieving state-of-the-art performance and providing insights into the decision-making process.