Deep reinforcement learning (DRL) has achieved impressive successes in various domains, but its vulnerability to adversarial attacks raises concerns about its reliability in safety-critical applications. This paper presents a novel approach to enhance the adversarial robustness of DRL agents by incorporating uncertainty-aware policy regularization. We introduce a new regularization term that encourages the policy to be more cautious in regions of high uncertainty, thereby reducing the attack surface. Experimental results on several benchmark environments demonstrate that our method significantly improves the robustness of DRL agents against a range of adversarial attacks, while maintaining their performance in the absence of attacks.