Cooperative multi-agent reinforcement learning (MARL) has shown promise in various applications, but existing methods often struggle to scale to large teams and complex tasks. This paper introduces a novel hierarchical attention network (HAN) architecture that enables efficient and effective cooperation among agents. Our HAN module incorporates both inter-agent and intra-agent attention mechanisms, allowing agents to selectively focus on relevant teammates and task components. We evaluate our approach on a range of MARL benchmarks, demonstrating improved performance and scalability compared to state-of-the-art methods.