Deep reinforcement learning (RL) has achieved impressive successes in complex tasks, but its lack of interpretability hinders its adoption in high-stakes applications. This paper proposes a novel framework, 'GraphXRL', which integrates contrastive graph embeddings with RL to provide explainable decision-making. By learning a graph representation of the state-action space, our approach can identify influential factors and relationships that contribute to the RL agent's policy. We demonstrate GraphXRL's effectiveness on a variety of Atari games and a real-world autonomous driving scenario, showcasing improved transparency and trustworthiness of the RL system.