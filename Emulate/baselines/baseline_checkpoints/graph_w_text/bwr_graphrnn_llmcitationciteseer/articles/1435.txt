Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the heterogeneity of modalities. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) that leverages the strengths of graph neural networks and attention mechanisms to capture complex relationships between modalities. HGAT learns to hierarchically aggregate features from each modality and selectively focus on relevant modalities for emotion recognition. Our experiments on the CMU-MOSEI dataset demonstrate that HGAT outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an improvement of 7.2% in F1-score.