Deep neural networks have achieved state-of-the-art performance in various machine learning tasks, but their success heavily relies on careful hyperparameter tuning. This paper proposes a novel Bayesian optimization approach that leverages Gaussian processes to efficiently search for optimal hyperparameters. Our method, 'GP-Hyper', adaptively balances exploration and exploitation by modeling the hyperparameter space as a probabilistic function. We demonstrate the effectiveness of GP-Hyper on several benchmark datasets, achieving competitive results with existing methods while reducing the number of evaluations by up to 50%. Our approach is particularly useful for large models and limited computational resources.