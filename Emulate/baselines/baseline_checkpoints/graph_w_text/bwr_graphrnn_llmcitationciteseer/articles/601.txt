Virtual reality (VR) has revolutionized human-computer interaction, but existing systems often neglect the importance of gaze-based input. This paper presents 'GazeAdapt', a novel framework that leverages eye-tracking data to adapt VR interactions to individual users' behavior. Our approach combines machine learning-based gaze prediction with real-time scene analysis to generate personalized interaction profiles. We evaluate GazeAdapt in a user study, demonstrating significant improvements in task completion time, accuracy, and user satisfaction. The results have implications for designing more inclusive and efficient VR experiences.