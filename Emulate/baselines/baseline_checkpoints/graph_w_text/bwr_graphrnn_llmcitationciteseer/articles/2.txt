Emotion recognition is crucial for developing empathetic human-computer interfaces. This paper presents EmoTract, a novel multimodal framework that leverages computer vision, speech recognition, and electroencephalography (EEG) to recognize emotions in real-time. EmoTract integrates a deep learning-based facial expression recognition module, a convolutional neural network (CNN) for speech emotion classification, and an EEG-based affective state detection module. Our experiments on a diverse dataset of 100 participants demonstrate that EmoTract outperforms state-of-the-art unimodal approaches, achieving an average emotion recognition accuracy of 87.5%. We envision EmoTract as a key component in developing more empathetic and responsive HCI systems.