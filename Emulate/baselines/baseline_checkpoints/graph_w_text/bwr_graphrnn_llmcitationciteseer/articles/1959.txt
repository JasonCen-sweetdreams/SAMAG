Entity disambiguation in knowledge graphs is a crucial step for accurate entity linking and question answering. Existing approaches rely on either hand-crafted features or simple graph-based methods that fail to capture complex contextual relationships. We propose GraphAttentionED, a novel entity disambiguation framework that leverages graph attention networks to learn contextualized entity representations. Our approach effectively integrates both local and global graph structure information, resulting in improved disambiguation accuracy. Experimental results on benchmark datasets demonstrate the superiority of GraphAttentionED over state-of-the-art methods, achieving an average F1-score improvement of 4.3%.