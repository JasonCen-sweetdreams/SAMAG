This paper addresses the challenge of coordinating multiple autonomous agents in complex, dynamic environments. We propose a decentralized partially observable Markov decision process (Dec-POMDP) framework that enables agents to learn coordinated policies without explicit communication. Our approach leverages a novel entropy-based reward function that encourages agents to explore and learn from each other's actions. Experimental results in a simulated search-and-rescue scenario demonstrate that Dec-POMDP outperforms existing decentralized methods, achieving improved task completion rates and reduced communication overhead.