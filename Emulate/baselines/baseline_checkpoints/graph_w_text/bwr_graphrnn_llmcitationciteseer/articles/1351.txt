This paper introduces a novel hierarchical reinforcement learning framework for coordinating autonomous agents in dynamic environments. Our approach, called Hierarchical-Actor-Critic (HAC), leverages a hierarchical policy representation to decompose complex tasks into simpler sub-tasks, enabling more efficient exploration and learning. We demonstrate the effectiveness of HAC in a simulated robotic soccer domain, where teams of agents must adapt to changing opponents and environmental conditions. Experimental results show that HAC outperforms state-of-the-art flat reinforcement learning methods in terms of task completion rate and overall team performance.