Trigger-action programming (TAP) has revolutionized end-user programming, but existing systems often neglect the needs of people with visual impairments. This paper presents 'AccessibleTAP', a novel adaptive TAP framework that leverages multimodal interaction, machine learning-driven inference, and real-time feedback to facilitate accessible programming. We conducted a user study with 20 participants with visual impairments, demonstrating significant improvements in programming speed, accuracy, and user satisfaction. Our results inform the design of more inclusive and adaptive TAP systems that can empower a broader range of users.