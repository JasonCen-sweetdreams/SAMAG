This paper presents a novel gaze-based intention recognition approach for intelligent wheelchair navigation. Our system utilizes a deep learning-based gaze estimation model to infer the user's intended direction of travel. We introduce a adaptive calibration method that accommodates individual differences in gaze patterns and wheelchair control styles. Experimental results with 20 participants demonstrate that our approach achieves an average recognition accuracy of 92.5% and reduces navigation errors by 35% compared to traditional joystick-based control. The proposed system has the potential to enhance the mobility and independence of individuals with severe motor impairments.