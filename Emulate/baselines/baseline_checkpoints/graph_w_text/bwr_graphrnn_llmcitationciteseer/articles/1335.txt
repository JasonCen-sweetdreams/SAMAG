Autonomous navigation in dynamic environments requires agents to adapt to changing circumstances while making decisions in real-time. This paper presents a hierarchical reinforcement learning framework, 'HRL-NAV', which leverages a novel decomposition of the navigation task into high-level planning and low-level control. The high-level planner utilizes a graph-based representation to reason about the environment, while the low-level controller employs a deep Q-network to execute actions. We evaluate HRL-NAV on a simulated driving scenario and demonstrate improved navigation performance and adaptability compared to flat reinforcement learning approaches.