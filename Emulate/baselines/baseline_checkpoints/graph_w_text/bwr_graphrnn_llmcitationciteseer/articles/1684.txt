Neural ranking models have become increasingly popular in e-commerce search due to their ability to capture complex query-document relationships. However, their computational requirements and memory footprint can be prohibitive for large-scale deployments. This paper proposes a novel index pruning technique, 'NP-Prune', that reduces the computational cost of neural ranking models by eliminating redundant and irrelevant index terms. We introduce a reinforcement learning-based approach to learn pruning policies that maximize search quality while minimizing computational overhead. Experimental results on a large e-commerce dataset demonstrate that NP-Prune achieves significant speedups and memory savings while preserving search accuracy.