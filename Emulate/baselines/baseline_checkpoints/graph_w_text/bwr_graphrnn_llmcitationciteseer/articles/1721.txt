Deep neural ranking models have achieved state-of-the-art performance in ad-hoc retrieval tasks. However, their effectiveness is often dependent on the quality of the input queries. Query expansion techniques have been shown to improve retrieval performance, but their impact on neural ranking models remains understudied. This paper investigates the effects of query expansion on several popular neural ranking models, including BERT and transformer-based architectures. We explore various expansion methods, including pseudo-relevance feedback and word embeddings-based techniques, and evaluate their impact on retrieval performance using standard benchmarks. Our results demonstrate that careful selection of query expansion methods can significantly improve the performance of neural ranking models, and provide insights into the optimal combination of expansion techniques and model architectures.