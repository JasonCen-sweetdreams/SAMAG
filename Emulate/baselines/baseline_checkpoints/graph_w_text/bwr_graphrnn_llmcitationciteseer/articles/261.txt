Visual Question Answering (VQA) models have achieved impressive performance, but their decision-making processes remain opaque. We propose a novel hierarchical attention-based framework, 'HARE', which leverages explainable reasoning to improve VQA performance and transparency. HARE employs a multi-scale attention mechanism that selectively focuses on relevant regions of the image and question, generating attention weights that provide insights into the model's decision-making process. Our experiments on the VQA 2.0 dataset demonstrate that HARE outperforms state-of-the-art models while providing interpretable explanations for its predictions.