Multi-task learning (MTL) has become increasingly popular in computer vision, but it often suffers from task interference and high computational costs. This paper proposes a novel hierarchical attention network (HAN) architecture that efficiently addresses these challenges. Our HAN leverages task-specific attention weights to selectively focus on relevant features and adaptively adjust the shared representation across tasks. We evaluate our approach on several benchmark datasets, demonstrating significant improvements in performance and efficiency compared to state-of-the-art MTL methods.