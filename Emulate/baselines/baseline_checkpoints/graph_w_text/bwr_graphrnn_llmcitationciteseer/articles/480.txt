Explainability is crucial in AI-driven medical diagnosis, where model interpretability can facilitate trust and understanding of life-critical decisions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates attention mechanisms at multiple scales to explain the diagnosis process. Our approach leverages feature importance scores to identify key biomarkers and highlight relevant patient information, enabling clinicians to understand the rationales behind AI-driven diagnoses. We evaluate HAN on a large-scale electronic health record dataset, demonstrating significant improvements in diagnosis accuracy and explainability compared to existing deep learning models.