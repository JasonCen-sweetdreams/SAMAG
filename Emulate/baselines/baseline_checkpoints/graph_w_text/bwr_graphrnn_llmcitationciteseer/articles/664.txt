Coordinating multiple autonomous agents in complex environments is a challenging problem with applications in robotics, smart cities, and IoT. This paper proposes a decentralized reinforcement learning framework, 'Decentralized Action-Value Estimation' (DAVE), which enables agents to learn coordinated policies without relying on a centralized controller. DAVE leverages an actor-critic architecture and a novel attention mechanism to facilitate communication and coordination among agents. We evaluate DAVE in a simulated urban traffic scenario, demonstrating improved coordination and reduced congestion compared to traditional decentralized approaches.