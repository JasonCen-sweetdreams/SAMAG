Dense passage retrieval (DPR) has shown promising results in open-domain question answering. However, its computational requirements and memory footprint limit its applicability to large-scale retrieval tasks. This paper introduces a novel query-adaptive document embedding (QADE) method that learns to adaptively adjust document representations based on the query context. QADE leverages a lightweight transformer architecture to generate query-dependent document embeddings, resulting in significant speedup and reduced memory usage while maintaining retrieval performance. Experimental results on the Natural Questions dataset demonstrate the effectiveness of QADE in improving the efficiency of DPR-based systems.