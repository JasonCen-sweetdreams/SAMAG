Deep learning models have achieved state-of-the-art performance in multi-modal sentiment analysis, but their lack of interpretability hinders their deployment in high-stakes applications. This paper proposes a novel hierarchical attention network (HAN) that incorporates explainable attention mechanisms to identify salient features in both visual and textual modalities. Our approach leverages a hierarchical fusion strategy to combine modality-specific attention weights, enabling the model to focus on relevant regions of interest in images and sentences. Experimental results on a large-scale multi-modal dataset demonstrate the effectiveness of our HAN model in improving both predictive performance and model interpretability.