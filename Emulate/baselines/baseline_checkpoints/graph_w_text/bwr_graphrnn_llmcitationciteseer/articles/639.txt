Individuals with motor impairments face significant barriers when interacting with web applications. This paper presents EyeGaze+, a novel gaze-based interface that leverages machine learning to adapt to the user's abilities and preferences. Our system incorporates a gaze-tracking module, which detects and interprets the user's eye movements to infer their intentions. We propose a probabilistic model that integrates gaze data with keyboard and mouse interactions to predict the user's goals. Evaluation results with 20 participants show that EyeGaze+ outperforms existing gaze-based interfaces, achieving an average accuracy of 92.5% in completing web tasks.