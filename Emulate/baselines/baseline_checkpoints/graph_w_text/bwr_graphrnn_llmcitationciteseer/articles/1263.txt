This paper proposes a novel attention mechanism for multimodal human-computer interaction that leverages eye-gaze information to disambiguate user intent. Our approach, 'EyeGuide', integrates a deep neural network-based eye-tracking model with a multimodal fusion framework to enable more accurate and efficient human-computer interaction. We evaluate EyeGuide on a dataset of 30 users performing various tasks on a multimodal interface and demonstrate a significant reduction in error rates and improvement in interaction speed compared to existing state-of-the-art methods.