This paper proposes a decentralized task allocation framework using multi-agent reinforcement learning. We consider a scenario where a team of agents must allocate tasks to maximize overall team performance, while communicating only with their local neighbors. We introduce a novel algorithm, 'MADRAL', which combines decentralized actor-critic methods with graph neural networks to learn task allocation policies. Experimental results in a simulated robotic search and rescue domain demonstrate that MADRAL outperforms existing decentralized task allocation algorithms in terms of team performance and adaptability to changing task requirements.