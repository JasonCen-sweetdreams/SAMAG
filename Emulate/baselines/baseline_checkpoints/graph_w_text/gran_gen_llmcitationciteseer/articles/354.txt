Open-domain question answering (ODQA) models rely on efficient passage retrieval to identify relevant documents containing the answer. This paper proposes a novel multi-modal embedding approach, 'MMEmb', which jointly learns dense representations of passages and questions from both textual and visual features. We demonstrate that MMEmb outperforms state-of-the-art retrieval methods on the Natural Questions dataset, achieving a 12.5% improvement in top-1 passage retrieval accuracy. Furthermore, we analyze the effectiveness of MMEmb in various ODQA pipelines, showcasing its potential to improve overall question answering performance.