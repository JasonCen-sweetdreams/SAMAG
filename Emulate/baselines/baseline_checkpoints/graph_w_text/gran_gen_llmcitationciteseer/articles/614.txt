Individuals with motor disabilities face significant challenges interacting with digital devices. This paper presents a novel gesture-based interface that leverages machine learning to recognize and interpret user intentions. Our approach utilizes a convolutional neural network to classify hand and finger gestures captured by a low-cost, wrist-mounted camera. We evaluate our system with a user study involving 20 participants with motor disabilities, demonstrating an average accuracy of 92.5% in gesture recognition and a significant reduction in interaction time. Our findings suggest that machine learning-driven gesture recognition can greatly enhance the accessibility and usability of digital technologies for individuals with motor disabilities.