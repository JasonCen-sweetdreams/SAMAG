Multi-agent reinforcement learning with graph neural networks (GNNs) has shown great promise in various applications. However, the vulnerability of these systems to adversarial attacks remains largely unexplored. This paper investigates targeted attacks on multi-agent systems, where an adversary manipulates the graph structure or node features to compromise the learning process. We propose a novel attack method, 'GraphFool', which leverages the graph topology to identify influential agents and craft customized perturbations. Our results demonstrate that GraphFool can significantly degrade the performance of the victim agents, highlighting the need for robustness measures in multi-agent reinforcement learning.