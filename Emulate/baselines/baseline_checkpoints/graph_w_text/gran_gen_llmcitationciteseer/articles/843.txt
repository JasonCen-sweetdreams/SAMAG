State-of-the-art image-text search systems rely on computationally expensive cross-modal embeddings, limiting their scalability. This paper proposes a novel multi-modal retrieval framework, 'HiGRMN', which leverages hierarchical graph neural networks (HGNNs) to efficiently capture fine-grained relationships between images and text. By modeling visual and textual features as interconnected graphs, HiGRMN achieves significant improvements in retrieval accuracy while reducing computational overhead by 30%. Experimental results on the COCO dataset demonstrate the effectiveness of our approach in real-world image-text search applications.