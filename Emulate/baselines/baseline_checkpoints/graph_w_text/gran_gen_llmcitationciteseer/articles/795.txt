Deep neural networks (DNNs) are vulnerable to adversarial attacks, which can lead to misclassification and security breaches. This paper proposes a novel approach for detecting adversarial attacks in DNNs using uncertainty-based anomaly detection. Our method, called 'UADEX', leverages the uncertainty estimates of DNNs to identify anomalous input samples that are likely to be adversarial. We demonstrate the effectiveness of UADEX on multiple benchmark datasets, achieving a detection rate of 95.2% with a false positive rate of 2.5%. Our approach outperforms existing state-of-the-art methods and provides a robust defense mechanism against adversarial attacks.