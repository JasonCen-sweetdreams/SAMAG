Deep reinforcement learning (DRL) has achieved impressive results in various domains, but its vulnerability to adversarial attacks raises concerns about its reliability in safety-critical applications. This paper investigates the robustness of DRL policies in partially observable environments, where the agent observes only a subset of the true state. We propose a novel attack framework, 'POE-Attack', which crafts perturbations to the observation space, misleading the agent into taking suboptimal actions. Our experiments on several benchmark domains demonstrate that POE-Attack significantly degrades the performance of state-of-the-art DRL algorithms, highlighting the need for robustification techniques in real-world deployments.