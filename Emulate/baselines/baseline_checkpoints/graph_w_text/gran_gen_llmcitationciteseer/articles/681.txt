Deep neural networks have been shown to be vulnerable to adversarial attacks, which can compromise their performance and reliability. This paper proposes a novel approach for detecting adversarial attacks using graph-based anomaly detection. We represent the neural network's output as a graph, where nodes correspond to neurons and edges represent the flow of activations. By analyzing the graph structure, we can identify anomalies indicative of adversarial attacks. Our experiments on several benchmark datasets demonstrate the effectiveness of our approach in detecting a wide range of attacks, including those using advanced techniques such as gradient masking. Our method can be easily integrated into existing neural network architectures, providing an additional layer of security against adversarial threats.