Multimodal sentiment analysis (MSA) requires effective fusion of features from diverse modalities such as text, images, and audio. This paper presents a novel deep learning framework, 'MAAN', which leverages adaptive attention mechanisms to dynamically weigh and integrate modality-specific features. MAAN incorporates a hierarchical attention structure that captures both local and global dependencies across modalities, leading to improved sentiment prediction accuracy. Experimental results on two benchmark datasets demonstrate the superiority of MAAN over state-of-the-art MSA methods, particularly in scenarios with noisy or missing modality data.