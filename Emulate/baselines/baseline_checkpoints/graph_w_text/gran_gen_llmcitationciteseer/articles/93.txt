Autonomous vehicle platooning has the potential to significantly improve highway safety and efficiency. However, coordinating the actions of multiple vehicles in real-time poses significant challenges. This paper proposes a novel multi-agent reinforcement learning framework, 'MARP', which enables autonomous vehicles to learn cooperative policies for platooning. MARP utilizes a decentralized actor-critic architecture, where each vehicle learns to make decisions based on local observations and communication with its neighbors. We evaluate MARP in a simulated highway environment and demonstrate improved platooning stability and reduced fuel consumption compared to traditional rule-based approaches.