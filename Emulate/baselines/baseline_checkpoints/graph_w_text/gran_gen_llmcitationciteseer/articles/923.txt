Augmented reality (AR) interfaces have the potential to revolutionize human-computer interaction, but existing visualization techniques often fail to effectively leverage multimodal input. This paper presents a novel framework for adaptive visualization in AR, which dynamically adjusts visual representations based on user behavior, context, and cognitive load. Our approach integrates machine learning-based user modeling with cognitive architectures to generate personalized, intuitive, and efficient visualizations. A user study demonstrates significant improvements in task completion time and user satisfaction compared to traditional visualization methods.