Skeleton-based action recognition has gained popularity in human-computer interaction applications. However, existing methods struggle to capture complex spatial-temporal relationships between joints. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) architecture, which leverages attention mechanisms to selectively focus on relevant joints and their interactions. We introduce a hierarchical graph representation that captures both local and global dependencies, enabling the model to recognize actions more accurately. Experiments on the NTU RGB+D and Kinetics datasets demonstrate the superiority of HGAT over state-of-the-art methods, achieving an average accuracy improvement of 3.5%.