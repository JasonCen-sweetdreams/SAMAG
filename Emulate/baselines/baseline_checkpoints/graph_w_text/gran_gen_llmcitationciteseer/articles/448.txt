Visual question answering (VQA) tasks require models to integrate information from both images and natural language. While existing approaches focus on learning complex multimodal representations, they often overlook the hierarchical structure of visual and linguistic data. This paper presents a novel Hierarchical Attention Network (HAN) architecture that leverages attention mechanisms to selectively focus on relevant regions in images and sentences. Our experiments on popular VQA benchmarks demonstrate that HAN achieves state-of-the-art performance while reducing computational costs by up to 30%. We also provide insights into the learned attention patterns, revealing the importance of object-centric reasoning in VQA.