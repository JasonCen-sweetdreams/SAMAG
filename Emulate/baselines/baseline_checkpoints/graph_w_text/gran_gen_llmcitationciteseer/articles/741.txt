Few-shot learning has emerged as a crucial challenge in AI, where models must adapt to new tasks with limited data. Graph Neural Networks (GNNs) have shown promise in this setting, but their performance often hinges on manual tuning of hyperparameters. This paper proposes a novel meta-learning approach, 'MAGNET', which adaptively learns graph structures and neural network weights from a few examples. Our method leverages a hierarchical graph attention mechanism, enabling efficient knowledge transfer across tasks. Experimental results on benchmark datasets demonstrate that MAGNET outperforms state-of-the-art few-shot learning methods, while requiring significantly fewer parameters and computations.