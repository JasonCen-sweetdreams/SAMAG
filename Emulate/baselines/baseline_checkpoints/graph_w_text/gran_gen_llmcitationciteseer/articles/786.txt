Explainability is crucial in multi-agent reinforcement learning (MARL) to understand how agents make decisions. This paper proposes a Hierarchical Attention Network (HAN) architecture for MARL, which incorporates attention mechanisms to selectively focus on relevant agents and their interactions. Our approach enables the identification of influential agents and the attribution of rewards to individual agents' actions, leading to more interpretable policies. We evaluate HAN on a suite of cooperative and competitive MARL benchmarks, demonstrating improved explainability and robustness compared to state-of-the-art methods.