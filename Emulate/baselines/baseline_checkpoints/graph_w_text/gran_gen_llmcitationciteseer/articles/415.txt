Multi-label classification is a fundamental problem in machine learning, where each instance can be associated with multiple labels. Existing approaches often suffer from high computational complexity and neglect label correlations. We propose a novel Hierarchical Attention Network (HAN) architecture that leverages label hierarchies to improve model efficiency and accuracy. HAN employs a hierarchical attention mechanism to selectively focus on relevant labels and capture label dependencies. Experimental results on benchmark datasets demonstrate that HAN outperforms state-of-the-art methods in terms of both accuracy and inference time.