Deep reinforcement learning (DRL) models have achieved remarkable successes in complex decision-making tasks, but their opacity hinders trust and understanding. This paper presents 'AttribuRL', a model-agnostic attribution framework for explaining DRL policies. By recursively applying feature importance measures, AttribuRL generates human-interpretable explanations for state-action pairs, facilitating error analysis and policy improvement. We evaluate AttribuRL on several Atari games and a real-world robotics task, demonstrating its scalability and ability to identify actionable insights.