The increasing adoption of transformer-based models in AI applications has raised concerns about their lack of transparency. This paper proposes a novel approach to explainability, dubbed 'TransformerLens', which leverages attention mechanisms to generate saliency maps that highlight input features contributing to the model's predictions. Our method outperforms existing techniques in terms of attribution quality and computational efficiency, as demonstrated on a range of NLP benchmarks. We also provide a user study that shows the effectiveness of TransformerLens in improving human understanding and trust in AI-driven decision-making systems.