Explainability is a crucial aspect of AI-driven decision-making systems, as it enables accountability and trust. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates attention mechanisms at multiple levels to provide transparent and interpretable explanations for AI-driven decisions. Our approach leverages attention weights to identify relevant input features and their contributions to the decision-making process. Experimental results on a real-world credit risk assessment dataset demonstrate the effectiveness of HAN in generating accurate and interpretable explanations, outperforming state-of-the-art explainability methods.