Multimodal sentiment analysis (MSA) aims to identify sentiment expressed through multiple modalities, such as text, images, and audio. Existing approaches often rely on early or late fusion, neglecting the complex interactions between modalities. This paper presents a novel hierarchical attention network (HAN) for MSA, which learns to selectively focus on relevant modalities and their interactions at multiple levels. Our HAN model comprises modality-specific attention modules and a cross-modal fusion layer, enabling the model to capture both intra- and inter-modality relationships. Experimental results on several benchmark datasets demonstrate the superiority of our approach over state-of-the-art methods, achieving improved accuracy and robustness in MSA tasks.