Conversational agents are becoming increasingly prevalent in various applications, but accurately recognizing user emotions remains a significant challenge. This paper proposes a novel hierarchical attention network (HAN) framework that leverages multi-modal input (text, audio, and video) to improve emotion recognition. Our approach incorporates a hierarchical attention mechanism that selectively focuses on relevant modalities and utterance segments, enabling more accurate emotion detection. Experimental results on a large-scale conversational dataset demonstrate that our HAN framework outperforms existing state-of-the-art methods, achieving an F1-score of 92.1% on the emotion recognition task.