Bayesian neural networks (BNNs) offer a promising approach to modeling uncertainty in deep learning, but their high computational cost hinders their adoption in real-world applications. This paper presents a novel pruning method for BNNs, leveraging hierarchical masking to identify and remove redundant weights. Our approach exploits the inherent structure of BNNs to efficiently prune entire sub-networks, resulting in significant reductions in computational cost and memory usage. Experimental results on several benchmark datasets demonstrate that our method achieves state-of-the-art performance in terms of accuracy and uncertainty estimation while maintaining a drastic reduction in model size.