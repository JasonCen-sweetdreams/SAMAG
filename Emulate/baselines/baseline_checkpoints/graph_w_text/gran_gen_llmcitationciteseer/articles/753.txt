Few-shot learning has witnessed significant progress with the advent of meta-learning and attention mechanisms. However, the lack of explainability in these models hinders their adoption in high-stakes applications. This paper proposes a novel Hierarchical Attention Network (HAN) that integrates self-attention and class-attention mechanisms to provide interpretable few-shot classification. Our approach learns to selectively focus on relevant features and instances, generating attention maps that explain the model's predictions. Experimental results on mini-ImageNet and tiered-ImageNet datasets demonstrate the effectiveness of HAN in achieving state-of-the-art performance while providing transparent decision-making processes.