Query expansion (QE) is a crucial step in ad-hoc retrieval, but traditional methods often rely on simplistic term co-occurrence statistics. This paper proposes a novel QE approach that leverages pre-trained deep neural language models (DLMs) to capture nuanced semantic relationships between terms. We fine-tune a BERT-based DLM on a large corpus of text and use its contextualized embeddings to generate expanded queries. Experimental results on several TREC datasets demonstrate significant improvements in retrieval effectiveness, particularly for short and vague queries, compared to state-of-the-art QE methods.