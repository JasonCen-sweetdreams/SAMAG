Distributed database systems are increasingly used to handle large-scale data storage and querying. However, optimizing query performance remains a significant challenge. This paper proposes a novel approach to index selection using machine learning techniques. We develop a framework that leverages query workload patterns and data distribution characteristics to predict optimal index configurations. Our experiments on a real-world dataset demonstrate that our approach outperforms traditional rule-based index selection methods, reducing query latency by up to 35%. We also explore the applicability of our framework to various distributed database systems.