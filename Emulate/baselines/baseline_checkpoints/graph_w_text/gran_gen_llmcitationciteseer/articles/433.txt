Human activity recognition (HAR) using multi-modal sensors requires effective fusion of heterogeneous data streams. This paper proposes a novel Hierarchical Graph Attention Network (H-GAT) architecture that adaptively integrates feature representations from various sensor modalities. Our approach leverages graph attention mechanisms to model complex relationships between sensor readings and activity labels, while hierarchical fusion enables the capture of both local and global contextual information. Experiments on a large-scale HAR dataset demonstrate that H-GAT achieves state-of-the-art performance, outperforming existing fusion techniques by up to 12% in terms of activity recognition accuracy.