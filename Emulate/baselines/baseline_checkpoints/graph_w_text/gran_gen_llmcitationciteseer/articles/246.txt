Backdoor attacks on deep neural networks (DNNs) have become a significant concern in machine learning. Existing defense methods often focus on detecting and removing backdoors, but these approaches can be evaded by sophisticated attackers. We propose a novel adversarial training framework, 'BackdoorGuard', which proactively injects artificial backdoors into the training data to enhance the model's robustness against such attacks. Our experiments demonstrate that BackdoorGuard significantly improves the resilience of DNNs to backdoor attacks, outperforming state-of-the-art defense methods in various scenarios.