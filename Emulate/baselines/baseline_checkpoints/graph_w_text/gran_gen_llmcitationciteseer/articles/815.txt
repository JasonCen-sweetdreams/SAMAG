This paper introduces EmoTract, a novel affective state modeling framework for human-computer interaction. EmoTract leverages multimodal sensor data from physiological and facial expression sensors to infer users' emotional states in real-time. Our approach incorporates a hierarchical Bayesian network that captures the dynamics of affective states and their influence on user behavior. We evaluate EmoTract in a user study, demonstrating its ability to accurately predict user frustration and adapt the interaction to mitigate negative emotional responses. The results show improved user experience and task performance, highlighting the potential of EmoTract for personalized HCI.