Gesture recognition systems have transformed human-computer interaction, but they often struggle to accommodate users with motor impairments. This paper presents a multimodal approach that combines computer vision, machine learning, and electromyography (EMG) to recognize gestures from users with diverse abilities. Our system, 'IncluGesture', leverages a novel feature fusion technique to integrate data from multiple sensors, enabling accurate recognition of gestures even in the presence of motor impairments. We evaluate IncluGesture with a diverse user group and demonstrate significant improvements in recognition accuracy and user experience compared to state-of-the-art methods.