Cooperative multi-agent reinforcement learning is a challenging problem in AI research, requiring agents to learn complex coordination strategies. We propose a novel Hierarchical Attention Network (HAN) architecture, which enables agents to selectively focus on relevant teammates and tasks. HAN consists of a hierarchical attention mechanism and a novel ' teammate-aware' reward function. Our experiments show that HAN outperforms state-of-the-art methods in several cooperative game environments, demonstrating its ability to learn effective coordination strategies in complex, dynamic scenarios.