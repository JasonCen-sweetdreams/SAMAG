Conversational agents require accurate emotion recognition to provide empathetic and personalized responses. Existing approaches often rely on a single modality, such as speech or text, which can be limiting. This paper proposes a novel hierarchical attention network (HAN) that integrates multimodal features from speech, text, and vision to recognize emotions in conversational agents. Our HAN model learns to attend to relevant modalities and time steps, improving emotion recognition accuracy and robustness. Experimental results on the CMU-MOSEI dataset demonstrate the effectiveness of our approach, outperforming state-of-the-art methods by 12.5% in terms of F1-score.