As autonomous vehicles (AVs) become increasingly prevalent, efficient coordination mechanisms are crucial for safe and efficient traffic flow. This paper presents MARL-COORD, a novel multi-agent reinforcement learning framework that enables AVs to adaptively learn coordination strategies in dynamic environments. We propose a decentralized, graph-based Q-learning approach that leverages vehicle-to-vehicle communication and sparse rewards. Experimental results in a realistic simulation environment demonstrate that MARL-COORD outperforms existing methods in terms of reduced congestion, travel time, and fuel consumption.