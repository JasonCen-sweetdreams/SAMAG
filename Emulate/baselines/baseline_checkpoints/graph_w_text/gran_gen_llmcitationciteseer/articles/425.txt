Event extraction from multi-modal data (e.g., text, images, videos) is a challenging task due to the complexity of capturing cross-modal relationships. This paper introduces HAGNet, a hierarchical attention graph neural network that jointly learns intra-modal and inter-modal representations. Our approach leverages self-attention mechanisms to model node interactions within each modality and across modalities. Experimental results on a large-scale event extraction benchmark demonstrate that HAGNet outperforms state-of-the-art methods by a significant margin, achieving an F1-score improvement of 12.5% on average.