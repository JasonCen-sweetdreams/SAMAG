Autonomous robotics requires agents to learn complex tasks in diverse environments. We propose a hierarchical reinforcement learning framework, 'HIRL-Transfer', that leverages transfer learning to adapt policies across tasks and domains. Our approach combines a high-level task encoder with a low-level motor controller, enabling the agent to transfer knowledge from simulation to real-world scenarios. Experimental results on a robotic arm platform demonstrate significant improvements in task completion rates and reduced learning times compared to flat reinforcement learning methods.