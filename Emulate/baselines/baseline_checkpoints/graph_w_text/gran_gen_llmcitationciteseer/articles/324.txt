Zero-shot learning (ZSL) enables models to recognize unseen classes without additional training data. However, existing ZSL methods often rely on auxiliary datasets or complex optimization procedures, limiting their applicability to real-world scenarios. This paper presents a meta-learning framework, 'MetaZSL', which tackles ZSL by learning to adapt to new classes from a few exemplar images. Our approach leverages a novel task-aware attention mechanism to selectively focus on relevant feature dimensions, resulting in improved generalization to unseen classes. Experiments on popular image classification benchmarks demonstrate that MetaZSL outperforms state-of-the-art ZSL methods while requiring minimal additional computational resources.