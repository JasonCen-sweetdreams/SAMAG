Deep neural networks have achieved state-of-the-art performance in various machine learning tasks, but their success heavily relies on careful hyperparameter tuning. Bayesian optimization has emerged as a powerful approach to hyperparameter tuning, but its computational cost can be prohibitively high. This paper proposes a novel Bayesian optimization method that leverages Gaussian process priors to model the objective function. We introduce a sparse approximation scheme that reduces the computational cost of Gaussian process inference while maintaining its accuracy. Experimental results on several benchmark datasets demonstrate that our approach outperforms existing Bayesian optimization methods and achieves comparable performance to state-of-the-art methods with significantly reduced computational cost.