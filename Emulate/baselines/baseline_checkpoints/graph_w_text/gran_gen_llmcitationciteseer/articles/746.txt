Emotion recognition from multi-modal inputs (e.g., speech, text, vision) remains a challenging task due to the complexity of human emotions and the heterogeneity of modalities. This paper presents a novel Hierarchical Attention Network (HAN) that adaptively integrates and weights features from different modalities. Our HAN consists of multiple attention levels, each focusing on a specific modality or interaction between modalities. Experimental results on the IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving a 12.1% improvement in F1-score.