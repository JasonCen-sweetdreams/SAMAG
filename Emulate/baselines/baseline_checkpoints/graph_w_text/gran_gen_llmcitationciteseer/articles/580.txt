This paper presents a novel decentralized task allocation framework for multi-agent systems using Markov decision processes (MDPs). We model each agent as an MDP, and propose a distributed value function approximation algorithm to learn optimal task allocations. Our approach leverages the agents' local observations and communication with neighboring agents to make decentralized decisions. We evaluate our framework on a simulated disaster response scenario, demonstrating improved task completion rates and reduced communication overhead compared to centralized allocation methods. Our results have implications for large-scale multi-agent systems in real-world applications such as search and rescue, surveillance, and smart cities.