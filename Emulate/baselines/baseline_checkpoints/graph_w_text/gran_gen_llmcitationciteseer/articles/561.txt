Emotion recognition in human-computer interaction (HCI) is crucial for developing empathetic systems. Existing approaches often rely on single modalities, neglecting the complementary information from multiple sources. This paper proposes a Hierarchical Attention Network (HAN) for multimodal emotion recognition, integrating facial expressions, speech, and physiological signals. Our HAN architecture learns to selectively focus on relevant modalities and features, outperforming state-of-the-art approaches on the EMOTIC dataset. We also explore the interpretability of HAN, providing insights into the emotional cues utilized by the model.