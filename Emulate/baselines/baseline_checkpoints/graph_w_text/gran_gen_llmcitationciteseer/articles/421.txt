Dialogue state tracking (DST) is a crucial component of multimodal conversational systems. Existing approaches struggle to effectively integrate visual and linguistic cues, leading to suboptimal performance. We propose a novel Hierarchical Graph Attention Network (HiGAT) that leverages both modalities to learn a comprehensive representation of the conversation state. HiGAT exploits the graph structure of the dialogue to model relationships between entities, intent, and slots, and incorporates multimodal attention to selectively focus on relevant input features. Experimental results on the DSTC8 benchmark demonstrate state-of-the-art performance, with significant gains over existing multimodal DST models.