Multi-hop question answering (QA) models have achieved impressive results, but their lack of transparency hinders their reliability in real-world applications. This paper presents a novel hierarchical attention network (HAN) architecture that provides interpretable reasoning for multi-hop QA tasks. Our model employs a stacked attention mechanism to selectively focus on relevant context sentences, entities, and relationships, allowing for explicit tracking of the reasoning process. Experimental results on the HotPotQA dataset demonstrate that our HAN model outperforms state-of-the-art approaches while offering enhanced explainability and transparency.