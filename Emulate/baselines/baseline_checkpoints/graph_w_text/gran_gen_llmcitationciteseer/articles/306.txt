Developing dialogue systems that can effectively engage with humans requires understanding the nuances of multi-modal input, including speech, text, and vision. This paper presents a novel hierarchical attention network (HAN) architecture that learns to selectively focus on relevant modalities and contextual information to generate more accurate and explainable responses. We also introduce a new explanation module that provides insights into the model's decision-making process, improving transparency and trust in human-machine interactions. Experiments on a large-scale multi-modal dialogue dataset demonstrate the effectiveness of our approach, achieving state-of-the-art results in response accuracy and human likeness.