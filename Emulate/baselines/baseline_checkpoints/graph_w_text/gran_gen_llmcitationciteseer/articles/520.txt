Emotion recognition from multimodal inputs (e.g., text, audio, and vision) is a challenging task due to the complexity and diversity of human emotions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that captures both intra-modality and inter-modality relationships. Our HAN model consists of three stages: (1) modality-specific feature extraction, (2) attention-based feature fusion, and (3) hierarchical attention for emotion classification. Experimental results on the Multimodal Emotion Recognition (MER) benchmark dataset demonstrate that our approach outperforms state-of-the-art methods by 12.5% in terms of weighted F1-score.