Autonomous vehicles (AVs) rely on reinforcement learning (RL) to make critical decisions, but the lack of transparency in RL models poses significant safety concerns. This paper proposes an explainable RL framework, 'XRL', that integrates model-based and model-free RL with attention mechanisms to generate visual explanations for AV decision-making. We evaluate XRL on a real-world driving dataset and demonstrate improved interpretability and safety compared to state-of-the-art RL methods. Our approach has significant implications for the development of trustworthy and accountable AV systems.