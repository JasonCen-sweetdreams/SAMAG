Coordinating multiple heterogeneous agents in complex environments is a challenging problem in multi-agent systems. This paper proposes a decentralized multi-agent deep reinforcement learning framework, 'DMARL-COM', that enables agents to learn effective coordination strategies without relying on centralized controllers or communication. We introduce a novel attention-based agent representation that captures the heterogeneity of agents and their interactions. Experimental results on a real-world traffic management scenario demonstrate that DMARL-COM outperforms state-of-the-art decentralized methods in terms of coordination efficiency and scalability.