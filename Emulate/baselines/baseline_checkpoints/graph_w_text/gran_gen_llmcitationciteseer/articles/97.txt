Emotion recognition from multi-modal inputs (e.g., audio, video, text) is a challenging task due to the inherent complexity of human emotions and the variability of modalities. This paper presents a novel Hierarchical Attention Network (HAN) architecture that effectively fuses and selectively focuses on relevant modalities to improve emotion recognition. Our HAN model employs a hierarchical structure to capture intra-modality and inter-modality relationships, and incorporates attention mechanisms to dynamically weight the importance of each modality. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs.