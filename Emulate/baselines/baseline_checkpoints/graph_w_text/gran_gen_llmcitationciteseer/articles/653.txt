Zero-shot learning (ZSL) has gained significant attention in recent years, enabling object detection models to recognize unseen classes without additional training data. However, existing ZSL methods suffer from poor performance on real-world datasets due to the lack of robustness to noisy or limited training data. This paper proposes a novel contrastive language-image pre-training approach, dubbed 'CLIP-ZSL', which leverages large-scale language-image datasets to learn a shared representation space for both visual and linguistic inputs. Our method achieves state-of-the-art performance on several real-world object detection benchmarks, outperforming existing ZSL methods by up to 15% in mean average precision.