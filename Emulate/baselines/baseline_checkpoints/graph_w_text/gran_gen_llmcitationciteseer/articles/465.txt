Deep neural networks are vulnerable to adversarial attacks, which can significantly degrade their performance. This paper proposes a neural architecture search (NAS) framework, 'RobuNAS', that incorporates robustness awareness into the search process. We introduce a novel search objective that minimizes the expected loss under adversarial perturbations, while also considering the model's clean accuracy and computational cost. Experiments on benchmark datasets show that RobuNAS discovers architectures that are more robust to various types of attacks, outperforming state-of-the-art models trained with adversarial training and other defense mechanisms.