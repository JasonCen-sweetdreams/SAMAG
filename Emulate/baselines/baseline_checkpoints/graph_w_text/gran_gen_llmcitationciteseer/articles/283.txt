Neural search models have shown promising results in information retrieval tasks, but often rely on predefined query representations. This paper proposes a novel query expansion approach, dubbed 'ContrastiveQE', which leverages contrastive learning to generate semantically enriched query embeddings. Our method learns to distinguish relevant from non-relevant documents, enabling the model to capture nuanced query semantics and improve retrieval performance. Experiments on several benchmark datasets demonstrate that ContrastiveQE outperforms state-of-the-art query expansion techniques, achieving significant gains in retrieval accuracy and efficiency.