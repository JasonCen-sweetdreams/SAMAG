Deep neural networks (DNNs) are vulnerable to adversarial attacks, which can be imperceptible to humans. Existing detection methods often rely on point estimates of model uncertainty, neglecting the inherent Bayesian nature of neural networks. This paper proposes a novel framework, 'BayesDefend', that leverages Bayesian uncertainty estimation to detect adversarial perturbations. By approximating the posterior distribution of model parameters, we can quantify the uncertainty of predictions and identify inputs with high uncertainty as potential attacks. Our experiments on ImageNet and CIFAR-10 datasets demonstrate that BayesDefend outperforms state-of-the-art detection methods, achieving an average detection rate of 95.2% with a false positive rate of 2.1%.