Deep reinforcement learning (DRL) has achieved remarkable success in various domains, but the lack of transparency in decision-making processes hinders its widespread adoption. This paper presents a novel approach to improving the explainability of DRL agents by incorporating attention-based feature importance. We propose a model-agnostic technique that generates feature importance scores, enabling the identification of crucial state features contributing to the agent's policy. Experimental results on Atari games and a real-world robotics dataset demonstrate that our approach provides meaningful insights into the decision-making process, improving the trustworthiness and accountability of DRL systems.