Accurate human emotion recognition is crucial in various applications, including affective computing, human-computer interaction, and mental health monitoring. This paper proposes a multimodal fusion approach that combines electroencephalography (EEG) signals with facial expression analysis to recognize human emotions in real-time. We present a novel neural network architecture that integrates EEG-based affective state estimation with computer vision-based facial feature extraction. Experimental results on a large-scale emotion dataset demonstrate the superiority of our approach over unimodal methods, achieving an average emotion recognition accuracy of 92.5%.