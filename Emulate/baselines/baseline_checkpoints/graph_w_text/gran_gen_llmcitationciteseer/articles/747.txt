Multimodal emotion recognition has gained significant attention in recent years, but existing approaches often lack interpretability and may not generalize well to unseen scenarios. This paper proposes a novel hierarchical attention network (HAN) that integrates visual, acoustic, and linguistic features for emotion recognition. Our HAN model employs a top-down attention mechanism to selectively focus on relevant modalities and regions, allowing for decomposition of the recognition process and improved explainability. Experimental results on the EmoReact dataset demonstrate that our approach outperforms state-of-the-art multimodal fusion methods and provides insightful visualizations of the decision-making process.