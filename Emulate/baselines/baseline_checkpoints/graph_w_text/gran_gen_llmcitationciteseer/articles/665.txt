As autonomous systems become increasingly prevalent, there is a growing need for explainable decision-making to ensure transparency and accountability. This paper presents a novel multi-modal fusion framework, 'EXPLAIN', which integrates visual, textual, and auditory features to provide interpretable explanations for autonomous decision-making. We leverage attention-based neural networks to selectively weigh and fuse modality-specific features, generating comprehensive explanations for complex decisions. Experimental results on a real-world autonomous driving dataset demonstrate the effectiveness of EXPLAIN in improving decision transparency and user trust.