We present a novel wearable-based gesture recognition system that leverages the strengths of deep learning and sensor fusion. Our approach combines data from multiple sensors, including accelerometers, gyroscopes, and electromyography, to recognize a range of hand and finger gestures. We propose a multi-modal fusion architecture that integrates convolutional neural networks and recurrent neural networks to capture both spatial and temporal patterns in the sensor data. Experimental results demonstrate an average recognition accuracy of 95.2% across 12 different gestures, outperforming state-of-the-art approaches that rely on individual sensors or modalities.