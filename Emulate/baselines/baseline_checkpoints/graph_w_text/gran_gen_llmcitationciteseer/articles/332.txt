In dynamic task environments, agents must adapt to changing conditions and coordinate with each other to achieve common goals. This paper presents a novel framework for adaptive agent coordination, which leverages reinforcement learning to learn effective coordination strategies in real-time. Our approach, dubbed 'AdaCoor', integrates a task-oriented attention mechanism with a graph neural network to model inter-agent dependencies and optimize collective performance. Experimental results on a variety of benchmark scenarios demonstrate the robustness and scalability of AdaCoor in the face of unexpected environmental changes and agent failures.