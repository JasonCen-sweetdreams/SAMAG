Deep reinforcement learning (DRL) has achieved remarkable success in various applications, but the lack of transparency in decision-making processes hinders its adoption in high-stakes domains. This paper presents a novel, model-agnostic approach to explainability in DRL, dubbed 'SaliencyRL'. By generating saliency maps that highlight the most influential input features contributing to an agent's decisions, SaliencyRL provides actionable insights into the policy's behavior. We demonstrate the effectiveness of SaliencyRL on several Atari games and a real-world robotics task, showcasing its potential to facilitate trust and understanding in DRL systems.