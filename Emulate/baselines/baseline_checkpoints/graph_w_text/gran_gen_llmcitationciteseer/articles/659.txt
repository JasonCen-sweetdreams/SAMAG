Multimodal retrieval has gained significance with the increasing availability of multimedia data. Existing query expansion techniques struggle to capture the complex relationships between different modalities. This paper proposes a novel graph-based embedding approach, 'Multimodal Graph Embeddings' (MGE), to encode the interactions between text, image, and audio modalities. MGE leverages a graph neural network to learn joint representations, enabling effective query expansion for multimodal retrieval. Experimental results on the Wikipedia dataset demonstrate significant improvements in retrieval performance compared to state-of-the-art methods, especially for queries with limited textual information.