Deep neural networks have been shown to be vulnerable to adversarial attacks, which can compromise their performance and security. This paper proposes a novel approach to detect adversarial attacks in deep neural networks using graph-based anomaly detection techniques. We model the neural network's activation patterns as a graph and utilize graph-based metrics to identify anomalies indicative of adversarial attacks. Our approach is evaluated on several benchmark datasets and demonstrates improved detection accuracy compared to existing methods. We further analyze the robustness of our approach against various types of attacks and provide insights into the vulnerability of different neural network architectures.