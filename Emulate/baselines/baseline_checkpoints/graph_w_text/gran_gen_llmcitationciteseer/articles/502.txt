In multi-agent systems, explaining the decision-making process of individual agents is crucial for trust and accountability. We propose a novel Hierarchical Graph Attention Network (HGAT) architecture that learns to coordinate agents while providing interpretable explanations. HGAT leverages hierarchical graph attention to model complex agent interactions and incorporates a novel explainability module that generates saliency maps highlighting influential agents and their relationships. Our experiments on a real-world traffic management dataset demonstrate that HGAT outperforms state-of-the-art methods in terms of both coordination efficiency and explanation quality.