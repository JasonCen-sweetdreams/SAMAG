Integrating multiple modalities (e.g., vision, language, and audio) in AI systems often relies on knowledge graphs (KGs) to capture complex relationships. However, existing KG embedding methods struggle with heterogeneous data and noisy annotations. This paper proposes a novel contrastive learning framework, 'MultiModalCE', which jointly learns KG embeddings and modality-specific representations. By leveraging large-scale, unlabeled datasets and a carefully designed contrastive loss, MultiModalCE outperforms state-of-the-art methods in both link prediction and entity classification tasks on several benchmark datasets.