Emotion recognition from multi-modal inputs, such as speech, text, and vision, is a challenging task due to the complexity of human emotions and the heterogeneity of modalities. This paper proposes a novel Hierarchical Attention Network (HAN) that captures intra- and inter-modal relationships to recognize emotions more accurately. Our HAN model incorporates a hierarchical fusion mechanism that adaptively weights modalities based on their contributions to emotion recognition. Experimental results on the CMU-MultimodalSDK dataset demonstrate that our approach outperforms state-of-the-art methods in terms of emotion recognition accuracy and provides interpretable results through attention visualization.