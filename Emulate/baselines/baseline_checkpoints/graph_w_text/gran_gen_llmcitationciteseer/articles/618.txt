Deep learning models have achieved state-of-the-art performance in various applications, but their hyperparameter tuning remains a challenging and time-consuming task. This paper proposes a novel Bayesian optimization framework that leverages transfer learning to adapt the search space of hyperparameters. Our approach, 'HyperTL', uses a surrogate model pre-trained on a large dataset to warm-start the search process, reducing the number of evaluations required to find optimal hyperparameters. We demonstrate the efficacy of HyperTL on several benchmark datasets, showing significant speedups and improved model performance compared to existing Bayesian optimization methods.