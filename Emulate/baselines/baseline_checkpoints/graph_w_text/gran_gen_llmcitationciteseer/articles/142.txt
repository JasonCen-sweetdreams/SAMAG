Coordinating multi-agent systems (MAS) in complex, dynamic environments remains a challenging problem. This paper proposes a decentralized reinforcement learning framework, 'Dec-MAS', which enables heterogeneous agents to learn cooperative policies without relying on centralized controllers or explicit communication. We introduce a novel, graph-based neural network architecture that incorporates local observations and neighbor information to facilitate decentralized decision-making. Experimental results on a simulated disaster response scenario demonstrate that Dec-MAS achieves improved coordination and adaptability compared to traditional, model-based approaches.