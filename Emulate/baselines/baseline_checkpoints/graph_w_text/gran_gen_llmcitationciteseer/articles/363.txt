Visual question answering (VQA) models often struggle to provide interpretable explanations for their predictions. We propose a novel hierarchical attention-based approach, 'HAT-VQA', which leverages self-attention mechanisms to generate explanations at multiple levels of abstraction. Our framework consists of a question-aware visual encoder, a hierarchical attention network, and a reasoning module that predicts answers and explanations jointly. Experimental results on the VQA-X dataset demonstrate that HAT-VQA outperforms state-of-the-art VQA models while providing more accurate and informative explanations.