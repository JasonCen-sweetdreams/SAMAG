Deep reinforcement learning (DRL) has achieved remarkable success in various applications, but the lack of interpretability hinders its adoption in high-stakes domains. This paper introduces a novel hierarchical attention mechanism, 'HAT', to provide explanations for DRL policies. HAT generates attention weights at multiple levels of abstraction, enabling the identification of key state features and their relationships that contribute to the policy's decisions. We demonstrate the effectiveness of HAT on several Atari games and a real-world autonomous driving scenario, showing that it improves the transparency and trustworthiness of DRL agents.