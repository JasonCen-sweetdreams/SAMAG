This paper presents an affective modeling framework that leverages emotional intelligence to enhance personalized human-computer interaction. Our approach combines multimodal sensing and machine learning to recognize users' emotional states and adapt the interaction accordingly. We conducted a user study with 50 participants and demonstrated that our framework improves user satisfaction and reduces frustration in emotionally charged tasks. The proposed model can be integrated into various HCI applications, such as virtual assistants, gaming, and mental health support systems.