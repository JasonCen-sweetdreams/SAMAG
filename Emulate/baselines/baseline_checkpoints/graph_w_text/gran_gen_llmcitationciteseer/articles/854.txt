Sentiment analysis from multi-modal data (e.g., text, images, and audio) has become increasingly important in various applications. However, existing methods typically focus on a single modality or rely on early fusion, which may lead to suboptimal performance. We propose a novel Hierarchical Attention Network (HAN) that leverages both intra- and inter-modal attention mechanisms to capture complex relationships between modalities. Our experiments on a large-scale multi-modal dataset demonstrate that HAN outperforms state-of-the-art methods in sentiment analysis tasks, achieving a significant improvement in accuracy and F1-score.