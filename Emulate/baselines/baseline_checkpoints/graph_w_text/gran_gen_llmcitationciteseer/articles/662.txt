Neural architecture search (NAS) has emerged as a crucial technique for automating the design of deep neural networks. However, existing methods often suffer from high computational overhead and limited exploration capabilities. To address these limitations, we propose a novel NAS framework, 'GraphNAS', which leverages graph-based reinforcement learning to efficiently explore the architecture space. Our approach represents neural architectures as graphs and utilizes a reinforcement learning agent to search for optimal architectures. Experimental results on several benchmark datasets demonstrate that GraphNAS achieves state-of-the-art performance while reducing the search time by up to 50% compared to existing methods.