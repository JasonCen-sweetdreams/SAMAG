Virtual reality (VR) systems often rely on manual input devices, leading to user fatigue and decreased immersion. This paper presents 'GazeAdapt', an innovative eye-gaze driven interface that dynamically adapts to user behavior in VR environments. Our approach utilizes machine learning-based gaze prediction and real-time pupillometry to detect user attention and intentions. We evaluate GazeAdapt in a user study, demonstrating significant improvements in task efficiency, user satisfaction, and reduced cognitive load. The proposed system has far-reaching implications for enhancing accessibility and overall user experience in VR applications.