Dense passage retrieval (DPR) models have shown promising results in open-domain question answering tasks. However, they often suffer from high computational costs and slow inference speeds, making them impractical for large-scale applications. This paper proposes a hierarchical query expansion (HQE) approach that reduces the number of passages to be scored by the DPR model, resulting in significant speedups. HQE leverages a two-stage framework, where the first stage uses a lightweight neural retriever to select a subset of relevant passages, and the second stage applies DPR to the selected passages. Experimental results on the NQ and Trivia datasets demonstrate that HQE achieves comparable retrieval performance to state-of-the-art DPR models while reducing inference time by up to 75%.