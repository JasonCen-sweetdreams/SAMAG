Dialogue systems have made significant strides with the integration of AI models, but their lack of transparency hinders human trust. This paper proposes 'HARE', a novel hierarchical attention-based framework for explainable reasoning in multi-agent dialogue systems. HARE employs a cognitive graph-based architecture to model agent interactions, incorporating attention mechanisms to identify salient context elements. We demonstrate that HARE outperforms state-of-the-art models in generating coherent and interpretable dialogue responses, while providing explicit explanations for its reasoning process. Our approach has far-reaching implications for deploying trustworthy AI systems in real-world applications.