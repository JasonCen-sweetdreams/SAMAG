Deep reinforcement learning (DRL) agents are vulnerable to adversarial attacks, which can manipulate the state and action spaces to compromise the learning process. We propose a novel approach to detect such attacks using graph convolutional networks (GCNs). Our method, 'AdvGuard', constructs a graph representation of the agent's experience and applies GCNs to identify anomalous patterns indicative of adversarial behavior. We evaluate AdvGuard on a range of DRL environments and demonstrate its effectiveness in detecting both white-box and black-box attacks, outperforming existing detection methods.