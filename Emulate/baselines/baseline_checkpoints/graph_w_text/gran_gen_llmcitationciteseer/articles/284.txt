Multimodal sentiment analysis in social media remains a challenging task due to the complexity of integrating textual, visual, and audio features. We propose a novel hierarchical attention network (HAN) that learns to selectively focus on relevant multimodal inputs and capture nuanced sentiment expressions. Our HAN model consists of a stacked attention mechanism that recursively aggregates features from multiple modalities, enabling the capture of both local and global contextual dependencies. Experimental results on a large-scale social media dataset demonstrate significant improvements in sentiment analysis accuracy compared to state-of-the-art multimodal fusion methods.