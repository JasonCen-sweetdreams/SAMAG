Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) remains a challenging task due to the complexity of human emotional expressions. This paper proposes a novel Hierarchical Attention Network (HAN) that leverages both local and global attention mechanisms to selectively focus on relevant modalities and features. Our approach enables explainable emotion recognition by generating attention-based visualizations of the decision-making process. Experimental results on the CMU-MOSEI dataset demonstrate the effectiveness of HAN in achieving state-of-the-art performance while providing insights into the emotional cues utilized by the model.