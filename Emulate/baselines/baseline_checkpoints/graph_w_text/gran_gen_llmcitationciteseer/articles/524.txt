We present a novel approach to task allocation for heterogeneous agent teams using multi-agent reinforcement learning (MARL). Our framework, 'AgentTA', leverages a decentralized, communication-efficient architecture to enable agents with varying capabilities to learn joint policies for task allocation. We introduce a novel reward function that balances individual agent utility with global team performance, and demonstrate improved allocation efficiency and adaptability compared to traditional, centralized methods. Experimental results on a simulated disaster response domain show that AgentTA outperforms state-of-the-art MARL baselines in dynamic, real-world-inspired scenarios.