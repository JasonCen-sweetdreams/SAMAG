Eye-tracking technology is crucial for immersive virtual reality (VR) experiences, but current systems suffer from accuracy limitations. This paper presents EyeGaze+, a novel deep learning-based framework that enhances eye-tracking accuracy in VR by leveraging multimodal fusion of eye movement, head pose, and scene context. We propose a hierarchical attention network that adaptively weighs the contributions of each modality to improve gaze estimation. Experimental evaluation on a large-scale VR dataset shows that EyeGaze+ outperforms state-of-the-art methods, achieving a 25% reduction in gaze error and enabling more precise foveated rendering.