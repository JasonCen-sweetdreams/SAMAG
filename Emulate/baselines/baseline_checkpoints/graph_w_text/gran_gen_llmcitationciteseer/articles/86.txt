We propose a novel hierarchical attention network (HAN) for multimodal fusion in visual question answering (VQA). Unlike existing approaches that flatten visual and textual features, our HAN hierarchically fuses them using attention mechanisms at multiple scales. This enables our model to capture complex contextual relationships between images and questions. We demonstrate improved performance on the VQA-CP v2 benchmark, achieving state-of-the-art results on the 'number' and 'other' question types. Our ablation study shows that the hierarchical attention mechanism is crucial for capturing nuanced multimodal interactions.