Autonomous vehicles rely on accurate perception models to navigate complex scenarios. However, collecting and annotating data for every possible situation is impractical. This paper presents a novel zero-shot learning framework, 'TaskGen', which leverages task-conditioned generative models to synthesize novel scenarios and adapt perception models without additional labeled data. Our approach combines a conditional GAN with a task encoder, enabling the generation of realistic, task-specific scenes that improve the robustness of perception models. Experimental results on the KITTI and Waymo datasets demonstrate that TaskGen outperforms state-of-the-art zero-shot learning methods in object detection and semantic segmentation tasks.