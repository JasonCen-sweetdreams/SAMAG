Few-shot relation extraction (FSRE) is a challenging task in natural language processing, where a model must learn to extract relevant relations from a few examples. This paper proposes a meta-learning approach that leverages graph neural networks (GNNs) to learn a relational representation space. Our model, MetaRel, uses episodic training to simulate FSRE tasks and adapt to new relations. We demonstrate that MetaRel outperforms state-of-the-art FSRE methods on several benchmark datasets, achieving an average F1-score improvement of 12.5%. Our analysis shows that MetaRel's performance gain is attributed to its ability to capture complex relational patterns and generalize to unseen relations.