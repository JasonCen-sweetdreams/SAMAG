Multi-modal sensor data fusion is crucial in various applications, including surveillance, healthcare, and robotics. This paper proposes a novel hierarchical attention-based graph neural network (HAGNN) to fuse and process multi-modal data from heterogeneous sensors. Our approach leverages graph attention mechanisms to model complex relationships between sensor modalities and captures hierarchical patterns in the data. Experiments on a large-scale dataset demonstrate that HAGNN outperforms state-of-the-art methods in terms of accuracy, robustness, and computational efficiency.