Deep neural networks are vulnerable to adversarial attacks that can mislead the model's predictions. Existing detection methods often rely on hand-crafted features or statistical analysis. This paper proposes a novel approach, 'GraphAlert', which leverages explainable graph attention mechanisms to identify potential attacks. By analyzing the attention weights and graph structure, our method can detect adversarial samples with high accuracy. We evaluate GraphAlert on several image classification benchmarks and demonstrate its effectiveness in detecting various types of attacks, including pixel-level and semantic-level perturbations.