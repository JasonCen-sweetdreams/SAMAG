Question answering (QA) over knowledge graphs (KGs) relies on effective KG embeddings that capture complex relationships between entities. This paper proposes a novel hierarchical attention mechanism, 'HAT', which selectively focuses on relevant subgraphs and entity properties when learning KG embeddings. Our approach leverages a combination of graph attention and hierarchical clustering to reduce dimensionality and improve embedding quality. Experimental results on benchmark QA datasets demonstrate that HAT outperforms state-of-the-art KG embedding methods, achieving higher accuracy and better generalizability to unseen entities and relations.