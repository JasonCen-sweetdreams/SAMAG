Multi-agent reinforcement learning has been increasingly applied to autonomous systems, such as autonomous vehicles and drones. However, the robustness of these systems to adversarial attacks remains largely unexplored. This paper investigates the vulnerability of multi-agent reinforcement learning algorithms to targeted attacks, which can manipulate the behavior of individual agents to compromise the overall system. We propose a novel attack framework, 'MAARA', which leverages the interaction between agents to amplify the adversarial effect. Our experiments demonstrate the efficacy of MAARA in compromising the performance of state-of-the-art multi-agent reinforcement learning algorithms, highlighting the need for robustness testing in these systems.