Explainability is a crucial aspect of multi-agent decision-making, particularly in real-world applications like autonomous vehicles and smart grids. This paper presents a novel hierarchical attention network (HAN) architecture that learns to generate interpretable explanations for joint agent decisions. Our approach leverages agent-specific attention mechanisms to selectively focus on relevant features and interactions, enabling the identification of key contributing factors to the collective decision. Experimental results on a suite of multi-agent benchmarks demonstrate the effectiveness of HAN in improving both decision quality and explainability, outperforming existing baselines.