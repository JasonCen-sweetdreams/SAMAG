Conversational interfaces have become ubiquitous, but understanding user preferences remains a significant challenge. This paper proposes a novel approach using reinforcement learning to elicit user preferences in conversational systems. We introduce a hierarchical reinforcement learning framework that learns to ask context-dependent questions to users, iteratively refining the understanding of their preferences. Our evaluation on a large-scale conversational dataset demonstrates that our approach significantly outperforms existing methods in terms of preference accuracy and user satisfaction.