Sentiment analysis in online reviews has become crucial for businesses to understand customer opinions. However, existing approaches often struggle with handling multimodal data (e.g., text, images, and videos) and capturing complex sentiment relationships. We propose a novel Hierarchical Attention Network (HAN) that integrates multimodal features and learns to focus on relevant segments of the input data. Our HAN model consists of three attention layers: modality-level, segment-level, and token-level attention, enabling it to capture sentiment patterns at different granularities. Experimental results on a large-scale multimodal review dataset demonstrate improved sentiment analysis performance and robustness compared to state-of-the-art multimodal fusion methods.