Gesture recognition has numerous applications in human-computer interaction, but existing systems often struggle with variability in user gestures and sensor noise. This paper presents a novel approach that combines computer vision, electromyography, and inertial measurement unit data to recognize gestures. We propose a Bayesian neural network that models uncertainty in the fusion process, enabling the system to adapt to individual users and contexts. Our experiments on a large-scale gesture dataset demonstrate improved recognition accuracy and robustness to sensor noise, paving the way for more reliable and personalized HCI systems.