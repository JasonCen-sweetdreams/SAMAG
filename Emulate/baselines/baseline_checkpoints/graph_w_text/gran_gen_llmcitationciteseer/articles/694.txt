Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a Hierarchical Attention Network (HAN) to recognize emotions from multi-modal data. Our HAN model consists of modality-specific attention modules that learn to focus on relevant features and a hierarchical fusion module that integrates the outputs from each modality. We also introduce a novel explainability technique, called Emotion Attribution, to provide insights into the decision-making process of our model. Experimental results on a benchmark dataset show that our approach outperforms state-of-the-art methods in terms of emotion recognition accuracy and provides meaningful explanations for the predicted emotions.