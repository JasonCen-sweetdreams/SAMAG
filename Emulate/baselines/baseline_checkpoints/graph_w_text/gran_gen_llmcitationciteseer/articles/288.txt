Knowledge graph embeddings have become a crucial component in various AI applications, but existing methods often struggle to capture complex relational structures. In this paper, we propose a novel hierarchical relation attention mechanism, 'HieraRel', which learns to selectively focus on relevant relations at different abstraction levels. Our approach leverages a graph neural network to encode entity representations and a hierarchical attention module to weigh relation importance. Experimental results on benchmark datasets demonstrate that HieraRel outperforms state-of-the-art methods in link prediction, entity classification, and question answering tasks.