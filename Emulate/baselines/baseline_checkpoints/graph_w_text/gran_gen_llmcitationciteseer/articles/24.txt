Autonomous vehicles require rapid decision-making in complex environments. We propose a novel hierarchical reinforcement learning (HRL) framework, 'HierAct', which leverages a dual-agent architecture to optimize action selection and planning in real-time. Our approach integrates a high-level, model-based agent with a low-level, model-free agent, enabling efficient exploration-exploitation trade-offs and improved adaptability to dynamic scenarios. Experimental results on a high-fidelity driving simulator demonstrate HierAct's superiority over flat RL baselines in terms of task completion rate and safety metrics.