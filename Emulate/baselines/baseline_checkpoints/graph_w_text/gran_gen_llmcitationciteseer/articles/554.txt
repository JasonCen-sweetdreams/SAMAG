Deep learning models have achieved state-of-the-art performance on various graph-structured data tasks, but their robustness against adversarial attacks remains a significant concern. In this paper, we present a comprehensive analysis of the robustness of graph neural networks (GNNs) against different types of adversarial attacks, including node injection, edge perturbation, and feature manipulation. We propose a novel attack-agnostic defense mechanism, 'GraphShield', which leverages graph autoencoders to detect and correct adversarial perturbations. Experimental results on several benchmark datasets demonstrate the effectiveness of GraphShield in improving the robustness of GNNs against various adversarial attacks.