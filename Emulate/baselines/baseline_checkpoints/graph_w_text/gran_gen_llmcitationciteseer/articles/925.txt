Continuous affective state recognition is crucial for human-computer interaction, mental health monitoring, and personalized systems. This paper introduces EmotionAware, a novel framework that leverages multi-modal fusion of physiological (EEG, ECG) and behavioral (facial expressions, speech) cues to recognize emotions in real-time. We propose a deep learning architecture that integrates convolutional and recurrent neural networks to handle the heterogeneous data streams. Experimental results on a dataset of 50 participants demonstrate that EmotionAware achieves an average recognition accuracy of 87.2% for seven emotions, outperforming state-of-the-art systems that rely on single modalities.