Multi-agent systems (MAS) are increasingly used in complex domains, such as smart cities and autonomous vehicles, where dynamic task allocation is crucial. This paper proposes a hierarchical reinforcement learning (HRL) framework for coordinating MAS. Our approach combines a high-level task allocator with low-level agent controllers, enabling efficient adaptation to changing task requirements and environmental conditions. We evaluate our framework on a simulated urban surveillance scenario, demonstrating improved task completion rates and reduced agent collisions compared to traditional decentralized approaches.