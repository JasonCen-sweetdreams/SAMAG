As the global population ages, there is a growing need for virtual assistants that can effectively interact with older adults. This paper presents a study on designing emotion-aware virtual assistants that can recognize and respond to the emotional states of older adults through voice-based interaction. We developed a multimodal framework that integrates speech recognition, natural language processing, and affective computing to detect emotional cues in voice inputs. Our user study with 30 older adults shows that our approach can improve the user experience and reduce frustration with virtual assistants. We discuss the implications of our findings for designing more empathetic and supportive virtual assistants for older adults.