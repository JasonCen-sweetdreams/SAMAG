Multimodal search systems, which simultaneously process text and visual features, have become increasingly popular. However, existing neural ranking models struggle to effectively integrate both modalities. This paper proposes a novel hybrid neural ranking framework, 'MM-Rank', which combines the strengths of transformer-based language models and convolutional neural networks (CNNs) for visual feature extraction. MM-Rank employs a shared semantic space to align text and image representations, enabling more accurate ranking and retrieval. Experimental results on the Flickr30k dataset demonstrate significant improvements in retrieval effectiveness over state-of-the-art baselines, particularly for multimodal queries.