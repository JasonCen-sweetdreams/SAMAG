Multimodal sentiment analysis has gained significant attention in recent years, as it enables the analysis of opinions expressed through different modalities, such as text, images, and audio. However, existing approaches often treat each modality independently, overlooking the complex interactions between them. This paper proposes a Hierarchical Attention Network (HAN) that jointly learns to extract and fuse features from multiple modalities. Our HAN model consists of modality-specific attention layers that learn to weigh the importance of each modality, followed by a hierarchical fusion layer that captures the interactions between modalities. Experimental results on several benchmark datasets demonstrate that our approach outperforms state-of-the-art multimodal sentiment analysis models, particularly in scenarios with noisy or incomplete data.