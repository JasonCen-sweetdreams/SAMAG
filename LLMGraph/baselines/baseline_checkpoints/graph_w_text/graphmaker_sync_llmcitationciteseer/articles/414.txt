Multimodal emotion recognition is a challenging task that requires integrating and fusing information from diverse modalities such as speech, text, and vision. This paper proposes a novel hierarchical attention-based transformer architecture, 'MERTA', that leverages the strengths of each modality to improve emotion recognition accuracy. MERTA employs a modal-agnostic attention mechanism that adaptively weights the importance of each modality based on the input data. Our experiments on three benchmark datasets demonstrate that MERTA outperforms state-of-the-art methods by 4.2% in terms of weighted F1-score, while reducing computational complexity by 30%.