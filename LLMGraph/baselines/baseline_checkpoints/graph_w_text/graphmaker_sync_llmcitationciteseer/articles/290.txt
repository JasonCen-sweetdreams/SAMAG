Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task, particularly when explaining the decision-making process. This paper presents a novel hierarchical attention framework, 'HierAttnEmo', which leverages the strengths of both transformer-based architectures and graph neural networks. HierAttnEmo learns to focus on relevant modalities and features, providing interpretable emotion recognition results. Our experiments on the CMU-MOSEI dataset demonstrate the effectiveness of our approach, achieving state-of-the-art performance while offering insights into the emotional cues used by the model.