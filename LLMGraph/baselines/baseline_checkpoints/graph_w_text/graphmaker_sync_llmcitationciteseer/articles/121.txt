Cloud computing providers face the challenge of efficiently allocating resources to meet dynamic workload demands while minimizing costs. This paper proposes a novel hierarchical reinforcement learning framework, 'HRL-Cloud', which learns to optimize resource allocation in a decentralized manner. We introduce a hierarchical policy that balances exploration and exploitation across multiple timescales, facilitating adaptation to changing workload patterns. Experiments on a real-world cloud dataset demonstrate that HRL-Cloud outperforms traditional rule-based allocation methods, achieving an average of 23% cost savings while maintaining 99.99% SLA compliance.