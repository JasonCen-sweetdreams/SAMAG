Individuals with motor impairments often face difficulties interacting with gesture-based systems. This paper presents an adaptive gesture recognition framework, 'EaseRecognize', which leverages machine learning and human-computer interaction principles to accommodate diverse motor abilities. Our approach involves real-time adaptation of gesture models based on user performance, incorporating feedback from wearable sensors and eye-tracking data. We conducted a user study with 20 participants, demonstrating significant improvements in recognition accuracy and user experience for individuals with motor impairments. The results have implications for the design of more inclusive and accessible human-computer interfaces.