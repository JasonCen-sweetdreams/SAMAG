Distributed database systems have become increasingly popular for handling large-scale data storage and processing. However, query optimization remains a significant challenge in such systems. This paper proposes a novel approach to query optimization using machine learning techniques. We develop a predictive model that learns from historical query execution times and optimizes query plans based on the predicted execution costs. Our approach is demonstrated to achieve significant performance improvements over traditional query optimization methods in a real-world distributed database system.