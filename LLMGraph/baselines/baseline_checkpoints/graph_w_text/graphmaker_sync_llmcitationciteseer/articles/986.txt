Multimodal sentiment analysis has drawn increasing attention in recent years, but most existing approaches neglect the uncertainty inherent in model predictions. This paper introduces a novel hierarchical attention network (HAN) that jointly learns sentiment representations from text, image, and acoustic modalities. We propose a Bayesian extension to HAN, which enables uncertainty estimation for sentiment predictions. Experimental results on the CMU-MOSI dataset demonstrate that our approach outperforms state-of-the-art multimodal sentiment analysis models, while providing calibrated uncertainty estimates that can facilitate more informed decision-making in real-world applications.