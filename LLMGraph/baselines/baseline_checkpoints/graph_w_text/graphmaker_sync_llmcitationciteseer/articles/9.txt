Autonomous vehicles (AVs) require sophisticated control systems to navigate complex, dynamic environments. This paper presents a hierarchical reinforcement learning (HRL) framework, 'DynaDrive', which integrates high-level decision-making with low-level control policies. DynaDrive leverages a modular architecture to encode domain knowledge and incorporates a novel, graph-based exploration strategy to adapt to changing scenarios. Experimental results on a high-fidelity simulator demonstrate improved safety, efficiency, and adaptability of DynaDrive compared to state-of-the-art baselines in various urban and highway driving scenarios.