Multi-modal sentiment analysis has gained significant attention in recent years, but existing approaches often suffer from high computational costs and limited scalability. This paper introduces Hierarchical Attention Networks (HANs), a novel neural architecture that leverages the strengths of both visual and textual modalities. HANs employ a hierarchical attention mechanism that adaptively weights feature importance across modalities and spatial regions, enabling efficient sentiment prediction. Experimental results on three benchmark datasets demonstrate the superiority of HANs over state-of-the-art methods in terms of accuracy and computational efficiency.