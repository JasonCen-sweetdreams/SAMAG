The increasing adoption of AI-driven NLP models in high-stakes applications has raised concerns about their interpretability. This paper proposes a novel hierarchical attention network (HAN) architecture that enables explainable AI-driven NLP. Our HAN model integrates a self-attention mechanism with a hierarchical graph neural network to capture complex contextual relationships in input texts. We demonstrate the effectiveness of our approach on several benchmark datasets, achieving state-of-the-art results in sentiment analysis and question answering while providing visualizable attention weights that facilitate model interpretability.