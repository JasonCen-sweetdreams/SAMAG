Explainable AI has become crucial in various applications, including visual question answering (VQA). This paper proposes a novel Hierarchical Attention-based Multimodal Fusion (HAMF) framework that provides interpretable results in VQA tasks. HAMF integrates visual and textual features using a hierarchical attention mechanism, enabling the model to focus on relevant regions and words when answering questions. Our experimental results on the VQA-X dataset demonstrate that HAMF outperforms state-of-the-art VQA models while providing insightful attention visualizations that reveal the decision-making process.