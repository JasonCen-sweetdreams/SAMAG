Cooperative task allocation in dynamic environments is a challenging problem that requires efficient decision-making and adaptability. This paper proposes a novel multi-agent reinforcement learning framework, 'CoopStar', which enables agents to learn cooperative policies for task allocation in uncertain and changing environments. We introduce a decentralized Q-network architecture that incorporates communication and trust mechanisms, allowing agents to share knowledge and adapt to new situations. Experimental results on a simulated search-and-rescue scenario demonstrate that CoopStar outperforms traditional centralized approaches and achieves significant improvements in task completion rates and response times.