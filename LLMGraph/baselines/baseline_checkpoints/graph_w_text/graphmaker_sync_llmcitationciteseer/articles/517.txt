Virtual reality (VR) systems often rely on precise motor control to interpret user gestures, leading to fatigue and decreased immersion. We propose a novel approach using deep reinforcement learning to decouple gestural input from motor control, enabling users to interact with virtual objects using subtle, intuitive movements. Our method, GestureRL, employs a hierarchical policy architecture that learns to recognize gestures from vision-based input and translates them into precise motor commands. Experimental results show that GestureRL achieves a 35% reduction in user fatigue while maintaining high accuracy in gesture recognition, paving the way for more accessible and engaging VR experiences.