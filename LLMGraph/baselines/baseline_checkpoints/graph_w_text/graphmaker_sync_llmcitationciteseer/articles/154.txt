Multi-agent systems (MAS) have become increasingly prevalent in real-world applications, but the lack of explainability in their decision-making processes hinders trust and accountability. This paper proposes a novel hierarchical attention network (HAN) architecture, 'ExplainMAS', which learns to selectively focus on relevant agents and their interactions to generate interpretable explanations for joint decision-making. We evaluate ExplainMAS on a variety of cooperative and competitive MAS benchmarks, demonstrating significant improvements in explanation quality and decision-making performance compared to state-of-the-art approaches.