Emotion recognition is a crucial aspect of human-robot interaction, enabling robots to respond empathetically to users. However, existing approaches struggle to effectively fuse and interpret multi-modal cues from speech, text, and vision. This paper presents a novel Hierarchical Attention Network (HAN) that learns to selectively focus on salient modalities and contextual features. Our HAN architecture comprises a feature extraction module, modality-aware attention, and a hierarchical fusion mechanism. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, with improved robustness to noisy or missing data.