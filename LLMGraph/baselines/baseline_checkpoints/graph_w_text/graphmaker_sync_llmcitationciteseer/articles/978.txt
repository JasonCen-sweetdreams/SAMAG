Conversational agents require accurate emotion recognition to respond empathetically. We propose a novel Hierarchical Attention Network (HAN) architecture that integrates multimodal features from speech, text, and facial expressions to recognize emotions in human-agent conversations. Our HAN model captures both local and global dependencies across modalities, achieving state-of-the-art performance on the IEMOCAP dataset. We also introduce a new metric, Emotion Recognition F1-score (ERF1), to evaluate the model's ability to recognize subtle emotions. Experimental results demonstrate the effectiveness of our approach in improving conversational agents' emotional intelligence.