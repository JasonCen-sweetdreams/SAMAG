Sentiment analysis on social media platforms is challenging due to the multimodal nature of user-generated content. This paper proposes a novel hierarchical attention network (HAN) architecture that jointly learns from visual, textual, and acoustic features. Our HAN model employs a nested attention mechanism to capture local and global contextual relationships between modalities, enabling more accurate sentiment predictions. Experimental results on a large-scale, multimodal dataset demonstrate the effectiveness of our approach, outperforming state-of-the-art multimodal fusion techniques by 3.5% in F1-score.