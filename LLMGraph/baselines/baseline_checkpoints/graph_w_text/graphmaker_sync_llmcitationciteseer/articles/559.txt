Robot task planning in uncertain environments is a challenging problem due to the complexity of handling multiple tasks and uncertain outcomes. This paper presents a hierarchical reinforcement learning framework, 'HRL-Uncertainty', which integrates task planning and motion planning to achieve efficient and adaptive robot behavior. Our approach utilizes a high-level task planner to generate task sequences and a low-level motion planner to execute the tasks, while incorporating uncertainty estimation and risk-aware decision-making. Experimental results on a robotic arm platform demonstrate that HRL-Uncertainty outperforms existing methods in terms of task completion time and success rate in uncertain environments.