Deep neural networks have revolutionized numerous applications, but their performance heavily relies on careful hyperparameter tuning. Bayesian optimization has emerged as a popular approach for hyperparameter tuning, but its high computational cost limits its applicability. This paper proposes a novel, efficient Bayesian optimization framework, 'HyperBO', which leverages a hierarchical surrogate model to reduce the number of expensive neural network evaluations. We demonstrate HyperBO's effectiveness on various benchmark datasets, achieving state-of-the-art results while reducing the tuning time by up to 75% compared to existing methods.