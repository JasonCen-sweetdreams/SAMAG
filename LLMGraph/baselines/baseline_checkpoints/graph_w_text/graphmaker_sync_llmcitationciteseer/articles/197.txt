Graph neural networks (GNNs) have achieved state-of-the-art results in node classification tasks, but their computational complexity increases rapidly with graph size. This paper proposes a novel hierarchical graph attention network (HGAN) that leverages attention mechanisms to selectively focus on relevant nodes and subgraphs. By recursively applying attention at multiple scales, HGAN reduces the number of node embeddings to be computed, resulting in significant speedups. Experiments on several benchmark datasets demonstrate that HGAN achieves competitive accuracy to state-of-the-art GNNs while being up to 5x faster.