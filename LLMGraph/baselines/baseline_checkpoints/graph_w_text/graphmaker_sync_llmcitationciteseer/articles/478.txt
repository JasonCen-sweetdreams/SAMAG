Neural retrieval models have revolutionized open-domain question answering by leveraging large-scale datasets and deep learning architectures. However, their performance is often hindered by the lack of explicit relevance feedback from users. This paper proposes a novel approach to incorporating relevance feedback into neural retrieval models, using a multi-task learning framework that jointly optimizes retrieval and relevance prediction. We develop a large-scale dataset of annotated relevance labels and demonstrate significant improvements in retrieval accuracy and efficiency on several benchmark datasets. Our approach enables more effective and efficient question answering systems that can adapt to user preferences and feedback.