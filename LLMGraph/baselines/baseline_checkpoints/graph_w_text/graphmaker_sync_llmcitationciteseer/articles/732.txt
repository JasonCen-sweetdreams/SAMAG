Dialogue systems have made significant progress in generating coherent and informative responses. However, the lack of transparency and interpretability in these systems hinders their widespread adoption. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that incorporates explicit attention mechanisms to facilitate explainable dialogue generation. Our approach integrates a utterance-level attention module to model the conversation context and a token-level attention module to focus on salient input tokens. Experimental results on the DSTC7 benchmark dataset demonstrate that HAN outperforms state-of-the-art models in both response generation and explainability metrics.