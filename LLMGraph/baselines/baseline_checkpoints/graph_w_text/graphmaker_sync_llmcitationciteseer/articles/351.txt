Knowledge graph embedding (KGE) has emerged as a crucial technique for representing complex relationships in large-scale graphs. However, existing methods struggle to capture nuanced dependencies between entities and relationships. We propose 'AttentionKG', a novel KGE framework that integrates attention-based graph neural networks to selectively focus on relevant subgraphs during training. Our approach achieves state-of-the-art performance on multiple benchmark datasets, including WN18RR and FB15K-237, and demonstrates improved interpretability through attention visualizations.