Emotion recognition from multi-modal data (e.g., speech, text, vision) is a challenging task due to the complexity of emotional expressions. Existing approaches often rely on deep neural networks, which lack interpretability. We propose a hybrid attention network (HAN) that integrates neural and symbolic representations to recognize emotions from multi-modal input. HAN leverages attention mechanisms to selectively focus on relevant modalities and features, enabling explainable emotion recognition. Our experiments on the CMU-MOSEI dataset demonstrate that HAN outperforms state-of-the-art methods in terms of accuracy and provides transparent insights into the emotional cues used for recognition.