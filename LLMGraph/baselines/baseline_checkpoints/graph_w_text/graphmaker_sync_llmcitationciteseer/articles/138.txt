Neural Architecture Search (NAS) has emerged as a promising paradigm for automating the design of deep neural networks. However, existing NAS methods often rely on computationally expensive reinforcement learning or evolutionary algorithms. This paper proposes a novel Bayesian HyperNetwork (BHN) approach that efficiently searches for high-performing architectures in a probabilistic manner. Our BHN framework learns to predict the performance of candidate architectures using a surrogate model, which is trained on a small set of evaluated architectures. Experimental results on several benchmark datasets demonstrate that our approach achieves state-of-the-art NAS performance while reducing the computational cost by an order of magnitude.