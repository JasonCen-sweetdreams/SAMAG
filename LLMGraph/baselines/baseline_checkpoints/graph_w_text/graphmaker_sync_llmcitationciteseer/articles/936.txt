Neural architecture search (NAS) has revolutionized deep learning model design. However, existing methods suffer from high computational costs and limited exploration of the vast search space. This paper introduces GraphNAS, a novel NAS framework that leverages graph-based reinforcement learning to efficiently explore the architecture space. By representing architectures as graphs, we enable effective exploration-exploitation trade-offs and reduce the number of required evaluations. Experimental results on popular benchmarks demonstrate that GraphNAS achieves state-of-the-art performance while reducing search time by an order of magnitude.