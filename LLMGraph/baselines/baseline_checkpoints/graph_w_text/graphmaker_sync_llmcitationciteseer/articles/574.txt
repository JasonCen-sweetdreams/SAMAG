Traditional web search engines rely on text-based queries, neglecting the richness of multimodal content on the web. This paper proposes a novel hybrid retrieval model, 'Multimodal Fusion Network' (MFN), which integrates visual and textual features to retrieve relevant web pages. MFN employs a deep neural network to learn a shared representation space for both modalities, enabling effective fusion of query and document features. Experimental results on a large-scale web dataset demonstrate that MFN outperforms state-of-the-art text-based retrieval models, achieving a 15% increase in mean average precision.