Multimodal sentiment analysis (MSA) involves predicting sentiment from diverse input modalities such as text, images, and audio. Existing deep learning methods often rely on complex black-box models, making it challenging to understand their decision-making processes. This paper proposes a novel explainable hierarchical attention network (EHAN) for MSA, which integrates both local and global attention mechanisms to capture multimodal interactions. Our approach enables the identification of salient input features and modalities contributing to the predicted sentiment, thereby providing transparency and interpretability. Experimental results on benchmark datasets demonstrate EHAN's superior performance and explainability compared to state-of-the-art MSA models.