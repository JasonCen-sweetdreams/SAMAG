Deep neural networks have demonstrated remarkable performance in various machine learning tasks, but their vulnerability to adversarial attacks remains a significant concern. This paper proposes a novel approach to evaluate the robustness of deep learning models against adversarial attacks using Bayesian neural networks (BNNs). We leverage the probabilistic nature of BNNs to quantify the uncertainty in the model's predictions, enabling the detection of adversarial examples with high accuracy. Our experiments on multiple benchmark datasets demonstrate the effectiveness of our approach in identifying and mitigating the impact of adversarial attacks on model performance.