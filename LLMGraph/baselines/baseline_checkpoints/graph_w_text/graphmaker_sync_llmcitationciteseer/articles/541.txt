Traditional latent semantic analysis (LSA) techniques rely on bag-of-words representations, overlooking the semantic relationships between words. This paper presents a novel neural LSA approach, ATTN-LSA, which incorporates attention mechanisms to model word contexts. Our method learns to weight words based on their importance in the document, allowing for more accurate topic modeling. Experimental results on the TREC-8 dataset demonstrate significant improvements in retrieval precision and recall compared to traditional LSA and deep learning-based methods.