Multi-agent reinforcement learning (MARL) in partially observable environments is a challenging problem, as agents must learn to cooperate and communicate effectively despite limited information. This paper proposes a novel hierarchical attention network (HAN) architecture, which integrates attention mechanisms at both the intra-agent and inter-agent levels. Our approach enables agents to selectively focus on relevant information from their observations and coordinate with others to achieve common goals. Experimental results in a series of cooperative navigation tasks demonstrate that HAN outperforms existing MARL methods, improving both individual and team rewards in complex, dynamic environments.