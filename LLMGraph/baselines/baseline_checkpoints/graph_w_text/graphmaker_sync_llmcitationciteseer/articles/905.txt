Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task due to the inherent heterogeneity and complexity of human emotions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features, enabling efficient and accurate emotion recognition. Our approach outperforms state-of-the-art methods on two benchmark datasets, achieving a 12.5% improvement in weighted F1-score. We also demonstrate the robustness of HAN to varying levels of noise and missing modalities.