Effective human-robot interaction (HRI) requires robots to understand and respond to human emotions. We propose a novel multi-modal emotional intelligence framework, EmoGraph, which leverages graph-based attention networks to fuse facial expression, speech, and physiological signals. EmoGraph outperforms state-of-the-art methods on the RECOLA dataset, achieving a 15% improvement in emotion recognition accuracy. We demonstrate the framework's efficacy in a real-world HRI scenario, where robots adapt their behavior to comfort users experiencing emotional distress.