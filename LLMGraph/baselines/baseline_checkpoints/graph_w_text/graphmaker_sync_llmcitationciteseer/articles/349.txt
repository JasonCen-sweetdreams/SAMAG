Knowledge graph embedding (KGE) has become a crucial technique for various AI applications. However, existing KGE methods often suffer from inefficient modeling of complex relations. This paper proposes a novel adaptive relation attention mechanism, 'ARA-KGE', which dynamically adjusts attention weights based on the relational context. We demonstrate that ARA-KGE outperforms state-of-the-art KGE methods on benchmark datasets, including WN18RR and FB15k-237, with significant improvements in both link prediction and entity disambiguation tasks.