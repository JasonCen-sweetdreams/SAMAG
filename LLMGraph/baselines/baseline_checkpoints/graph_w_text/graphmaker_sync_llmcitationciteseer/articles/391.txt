Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the complexity of human emotions and the heterogeneity of modalities. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) framework that effectively integrates and represents multi-modal data. Our approach leverages graph attention mechanisms to capture intra- and inter-modal relationships, and a hierarchical architecture to model emotion hierarchies. Experimental results on the IEMOCAP and CMU-MOSEI datasets demonstrate that HGAT outperforms state-of-the-art methods in recognizing emotions from multi-modal data, with improved robustness to noisy or missing modalities.