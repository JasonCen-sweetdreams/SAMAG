Explainability in multi-agent reinforcement learning (MARL) is crucial for real-world applications. We propose a novel hierarchical attention network (HAN) architecture that learns to selectively focus on relevant agents and their interactions to improve policy explanations. Our approach extends traditional attention mechanisms to handle complex, multi-agent environments. Experimental results on a range of MARL benchmarks demonstrate that HAN-based agents achieve comparable performance to state-of-the-art methods while providing intuitive, interpretable explanations of their decision-making processes.