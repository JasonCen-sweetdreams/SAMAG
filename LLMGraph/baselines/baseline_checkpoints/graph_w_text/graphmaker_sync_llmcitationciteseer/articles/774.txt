Despite their impressive performance, deep neural networks remain vulnerable to adversarial attacks. In this paper, we propose an attention-based attribution method to generate explainable adversarial attacks. Our approach leverages attention mechanisms to identify the most critical input features contributing to the model's predictions. By perturbing these features, we craft targeted attacks that are more interpretable and effective. Experimental results on ImageNet and CIFAR-10 datasets demonstrate the efficacy of our method in generating visually meaningful attacks while maintaining a high attack success rate. Our work provides new insights into the adversarial vulnerability of deep models and paves the way for developing more robust and transparent AI systems.