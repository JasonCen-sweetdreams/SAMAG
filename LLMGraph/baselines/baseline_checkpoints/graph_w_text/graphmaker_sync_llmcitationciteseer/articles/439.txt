In multi-agent systems, learning effective communication and cooperation strategies is crucial for achieving common goals. This paper presents a novel hierarchical graph attention network (HGAT) framework for multi-agent reinforcement learning. HGAT leverages graph attention mechanisms to model complex agent interactions and hierarchically aggregates agent representations to capture both local and global dependencies. We evaluate HGAT on several benchmark environments and demonstrate significant improvements in task performance and convergence speed compared to state-of-the-art methods.