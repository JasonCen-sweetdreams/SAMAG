Recent advancements in neural ranking models have significantly improved the performance of information retrieval systems. However, the computational cost of these models can be prohibitively expensive, hindering their adoption in real-world applications. This paper proposes a novel query optimization framework that leverages the inherent structural properties of deep neural networks to reduce the computational overhead of inference. We introduce a hierarchical pruning strategy that selectively eliminates redundant computations, resulting in a 3x speedup in query processing time while maintaining retrieval accuracy. Experiments on a large-scale dataset demonstrate the efficacy of our approach in facilitating efficient query optimization for neural ranking models.