Emotion recognition has become a crucial aspect of human-computer interaction (HCI), enabling more empathetic and personalized systems. However, existing approaches often rely on complex, black-box models that struggle to provide transparent and interpretable results. This paper proposes a novel Hierarchical Attention Network (HAN) for multi-modal emotion recognition, integrating facial expressions, speech, and physiological signals. Our HAN architecture leverages attention mechanisms to selectively focus on relevant features and modalities, providing real-time explanations for emotion predictions. Experimental results on the SEMAINE database demonstrate improved recognition accuracy and robustness compared to state-of-the-art methods, while our visual attention heatmaps offer valuable insights into the emotional cues driving our model's decisions.