Deep learning models have achieved state-of-the-art performance in various applications, but their hyperparameter tuning remains a time-consuming and computationally expensive task. This paper proposes a novel Bayesian optimization approach that leverages transfer learning to adapt the search space of hyperparameters to the target task. Our method, called 'TransBO', uses a knowledge graph to encode the relationships between hyperparameters and model performance across different tasks. We demonstrate the effectiveness of TransBO on several benchmark datasets, achieving faster convergence and better model performance compared to existing Bayesian optimization methods.