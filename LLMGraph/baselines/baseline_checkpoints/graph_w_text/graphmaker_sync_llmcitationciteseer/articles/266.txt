Emotion recognition is a crucial aspect of human-computer interaction, but current approaches rely heavily on manual feature engineering and fail to capture the complexities of human emotions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates multimodal data from speech, text, and facial expressions. Our approach leverages attention mechanisms to selectively focus on salient features and hierarchically fuse information from different modalities. Experimental results on a large-scale multimodal dataset demonstrate that HAN outperforms state-of-the-art methods in recognizing emotions and achieves improved robustness to noisy or missing data.