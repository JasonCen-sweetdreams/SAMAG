Real-time analytics has become a crucial aspect of modern data-driven applications, requiring efficient processing of complex queries over distributed databases. This paper presents a novel query optimization framework, 'Raptor', that leverages machine learning and statistical modeling to predict query execution costs and optimize join orders. We introduce a novel cost model that incorporates data skew, correlation, and query concurrency, and demonstrate significant performance improvements over state-of-the-art optimizers on a range of real-world benchmarks.