This paper presents an innovative approach to gesture recognition that leverages eye-gaze information to improve the accuracy and accessibility of inclusive interfaces. Our system, called 'GazeGuide', uses a multimodal fusion of eye-tracking data and computer vision-based hand tracking to recognize gestures in real-time. We propose a novel adaptive framework that adjusts the recognition model based on the user's gaze patterns, enhancing the system's robustness to individual differences and variability in gesture execution. Experimental results with participants with and without motor impairments demonstrate the effectiveness of GazeGuide in facilitating more accurate and efficient gesture recognition.