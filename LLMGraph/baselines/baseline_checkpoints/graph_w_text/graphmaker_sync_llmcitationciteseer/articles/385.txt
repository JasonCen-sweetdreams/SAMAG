Deep neural networks have become increasingly complex, making optimization a crucial aspect of their training. This paper proposes an adaptive momentum-based optimization algorithm, 'AdaMom', which dynamically adjusts the momentum factor based on the gradient norm. AdaMom converges faster and achieves better generalization performance compared to existing gradient-based optimizers, including SGD with momentum and Adam. We provide theoretical convergence guarantees and validate our approach on various benchmark datasets, including CIFAR-10 and ImageNet.