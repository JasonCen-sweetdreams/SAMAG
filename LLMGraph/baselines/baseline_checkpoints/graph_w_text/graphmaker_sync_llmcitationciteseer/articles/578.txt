Distributed database systems have become increasingly popular for handling large-scale data storage and querying. However, optimizing query performance in these systems remains a challenging task. This paper proposes a novel approach that leverages machine learning techniques to improve query optimization. We develop a neural network-based model that predicts optimal query plans based on historical query patterns and system workload. Our experimental results show that the proposed approach outperforms traditional query optimization techniques by up to 30% in terms of query execution time and resource utilization.