Multimodal sentiment analysis on social media requires handling diverse inputs such as images, text, and audio. This paper proposes a novel hierarchical attention network (HAN) that leverages the strengths of both visual and textual features to improve sentiment prediction. Our HAN model consists of modality-specific attention modules that adaptively weight the importance of each input modality, followed by a multimodal fusion layer that integrates the outputs. Experiments on a large-scale social media dataset demonstrate that our approach outperforms state-of-the-art methods in terms of accuracy and robustness, especially in scenarios with noisy or incomplete data.