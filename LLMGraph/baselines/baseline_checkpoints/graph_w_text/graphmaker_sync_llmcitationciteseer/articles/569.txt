In multi-agent systems, efficient task allocation is crucial for achieving complex goals. This paper proposes a hierarchical reinforcement learning (HRL) framework for task allocation, which integrates a high-level, graph-based task allocator with low-level, agent-centric controllers. Our approach leverages the strengths of both planning and learning methods, enabling effective task assignment and adaptive execution. We evaluate our framework in a simulated disaster response scenario, demonstrating significant improvements in task completion rates and agent utilization compared to traditional, flat RL approaches.