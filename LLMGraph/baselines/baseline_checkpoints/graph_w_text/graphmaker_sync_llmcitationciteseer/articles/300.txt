Explainable AI (XAI) is crucial for trust-worthy decision-making in multi-agent systems. This paper proposes a novel hierarchical attention mechanism, 'HierAttn', that enables explainable reasoning in complex, dynamic environments. HierAttn integrates graph attention networks with hierarchical reinforcement learning to reason about agent interactions, goals, and intentions. We evaluate HierAttn on a real-world autonomous vehicle dataset, demonstrating improved explainability and decision-making performance compared to state-of-the-art XAI methods. Our approach has significant implications for human-AI collaboration and trust in safety-critical applications.