Class imbalance in long-tail datasets hinders the performance of deep learning models. We propose a novel hierarchical attention network (HAN) architecture that selectively focuses on tail classes by leveraging a hierarchical abstraction of the class distribution. Our approach adaptively adjusts the attention weights based on the class frequency, allowing the model to concentrate on the most informative samples. Experimental results on several long-tail benchmarks demonstrate that HAN consistently outperforms state-of-the-art methods, achieving improved accuracy and F1-score on tail classes while maintaining competitive performance on head classes.