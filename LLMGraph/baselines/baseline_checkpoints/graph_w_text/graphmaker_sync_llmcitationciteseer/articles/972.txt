The proliferation of edge devices has created a pressing need for efficient neural network models that can operate under severe resource constraints. This paper presents a novel pruning method, 'EdgePrune', which leverages a combination of magnitude-based pruning and knowledge distillation to reduce model size and computational complexity. Our approach exploits the inherent redundancy in deep neural networks to achieve significant reductions in memory footprint and energy consumption, while maintaining accuracy. Experimental results on various edge devices demonstrate that EdgePrune outperforms existing pruning methods, enabling real-time inference on resource-constrained devices.