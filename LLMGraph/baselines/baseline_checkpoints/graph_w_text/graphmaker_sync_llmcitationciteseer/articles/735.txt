Individuals with visual impairments face significant challenges when interacting with graphical user interfaces. This paper presents 'GAZE', a novel adaptive UI framework that leverages gaze tracking and machine learning to dynamically adjust UI elements for improved accessibility. We integrate a deep learning-based gaze estimation model with a user behavior analysis module to infer users' intentions and adapt the UI in real-time. Our user study with 20 visually impaired participants demonstrates that GAZE significantly reduces interaction time and improves overall user experience compared to traditional assistive technologies.