Deep reinforcement learning (DRL) has achieved remarkable success in complex control tasks, but its robustness to adversarial attacks remains a significant concern. This paper investigates the vulnerability of DRL policies to perturbations in high-dimensional state spaces, where the curse of dimensionality exacerbates the attack surface. We propose a novel methodology to generate adversarial attacks using a combination of transfer learning and gradient-based methods. Our experiments on several Atari games and robotic control tasks demonstrate that even state-of-the-art DRL algorithms can be significantly compromised by our attacks, highlighting the need for more robust policy architectures and training protocols.