Multi-task learning has emerged as a powerful paradigm for simultaneously learning multiple related tasks. However, existing approaches often suffer from scalability issues and require careful task selection. This paper presents a novel hierarchical task embedding framework, dubbed 'HTE', which enables efficient and effective multi-task learning at scale. By representing tasks as hierarchical clusters of embedded vectors, HTE can capture complex task relationships and adapt to new tasks with minimal additional computation. Our experiments on a suite of benchmark datasets demonstrate that HTE achieves state-of-the-art performance on multiple tasks while requiring significantly fewer parameters and training iterations compared to existing methods.