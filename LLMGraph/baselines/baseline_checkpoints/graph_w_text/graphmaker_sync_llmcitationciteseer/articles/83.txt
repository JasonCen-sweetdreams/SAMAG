Emotion recognition is a crucial aspect of human-computer interaction, enabling systems to respond empathetically and enhance user experience. This paper proposes a novel Hierarchical Attention Network (HAN) architecture for multi-modal emotion recognition, incorporating facial expressions, speech, and physiological signals. Our approach leverages attention mechanisms to selectively focus on relevant modalities and regions, improving recognition accuracy and robustness. Experimental results on a large, diverse dataset demonstrate the effectiveness of HAN in recognizing emotions across various contexts and user populations.