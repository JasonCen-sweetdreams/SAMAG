Deep reinforcement learning (DRL) has achieved remarkable success in robotics, but the lack of transparency in decision-making processes hinders its adoption in safety-critical applications. This paper proposes a novel model-agnostic explanation framework, 'RL-Explain', which generates interpretable explanations for DRL policies in robotics. We introduce a gradient-based attribution method that identifies the most relevant state features and action components contributing to the agent's decisions. Our experiments on robotic grasping and manipulation tasks demonstrate that RL-Explain provides accurate and informative explanations, improving the trustworthiness and accountability of DRL systems.