Dialogue systems often struggle to effectively integrate and process multi-modal inputs, such as speech, text, and visual cues. This paper presents a novel attention mechanism, 'HybridFocus', which adaptively weights and combines attention weights from different modalities. We demonstrate that HybridFocus significantly outperforms existing approaches in both utterance-level and dialogue-level evaluation metrics. Furthermore, we show that our proposed mechanism can be efficiently implemented using a combination of parallel computing and knowledge distillation, making it suitable for real-world deployment.