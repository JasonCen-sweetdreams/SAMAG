Virtual reality (VR) systems often rely on manual controllers or simplistic gaze-based interfaces, limiting the sense of immersion. We propose a novel gaze-based interaction framework that leverages deep reinforcement learning to predict user intentions and generate intuitive control signals. Our approach combines a convolutional neural network (CNN) with a deep Q-network (DQN) to learn a policy that maps gaze patterns to precise actions in the VR environment. Experimental results demonstrate improved interaction accuracy and user satisfaction compared to existing state-of-the-art methods.