Autonomous vehicles rely on reinforcement learning (RL) to make decisions in complex scenarios. However, the lack of explainability in RL models hinders trust and understanding of their decision-making processes. This paper proposes a novel XRL framework, 'CARMA', which integrates saliency mapping and attention mechanisms to provide interpretable representations of the RL agent's decision-making process. Evaluations on a real-world autonomous driving dataset demonstrate that CARMA improves model transparency without compromising decision-making performance. Our approach has significant implications for the development of trustworthy autonomous systems.