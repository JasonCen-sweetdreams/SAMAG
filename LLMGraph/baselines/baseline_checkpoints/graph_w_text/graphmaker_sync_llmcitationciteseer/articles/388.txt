Cooperative multi-agent systems have applications in various domains, including robotics, autonomous vehicles, and smart cities. This paper proposes a decentralized task allocation framework for such systems, using reinforcement learning to optimize the allocation process. Our approach, called 'MACTOR', leverages a distributed, asynchronous Q-learning algorithm to enable agents to learn from their experiences and adapt to changing environments. Experimental results demonstrate that MACTOR outperforms traditional, centralized allocation methods in terms of task completion efficiency and system resilience, especially in scenarios with partial observability and uncertainty.