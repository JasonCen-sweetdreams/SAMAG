Medical image analysis often involves multiple tasks, such as lesion detection, segmentation, and classification. We propose a novel multi-task learning framework, 'HierAtt', which leverages hierarchical attention to share knowledge across tasks and modalities. Our approach learns to selectively focus on relevant regions and features, reducing the need for task-specific annotations and improving overall performance. Experimental results on a large-scale dataset of brain MRIs demonstrate that HierAtt outperforms state-of-the-art methods in various tasks, while reducing computational costs and requiring fewer labeled samples.