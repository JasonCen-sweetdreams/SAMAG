Neural architecture search (NAS) has emerged as a promising technique for automating the design of deep neural networks. However, existing NAS methods often neglect the crucial role of hyperparameter optimization (HPO), leading to suboptimal performance and computational inefficiencies. This paper introduces CoTune, a novel framework that jointly optimizes neural architectures and hyperparameters using a differentiable search space. We demonstrate that CoTune achieves state-of-the-art performance on various image classification benchmarks while reducing the computational overhead of NAS by up to 75%. Furthermore, we provide insights into the interplay between architecture and hyperparameter search, highlighting the importance of co-tuning for efficient deep learning model development.