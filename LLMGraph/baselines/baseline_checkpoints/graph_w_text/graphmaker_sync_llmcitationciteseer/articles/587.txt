Multi-task learning has been widely adopted in various applications, but the lack of interpretability limits its usage in high-stakes domains. This paper proposes a hierarchical attention network (HAN) framework that learns task-specific and shared representations simultaneously, enabling explainable multi-task learning. Our approach incorporates a novel attention mechanism that dynamically weights task relationships, allowing the model to focus on relevant tasks and features. Experimental results on benchmark datasets demonstrate that HAN outperforms state-of-the-art models while providing insightful feature importance scores and task correlation analysis.