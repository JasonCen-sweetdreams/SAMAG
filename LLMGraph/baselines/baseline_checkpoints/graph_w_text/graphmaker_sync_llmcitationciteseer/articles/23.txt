Autonomous vehicles (AVs) rely on reinforcement learning (RL) to make decisions in complex environments. However, the lack of interpretability in RL models hinders trust and understanding of AV decision-making. This paper proposes a hierarchical explainable RL framework, 'HERO', which integrates attention-based feature importance with hierarchical policy decomposition. HERO enables fine-grained explanations of AV decisions, from high-level goals to low-level control actions. Experiments on a real-world driving dataset show that HERO improves explainability without sacrificing decision-making performance, paving the way for more transparent and trustworthy AV systems.