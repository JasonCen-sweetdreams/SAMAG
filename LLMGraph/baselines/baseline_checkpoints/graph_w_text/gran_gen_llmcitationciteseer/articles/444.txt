Traditional query expansion techniques rely solely on textual features, neglecting the rich contextual information available in multi-modal search scenarios. This paper presents a novel context-aware query expansion framework, 'CAQE', which incorporates visual and acoustic features to disambiguate user queries. By leveraging a multi-modal graph neural network, CAQE captures intricate relationships between query terms, visual objects, and audio patterns. Experimental results on a large-scale multi-modal dataset demonstrate significant improvements in search accuracy and user satisfaction compared to state-of-the-art query expansion methods.