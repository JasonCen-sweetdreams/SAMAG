In many real-world applications, multiple agents must cooperate to achieve a common goal in uncertain and dynamic environments. This paper proposes a novel framework for cooperative task allocation using multi-agent reinforcement learning. We formulate the problem as a decentralized partially observable Markov decision process and develop a hierarchical policy gradient algorithm that leverages graph neural networks to model agent interactions. Experimental results on a simulated disaster response scenario demonstrate improved task allocation efficiency and adaptability compared to existing methods, showcasing the potential of our approach for real-world applications.