Emotion recognition from multi-modal data, such as speech, text, and vision, remains a challenging task due to the complexity of human emotions and the need to model intricate relationships between modalities. We propose a novel Hierarchical Attention Graph Neural Network (HAGNN) architecture that leverages graph attention mechanisms to capture both intra- and inter-modality relationships. Experiments on the benchmark CMU-MOSEI dataset demonstrate that HAGNN outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving a significant improvement in F1-score and correlation coefficient.