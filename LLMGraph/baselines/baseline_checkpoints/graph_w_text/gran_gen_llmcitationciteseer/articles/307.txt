This paper presents a novel approach to intention recognition in virtual reality (VR) environments, leveraging gaze-tracking data to infer user goals and adapt the interface accordingly. We propose a hierarchical Bayesian model that integrates gaze patterns, head movements, and environmental cues to recognize user intentions in real-time. Our approach is evaluated in a VR-based puzzle game, demonstrating significant improvements in task completion time and user satisfaction compared to traditional, static interfaces. The proposed framework has implications for enhancing user experience and accessibility in VR applications.