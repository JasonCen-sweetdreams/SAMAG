Autonomous vehicles require efficient navigation strategies to handle complex scenarios. This paper proposes a hierarchical reinforcement learning (HRL) framework, 'AV-Nav', that leverages a novel state-abstraction mechanism to reduce the curse of dimensionality. AV-Nav employs a higher-level policy to select sub-goals, while a lower-level policy focuses on executing the selected sub-goals. We demonstrate the effectiveness of AV-Nav in simulations and real-world experiments, showcasing improved navigation efficiency and reduced collision rates compared to flat reinforcement learning baselines.