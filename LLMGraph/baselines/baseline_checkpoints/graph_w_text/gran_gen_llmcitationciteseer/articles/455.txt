Vision Transformers (ViTs) have achieved state-of-the-art performance in various computer vision tasks. However, their robustness against real-world attacks remains under-explored. This paper presents a comprehensive evaluation of ViTs against a range of real-world attacks, including physical-world attacks, such as stickers, graffiti, and print-then-digitize attacks. We demonstrate that ViTs are vulnerable to these attacks, achieving accuracy drops of up to 30%. Furthermore, we propose a novel adversarial training method that incorporates real-world attack simulations, resulting in improved robustness against these attacks without compromising clean accuracy.