Coordinating multi-agent systems in complex, dynamic environments is a challenging problem. This paper proposes a decentralized approach to solving partially observable Markov decision processes (POMDPs) in multi-agent systems. We introduce a novel method for learning decentralized policies using a combination of deep reinforcement learning and communication protocols. Our approach allows agents to share information and coordinate their actions in real-time, even in the presence of partial observability and uncertainty. Experimental results demonstrate the effectiveness of our approach in a variety of multi-agent scenarios, including robotic soccer and disaster response.