Individuals with motor impairments face significant challenges in interacting with digital systems. This paper presents EyeGazeAssist, a novel gaze-based assistive system that enables users to navigate and select on-screen items using only their eye movements. Our approach leverages a deep learning-based gaze estimation model, which is trained on a large dataset of eye tracking recordings. We also introduce a novel 'gaze dwell-time' based selection mechanism that adapts to the user's gaze pattern, reducing the need for explicit dwell-time settings. A user study with 20 participants with motor impairments demonstrates the effectiveness of EyeGazeAssist in improving interaction speed and accuracy.