Deep reinforcement learning (DRL) has achieved remarkable success in various domains, but its lack of transparency hinders trust and understanding. This paper proposes a novel hierarchical explainability framework, 'HERO', tailored to DRL in partially observable environments. HERO decomposes the agent's decision-making process into interpretable components, enabling the identification of influential observations, actions, and latent factors. We demonstrate HERO's effectiveness in Atari games and a real-world robotics task, showcasing improved interpretability without sacrificing performance.