Emotion recognition from multimodal inputs, such as speech, text, and facial expressions, is a crucial task in affective computing. Existing approaches often rely on fusion techniques, which can be computationally expensive and neglect the hierarchical relationships between modalities. We propose a novel Hierarchical Attention Network (HAN) that leverages attention mechanisms to selectively focus on relevant modalities and fuse them in a hierarchical manner. Experimental results on the CMU-MOSI dataset demonstrate that HAN outperforms state-of-the-art approaches in terms of accuracy and computational efficiency, while providing interpretable attention weights for understanding emotion recognition.