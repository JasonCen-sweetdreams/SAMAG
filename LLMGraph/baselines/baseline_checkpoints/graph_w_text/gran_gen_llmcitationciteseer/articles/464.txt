Multimodal sentiment analysis (MSA) combines computer vision and natural language processing to analyze user opinions from images, text, and audio. Existing approaches often rely on separate modality-specific models, leading to increased computational costs and reduced accuracy. We propose a novel Hybrid Attention Network (HAN) that integrates modality-agnostic and modality-specific attention mechanisms to selectively focus on relevant inputs. HAN achieves state-of-the-art performance on three benchmark MSA datasets while reducing inference time by up to 30%. Our approach has significant implications for real-world applications, such as opinion mining and customer feedback analysis.