Deep learning models for sentiment analysis often struggle to provide interpretable results, especially when dealing with multi-modal inputs (e.g., text, images, and audio). This paper proposes a novel hierarchical attention network (HAN) architecture that explicitly models the relationships between different modalities. Our approach learns to attend to relevant regions in each modality and fuses the representations using a hierarchical graph neural network. We demonstrate the effectiveness of our method on a benchmark dataset, achieving state-of-the-art results in terms of both accuracy and explainability. We also provide visualizations of the learned attention patterns, offering insights into the decision-making process of the model.