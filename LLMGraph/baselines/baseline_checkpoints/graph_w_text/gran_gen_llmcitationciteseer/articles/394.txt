Emotion recognition is a crucial aspect of human-computer interaction, enabling more empathetic and personalized systems. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates multi-modal inputs from facial expressions, speech, and physiological signals. Our HAN model learns to focus on relevant input modalities and temporal segments, improving emotion recognition accuracy in the presence of noisy or missing data. Experimental results on a large, publicly available dataset demonstrate the effectiveness of our approach, achieving state-of-the-art performance for multi-modal emotion recognition tasks.