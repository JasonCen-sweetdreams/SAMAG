Neural information retrieval (IR) models have achieved state-of-the-art results, but their computational requirements hinder their adoption in real-world applications. This paper presents a novel hierarchical document embedding approach, 'HiDE', which reduces the dimensionality of document representations while preserving their semantic meaning. Our method leverages a multi-level clustering strategy to group similar documents, and then learns compact embeddings for each cluster. Experimental results on the TREC-CAR dataset demonstrate that HiDE achieves comparable retrieval performance to dense passage retrieval models while requiring significantly fewer computations and memory.