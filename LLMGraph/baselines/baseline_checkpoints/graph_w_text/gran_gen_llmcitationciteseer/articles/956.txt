This paper presents a novel approach to coordinating multi-agent systems by integrating hierarchical planning and reinforcement learning. We propose a two-layer framework, where a high-level planner generates abstract plans that are then refined by a lower-level reinforcement learning agent. The hierarchical structure enables the agents to reason about long-term goals while adapting to dynamic environments. We evaluate our approach in a simulated robotic soccer domain, demonstrating improved coordination and task completion rates compared to existing methods. The proposed framework has applications in various multi-agent domains, such as autonomous vehicles, smart homes, and disaster response.