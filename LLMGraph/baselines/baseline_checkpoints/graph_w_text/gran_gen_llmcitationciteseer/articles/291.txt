Neural architecture search (NAS) has emerged as a promising technique for automating the design of deep neural networks. However, the computational overhead and memory requirements of NAS algorithms can be prohibitively expensive for resource-constrained devices. This paper proposes a novel NAS approach, 'ProgPrune', which leverages progressive pruning to reduce the search space and computational cost. Our method adaptively eliminates less promising architectures and focuses on the most promising candidates, resulting in significant speedups and memory savings. Experimental results on several benchmark datasets demonstrate that ProgPrune discovers competitive architectures while requiring only a fraction of the computational resources compared to state-of-the-art NAS methods.