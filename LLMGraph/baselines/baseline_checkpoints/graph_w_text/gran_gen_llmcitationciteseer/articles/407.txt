Virtual reality (VR) systems often rely on manual input devices, limiting the immersion and accessibility of VR experiences. This paper proposes a novel gaze-based interaction framework, 'GazeVR', which leverages deep learning to infer user intentions from eye movement data. We introduce a multi-modal neural network architecture that combines convolutional and recurrent layers to process gaze patterns, head movements, and contextual information. Our evaluation on a VR gaming dataset shows that GazeVR achieves high accuracy and low latency in recognizing user commands, enabling more natural and efficient interaction in VR environments.