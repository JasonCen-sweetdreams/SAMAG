Deep neural networks have achieved state-of-the-art performance in various machine learning tasks, but their success highly depends on careful hyperparameter tuning. Bayesian optimization has emerged as a popular approach for hyperparameter tuning, but its computational cost can be prohibitive for large models. This paper proposes a novel Bayesian optimization algorithm, 'HyperBO', which leverages a probabilistic kernel-based surrogate model to efficiently explore the hyperparameter space. We demonstrate the effectiveness of HyperBO on several benchmark datasets, showing significant improvements in tuning efficiency and model performance compared to existing methods.