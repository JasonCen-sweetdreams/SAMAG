Multi-agent systems increasingly rely on AI-driven decision-making, but the lack of transparency hinders trust and reliability. This paper presents a novel hierarchical graph attention network (HiGAT) architecture for explainable multi-agent cooperation. HiGAT integrates graph attention mechanisms with hierarchical reinforcement learning to learn interpretable policies for cooperative tasks. We demonstrate the effectiveness of HiGAT in a simulated multi-robot exploration scenario, achieving improved cooperation and interpretability compared to state-of-the-art methods. Our approach provides a promising direction for explainable AI in real-world multi-agent applications.