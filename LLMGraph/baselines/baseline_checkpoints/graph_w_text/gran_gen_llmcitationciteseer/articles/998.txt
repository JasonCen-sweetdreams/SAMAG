Neural ranking models have achieved state-of-the-art performance in ad-hoc retrieval tasks, but their effectiveness can be limited by the quality of the input queries. This paper investigates the impact of query expansion on neural ranking models, exploring the use of different expansion techniques and their effects on retrieval performance. Our experiments on the TREC Robust04 dataset show that query expansion can significantly improve the performance of neural ranking models, but the choice of expansion technique and hyperparameters can have a significant impact on the results. We also analyze the effects of query expansion on the robustness of neural ranking models to out-of-vocabulary queries.