Multi-modal question answering (MMQA) has gained significant attention in recent years, but existing models often struggle to effectively integrate knowledge from diverse sources. We propose a novel knowledge graph embedding (KGE) method, 'MM-KGE', which jointly learns entity and relation representations from visual, textual, and structural modalities. Our approach leverages a multi-task learning framework to optimize KGE objectives across multiple datasets, resulting in improved MMQA performance. Experimental results on benchmark datasets demonstrate the effectiveness of MM-KGE in capturing complex relationships between entities and answering multi-modal questions.