Emotion recognition from multi-modal inputs, such as speech, text, and vision, has numerous applications in human-computer interaction. However, existing approaches often suffer from high computational complexity and neglect hierarchical relationships between modalities. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages self-attention mechanisms to selectively focus on relevant modalities and features. Our experiments on the CMU-MOSEI dataset demonstrate that HAN achieves state-of-the-art performance in multi-modal emotion recognition while reducing computational overhead by 30% compared to existing approaches.