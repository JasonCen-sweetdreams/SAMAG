Human-computer interaction has seen significant advancements with the advent of gesture recognition. This paper presents a novel approach to real-time gesture recognition using graph convolutional networks (GCNs) and reinforcement learning. Our method, 'GesRec', represents gestures as graphs, leveraging spatial and temporal dependencies between joints. We introduce a reinforcement learning framework that optimizes GCN hyperparameters in real-time, adapting to varying user gestures and environments. Experimental results on a large-scale gesture dataset demonstrate GesRec's superiority in accuracy and latency compared to state-of-the-art methods, with applications in virtual reality, gaming, and healthcare.