Deep reinforcement learning (DRL) has achieved remarkable success in various applications, but its vulnerability to adversarial attacks poses a significant threat to its deployment. This paper introduces a novel graph-based anomaly detection (GAD) approach to detect adversarial attacks in DRL. Our method represents the agent's policy as a graph and utilizes graph-based metrics to identify anomalous patterns indicative of attacks. We evaluate our approach on several DRL benchmarks and demonstrate its effectiveness in detecting various types of attacks, including poisoning and evasion attacks. Our results show that GAD outperforms existing detection methods and provides a promising direction for securing DRL systems.