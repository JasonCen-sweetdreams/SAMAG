Multimodal sentiment analysis (MSA) has gained increasing attention due to the proliferation of multimodal data on social media. Existing MSA approaches often rely on early or late fusion methods, which neglect the complex interactions between modalities. To address this, we propose a novel cross-modal fusion framework based on graph attention networks (GATs). Our approach models the relationships between modalities as a graph, allowing for adaptive and selective information exchange. Experimental results on the CMU-MOSI and ICT-MMM datasets demonstrate that our approach achieves state-of-the-art performance, outperforming existing fusion methods by up to 5.2% in terms of F1-score.