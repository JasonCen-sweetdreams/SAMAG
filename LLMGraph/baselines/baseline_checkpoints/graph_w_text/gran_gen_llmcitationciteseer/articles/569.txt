Accurate emotion recognition is crucial for developing empathetic human-computer interfaces. This paper presents EmoReact, a novel multimodal affective computing framework that integrates computer vision, natural language processing, and physiological signal processing for real-time emotion recognition. EmoReact utilizes a convolutional neural network to extract facial features, a recurrent neural network to analyze linguistic cues, and a signal processing module to extract physiological features from wearable devices. We evaluate EmoReact on a large-scale multimodal dataset and demonstrate its superior performance compared to existing approaches, achieving an average F1-score of 0.92 across six emotions.