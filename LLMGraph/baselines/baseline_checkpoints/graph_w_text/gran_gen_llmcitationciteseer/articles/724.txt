Autonomous systems increasingly rely on AI-driven decision-making, yet the lack of transparency and interpretability hinders trust and accountability. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates attention mechanisms with explainability techniques to provide insights into the decision-making process. Our approach enables the identification of critical input features and their contributions to the output, facilitating the detection of biases and errors. Experimental evaluations on a real-world autonomous driving dataset demonstrate improved accuracy and explainability compared to state-of-the-art baselines.