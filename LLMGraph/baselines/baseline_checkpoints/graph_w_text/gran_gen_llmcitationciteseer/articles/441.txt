Deep neural networks (DNNs) are increasingly vulnerable to adversarial attacks, which can compromise their performance and reliability. Existing detection methods often rely on hand-crafted features or complex models, leading to limited generalizability and interpretability. This paper proposes a novel hierarchical temporal attention (HTA) mechanism that detects adversarial perturbations in DNNs by modeling the temporal dependencies between input sequences. Our approach leverages attention weights to highlight suspicious regions and adaptively adjusts the detection threshold based on the input complexity. Experimental results on benchmark datasets demonstrate that HTA outperforms state-of-the-art methods in detecting both white-box and black-box attacks, while providing insightful visualizations into the perturbation detection process.