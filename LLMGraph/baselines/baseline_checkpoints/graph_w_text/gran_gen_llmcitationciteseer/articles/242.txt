Multimodal sentiment analysis in social media platforms has gained significance due to the increasing use of images, videos, and audio along with text. This paper proposes a deep transfer learning framework, 'MultimodalSent', which leverages pre-trained convolutional neural networks (CNNs) and recurrent neural networks (RNNs) for feature extraction from vision and language modalities, respectively. We introduce a novel multimodal attention mechanism that adaptively weights the importance of each modality based on the input data. Experimental results on a large-scale social media dataset demonstrate that MultimodalSent outperforms state-of-the-art approaches in sentiment classification tasks.