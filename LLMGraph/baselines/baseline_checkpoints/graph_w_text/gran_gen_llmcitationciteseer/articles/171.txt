Deep learning models have achieved state-of-the-art performance in various applications, but their hyperparameter tuning remains a time-consuming and computationally expensive process. This paper proposes a Bayesian optimization approach, 'BOHB', which leverages a probabilistic search strategy to efficiently explore the hyperparameter space. We introduce a novel acquisition function that balances exploration and exploitation, and demonstrate its effectiveness in tuning hyperparameters for convolutional neural networks and recurrent neural networks. Experiments on several benchmark datasets show that BOHB outperforms popular hyperparameter tuning methods, including random search and grid search, in terms of both convergence speed and model performance.