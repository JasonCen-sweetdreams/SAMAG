This paper presents a novel multi-agent reinforcement learning framework for cooperative task allocation in dynamic environments. We propose a decentralized, communication-efficient approach that enables agents to learn task allocation strategies based on local observations and sparse rewards. Our method, called 'CoopTask', incorporates a hierarchical graph neural network to model agent interactions and a curriculum learning mechanism to adapt to changing environment conditions. Experimental results on a simulated Search-and-Rescue scenario demonstrate that CoopTask outperforms existing methods in terms of task completion rate and average reward.