Neural retrieval models have revolutionized the field of information retrieval, but their reliance on exact keyword matching can lead to suboptimal performance. This paper proposes a novel query expansion method, 'ContraQE', which leverages contrastive learning to capture semantic relationships between query terms and documents. By generating negative samples that are semantically distant from the query, ContraQE encourages the model to focus on relevant contexts and expands the query space more effectively. Experimental results on the TREC-COVID dataset demonstrate that ContraQE outperforms state-of-the-art query expansion techniques, achieving a 15% increase in retrieval accuracy.