Hyperparameter tuning is a critical step in deep learning model development, but it can be computationally expensive and time-consuming. This paper proposes a scalable Bayesian optimization approach for hyperparameter tuning, leveraging a novel acquisition function that balances exploration and exploitation. We demonstrate the effectiveness of our approach on several benchmark datasets, achieving state-of-the-art results in terms of model performance and tuning efficiency. Our method is particularly suitable for large-scale deep learning models and can be easily parallelized, making it an attractive solution for real-world applications.