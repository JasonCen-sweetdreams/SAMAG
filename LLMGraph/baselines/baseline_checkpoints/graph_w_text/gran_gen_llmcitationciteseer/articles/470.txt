Multimodal sentiment analysis (MSA) has gained significant attention in recent years, but existing approaches often struggle to effectively fuse information from disparate modalities. This paper proposes a novel hierarchical graph attention network (HGAT) framework for MSA, which leverages transfer learning to adapt to new modalities. Our approach integrates graph attention mechanisms to model complex relationships between modalities and adaptively weights their contributions to sentiment prediction. Experimental results on three benchmark datasets demonstrate that HGAT outperforms state-of-the-art methods, achieving an average improvement of 4.2% in accuracy and 3.5% in F1-score.