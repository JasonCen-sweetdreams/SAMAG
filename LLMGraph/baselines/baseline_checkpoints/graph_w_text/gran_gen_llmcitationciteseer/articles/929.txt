Hierarchical reinforcement learning (HRL) has shown promise in solving complex robotics tasks, but its application to real-world scenarios remains limited due to high sample complexity. This paper proposes a novel efficient HRL framework, 'EHR', which leverages a hierarchical policy decomposition and a state-abstraction mechanism to reduce the exploration burden. We demonstrate EHR's effectiveness on a range of robotics tasks, including robotic grasping and manipulation, and show significant improvements in learning efficiency and task success rates compared to flat RL methods. Our results pave the way for the deployment of HRL in real-world robotics applications.