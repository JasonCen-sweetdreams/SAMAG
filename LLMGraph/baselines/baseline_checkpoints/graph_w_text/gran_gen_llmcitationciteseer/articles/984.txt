In multi-agent systems, efficient task allocation is crucial for achieving global objectives. We propose a novel approach that leverages deep reinforcement learning to allocate tasks in real-time, taking into account the dynamic capabilities and availability of agents. Our method, called 'AgentDRL', uses a decentralized actor-critic framework to learn optimal task allocation policies. Experimental results on a simulated disaster response scenario demonstrate that AgentDRL outperforms traditional methods in terms of task completion time and agent utilization, while adapting to changes in the environment and agent availability.