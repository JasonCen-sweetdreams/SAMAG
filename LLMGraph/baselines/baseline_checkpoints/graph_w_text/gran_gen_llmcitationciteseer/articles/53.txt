Multimodal sentiment analysis has gained significance in recent years, but existing approaches often rely on complex models that lack transparency. This paper proposes a novel hierarchical attention network (HAN) that integrates visual, textual, and acoustic features for sentiment analysis. Our HAN model employs a self-attention mechanism to identify relevant features and modalities, providing interpretable results. Experimental results on the CMU-MOSI dataset demonstrate that our approach outperforms state-of-the-art methods in terms of accuracy and F1-score, while offering insights into the decision-making process.