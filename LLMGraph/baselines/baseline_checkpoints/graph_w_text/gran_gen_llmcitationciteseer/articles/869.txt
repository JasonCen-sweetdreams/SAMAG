Deep learning models have achieved state-of-the-art performance in sentiment analysis, but their lack of transparency hinders trust and adoption. This paper introduces Hierarchical Attention Networks (HANs), a novel explainable AI framework that leverages attention mechanisms to identify salient features and phrases contributing to sentiment predictions. We propose a hierarchical architecture that captures both local and global contextual dependencies, enabling the model to provide interpretable explanations for its predictions. Experimental results on benchmark datasets demonstrate that HANs outperform existing explainable methods while maintaining competitive sentiment analysis performance.