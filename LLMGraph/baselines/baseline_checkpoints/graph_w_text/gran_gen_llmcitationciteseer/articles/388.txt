Deep learning models have achieved state-of-the-art performance in multi-modal sentiment analysis tasks, but their lack of transparency hinders their adoption in high-stakes applications. This paper proposes a novel hierarchical attention network (HAN) that incorporates explainability mechanisms into the model architecture. Our HAN model learns to selectively focus on relevant modalities and regions of interest, providing interpretable visualizations of the decision-making process. Experimental results on a large-scale multi-modal sentiment dataset demonstrate the effectiveness of our approach, achieving improved performance and explainability over existing methods.