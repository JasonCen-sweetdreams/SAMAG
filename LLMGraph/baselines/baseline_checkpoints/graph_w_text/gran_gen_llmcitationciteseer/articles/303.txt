Deep neural networks have revolutionized various applications, but their performance heavily relies on careful hyperparameter tuning. This process is often time-consuming and computationally expensive. We propose a Bayesian optimization approach using Gaussian processes to efficiently search for optimal hyperparameters. Our method, 'GP-BO', leverages the uncertainty estimates of Gaussian processes to adaptively select the most informative hyperparameter settings. Experimental results on several benchmark datasets demonstrate that GP-BO achieves comparable or better performance than state-of-the-art methods while reducing the number of evaluations by up to 50%. We also provide theoretical guarantees on the convergence of our algorithm.