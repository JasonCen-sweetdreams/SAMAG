Explainability is a crucial aspect in multi-agent dialogue systems, as it enhances transparency and trust in decision-making processes. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that incorporates attention mechanisms at both utterance and dialogue levels. Our approach enables the model to selectively focus on relevant contexts and generate interpretable explanations for its responses. Experimental results on the Multi-WOZ dataset demonstrate that HAN outperforms state-of-the-art models in terms of dialogue coherence and explanation quality.