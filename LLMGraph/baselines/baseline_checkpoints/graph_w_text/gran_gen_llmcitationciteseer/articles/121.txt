Multi-modal fusion of visual and textual features has become a crucial task in various computer vision applications. This paper presents a novel Hierarchical Attention Network (HAN) architecture that efficiently fuses features from different modalities. Our approach leverages self-attention mechanisms to capture intra-modal and inter-modal relationships, enabling the model to selectively focus on relevant features. We evaluate HAN on several benchmark datasets, demonstrating significant improvements in performance and computational efficiency compared to state-of-the-art methods.