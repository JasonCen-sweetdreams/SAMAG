Few-shot natural language processing (NLP) tasks require models to generalize to unseen tasks with limited labeled data. We propose a novel meta-learning approach, 'HATE', which employs hierarchical adaptive task embeddings to capture task relationships and transfer knowledge across tasks. Our method leverages a hierarchical neural network to learn task embeddings, which are then adapted to new tasks using a meta-learning objective. We evaluate HATE on several few-shot NLP benchmarks and demonstrate improved performance over state-of-the-art methods, achieving an average relative improvement of 12.5% in accuracy.