Multimodal interfaces have become increasingly prevalent, but their complexity can lead to cognitive overload and exclusion of users with diverse abilities. This paper presents a novel visualization framework, 'VisInclu', which adaptively generates visual representations of multimodal data to support inclusive decision-making. VisInclu integrates machine learning-driven prediction models with participatory design principles to dynamically adjust visualization complexity, color schemes, and interaction modalities based on user preferences and abilities. Our user study with 30 participants demonstrates that VisInclu significantly improves decision-making accuracy and user satisfaction for individuals with visual impairments, dyslexia, and aging-related cognitive decline.