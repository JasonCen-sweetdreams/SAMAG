Voice assistants have become ubiquitous in modern life, but their interaction modalities often exclude users with disabilities. This paper presents a novel, multimodal framework for designing inclusive voice assistants that accommodate diverse abilities. We combine computer vision, natural language processing, and gesture recognition to enable users to interact with voice assistants through a range of modalities, including speech, sign language, and gestures. Our user study with 30 participants with disabilities shows that our approach significantly improves interaction accuracy, efficiency, and overall user experience compared to traditional voice-only assistants.