Deep reinforcement learning (RL) has achieved remarkable success in various domains, but its brittleness in partially observable environments remains a significant challenge. This paper introduces HAN-RL, a novel hierarchical attention network architecture that incorporates both spatial and temporal attention mechanisms to reason about uncertain state information. We demonstrate that HAN-RL improves the interpretability and performance of RL agents in several benchmark environments, including the Atari and MuJoCo suites. Furthermore, we provide a theoretical analysis of the attention mechanisms, showing that they facilitate more efficient exploration and improved policy convergence.