Multi-task dialogue management has become a crucial component of conversational AI systems. However, existing approaches often rely on black-box models, hindering transparency and interpretability. This paper presents a novel hierarchical attention network (HAN) architecture that jointly learns multiple dialogue tasks while providing explainable results. Our HAN model employs a hierarchical attention mechanism to selectively focus on relevant contextual information, enabling the generation of task-specific explanations. Experimental results on the MultiWOZ dataset demonstrate that our approach outperforms state-of-the-art models in terms of task performance while offering insights into the decision-making process.