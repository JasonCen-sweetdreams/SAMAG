Virtual assistants (VAs) have transformed the way people interact with technology, but their lack of emotional intelligence (EI) can lead to frustration and abandonment, particularly for individuals with disabilities. This paper presents an EI framework for VAs that incorporates empathy, self-awareness, and social skills. We develop a multimodal affect recognition system that detects users' emotional states from speech, facial expressions, and physiological signals. Our user study with 30 participants with disabilities shows that our EI-VA significantly improves user satisfaction, reduces anxiety, and increases task completion rates compared to a traditional VA. We discuss the implications of our work for designing more inclusive and supportive interfaces.