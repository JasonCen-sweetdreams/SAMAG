Cooperative multi-agent systems have numerous applications in domains like robotics, smart cities, and IoT. However, efficient task allocation remains a challenging problem, especially in dynamic environments. This paper proposes a novel approach, 'Deep-Coop', which leverages deep reinforcement learning to learn distributed task allocation policies for cooperative multi-agent systems. Our method utilizes a decentralized actor-critic architecture, where each agent learns to make decisions based on local observations and communication with neighbors. Experimental results demonstrate that Deep-Coop outperforms traditional methods in terms of task completion rate, latency, and adaptability to environmental changes.