Understanding sentiment from multi-modal inputs (e.g., text, images, audio) is crucial for AI-driven decision-making. However, existing methods often rely on complex neural networks with opaque decision-making processes. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates attention mechanisms across modalities and layers, enabling explainable sentiment analysis. Our approach leverages modality-specific attention to weigh feature importance and provides visualizations of attention weights for interpretability. Experimental results on a large-scale multi-modal dataset demonstrate improved sentiment accuracy and robustness compared to state-of-the-art models.