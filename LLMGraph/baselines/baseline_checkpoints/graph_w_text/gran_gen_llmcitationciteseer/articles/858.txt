Explainability is a crucial aspect of trustworthy AI, particularly in safety-critical autonomous systems. This paper presents a novel hierarchical reinforcement learning framework, 'HERO', which integrates model-based and model-free approaches to provide interpretable decision-making. HERO learns to decompose complex tasks into sub-goals, allowing for both global and local explanations of the agent's behavior. We demonstrate the efficacy of HERO in a real-world autonomous driving scenario, showcasing improved transparency and performance compared to existing state-of-the-art methods.