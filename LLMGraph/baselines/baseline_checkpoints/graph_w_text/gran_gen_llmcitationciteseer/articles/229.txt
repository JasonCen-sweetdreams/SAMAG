Deep reinforcement learning (DRL) has shown remarkable success in various applications, but its vulnerability to adversarial attacks remains a significant concern. This paper proposes a novel approach to detect adversarial attacks in DRL using temporal graph convolutional networks (T-GCNs). Our method, dubbed 'T-Guard', leverages the graph structure of DRL policies to identify abnormal behavior patterns indicative of attacks. We demonstrate the effectiveness of T-Guard in detecting various types of attacks, including poisoning and evasion attacks, on a range of DRL benchmarks. Our results show that T-Guard outperforms state-of-the-art detection methods, achieving an average detection accuracy of 92.5%.