Neural architecture search (NAS) has revolutionized the field of deep learning, but its computational cost remains a significant bottleneck. This paper introduces a novel Hierarchical Graph Attention (HGA) framework that efficiently searches for optimal neural architectures. HGA leverages graph attention mechanisms to capture complex hierarchical relationships between architecture components, thereby reducing the search space and improving search efficiency. We demonstrate the efficacy of HGA on several benchmark datasets, achieving state-of-the-art performance in NAS tasks while reducing computational overhead by up to 50%.