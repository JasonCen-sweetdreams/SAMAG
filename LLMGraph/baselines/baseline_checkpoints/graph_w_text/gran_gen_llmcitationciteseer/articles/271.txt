Accurate recognition of human emotional states is crucial for developing empathetic human-computer interfaces. This paper presents a novel multimodal approach that combines physiological signals (heart rate, skin conductance) with facial features (facial action units, facial landmarks) to recognize emotional states. We propose a deep fusion framework that exploits the complementary strengths of each modality, resulting in improved recognition accuracy and robustness to noisy or incomplete data. Our experiments on a large, diverse dataset demonstrate the effectiveness of our approach in recognizing seven basic emotions, outperforming state-of-the-art unimodal and multimodal baselines.