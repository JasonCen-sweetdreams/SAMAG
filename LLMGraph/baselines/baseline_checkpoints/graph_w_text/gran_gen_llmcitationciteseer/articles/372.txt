As autonomous vehicles increasingly rely on deep reinforcement learning (DRL) to make critical decisions, it is essential to develop explainability techniques that provide insights into their decision-making processes. This paper proposes a novel method, 'RL-Explain', which leverages attention mechanisms and feature importance to generate interpretable explanations for DRL policies in real-time. We demonstrate the effectiveness of RL-Explain on a variety of autonomous driving scenarios, showcasing its ability to identify salient features and provide actionable explanations for human operators. Our results highlight the potential of RL-Explain to increase trust and transparency in autonomous vehicle systems.