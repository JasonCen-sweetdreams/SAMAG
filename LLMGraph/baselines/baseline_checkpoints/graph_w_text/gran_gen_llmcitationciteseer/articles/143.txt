Multimodal sentiment analysis has gained significant attention in recent years, but the lack of interpretability in deep learning models hinders their adoption in real-world applications. This paper proposes a novel hierarchical attention network (HAN) that integrates visual, textual, and acoustic features for multimodal sentiment analysis. Our HAN model employs a hierarchical attention mechanism to selectively focus on relevant features and modalities, providing insights into the decision-making process. Experimental results on the CMU-MOSI dataset demonstrate that our approach outperforms state-of-the-art methods in terms of accuracy and explainability, enabling more trustworthy AI systems.