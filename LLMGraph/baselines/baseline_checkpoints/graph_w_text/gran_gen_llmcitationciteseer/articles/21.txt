Wheelchair navigation remains a significant challenge for individuals with mobility impairments. This paper presents an innovative, gaze-based interface designed to improve wheelchair control and navigation. Our system, 'GazeNav', utilizes a wearable eye-tracking device to detect and interpret user gaze patterns, translating them into wheelchair commands. We conducted a user study with 15 participants, demonstrating significant improvements in navigation accuracy and user satisfaction compared to traditional joystick-based interfaces. GazeNav's performance was further enhanced by integrating machine learning-based gaze prediction and path planning algorithms.