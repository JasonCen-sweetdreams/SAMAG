Neural architecture search (NAS) has emerged as a promising approach for automating the design of deep neural networks. However, existing methods often rely on costly and time-consuming evaluations of candidate architectures. This paper proposes a novel graph-based reinforcement learning framework, 'GraphNAS', which leverages the structural properties of neural networks to guide the search process. By representing architectures as graphs and learning a policy to traverse the graph space, GraphNAS achieves a significant reduction in search time and computational resources while maintaining competitive performance on several benchmark datasets.