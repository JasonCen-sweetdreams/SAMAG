Multimodal sentiment analysis (MSA) is a crucial task in human-computer interaction, as it enables computers to understand user emotions and opinions from various modalities. This paper proposes a hierarchical attention network (HAN) for MSA, which integrates visual, acoustic, and textual features. Our HAN model consists of modality-specific attention modules that learn to weight features based on their importance, followed by a hierarchical fusion module that combines the weighted features. Experimental results on the CMU-MOSI dataset demonstrate that our approach outperforms state-of-the-art methods in MSA, achieving an accuracy of 83.2% and a F1-score of 85.1%. We also conduct an ablation study to analyze the effectiveness of each attention module.