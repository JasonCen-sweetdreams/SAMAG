Multimodal fusion is a crucial component of Visual Question Answering (VQA) systems, but existing approaches often lack transparency and interpretability. We propose a novel Hierarchical Attention Network (HAN) that integrates visual and textual features through a hierarchical attention mechanism. Our approach enables the model to focus on relevant regions of the image and corresponding question words, providing insight into the decision-making process. Experimental results on the VQA-CP v2 dataset demonstrate that HAN outperforms state-of-the-art models in terms of accuracy and achieves better explainability through visualizations of attention weights.