Emotion recognition in conversational agents is a crucial aspect of human-computer interaction. This paper presents a novel hierarchical attention network (HAN) architecture that leverages multi-modal inputs (text, speech, and vision) to recognize emotions in human-agent conversations. Our HAN model uses a hierarchical framework to capture intra-modal and inter-modal relationships, allowing it to better distinguish between subtle emotional cues. Experimental results on a large-scale dataset of human-agent conversations demonstrate that our approach outperforms state-of-the-art methods in emotion recognition accuracy, especially in scenarios with noisy or incomplete input data.