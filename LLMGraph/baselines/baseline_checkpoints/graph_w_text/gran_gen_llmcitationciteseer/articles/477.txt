Emotion recognition is a crucial aspect of human-computer interaction (HCI), enabling systems to respond empathetically to user needs. This paper presents EmoReact, a novel multimodal emotion recognition framework that integrates computer vision, speech processing, and natural language processing to analyze user emotions. EmoReact employs a hierarchical attention mechanism to fuse features from different modalities, achieving improved recognition accuracy compared to unimodal approaches. We evaluate EmoReact on a large, publicly available dataset and demonstrate its effectiveness in enhancing user experience in various HCI applications, including virtual assistants and affective gaming.