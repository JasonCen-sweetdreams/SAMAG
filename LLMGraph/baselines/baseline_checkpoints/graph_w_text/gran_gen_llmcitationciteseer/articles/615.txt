Multimodal information retrieval (MMIR) systems struggle to effectively capture the semantic relationships between textual and visual features in queries. We propose a context-aware query expansion framework, CAQE, which leverages multimodal attention mechanisms to identify relevant contextual information. CAQE dynamically adapts to the query's semantic scope, enabling the incorporation of relevant visual and textual features to improve retrieval accuracy. Our experiments on a multimodal dataset show significant improvements in mean average precision and retrieval efficiency compared to state-of-the-art MMIR methods.