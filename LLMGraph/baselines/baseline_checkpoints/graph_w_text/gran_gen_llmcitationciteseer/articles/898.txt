Knowledge graph embedding (KGE) models have shown promise in various AI applications, but their performance is often hindered by the complexity of relation semantics. This paper presents a novel KGE approach, 'RelAttn', which leverages adaptive attention mechanisms to selectively focus on relevant relations during training. Our method dynamically adjusts attention weights based on the local neighborhood of each entity, enabling more effective capture of relation patterns. Experimental results on several benchmark datasets demonstrate that RelAttn outperforms state-of-the-art KGE methods in link prediction and triple classification tasks while reducing computational overhead.