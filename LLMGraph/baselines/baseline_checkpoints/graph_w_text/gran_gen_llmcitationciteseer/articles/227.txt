Time-series forecasting models often lack transparency, making it challenging to understand their predictions. This paper presents 'HATSF', a hierarchical attention network that integrates interpretable components for explainable forecasting. Our approach encodes temporal dependencies using a novel hierarchical attention mechanism, allowing the model to selectively focus on relevant segments of the input sequence. We evaluate HATSF on several benchmark datasets and demonstrate its ability to provide accurate forecasts while offering insights into the decision-making process. Experimental results show that HATSF outperforms state-of-the-art models in terms of forecasting accuracy and interpretability.