State-of-the-art neural retrieval models often require computationally expensive re-ranking of large document collections. This paper proposes a novel query expansion approach, 'AttentiveXP', which leverages attention mechanisms to selectively incorporate relevant terms from the top-ranked documents into the query representation. Our method reduces the number of documents to be re-ranked while preserving retrieval effectiveness. Experimental results on the TREC-COVID dataset demonstrate that AttentiveXP achieves significant speedups over state-of-the-art neural models while maintaining comparable retrieval performance.