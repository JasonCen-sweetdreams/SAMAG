Virtual voice assistants have become ubiquitous, but user frustration remains a significant obstacle to widespread adoption. This paper presents a multimodal approach to understanding user frustration in voice-based interactions. We collect a dataset of 100 users interacting with a popular virtual assistant, capturing audio, video, and physiological signals. Our analysis reveals that frustration is characterized by distinct patterns of speech, facial expressions, and physiological arousal. We develop a machine learning model that predicts user frustration with high accuracy, and demonstrate the effectiveness of our approach in improving user experience through personalized feedback and error mitigation strategies.