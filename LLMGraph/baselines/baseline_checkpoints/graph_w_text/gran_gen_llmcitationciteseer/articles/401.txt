Distributed relational databases have become increasingly popular for large-scale data storage and querying. However, optimizing subquery processing in these systems remains a challenging problem. This paper proposes a novel approach that leverages reinforcement learning to optimize subquery execution plans. We design a rewards function that takes into account both query latency and resource utilization, and demonstrate that our approach outperforms traditional rule-based and cost-based optimizers in terms of query performance and resource efficiency. Experimental results on a real-world dataset show that our approach reduces average query latency by up to 35% and improves resource utilization by up to 25%.