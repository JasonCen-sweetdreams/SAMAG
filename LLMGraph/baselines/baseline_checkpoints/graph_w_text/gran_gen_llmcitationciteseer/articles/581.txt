Multi-hop question answering (MHQA) requires models to perform complex reasoning across multiple sentences. While transformer-based architectures have shown promising results, they often lack interpretability and transparency. This paper proposes a novel Hierarchical Attention-based Explainable Reasoning (HAER) framework that integrates attention mechanisms with symbolic reasoning to generate interpretable explanations for MHQA tasks. Our approach leverages graph-based attention to identify relevant sentences and entities, and then applies a hierarchical reasoning module to generate step-by-step explanations. Experimental results on the HotPotQA dataset demonstrate that HAER outperforms state-of-the-art models in terms of accuracy and explanation quality.