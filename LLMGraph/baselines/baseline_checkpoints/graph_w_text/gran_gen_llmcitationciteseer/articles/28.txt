Dialogue systems have seen significant advancements with the integration of multi-modal inputs, such as text, speech, and vision. However, processing and fusing these modalities efficiently remains a challenge. We propose a novel Hierarchical Attention Network (HAN) architecture that leverages attention mechanisms at multiple scales to selectively focus on relevant modalities and context. Our experiments on the Multi-Modal Dialogue Dataset (M2DD) demonstrate improved response generation quality and reduced computational overhead compared to existing state-of-the-art models.