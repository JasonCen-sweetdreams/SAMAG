Deep neural networks are prone to overfitting when trained on noisy labels. This paper presents an adaptive batch normalization (AdaBN) technique that selectively adjusts the normalization statistics based on label confidence. We develop a probabilistic framework to estimate label noise and dynamically adjust the batch normalization layer to mitigate its impact. Experiments on several benchmark datasets demonstrate that AdaBN improves the robustness of deep models to noisy labels, achieving state-of-the-art performance on noisy CIFAR-10 and Clothing1M.