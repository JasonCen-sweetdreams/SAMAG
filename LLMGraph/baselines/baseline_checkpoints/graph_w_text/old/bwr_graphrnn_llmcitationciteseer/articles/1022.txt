Deep reinforcement learning (DRL) has achieved remarkable success in various domains, but the lack of interpretability hinders its widespread adoption. This paper proposes a novel approach, Hierarchical Attention-based Explainability (HAX), to provide insights into DRL decision-making processes. HAX leverages hierarchical attention mechanisms to identify salient state features and actions contributing to the agent's policy. We evaluate HAX on several Atari games and demonstrate its ability to generate faithful explanations for DRL policies, thereby improving their trustworthiness and transparency. Furthermore, we show that HAX can be used to identify and correct biases in DRL models.