Explainability is a crucial aspect of multi-agent reinforcement learning (MARL) as it enables understanding of complex agent interactions. This paper proposes a novel hierarchical attention network (HAN) architecture that learns to selectively focus on relevant agents, states, and actions to improve explainability in MARL. We evaluate our approach on a range of cooperative and competitive MARL tasks, demonstrating improved performance and interpretability compared to state-of-the-art baselines. Our method provides fine-grained insights into agent decision-making, facilitating the development of more transparent and trustworthy MARL systems.