Emotion recognition from multi-modal inputs is a challenging task, especially when dealing with ambiguous or conflicting cues. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages both local and global dependencies across modalities to improve emotion recognition accuracy. Our approach incorporates explainability techniques to provide insights into the decision-making process, enabling the identification of modalities and features that contribute most to emotion prediction. Experimental results on the CMU-MOSEI dataset demonstrate that our HAN outperforms state-of-the-art methods while providing interpretable results.