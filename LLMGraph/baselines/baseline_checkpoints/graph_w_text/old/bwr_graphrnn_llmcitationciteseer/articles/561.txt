Explainability is crucial in multi-agent reinforcement learning (MARL) as it enables understanding of complex decision-making processes. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates attention mechanisms at both local and global levels to provide interpretable representations of agent interactions. Our approach, 'ExplainMARL', leverages graph neural networks to model agent relationships and attention weights to identify key contributors to joint actions. We evaluate ExplainMARL on a range of MARL benchmarks, demonstrating improved performance and explainability compared to state-of-the-art methods.