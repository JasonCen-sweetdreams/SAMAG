Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the complexity and variability of human emotions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that models the interactions between different modalities and captures both local and global contextual information. Our approach achieves state-of-the-art performance on two benchmark datasets and provides interpretable results through attention-based visualization. We demonstrate that HAN can identify salient features in each modality that contribute to emotion recognition, facilitating more explainable AI systems.