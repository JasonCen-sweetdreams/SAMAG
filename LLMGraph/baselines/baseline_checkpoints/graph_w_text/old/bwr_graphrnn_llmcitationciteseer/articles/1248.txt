Individuals with motor impairments often face significant barriers when interacting with virtual assistants. This paper presents a novel gaze-based interaction system, 'GazeVA', which enables users to control virtual assistants using only their gaze. Our approach leverages a deep neural network to detect and track the user's gaze, and a probabilistic model to infer the user's intended command. We evaluate GazeVA with 20 participants with motor impairments and demonstrate a significant reduction in interaction time and error rate compared to traditional voice-based interfaces.