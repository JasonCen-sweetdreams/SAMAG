Accurate affective state recognition is crucial in human-computer interaction (HCI) applications. This paper presents EmoReact, a novel multimodal fusion approach that combines facial expression analysis with physiological signal processing to recognize emotions. Our framework leverages a deep neural network to extract features from facial action units, heart rate variability, and skin conductance. We evaluate EmoReact on a dataset of 100 participants and demonstrate a significant improvement in emotion recognition accuracy (average F1-score: 0.87) compared to unimodal approaches. EmoReact has potential applications in affective computing, mental health monitoring, and human-centered AI systems.