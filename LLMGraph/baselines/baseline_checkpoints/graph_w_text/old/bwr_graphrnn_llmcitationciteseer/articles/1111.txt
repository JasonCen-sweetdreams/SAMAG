Emotion recognition from multi-modal inputs, such as speech, text, and vision, is a challenging task in affective computing. Existing approaches often rely on early fusion of modalities, leading to increased computational complexity and decreased performance. This paper introduces a novel hierarchical attention network (HAN) that leverages late fusion and modality-specific attention mechanisms to selectively focus on relevant features. We evaluate HAN on three benchmark datasets and demonstrate significant improvements in emotion recognition accuracy and computational efficiency compared to state-of-the-art methods.