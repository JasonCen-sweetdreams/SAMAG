Graph neural networks (GNNs) have shown remarkable success in node classification tasks, but their performance degrades significantly in few-shot settings. This paper proposes a novel hierarchical attention mechanism for GNNs, dubbed HAGNN, which leverages both node-level and graph-level attention to adaptively focus on relevant subgraphs and node features. We evaluate HAGNN on several benchmark datasets and demonstrate its superior performance in few-shot node classification, outperforming state-of-the-art methods by up to 15%. We also provide an ablation study to validate the effectiveness of our hierarchical attention mechanism.