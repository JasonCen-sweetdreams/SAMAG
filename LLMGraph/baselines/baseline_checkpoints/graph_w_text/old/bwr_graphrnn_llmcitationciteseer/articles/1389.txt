In dynamic multi-agent systems, task allocation is a challenging problem due to the need to balance individual agent goals with global system objectives. This paper introduces a hierarchical reinforcement learning framework, 'HRL-TaskAlloc', that enables autonomous agents to adapt to changing task requirements and coordinate their actions effectively. Our approach combines a high-level task allocation policy with low-level agent control policies, learned through a novel hierarchical Q-network architecture. Experimental results on a simulated disaster response scenario demonstrate that HRL-TaskAlloc outperforms traditional decentralized task allocation methods in terms of task completion efficiency and system stability.