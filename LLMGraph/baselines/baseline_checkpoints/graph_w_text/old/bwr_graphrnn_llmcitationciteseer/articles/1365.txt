Personalized recommendation systems often face the exploration-exploitation dilemma, where they must balance exploring new items to learn user preferences and exploiting existing knowledge to maximize immediate rewards. This paper proposes a novel online learning framework, 'MAB-Rec', that leverages multi-armed bandits to adaptively explore and exploit items in real-time. We introduce a contextual Thompson sampling strategy that incorporates item features and user behavior, enabling MAB-Rec to efficiently learn user preferences and improve recommendation accuracy. Experiments on a large-scale e-commerce dataset demonstrate that MAB-Rec outperforms state-of-the-art methods in terms of cumulative rewards and user engagement.