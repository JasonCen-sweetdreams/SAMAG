Graph neural networks (GNNs) have shown promising results in various applications, but their robustness against adversarial attacks remains a concern. This paper presents a comprehensive analysis of the vulnerabilities of GNNs to graph-structured attacks, including node injection, edge perturbation, and attribute manipulation. We propose a novel attack framework, 'GraphFool', which leverages reinforcement learning to generate perturbations that maximize the misclassification rate. Our experiments on popular graph datasets demonstrate that GraphFool outperforms existing attacks, and we discuss the implications for designing more robust GNN architectures.