Virtual reality (VR) has the potential to revolutionize various aspects of our lives, but existing VR systems often fail to account for users with disabilities. This paper presents EmoReact, a novel affective gesture recognition system that enables users with mobility impairments to interact with VR environments using emotional cues. EmoReact leverages computer vision and machine learning techniques to recognize and classify emotional states from facial expressions, allowing users to navigate and interact with virtual objects in a more natural and inclusive way. Our user study with participants with mobility impairments demonstrates the effectiveness of EmoReact in enhancing the overall VR experience.