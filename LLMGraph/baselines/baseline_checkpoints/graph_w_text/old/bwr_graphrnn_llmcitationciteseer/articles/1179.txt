Explainability is crucial in multi-agent systems, where understanding agent decisions is essential for trust and safety. This paper presents HAN-MARL, a hierarchical attention network architecture that learns to explain multi-agent reinforcement learning policies. HAN-MARL employs a novel attention mechanism to selectively focus on relevant agents, states, and actions, generating interpretable explanations for agent decisions. Experimental results on a cooperative navigation task demonstrate that HAN-MARL outperforms state-of-the-art MARL methods in terms of explanation quality and policy performance.