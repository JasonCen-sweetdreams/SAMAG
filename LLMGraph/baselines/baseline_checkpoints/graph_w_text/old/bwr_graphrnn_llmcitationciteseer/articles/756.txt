In large-scale multi-agent systems, task allocation is a crucial problem that requires efficient and adaptive decision-making. This paper proposes a novel hierarchical reinforcement learning framework, 'HRL-MA', which enables agents to learn scalable task allocation policies. Our approach combines a high-level task allocator with low-level agent controllers, allowing agents to adapt to changing task demands and environmental conditions. We evaluate HRL-MA on a simulated disaster response scenario, demonstrating improved task completion rates and reduced communication overhead compared to existing decentralized task allocation methods.