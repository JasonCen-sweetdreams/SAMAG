Intelligent virtual assistants (IVAs) are increasingly prevalent in various domains, but their inability to recognize users' emotional states hinders their effectiveness. This paper presents an innovative approach to recognizing users' emotional states using eye-tracking data. We propose a novel deep learning model, 'EmoEye', which leverages multimodal fusion of eye-tracking features, facial expressions, and linguistic cues to recognize users' emotional states. Our approach achieves a recognition accuracy of 87.5% on a benchmark dataset, outperforming state-of-the-art methods. We also demonstrate the feasibility of integrating EmoEye with an IVA to provide personalized support and enhance user experience.