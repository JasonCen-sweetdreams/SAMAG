Deep neural networks (DNNs) have achieved state-of-the-art performance in various machine learning tasks, but their success heavily relies on careful hyperparameter tuning. This process can be computationally expensive and requires significant expertise. In this paper, we propose a Bayesian optimization (BO) framework for efficient hyperparameter tuning of DNNs. Our approach leverages a novel acquisition function that balances exploration and exploitation, resulting in faster convergence to optimal hyperparameters. We demonstrate the effectiveness of our method on several benchmark datasets, achieving comparable or better performance than existing BO methods while reducing the number of evaluations by up to 50%.