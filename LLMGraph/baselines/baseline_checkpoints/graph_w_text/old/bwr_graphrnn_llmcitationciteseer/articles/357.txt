Emotion recognition from multi-modal inputs, such as speech, text, and facial expressions, is a challenging task in affective computing. This paper proposes a novel Hierarchical Attention Network (HAN) that integrates features from different modalities and learns to focus on the most informative regions. Our approach leverages a hierarchical structure to model intra- and inter-modal relationships, enabling efficient emotion recognition. Experimental results on the IEMOCAP and CMU-MOSEI datasets demonstrate that our HAN outperforms state-of-the-art methods, achieving a 12.5% improvement in weighted F1-score while reducing computational costs by 30%.