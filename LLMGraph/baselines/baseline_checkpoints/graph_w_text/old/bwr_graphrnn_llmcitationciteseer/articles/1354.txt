Virtual reality (VR) has transformed the way we interact with digital information, but current interaction modalities often fall short in terms of naturalness and intuitiveness. This paper presents a novel gaze-based interaction modelling approach that leverages machine learning and computer vision techniques to infer user intentions from eye movements. Our proposed framework, 'GazeGuide', incorporates a probabilistic graphical model to reason about user goals and preferences, enabling more effective and efficient interaction in VR environments. Evaluation results from a user study demonstrate significant improvements in task completion time and user satisfaction compared to traditional controller-based interfaces.