Emotion recognition in conversational agents is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel Hierarchical Attention Network (HAN) that integrates acoustic, lexical, and visual features to recognize emotions in human-agent conversations. The HAN model consists of two levels of attention: modality-level attention to select relevant modalities and feature-level attention to focus on salient features. Experimental results on the IEMOCAP dataset show that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an average F1-score of 0.85.