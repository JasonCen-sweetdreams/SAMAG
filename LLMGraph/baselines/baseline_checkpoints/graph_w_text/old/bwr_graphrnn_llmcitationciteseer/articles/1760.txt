Emotion recognition from multimodal data, such as speech, text, and vision, is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) that leverages the strengths of graph neural networks and attention mechanisms to model the intricate relationships between modalities. Our approach consists of a hierarchical graph structure that captures both intra-modality and inter-modality interactions, and a graph attention mechanism that adaptively weights the importance of each modality. Experimental results on three benchmark datasets demonstrate that HGAT outperforms state-of-the-art methods in multitask emotion recognition and achieves robustness to noisy or missing modalities.