Few-shot learning remains a significant challenge in machine learning, where models must adapt to new tasks with limited labeled data. This paper presents a novel hierarchical attention mechanism, 'HierAttn', which improves few-shot learning efficiency by selectively focusing on relevant task-specific features. HierAttn employs a two-stage attention process: (1) a task-agnostic attention module that identifies universally important features, and (2) a task-specific attention module that refines feature selection based on the target task. Experimental results on benchmark datasets demonstrate that HierAttn outperforms state-of-the-art few-shot learning methods, achieving higher accuracy and faster adaptation to new tasks.