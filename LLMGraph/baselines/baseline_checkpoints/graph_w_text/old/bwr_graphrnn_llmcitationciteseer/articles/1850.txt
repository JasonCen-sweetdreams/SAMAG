This paper presents a novel approach to coordinating multi-agent systems in dynamic environments with uncertain task requirements. We introduce a hierarchical reinforcement learning framework, 'HierMA', which enables agents to learn both high-level task allocation policies and low-level control strategies. HierMA leverages a centralized coordinator to allocate tasks to agents based on their learned capabilities and preferences, while agents adapt to changing task demands through decentralized reinforcement learning. Experimental results on a simulated rescue scenario demonstrate that HierMA outperforms traditional planning-based approaches in terms of task completion efficiency and adaptability to unexpected events.