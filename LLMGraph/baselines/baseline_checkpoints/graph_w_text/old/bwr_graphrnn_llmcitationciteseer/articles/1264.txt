Graph neural networks (GNNs) have achieved state-of-the-art performance in various graph-based tasks. However, their vulnerability to adversarial attacks has raised concerns. This paper presents a comprehensive study of defense mechanisms against adversarial attacks on GNNs. We evaluate the effectiveness of five popular defense strategies, including graph purification, adversarial training, and graph attention-based methods. Our experiments on three benchmark datasets demonstrate that the proposed graph attention-based defense mechanism outperforms existing methods in terms of robustness and accuracy. We also analyze the trade-offs between defense mechanisms and provide recommendations for practitioners.