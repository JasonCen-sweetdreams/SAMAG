Distributed relational databases have become increasingly popular for handling large-scale data storage and processing. However, query optimization remains a significant challenge in such systems. This paper proposes a novel approach that leverages machine learning techniques to optimize query execution plans in distributed relational databases. We design a learning model that predicts the optimal plan based on query patterns, data distribution, and system workload. Experimental results on a real-world dataset demonstrate that our approach outperforms traditional query optimization techniques by up to 30% in terms of query execution time.