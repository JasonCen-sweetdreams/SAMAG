Emotion recognition from multi-modal data (e.g., speech, text, vision) is crucial in human-computer interaction. We propose a novel Hierarchical Attention Network (HAN) that effectively fuses modalities and captures complex emotional cues. Our HAN consists of a hierarchical encoder that learns to weigh and combine modality-specific features, followed by a multi-task learning framework that jointly predicts emotion categories and intensities. Experimental results on the SEMAINE and IEMOCAP datasets demonstrate significant improvements over state-of-the-art methods, achieving a 12.5% increase in accuracy and a 15.2% reduction in mean absolute error.