Sentiment analysis from multi-modal data (e.g., text, images, and videos) is a challenging task due to the complexity of interactions between different modalities. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) model that captures both intra- and inter-modality relationships. HGAT consists of two stages: (1) modality-specific graph attention networks that learn to weight the importance of different features within each modality, and (2) a hierarchical fusion module that integrates the modality-specific representations using a graph-based attention mechanism. Experimental results on two benchmark datasets demonstrate that HGAT outperforms state-of-the-art methods in multi-modal sentiment analysis tasks.