Dialogue state tracking (DST) is a crucial component of task-oriented dialogue systems, but existing approaches often lack interpretability. We propose a novel hierarchical attention network (HAT) that incorporates both local and global contextual information to improve DST performance. Our architecture consists of a hierarchical encoder that captures utterance-level and conversation-level dependencies, followed by a attention-based decoder that generates the dialogue state. We evaluate HAT on the MultiWOZ dataset and demonstrate significant improvements in DST accuracy and explainability compared to state-of-the-art models.