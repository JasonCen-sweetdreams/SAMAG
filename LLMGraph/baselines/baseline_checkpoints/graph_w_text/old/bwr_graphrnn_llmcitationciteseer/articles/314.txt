Virtual reality (VR) systems rely on accurate gesture recognition to provide an immersive user experience. This paper proposes a novel deep learning-based approach to hand pose estimation, 'HandPoseNet', which significantly improves gesture recognition accuracy in VR environments. Our approach leverages a convolutional neural network (CNN) to estimate 3D hand poses from 2D RGB images, and integrates a recurrent neural network (RNN) to model temporal dependencies between hand gestures. Experimental results on a large-scale VR gesture dataset demonstrate that HandPoseNet outperforms state-of-the-art methods by 15% in terms of gesture recognition accuracy, and reduces latency by 30%.