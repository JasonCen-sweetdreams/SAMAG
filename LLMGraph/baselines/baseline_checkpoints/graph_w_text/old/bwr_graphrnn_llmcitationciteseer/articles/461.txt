Relation extraction from text is a fundamental task in natural language processing. However, existing approaches often struggle with limited annotated data and heterogeneous graph structures. This paper proposes a novel meta-learning framework, 'HeteGraph', which leverages graph attention mechanisms to adapt to few-shot relation extraction tasks. We design a hierarchical attention module to capture both entity and relation dependencies, and a meta-optimizer to learn task-agnostic initializations. Experimental results on several benchmark datasets demonstrate that HeteGraph significantly outperforms state-of-the-art methods in few-shot settings, achieving near-supervised performance with only 5-10 labeled examples per relation.