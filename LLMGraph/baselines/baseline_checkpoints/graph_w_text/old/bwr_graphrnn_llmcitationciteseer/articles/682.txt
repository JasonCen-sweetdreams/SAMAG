Emotion recognition from multi-modal data, such as audio, video, and text, is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages the strengths of each modality to recognize emotions more accurately. Our approach uses a multi-level attention mechanism to selectively focus on relevant features from each modality, enabling the model to capture subtle emotional cues. Experimental results on three benchmark datasets demonstrate that our HAN model outperforms state-of-the-art methods in terms of recognition accuracy and computational efficiency.