Multimodal sentiment analysis is a challenging task due to the complexity of fusing and aligning features from diverse modalities such as text, images, and audio. We propose a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features at multiple levels of abstraction. Our HAN model consists of modality-specific attention modules and a fusion attention layer that adaptively weights the contributions of each modality. Experimental results on a benchmark multimodal dataset demonstrate that our approach outperforms state-of-the-art methods, achieving improved sentiment classification accuracy and robustness to noisy or missing modalities.