Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel hierarchical attention network (HAN) that selectively focuses on relevant modalities and segments of data to recognize emotions more efficiently. Our HAN model consists of modality-specific attention modules and a hierarchical fusion mechanism that adaptively weights the modalities based on their importance. Experimental results on benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in terms of accuracy and computational efficiency.