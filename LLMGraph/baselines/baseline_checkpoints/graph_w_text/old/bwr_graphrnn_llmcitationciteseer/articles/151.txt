Explainable AI (XAI) has gained significant attention in recent years, particularly in high-stakes applications where model interpretability is crucial. This paper proposes a novel multimodal fusion framework, 'HAFEX', which leverages hierarchical attention mechanisms to integrate visual, textual, and numerical features for improved explainability. HAFEX uses a modular architecture to generate feature importance scores, facilitating model-agnostic explanations for AI-driven decision-making processes. Experimental results on a real-world healthcare dataset demonstrate the effectiveness of HAFEX in enhancing model transparency and performance.