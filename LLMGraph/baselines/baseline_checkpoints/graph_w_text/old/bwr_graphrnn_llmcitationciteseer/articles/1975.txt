Query expansion is a crucial step in information retrieval (IR) systems, but its effectiveness is often limited in low-resource languages due to the scarcity of relevant document collections. This paper proposes a neural-based approach, 'NQE-LRL', that leverages a multi-task learning framework to jointly learn query expansion and document ranking. Our model incorporates a novel attention mechanism to adaptively weigh the importance of query terms and expansion candidates, resulting in improved retrieval performance. Experiments on several low-resource languages demonstrate the superiority of NQE-LRL over traditional query expansion methods, achieving a significant increase in mean average precision (MAP) and normalized discounted cumulative gain (NDCG).