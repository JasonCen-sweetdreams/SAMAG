As AI systems become increasingly pervasive, understanding their decision-making processes is crucial for building trust and facilitating human-AI collaboration. This paper presents a novel hierarchical attention-based framework, 'HARE', which generates interpretable explanations for AI-driven reasoning. By recursively applying attention mechanisms to identify salient pieces of information, HARE provides a transparent and hierarchical representation of the AI's decision-making process. Experimental results on a range of benchmark datasets demonstrate that HARE outperforms state-of-the-art explanation methods in terms of fidelity, relevance, and user satisfaction.