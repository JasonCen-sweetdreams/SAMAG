This paper presents a novel approach to distributed multi-agent planning, which integrates hierarchical task networks (HTNs) with reinforcement learning (RL). Our framework, called DMARL, enables agents to learn complex tasks by decomposing them into smaller subtasks and coordinating with other agents to achieve common goals. We demonstrate the effectiveness of DMARL in a simulated search-and-rescue scenario, where agents learn to adapt to dynamic environments and improve their planning and execution over time. Experimental results show that DMARL outperforms traditional planning-based approaches in terms of task completion rate and resource utilization.