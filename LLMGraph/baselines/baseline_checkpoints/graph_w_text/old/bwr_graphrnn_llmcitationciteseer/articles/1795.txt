Visual question answering (VQA) tasks require models to jointly process visual and linguistic inputs. Recent advances in multi-modal fusion have improved performance, but computational costs remain high. We propose a hierarchical attention network (HAN) that efficiently integrates visual and textual features using a novel, adaptive attention mechanism. Our approach selectively focuses on relevant regions of the image and question, reducing unnecessary computations. Experimental results on the VQA 2.0 dataset demonstrate that HAN outperforms state-of-the-art methods while achieving significant speedup and reduced memory usage.