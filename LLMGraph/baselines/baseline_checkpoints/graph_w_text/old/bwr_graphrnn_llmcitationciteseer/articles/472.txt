Neural retrieval models have shown remarkable performance in dense passage retrieval, but their computational overhead hinders their adoption in large-scale search applications. This paper proposes an efficient indexing method, dubbed 'NeuralInverter', which reduces the latency of neural retrieval models by up to 75% without sacrificing accuracy. NeuralInverter leverages a novel combination of hierarchical quantization and inverted indexing to enable fast and efficient nearest-neighbor search in the dense vector space. Our experiments on the BEIR benchmark demonstrate the efficacy of NeuralInverter in reducing the inference time of state-of-the-art neural retrieval models.