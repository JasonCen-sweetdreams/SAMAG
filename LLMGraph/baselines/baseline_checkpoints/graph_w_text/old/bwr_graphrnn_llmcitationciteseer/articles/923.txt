Deep reinforcement learning (DRL) has achieved state-of-the-art performance in various domains, but its vulnerability to adversarial attacks raises concerns. This paper explores targeted attacks on DRL policies, demonstrating that an adversary can significantly degrade the agent's performance by perturbing the environment state. We propose a novel defense mechanism, 'AdvShield', which leverages adversarial training and input preprocessing to improve the robustness of DRL policies. Experimental results on the CartPole and Atari benchmarks show that AdvShield can effectively counter adversarial attacks while maintaining the agent's performance in the absence of attacks.