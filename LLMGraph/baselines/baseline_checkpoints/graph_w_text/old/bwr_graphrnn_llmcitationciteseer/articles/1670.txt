Multimodal sentiment analysis has gained popularity in recent years, but existing methods often lack interpretability. This paper proposes a novel hierarchical attention network (HAN) architecture that incorporates both visual and textual features to analyze sentiment in multimodal data. Our approach learns to focus on relevant regions of images and corresponding text segments, generating attention weights that provide insights into the decision-making process. We evaluate our method on a large-scale multimodal dataset and demonstrate improved performance and explainability compared to state-of-the-art baselines.