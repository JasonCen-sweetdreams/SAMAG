Multi-label classification is a fundamental problem in machine learning, where an instance can belong to multiple classes simultaneously. However, existing approaches suffer from high computational complexity and ignore the inherent label correlations. This paper proposes a novel hierarchical attention network (HAN) that efficiently models label relationships and instance-level features. Our HAN architecture consists of a label-aware self-attention mechanism and a hierarchical fusion module, enabling the model to selectively focus on relevant labels and features. Experimental results on benchmark datasets demonstrate that our approach achieves state-of-the-art performance with significant computational savings.