Neural search models have shown promising results in information retrieval tasks, but often suffer from limited query representation capacity. This paper proposes a novel query expansion approach, 'ConEx', that leverages contrastive learning to generate effective expansion terms. By training a contrastive model to distinguish relevant from non-relevant documents, we learn a dense representation of query semantics that can be used to generate high-quality expansion terms. Experimental results on several benchmark datasets demonstrate significant improvements in retrieval performance over state-of-the-art query expansion methods.