Emotion recognition from multi-modal inputs (e.g., audio, video, text) is a challenging task due to the complexity of human emotions and the heterogeneity of modalities. This paper introduces a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and features for emotion recognition. Our HAN model consists of a modal attention mechanism to weigh the importance of each modality and a feature attention mechanism to identify salient features within each modality. Experimental results on the CMU-MOSEI dataset demonstrate that our approach achieves state-of-the-art performance and provides interpretable attention weights for explainable emotion recognition.