This paper presents a novel Hierarchical Attention Network (HAN) for multimodal emotion recognition, which leverages visual, acoustic, and linguistic cues to predict emotions in human-computer interactions. Our approach incorporates explainability techniques to provide insights into the decision-making process, enabling the identification of modality-specific emotion indicators. Experimental results on a large-scale multimodal dataset demonstrate the superiority of HAN over state-of-the-art methods, achieving a 12% improvement in emotion recognition accuracy while providing interpretable results.