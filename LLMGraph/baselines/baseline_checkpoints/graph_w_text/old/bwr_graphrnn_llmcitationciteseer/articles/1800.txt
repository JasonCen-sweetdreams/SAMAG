Multimodal sentiment analysis (MSA) combines visual and textual features to infer user sentiment. While deep learning models achieve high accuracy, their black-box nature hinders trust and interpretability. This paper proposes a novel hierarchical attention network (HAN) that integrates multimodal features and attention mechanisms to generate transparent and explainable sentiment predictions. Our HAN model incorporates a sentiment-aware attention module that adaptively weights visual and textual features based on their relevance to the sentiment polarity. Experimental results on two benchmark datasets demonstrate the effectiveness of our approach in improving MSA performance and providing insights into the decision-making process.