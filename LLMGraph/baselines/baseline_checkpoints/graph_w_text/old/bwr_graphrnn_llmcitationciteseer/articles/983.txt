Emotion recognition from multi-modal inputs, such as speech, text, and vision, remains a challenging task due to the inherent complexity and ambiguity of human emotions. This paper presents a novel hierarchical attention network (HAN) framework that leverages the strengths of each modality to recognize emotions more accurately. Our HAN architecture uses attention mechanisms to selectively focus on relevant features and modalities, providing explainable and interpretable results. Experimental evaluations on several benchmark datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance and providing insights into the relationships between modalities and emotions.