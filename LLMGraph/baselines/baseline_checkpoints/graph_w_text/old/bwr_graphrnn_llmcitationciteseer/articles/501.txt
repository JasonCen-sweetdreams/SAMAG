Distributed database systems have become increasingly popular due to their scalability and fault-tolerance. However, query optimization in such systems remains a challenging task. This paper proposes a novel approach that leverages machine learning to optimize query plans in distributed databases. We design a reinforcement learning framework that learns to predict optimal query plans based on historical query patterns and system workload. Experimental results on a real-world dataset demonstrate that our approach outperforms state-of-the-art query optimizers by up to 30% in terms of query execution time.