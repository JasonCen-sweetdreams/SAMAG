Visual question answering (VQA) models have achieved impressive performance, but their lack of transparency hinders their adoption in critical applications. We propose a novel hierarchical attention network (HAN) that generates interpretable attention maps and explanations for its question-answering process. Our HAN model employs a multi-scale attention mechanism that selectively focuses on relevant regions of the image and identifies key objects, attributes, and relationships. We evaluate our approach on the VQA-X dataset and demonstrate improved performance and explanation quality compared to state-of-the-art VQA models.