In virtual reality (VR) applications, accurate gaze estimation is crucial for foveated rendering, user experience analysis, and interaction design. This paper presents EyeGazeTracker, a deep learning-based approach that leverages convolutional neural networks (CNNs) and transfer learning to estimate gaze directions from VR headset-mounted cameras. Our method achieves state-of-the-art accuracy, outperforming existing gaze estimation techniques in VR scenarios. We also introduce a novel dataset, VR-Gaze, consisting of 10,000 annotated images, which we make publicly available for future research.