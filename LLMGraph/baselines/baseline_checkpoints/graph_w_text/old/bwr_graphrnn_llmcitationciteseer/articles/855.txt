Emotion recognition in human-robot interaction (HRI) is crucial for developing empathetic robots. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates multi-modal features from speech, text, and vision to recognize emotions in HRI. Our HAN model learns to focus on relevant modalities and time segments, improving robustness to noisy or missing data. We evaluate our approach on a newly collected dataset of human-robot interactions and demonstrate significant improvements in emotion recognition accuracy compared to state-of-the-art uni-modal and fusion-based methods.