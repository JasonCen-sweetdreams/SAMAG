Voice assistants have become ubiquitous, but users with dysarthria, a speech disorder characterized by impaired articulation, often struggle to interact with these systems. This paper presents a novel multimodal fusion approach that combines acoustic, linguistic, and visual features to improve the recognition accuracy of voice commands for users with dysarthria. We evaluate our approach using a dataset of 100 participants with dysarthria and demonstrate significant improvements in recognition accuracy and user satisfaction compared to traditional speech recognition systems. Our findings have implications for designing more inclusive and accessible voice assistants that can benefit individuals with speech disabilities.