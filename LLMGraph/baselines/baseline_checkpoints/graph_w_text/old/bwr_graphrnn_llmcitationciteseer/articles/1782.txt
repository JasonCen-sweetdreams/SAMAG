This paper presents EmoReact, a novel framework for affective state inference from multimodal interaction cues, aiming to enrich human-computer interaction (HCI). EmoReact fuses computer vision, speech processing, and machine learning techniques to analyze facial expressions, vocal tone, and linguistic patterns from user interactions. Our approach leverages a hierarchical attention mechanism to weigh the contributions of each modality, enabling accurate recognition of emotions such as joy, anger, and frustration. We evaluate EmoReact on a publicly available dataset, demonstrating improved affective state recognition compared to state-of-the-art methods. The implications of EmoReact for adaptive HCI systems, such as personalized recommendations and empathetic interfaces, are discussed.