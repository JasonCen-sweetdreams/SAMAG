Traditional ranking models in ad-hoc retrieval rely on handcrafted features and lexical matching, which can be limited in capturing complex semantic relationships. This paper proposes a novel deep neural ranking model, 'DeepRank', which leverages pre-trained language models and a hierarchical attention mechanism to capture document relevance. Our experiments on the TREC 2004 dataset demonstrate that DeepRank significantly outperforms state-of-the-art models in terms of mean average precision and normalized discounted cumulative gain. We also explore the effect of different pre-training objectives and attention weights on model performance.