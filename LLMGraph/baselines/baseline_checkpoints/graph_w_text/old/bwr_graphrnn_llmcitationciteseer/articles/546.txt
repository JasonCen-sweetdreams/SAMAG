Explainability is a crucial aspect of multi-agent reinforcement learning (MARL) systems, as they interact with humans in complex environments. This paper proposes a novel Hierarchical Attention Network (HAN) architecture to learn interpretable representations of agent interactions. Our approach combines attention mechanisms with hierarchical clustering to identify influential agents and their interactions. We evaluate HAN on two MARL benchmarks, demonstrating improved explainability and superior performance compared to state-of-the-art methods. We also provide visualizations of attention weights, enabling humans to understand agent decision-making processes.