Multimodal information retrieval systems face challenges in capturing the semantic relationships between different modalities. This paper presents a novel approach to query expansion, leveraging semantic embeddings to incorporate multimodal context into the retrieval process. Our proposed method, 'SemEx', learns a shared embedding space for text, image, and video features, enabling efficient computation of semantic similarities between queries and documents. Experimental results on a large-scale multimodal dataset demonstrate significant improvements in retrieval performance, particularly for queries with limited textual context.