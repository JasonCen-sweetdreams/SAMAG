Sentiment analysis has become increasingly important in natural language processing, but existing approaches often struggle with multi-modal data that combines text, images, and audio. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) architecture that integrates graph neural networks and attention mechanisms to capture complex relationships between modalities. Our approach outperforms state-of-the-art methods on three benchmark datasets, demonstrating improved sentiment prediction accuracy and robustness to noisy or missing modalities. We also provide insights into the learned attention weights, revealing interpretable patterns of modality importance.