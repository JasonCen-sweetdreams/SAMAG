Visual question answering (VQA) tasks require the integration of visual and textual information to generate accurate responses. This paper proposes a novel hierarchical attention network (HAN) that facilitates efficient multi-modal fusion in VQA. Our approach leverages a stacked attention mechanism to selectively focus on relevant regions of the image and corresponding question words, allowing for more effective feature integration. Experimental results on the VQA 2.0 dataset demonstrate that our HAN model achieves state-of-the-art performance while reducing computational overhead by 30% compared to existing fusion-based approaches.