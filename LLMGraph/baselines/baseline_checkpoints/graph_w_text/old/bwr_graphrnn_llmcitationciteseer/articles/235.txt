Multi-modal emotion recognition (MER) has gained significant attention in recent years, but existing approaches often suffer from high computational costs and limited generalizability. This paper proposes a novel hierarchical attention network (HAN) that leverages the complementary information from different modalities (e.g., speech, text, and vision) to improve MER performance. Our HAN architecture consists of modality-specific attention modules and a hierarchical fusion mechanism, enabling efficient and adaptive feature extraction. Experimental results on three benchmark datasets demonstrate the superiority of our approach over state-of-the-art MER models, achieving an average improvement of 12.5% in F1-score while reducing inference time by 30%.