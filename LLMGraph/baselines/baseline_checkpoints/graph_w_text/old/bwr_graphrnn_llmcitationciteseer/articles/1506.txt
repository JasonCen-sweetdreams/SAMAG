Accurate medical diagnosis relies on effectively combining information from multiple modalities, such as medical images, clinical reports, and genomic data. This paper proposes a novel hierarchical attention network (HAN) architecture, which learns to selectively focus on relevant features and modalities during the fusion process. Our approach achieves state-of-the-art performance on a benchmark dataset of multi-modality medical diagnoses, while providing interpretable attention weights that enable explainability. We demonstrate the applicability of HAN to various medical diagnosis tasks, including disease classification, segmentation, and survival analysis.