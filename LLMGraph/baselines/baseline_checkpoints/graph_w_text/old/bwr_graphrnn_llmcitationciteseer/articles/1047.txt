Decentralized task allocation for autonomous agents is a challenging problem in multi-agent systems. Existing methods often rely on centralized controllers or assume complete knowledge of the environment. This paper proposes a novel framework, 'Dec-MARL', that leverages multi-agent reinforcement learning to enable decentralized task allocation. Each agent learns to optimize its own policy while communicating with its neighbors to achieve global optimality. We evaluate Dec-MARL on a simulated disaster response scenario and demonstrate significant improvements in task completion time and agent utility compared to existing decentralized methods.