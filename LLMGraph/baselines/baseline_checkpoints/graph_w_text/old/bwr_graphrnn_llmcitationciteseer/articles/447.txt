Robot arm control systems often rely on hand-tuned controllers or model-based reinforcement learning (RL) methods, which can be inefficient and limiting. This paper proposes a hierarchical RL framework, 'HierArm', that leverages a high-level task encoder to decompose complex tasks into simpler sub-tasks, allowing for more efficient exploration and policy learning. We demonstrate the effectiveness of HierArm on a real-world robot arm platform, achieving improved task completion rates and reduced energy consumption compared to flat RL baselines.