Deep reinforcement learning (DRL) has shown promise in autonomous vehicle control, but existing methods suffer from high computational complexity and limited generalizability. This paper presents a novel approach that leverages graph neural networks (GNNs) to learn a compact and transferable representation of the environment. Our method, 'GraphDRL', incorporates GNNs into the DRL framework to model complex relationships between objects in the scene. We demonstrate significant improvements in control policy efficiency and adaptability to new scenarios, achieving state-of-the-art performance on the CARLA benchmark.