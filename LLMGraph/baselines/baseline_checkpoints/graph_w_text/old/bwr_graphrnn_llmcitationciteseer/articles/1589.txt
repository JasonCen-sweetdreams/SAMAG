Distributed database systems have become increasingly popular for handling large-scale data processing tasks. However, optimizing queries in such systems remains a challenging task due to the complexity of data distribution and query patterns. This paper proposes a novel approach to query optimization using machine learning techniques. We design a reinforcement learning framework that learns to optimize queries based on runtime statistics and query patterns. Our approach outperforms traditional query optimization techniques in terms of query execution time and resource utilization. Experiments on a real-world distributed database system demonstrate the effectiveness of our approach.