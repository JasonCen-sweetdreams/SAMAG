Autonomous driving in dynamic environments requires adaptability to changing scenarios and agent interactions. This paper presents a meta-reinforcement learning approach, 'DynAdapt', that enables autonomous vehicles to learn from experience and adapt to new situations. We introduce a novel meta-policy that leverages graph neural networks to model agent interactions and a hierarchical exploration strategy to balance exploitation and exploration. Experimental results on a realistic simulation platform demonstrate that DynAdapt outperforms traditional reinforcement learning methods in terms of navigation efficiency and safety in dynamic environments.