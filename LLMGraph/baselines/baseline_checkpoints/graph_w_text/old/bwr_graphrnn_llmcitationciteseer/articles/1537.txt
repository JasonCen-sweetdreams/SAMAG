Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the variability and complexity of human emotions. This paper proposes a novel Hierarchical Attention Network (HAN) that leverages the strengths of each modality to improve recognition accuracy. Our HAN model consists of modality-specific attention layers that selectively focus on relevant features, followed by a hierarchical fusion mechanism that integrates the outputs from each modality. Experimental results on the benchmark IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art methods, achieving an average F1-score of 85.2% across all emotion categories.