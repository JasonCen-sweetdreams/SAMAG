We propose a decentralized multi-agent reinforcement learning (MARL) framework for autonomous traffic control, where multiple agents learn to optimize traffic flow in a distributed manner. Our approach, called 'Traffic Harmony', leverages graph neural networks to model the complex interactions between agents and the environment. We introduce a novel communication protocol that enables agents to share information and coordinate their actions in real-time, while ensuring scalability and robustness to varying traffic conditions. Experimental results on a simulated traffic network demonstrate that Traffic Harmony outperforms traditional centralized control methods in terms of reduced congestion and travel times.