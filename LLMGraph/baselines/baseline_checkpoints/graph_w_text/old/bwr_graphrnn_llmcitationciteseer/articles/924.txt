Emotion recognition in conversational AI is a challenging task due to the complexity of human emotions and the variability of input modalities. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates acoustic, linguistic, and visual features to recognize emotions in human-computer interactions. Our approach applies attention mechanisms at multiple levels to selectively focus on salient features and contextual dependencies. Experimental results on a large-scale multimodal emotion recognition dataset demonstrate the superiority of HAN over state-of-the-art methods, achieving a 12.5% relative improvement in weighted F1-score.