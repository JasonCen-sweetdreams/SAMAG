Emotion recognition in human-computer interaction (HCI) is a challenging task due to the complexity of human emotions and the multimodal nature of emotional expressions. This paper proposes a novel hierarchical attention network (HAN) framework that integrates visual, audio, and textual features to recognize emotions in HCI. Our HAN model employs a bottom-up attention mechanism to selectively focus on relevant features across modalities and a top-down attention mechanism to capture emotional context. Experimental results on a large-scale multimodal dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions and achieving better interpretability.