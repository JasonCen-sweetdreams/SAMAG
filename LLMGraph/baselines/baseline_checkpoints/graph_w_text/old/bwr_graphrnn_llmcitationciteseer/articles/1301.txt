 Gesture recognition systems have improved significantly, but they often fail to accommodate users with disabilities. This paper presents a novel approach to designing adaptive gesture recognition models that can learn from and adapt to individual users' abilities. We propose a multi-modal fusion framework that combines computer vision, machine learning, and cognitive modeling to recognize gestures in a user-centered manner. Our approach is evaluated through a user study involving participants with varying abilities, demonstrating improved recognition accuracy and user experience. The results have implications for designing more inclusive and accessible human-computer interfaces.