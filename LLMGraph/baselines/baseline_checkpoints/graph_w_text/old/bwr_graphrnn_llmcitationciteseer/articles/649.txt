Knowledge graph embedding (KGE) is a crucial step in various AI applications, but existing methods often struggle with scalability and performance on large, complex graphs. This paper presents a novel KGE approach, HGA-KGE, which leverages hierarchical graph attention networks to capture both local and global structural patterns. By recursively applying attention mechanisms at different granularities, HGA-KGE effectively encodes long-range dependencies and improves entity representation. Experimental results on popular benchmarks demonstrate the superiority of HGA-KGE over state-of-the-art methods in terms of link prediction and entity classification tasks.