Explainability is crucial for real-world deployment of reinforcement learning (RL) agents in autonomous vehicles, as it enables understanding and trust in their decision-making processes. This paper introduces 'ExplainRL', a novel framework that generates interpretable explanations for RL policies in complex, dynamic environments. We propose a hierarchical attention mechanism that highlights critical state features and agent actions contributing to the decision-making process. Experimental results on a simulated autonomous driving environment demonstrate that ExplainRL improves transparency and accountability of RL-based decision-making, paving the way for widespread adoption in the automotive industry.