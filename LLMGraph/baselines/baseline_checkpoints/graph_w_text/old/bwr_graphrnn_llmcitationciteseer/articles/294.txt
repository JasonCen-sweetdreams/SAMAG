Sentiment analysis tasks often involve multi-modal inputs, such as text, images, and audio. However, existing approaches lack interpretability and fail to capture complex relationships between modalities. We propose Hierarchical Graph Attention Networks (HGAT), a novel framework that models multi-modal interactions using a hierarchical graph structure. HGAT incorporates attention mechanisms to selectively focus on relevant nodes and modalities, enabling interpretable sentiment predictions. Experimental results on a new multi-modal dataset demonstrate that HGAT outperforms state-of-the-art methods while providing insightful visualizations of modality-specific sentiment contributions.