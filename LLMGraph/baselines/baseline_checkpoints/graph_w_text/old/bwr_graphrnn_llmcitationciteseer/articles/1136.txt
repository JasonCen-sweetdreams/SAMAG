Multimodal sentiment analysis has gained popularity with the increasing availability of multimedia data. However, existing approaches often fail to effectively capture complex relationships between modalities. This paper proposes a novel hierarchical attention framework, 'HIMA', that leverages both intra-modal and inter-modal attention mechanisms to selectively focus on relevant features. We evaluate HIMA on three benchmark datasets, demonstrating significant improvements in sentiment analysis accuracy and robustness compared to state-of-the-art multimodal fusion methods.