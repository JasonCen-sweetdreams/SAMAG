With the proliferation of edge computing, there is a growing need for efficient neural networks that can run on resource-constrained devices. This paper proposes a novel neural architecture search (NAS) framework, 'EdgeNAS', which leverages a combination of reinforcement learning and gradient-based optimization to search for optimal architectures under device-specific constraints. Our approach reduces the search space by 3x compared to existing NAS methods, while achieving state-of-the-art accuracy on multiple edge computing benchmarks. We demonstrate the efficacy of EdgeNAS on a real-world edge computing platform, showcasing significant reductions in latency and energy consumption.