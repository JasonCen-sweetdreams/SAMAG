Explainability is a crucial aspect of modern recommendation systems, as it fosters trust and transparency in AI-driven decision-making. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) model that leverages graph-based representations of user-item interactions to provide personalized explanations. HGAT learns to attend to relevant nodes and edges in the graph, generating interpretable recommendations that highlight the most influential factors. Experimental results on three benchmark datasets demonstrate significant improvements in recommendation accuracy and explanation quality compared to state-of-the-art methods.