Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a challenging task, particularly when explaining the reasoning behind the predicted emotions. This paper introduces a novel Hierarchical Attention Network (HAN) architecture that incorporates attention mechanisms at multiple levels to selectively focus on relevant modalities, features, and temporal segments. Our approach achieves state-of-the-art performance on the EmoReact dataset, while providing interpretable attention weights that reveal the contribution of each modality to the predicted emotions. We demonstrate the effectiveness of HAN in identifying subtle emotional cues and providing personalized feedback in human-computer interaction scenarios.