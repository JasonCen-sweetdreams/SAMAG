Emotion recognition is crucial in human-robot interaction (HRI) to ensure empathetic and personalized responses. This paper proposes a hierarchical attention network (HAN) to recognize emotions from multimodal input, including facial expressions, speech, and physiological signals. Our HAN model learns to selectively focus on relevant modalities and time segments, outperforming state-of-the-art methods in emotion recognition accuracy. We evaluate our approach on a novel HRI dataset, featuring diverse emotional scenarios and robot feedback mechanisms, demonstrating improved user experience and satisfaction.