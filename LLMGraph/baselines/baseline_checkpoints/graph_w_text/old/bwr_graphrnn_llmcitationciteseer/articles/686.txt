Multimodal sentiment analysis has gained significant attention in recent years, but existing approaches often struggle to provide interpretable results. This paper proposes a novel hierarchical attention network (HAN) that integrates visual, textual, and acoustic features for sentiment analysis. Our HAN utilizes attention mechanisms at multiple levels to selectively focus on relevant input features, enabling the model to provide explicit explanations for its predictions. We evaluate our approach on a multimodal benchmark dataset and demonstrate its effectiveness in improving both sentiment accuracy and explainability. Furthermore, we conduct an extensive ablation study to investigate the contributions of individual components to the overall performance.