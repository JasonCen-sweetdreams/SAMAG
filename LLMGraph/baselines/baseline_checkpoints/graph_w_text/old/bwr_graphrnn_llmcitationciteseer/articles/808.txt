Gesture-based interfaces have become increasingly popular, but users with motor impairments often struggle to interact with them. This paper presents a novel approach to designing adaptive accessibility features for gesture-based interfaces. We propose a machine learning model that learns to recognize and adapt to individual users' abilities and preferences in real-time. Our approach combines computer vision and machine learning techniques to analyze user gestures and provide personalized accommodations, such as gesture simplification, speed adjustment, and feedback customization. A user study with participants with motor impairments demonstrates significant improvements in interaction accuracy and user satisfaction.