Multimodal sentiment analysis (MSA) has gained significance in human-computer interaction, but existing models often lack transparency and interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates visual, acoustic, and textual features for MSA. Our approach employs a hierarchical attention mechanism to weigh the importance of different modalities and features, enabling explainable sentiment predictions. Experimental results on three benchmark datasets demonstrate that our HAN model outperforms state-of-the-art methods in terms of accuracy and F1-score, while providing insightful attention visualizations for interpretable decision-making.