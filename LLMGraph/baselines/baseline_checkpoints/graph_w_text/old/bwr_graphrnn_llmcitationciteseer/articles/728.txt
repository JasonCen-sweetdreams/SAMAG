This paper presents a novel decentralized task allocation framework for multi-agent systems using reinforcement learning. The proposed approach, called 'DRL-TA', enables agents to learn optimal task allocation policies in a distributed manner, without relying on a centralized controller. We model the task allocation problem as a Markov game and employ a decentralized actor-critic algorithm to learn the policies. Experimental results on a simulated disaster response scenario demonstrate that DRL-TA outperforms traditional centralized allocation methods in terms of task completion time and overall system efficiency.