Multi-agent systems often rely on complex decision-making processes that are difficult to interpret. This paper proposes a novel hierarchical attention network (HAN) architecture for explainable decision-making in multi-agent settings. Our approach enables agents to selectively focus on relevant features and communicate effectively with each other. We evaluate our method on a series of multi-agent coordination tasks and demonstrate improved performance and interpretability compared to existing approaches. Our results have implications for the development of transparent and trustworthy AI systems in real-world applications.