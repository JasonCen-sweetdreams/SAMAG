Sentiment analysis on multi-modal data (e.g., text, images, and videos) is crucial for various applications, but existing models suffer from high computational costs and limited interpretability. We propose a hierarchical attention network (HAN) that leverages intra-modal and inter-modal relationships to efficiently process large-scale multi-modal datasets. Our HAN model incorporates a novel attention mechanism that adaptively weights modalities based on their relevance to the sentiment task. Experimental results on benchmark datasets demonstrate that our approach achieves state-of-the-art performance while reducing computational costs by up to 40% compared to existing methods.