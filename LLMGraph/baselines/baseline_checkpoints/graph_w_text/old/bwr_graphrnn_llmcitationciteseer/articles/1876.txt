Distributed database systems have become increasingly popular for handling large-scale data processing tasks. However, query optimization in these systems remains a challenging problem. This paper proposes a novel approach to query optimization using reinforcement learning. We design a reward function that captures the trade-off between query latency and resource utilization, and train an agent to optimize query plans using a deep Q-network. Our experiments on a real-world distributed database system demonstrate that our approach outperforms traditional query optimization techniques by up to 30% in terms of query latency, while reducing resource utilization by up to 25%.