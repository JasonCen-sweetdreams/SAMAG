Explainability is crucial in multi-agent systems, where decision-making processes involve complex interactions among agents. This paper introduces a novel Hierarchical Attention Network (HAN) architecture that learns to identify and weight relevant factors influencing agent decisions. Our approach leverages attention mechanisms to disentangle the contributions of different agents, environments, and timesteps, providing transparent and interpretable decision-making processes. Experimental results on a real-world autonomous vehicle dataset demonstrate the effectiveness of HAN in improving decision accuracy and explainability compared to existing methods.