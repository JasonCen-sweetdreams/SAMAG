Emotion recognition from multi-modal data (e.g., speech, text, and vision) is crucial in human-computer interaction. However, existing methods often rely on early fusion or simple concatenation of modalities, neglecting the complex relationships between them. We propose a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and features at multiple levels of abstraction. Experiments on three benchmark datasets demonstrate that HAN outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an average F1-score improvement of 7.2%. We also provide insights into the learned attention patterns, shedding light on the importance of each modality in emotion recognition.