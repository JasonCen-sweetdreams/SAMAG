Natural language inference (NLI) has made significant strides with the advent of deep learning models. However, their lack of interpretability hinders trust and understanding in their decision-making processes. This paper proposes a novel hierarchical attention network (HAN) architecture that incorporates explainability into NLI. Our model leverages attention weights to identify salient contextual clues and semantic roles, providing transparent reasoning behind its predictions. Experimental results on benchmark datasets demonstrate that HAN achieves state-of-the-art performance while offering insightful visualizations of its inference process.