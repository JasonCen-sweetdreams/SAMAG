Emotion recognition from multi-modal data sources (e.g., text, audio, vision) is a challenging task due to the inherent complexity of emotional expressions. This paper introduces a novel hierarchical graph attention network (HGAT) that leverages both intra- and inter-modal relationships to capture nuanced emotional cues. Our HGAT model consists of a graph attention encoder that aggregates features from each modality, followed by a hierarchical fusion mechanism that adaptively weighs the importance of each modality. Experimental results on three benchmark datasets demonstrate that HGAT outperforms state-of-the-art methods in recognizing emotions from multi-modal data, with significant improvements in F1-score and correlation coefficient.