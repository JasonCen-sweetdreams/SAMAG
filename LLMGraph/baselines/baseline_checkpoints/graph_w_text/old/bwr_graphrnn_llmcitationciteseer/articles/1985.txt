Emotion recognition from multi-modal data, such as speech, text, and vision, is a complex task that requires effective fusion of heterogeneous features. This paper proposes a hierarchical attention network (HAN) that selectively focuses on relevant modalities and features to improve emotion recognition accuracy. We introduce a novel modal attention mechanism that adaptively weights the contributions of each modality based on their relative importance in the input data. Experiments on two benchmark datasets demonstrate that our HAN model outperforms state-of-the-art methods in terms of recognition accuracy and provides interpretable insights into the emotional cues used for recognition.