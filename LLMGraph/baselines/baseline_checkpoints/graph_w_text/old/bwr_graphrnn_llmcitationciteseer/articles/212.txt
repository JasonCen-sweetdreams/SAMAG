Multi-modal dialogue systems require the integration of visual and linguistic information to generate contextually relevant responses. This paper presents a novel knowledge graph embedding approach, 'KG-Align', that leverages hierarchical graph attention to jointly learn entity and relationship representations from large-scale knowledge graphs. We demonstrate the effectiveness of KG-Align in improving response generation quality and reducing inference latency in a multi-modal dialogue system. Experimental results on the Visual Dialog dataset show that KG-Align outperforms state-of-the-art methods in both accuracy and efficiency metrics.