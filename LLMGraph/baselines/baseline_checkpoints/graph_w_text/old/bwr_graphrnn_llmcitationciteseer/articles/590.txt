Emotion recognition in conversational AI is a challenging task due to the complexity of human emotions and the variability of modalities involved. This paper proposes a novel hierarchical attention framework, 'HEMA', which integrates multimodal features from speech, text, and facial expressions to recognize emotions in conversations. HEMA employs a hierarchical attention mechanism to selectively focus on relevant modalities and temporal segments, leading to improved emotion recognition accuracy. Experimental results on the IEMOCAP dataset demonstrate that HEMA outperforms state-of-the-art multimodal fusion approaches and achieves robust performance in real-world conversational scenarios.