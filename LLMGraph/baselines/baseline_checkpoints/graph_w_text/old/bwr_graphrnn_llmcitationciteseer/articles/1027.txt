Emotion recognition from multi-modal data (e.g., speech, text, and vision) has been a long-standing challenge. This paper presents a novel hierarchical graph attention network (HGAN) that leverages the complementary information from different modalities. Our approach first constructs a graph to model the relationships between modalities and then applies a hierarchical attention mechanism to selectively focus on relevant modalities and features. Experimental results on the CMU-MOSEI dataset demonstrate that HGAN outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an improvement of 12.5% in F1-score.