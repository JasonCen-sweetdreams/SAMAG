In multi-agent reinforcement learning, it is crucial to understand the decision-making process of individual agents to ensure cooperation and avoid conflicts. We propose a hierarchical attention network (HAN) that learns to focus on relevant agents and their interactions, providing interpretable policy explanations. Our HAN architecture consists of two levels of attention: agent-level attention that identifies influential agents, and interaction-level attention that highlights critical interactions. Experimental results on a variety of multi-agent environments demonstrate that our approach outperforms state-of-the-art methods in terms of both policy performance and explanation quality.