Autonomous vehicles require efficient decision-making algorithms to navigate complex scenarios. This paper proposes a hierarchical reinforcement learning framework, 'HRL-AV', which leverages a two-level architecture to balance exploration and exploitation. The high-level policy optimizes long-term goals, such as route planning, while the low-level policy focuses on short-term actions, like lane changing and obstacle avoidance. We demonstrate the effectiveness of HRL-AV in simulated urban environments, achieving improved decision-making and reduced crashes compared to flat reinforcement learning approaches.