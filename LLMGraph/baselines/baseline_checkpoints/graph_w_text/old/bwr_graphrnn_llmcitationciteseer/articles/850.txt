Emotion recognition from multi-modal inputs, such as speech, text, and vision, is a challenging task due to the complexities of human emotional expression. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that selectively focuses on relevant features from each modality and hierarchically integrates them to recognize emotions. Our approach outperforms state-of-the-art methods on the IEMOCAP dataset, achieving an accuracy of 84.2% with a 30% reduction in computational cost. We also demonstrate the model's robustness to noisy or missing inputs, making it suitable for real-world applications.