Virtual reality (VR) has the potential to revolutionize user experiences, but existing interaction modalities can be inaccessible to individuals with motor impairments. This paper presents a novel gaze-based interaction system for VR, leveraging machine learning-based eye tracking and pupillary response analysis. Our approach enables users to navigate and interact with virtual environments using only their gaze, achieving an average accuracy of 92.5% in user studies. We also demonstrate the applicability of our system in a virtual museum scenario, showcasing its potential for enhancing accessibility in various domains.