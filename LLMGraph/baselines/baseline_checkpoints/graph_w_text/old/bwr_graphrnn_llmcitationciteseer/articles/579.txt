Developing intelligent dialogue systems that can engage in multi-modal conversations with humans requires a deep understanding of contextual information. This paper presents a novel hierarchical attention network (HAN) architecture that integrates visual, acoustic, and textual features to generate context-aware responses. Our approach leverages self-attention mechanisms to selectively focus on relevant modalities and utterance segments, enabling the model to provide explanations for its responses. Experimental results on the Multi-Modal Dialogue (MMD) dataset demonstrate that our HAN model outperforms state-of-the-art baselines in terms of response accuracy and explanation quality.