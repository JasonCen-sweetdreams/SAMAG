Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a crucial task in affective computing. Existing approaches often rely on complex fusion strategies or large-scale neural networks, leading to high computational costs. We propose a novel Hierarchical Attention Network (HAN) that efficiently integrates information from multiple modalities. Our model employs a hierarchical attention mechanism to selectively focus on relevant features at different levels of abstraction, reducing the dimensionality of the input space. Experimental results on the benchmark EmoReact dataset demonstrate that HAN outperforms state-of-the-art methods in terms of accuracy and inference speed.