Emotion recognition is a crucial aspect of human-computer interaction, enabling more empathetic and personalized systems. This paper proposes a novel hierarchical attention network (HAN) architecture for multimodal emotion recognition, leveraging facial expressions, speech, and physiological signals. Our approach learns to selectively focus on informative modalities and features, improving recognition accuracy and robustness. Experiments on a large, publicly available dataset demonstrate the effectiveness of HAN in recognizing emotions in real-world scenarios, outperforming state-of-the-art multimodal fusion methods.