Deep reinforcement learning (DRL) has achieved remarkable success in various applications, but its opacity hinders trust and understanding. This paper presents a novel explainability framework, 'RL-Explain', which generates interpretable explanations for DRL policies. We introduce a model-agnostic approach that leverages saliency maps and attention mechanisms to identify critical state features and actions contributing to the policy's decision-making process. Experimental results on Atari games and a real-world robotics task demonstrate that RL-Explain improves the transparency and accountability of DRL agents while maintaining their performance.