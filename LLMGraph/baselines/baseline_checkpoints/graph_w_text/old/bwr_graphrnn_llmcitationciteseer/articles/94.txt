Explainable AI (XAI) techniques, such as LIME and SHAP, aim to provide insights into machine learning model decisions. However, their robustness against adversarial attacks remains understudied. This paper presents a systematic evaluation of the vulnerability of LIME and SHAP to targeted attacks, designed to manipulate model explanations. Our results show that both techniques can be deceived by carefully crafted perturbations, leading to misleading explanations. We propose a novel attack framework, 'Explainability-Fool', which generates adversarial examples that exploit the underlying assumptions of XAI methods. Our findings highlight the need for more robust and secure XAI techniques in high-stakes applications.