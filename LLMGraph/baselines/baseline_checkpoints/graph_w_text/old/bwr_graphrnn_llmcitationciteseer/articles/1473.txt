Deep neural networks have achieved state-of-the-art performance in various machine learning tasks, but their performance is heavily dependent on the choice of hyperparameters. Bayesian optimization has been shown to be an effective method for hyperparameter tuning, but it can be computationally expensive. This paper proposes a novel approach to Bayesian optimization that leverages gradient-based acquisition functions and early stopping to reduce the computational cost. We evaluate our approach on several benchmark datasets and show that it achieves comparable or better performance than existing methods while requiring significantly fewer evaluations.