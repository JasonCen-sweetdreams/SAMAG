Multimodal emotion recognition is a challenging task that requires processing and integrating information from multiple sources, such as speech, text, and vision. This paper proposes a novel hierarchical attention network (HAN) that adaptively weighs and combines the output from modality-specific encoders. Our HAN model uses a stacked attention mechanism to selectively focus on relevant regions of the input data, both within and across modalities. Experimental results on the Multimodal Emotion Recognition (MER) benchmark dataset demonstrate that our approach outperforms state-of-the-art methods, achieving an average F1-score of 83.2% on the test set.