Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a crucial task in affective computing. However, existing approaches often suffer from high computational costs and neglect the inherent hierarchical structure of emotional cues. We propose a novel Hierarchical Attention Network (HAN) that leverages self-attention mechanisms to selectively focus on relevant modalities and emotional features at multiple scales. Experimental results on the IEMOCAP and SEMAINE datasets demonstrate that HAN achieves state-of-the-art performance in terms of emotion recognition accuracy and computational efficiency.