Neural architecture search (NAS) has revolutionized the field of deep learning, but its computational cost and reliance on large datasets hinder its applicability to few-shot learning scenarios. This paper presents a novel meta-learning approach, MetaNAS, which learns to search for optimal architectures across a variety of few-shot learning tasks. By leveraging a meta-learned initialization strategy and a lightweight search algorithm, MetaNAS achieves state-of-the-art performance on several few-shot image classification benchmarks while reducing the search cost by an order of magnitude. We demonstrate the versatility of MetaNAS by adapting it to different task distributions and neural architecture spaces.