Time series forecasting models often lack interpretability, making it challenging to understand their predictions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that incorporates both local and global attention mechanisms to generate explainable forecasts. Our approach learns to focus on relevant temporal dependencies and features, providing insights into the forecasting process. Experiments on several benchmark datasets demonstrate that HAN outperforms state-of-the-art models in terms of accuracy and obtains meaningful attention weights, enabling model interpretability.