Time series forecasting plays a vital role in various domains, including finance, healthcare, and climate science. Despite the success of deep learning models in this task, they often suffer from a lack of interpretability, making it challenging to understand the underlying decision-making process. This paper proposes a novel hierarchical attention network (HAN) architecture that incorporates explainability mechanisms to improve the transparency of time series forecasting models. Our approach utilizes a multi-level attention mechanism to selectively focus on relevant time series segments and features, enabling the model to provide feature importance scores and attention weights that justify its predictions. Experimental results on several benchmark datasets demonstrate the effectiveness of HAN in achieving state-of-the-art forecasting performance while providing meaningful insights into the model's decision-making process.