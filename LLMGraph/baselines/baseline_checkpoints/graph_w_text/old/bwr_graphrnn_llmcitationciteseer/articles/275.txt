Neural retrieval models have achieved state-of-the-art performance in various information retrieval tasks. However, their effectiveness heavily relies on the quality of the training data and the relevance feedback mechanism. This paper proposes a novel hierarchical relevance feedback framework, HRF-Net, which leverages both explicit and implicit user feedback to refine the neural model's understanding of relevance. Our experiments on the TREC-2020 dataset demonstrate that HRF-Net outperforms existing relevance feedback methods, particularly in cold-start scenarios where user feedback is limited. We also provide an in-depth analysis of the feedback hierarchies and their impact on model performance.