Multimodal sentiment analysis (MSA) is a challenging task due to the inherent ambiguity and uncertainty in human emotions. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates linguistic, visual, and acoustic features to capture complex sentiment dynamics. We introduce a Bayesian uncertainty estimation module to quantify the model's confidence in its predictions, enabling the identification of uncertain or ambiguous samples. Experimental results on the CMU-MOSI dataset demonstrate that our approach outperforms state-of-the-art MSA methods and provides valuable insights into the uncertainty of sentiment predictions.