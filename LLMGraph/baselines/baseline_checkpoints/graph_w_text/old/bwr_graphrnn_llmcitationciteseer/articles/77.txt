Neural search models have shown promising results in information retrieval tasks, but often struggle with limited query context. This paper proposes a novel query expansion approach that leverages contrastive learning to capture semantic relationships between queries and documents. Our method, dubbed 'ContrastiveQE', uses a bi-encoder architecture to learn query and document embeddings that are optimized for mutual information. Experimental results on several benchmark datasets demonstrate significant improvements in retrieval performance, particularly for sparse queries, and outperform existing query expansion techniques.