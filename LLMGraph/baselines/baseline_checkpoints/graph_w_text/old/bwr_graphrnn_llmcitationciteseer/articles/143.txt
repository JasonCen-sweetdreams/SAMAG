Emotion recognition from multi-modal inputs, such as speech, text, and vision, remains a challenging task in affective computing. This paper proposes a novel hierarchical attention network (HAN) architecture that leverages the strengths of each modality to improve emotion recognition accuracy. Our HAN model consists of modality-specific attention modules that learn to weigh the importance of different features, followed by a higher-level attention layer that integrates the outputs from each modality. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods, achieving an average F1-score improvement of 12.5% across different emotion categories.