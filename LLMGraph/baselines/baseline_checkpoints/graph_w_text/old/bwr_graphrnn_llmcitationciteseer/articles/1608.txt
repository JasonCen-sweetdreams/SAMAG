Existing image-text retrieval models often rely on fixed, hand-crafted features or simplistic interaction mechanisms, limiting their performance. This paper introduces 'HiGATE', a novel multi-modal retrieval framework that leverages hierarchical graph attention to model complex relationships between images and texts. By iteratively refining node representations and attention weights, HiGATE captures nuanced semantic correspondences between visual and linguistic features. Experiments on several benchmark datasets demonstrate significant improvements in retrieval performance, particularly for challenging cases with ambiguous or abstract concepts.