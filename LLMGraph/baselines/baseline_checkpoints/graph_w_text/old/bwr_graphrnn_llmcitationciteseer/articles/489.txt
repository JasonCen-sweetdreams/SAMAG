Zero-shot learning (ZSL) enables image classification models to recognize unseen classes without additional training data. However, existing ZSL methods often rely on pre-defined class attributes or annotations, which can be costly to obtain. This paper proposes a novel contrastive meta-learning approach, 'MetaZSL', that learns to adapt to unseen classes without requiring explicit attribute annotations. Our method leverages a contrastive loss function to align the feature representations of seen and unseen classes, enabling effective zero-shot classification. Experimental results on benchmark datasets demonstrate that MetaZSL outperforms state-of-the-art ZSL methods, achieving up to 15% improvement in classification accuracy.