Graph neural networks (GNNs) have achieved state-of-the-art performance in node classification tasks, but their lack of interpretability hinders their adoption in high-stakes applications. This paper proposes a novel graph attention mechanism, 'XGAT', which incorporates attention weights and node features to provide explainable node representations. We demonstrate that XGAT improves the accuracy of node classification on benchmark datasets while providing insights into the importance of neighboring nodes and features. Our framework enables the identification of influential nodes and features, facilitating trustworthiness and accountability in GNN-based decision-making systems.