Emotion recognition is a challenging task due to the complexity of human emotions and the variability of expression modalities. This paper proposes a novel multi-modal fusion approach using graph attention networks (GATs) to integrate facial, speech, and text features for emotion recognition. We design a hierarchical graph architecture that captures intra-modal and inter-modal relationships, enabling the model to selectively focus on relevant features and modalities. Experimental results on a benchmark dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, with significant improvements in recognizing subtle emotions.