Open-domain question answering (ODQA) relies on efficient passage retrieval to reduce the search space for answer extraction. This paper proposes a novel hierarchical document representation (HDR) that captures both local and global contextual information. Our approach combines a chunk-based sentence encoder with a document-level graph neural network to learn dense representations of passages. Experimental results on the Natural Questions dataset demonstrate that HDR outperforms state-of-the-art dense retrieval models, achieving a 12% increase in top-20 passage retrieval accuracy while reducing computational overhead by 30%.