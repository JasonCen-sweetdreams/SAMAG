Anomaly detection in time-series data is crucial for various applications, but existing methods often lack interpretability. This paper proposes a hierarchical attention network (HAN) that detects anomalies while providing insights into the contributing factors. Our approach leverages self-attention mechanisms to model complex dependencies within and across time-series segments, enabling the identification of anomalous patterns. We evaluate HAN on several real-world datasets, demonstrating improved detection accuracy and explaining the anomalies through interpretable attention weights. The results show the effectiveness of HAN in identifying anomalies and facilitating root cause analysis.