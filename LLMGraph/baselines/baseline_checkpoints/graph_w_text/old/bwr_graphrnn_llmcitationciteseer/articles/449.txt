Conversational AI systems often struggle to provide transparent and interpretable dialogue management. We propose a novel hierarchical attention network (HAN) architecture that learns to generate responses while explicitly modeling the relationships between user inputs, system responses, and task-oriented goals. Our approach leverages a hierarchical attention mechanism to selectively focus on relevant context and goal-oriented information, enabling more explainable and effective dialogue management. Experimental results on a popular conversational dataset demonstrate significant improvements in both response quality and explainability compared to state-of-the-art baselines.