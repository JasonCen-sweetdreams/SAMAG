Neural search has gained popularity due to its ability to capture semantic relationships between queries and documents. However, query expansion techniques, which are crucial for improving retrieval performance, often rely on manual feature engineering or simplistic word embeddings. This paper proposes Context-Aware Embeddings (CAE), a novel query expansion approach that leverages pre-trained language models to capture contextual information. CAE encodes queries and documents as dense vectors, incorporating syntactic, semantic, and pragmatic features. Experimental results on several benchmark datasets demonstrate that CAE outperforms state-of-the-art query expansion methods, achieving significant improvements in recall and precision.