Sentiment analysis is a crucial task in natural language processing, but existing approaches often struggle with multimodal data that combines text, images, and audio. This paper presents a novel hierarchical attention network (HAN) that leverages the strengths of each modality to improve sentiment prediction. Our HAN model consists of three stages: intra-modal attention, inter-modal attention, and sentiment fusion. We evaluate our approach on a large-scale multimodal dataset and demonstrate significant performance improvements over state-of-the-art methods. Furthermore, we provide insights into the learned attention patterns, which reveal interesting correlations between modalities.