Query expansion is a crucial step in ad-hoc retrieval, where the goal is to reformulate the original query to better capture user intent. Existing methods rely on manual feature engineering and heuristics, leading to suboptimal performance. We propose a reinforcement learning framework, QERL, which learns to select the most informative expansion terms by interacting with a simulated retrieval environment. QERL uses a novel reward function that balances term relevance and diversity, and is trained using a combination of imitation learning and policy gradient methods. Experiments on several benchmark datasets demonstrate that QERL outperforms state-of-the-art query expansion techniques, with significant improvements in mean average precision and nDCG.