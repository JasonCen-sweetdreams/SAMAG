Emotion recognition is a crucial aspect of human-computer interaction, enabling systems to adapt to user needs and provide personalized experiences. This paper presents a novel hierarchical attention network (HAN) for multimodal emotion recognition, which leverages facial expressions, speech, and text inputs. Our HAN framework consists of modality-specific attention modules that learn to focus on salient features, followed by a higher-level fusion attention mechanism that integrates the modality-wise representations. Experimental results on the CMU-Multimodal SDK dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multimodal inputs, with a significant improvement in F1-score.