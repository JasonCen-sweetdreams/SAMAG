Virtual reality (VR) has revolutionized the way we interact with digital information, but current recommendation systems fail to fully leverage the rich gaze-based interaction data available in VR environments. This paper presents 'GazeRec', a novel personalized recommendation framework that incorporates gaze patterns, head movements, and user behavior to provide tailored suggestions in VR. We propose a multimodal fusion approach that combines gaze-based saliency maps with collaborative filtering, resulting in improved recommendation accuracy and user satisfaction. A user study with 50 participants demonstrates the effectiveness of GazeRec in enhancing user experience and engagement in VR applications.