Traditional human-computer interaction (HCI) systems often neglect the emotional and social needs of users with disabilities. This paper presents EmoReact, a novel affective gesture recognition framework that enables inclusive HCI for individuals with motor impairments. EmoReact leverages computer vision and machine learning to detect subtle emotional cues from facial expressions, head movements, and body language. We evaluate EmoReact in a user study with participants with cerebral palsy, demonstrating improved interaction accuracy and user satisfaction compared to traditional keyboard-and-mouse interfaces.