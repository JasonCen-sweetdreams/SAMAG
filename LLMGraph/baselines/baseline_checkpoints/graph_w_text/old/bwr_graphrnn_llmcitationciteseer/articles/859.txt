Multimodal sentiment analysis has gained significant attention in recent years, but existing methods often struggle to provide interpretable results. We propose a novel hierarchical attention network (HAN) that integrates visual and textual features to predict sentiment polarity. Our approach employs a modular architecture, where attention weights are learned at multiple levels to highlight salient regions in images and sentences. Experimental results on three benchmark datasets demonstrate that HAN outperforms state-of-the-art methods in terms of accuracy and correlation with human annotations. We also provide visualizations to illustrate the explainability of our model, revealing insights into the decision-making process.