Deep learning models have achieved state-of-the-art performance in multi-modal sentiment analysis tasks. However, the lack of interpretability and explainability hinders their adoption in high-stakes applications. We propose a novel Hierarchical Attention Network (HAN) architecture that incorporates explainable attention mechanisms to model the complex interactions between modalities. Our approach utilizes a hierarchical fusion strategy to integrate text, image, and audio features, enabling the identification of salient features that contribute to the sentiment prediction. Experimental results on the CMU-MultiModal dataset demonstrate the effectiveness of our approach in achieving competitive performance while providing insightful explanations for the predicted sentiment.