Emotion recognition from multi-modal data (e.g., speech, text, and vision) is a challenging task due to the complexity and variability of human emotions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features to improve emotion recognition accuracy. Our approach incorporates explainability techniques to provide insights into the decision-making process, enabling a better understanding of the emotional cues underlying human behavior. Experimental results on the CMU-MOSEI dataset demonstrate the effectiveness of HAN in recognizing emotions from multi-modal data, outperforming state-of-the-art methods by 12.5%.