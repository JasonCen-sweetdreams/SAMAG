Few-shot learning (FSL) has gained significant attention in recent years due to its ability to adapt to new tasks with limited labeled data. However, existing FSL methods often suffer from high computational costs and poor performance on complex datasets. This paper proposes a novel hierarchical attention network (HAN) architecture that leverages attention mechanisms at multiple scales to efficiently learn task-specific representations. Our experiments on benchmark FSL datasets demonstrate that HAN achieves state-of-the-art performance while reducing computational costs by up to 50% compared to existing methods.