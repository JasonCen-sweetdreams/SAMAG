Graph neural networks (GNNs) have achieved state-of-the-art performance in node classification tasks, but their scalability and interpretability remain significant challenges. This paper proposes a novel hierarchical graph attention network (H-GAT) architecture that leverages both node and graph-level attention mechanisms to selectively focus on relevant subgraphs and nodes. We demonstrate the efficacy of H-GAT on large-scale graph datasets, achieving improved accuracy and F1-scores compared to existing GNN models while providing interpretable attention weights for explainability.