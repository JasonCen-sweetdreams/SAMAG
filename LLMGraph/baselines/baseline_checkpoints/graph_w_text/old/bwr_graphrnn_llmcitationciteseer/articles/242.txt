Emotion recognition from multi-modal data (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features for emotion recognition. Our approach leverages self-attention mechanisms to model intra-modal and inter-modal relationships, enabling the network to provide explainable emotion recognition. Experimental results on the CMU-MOSEI dataset demonstrate the effectiveness of HAN in achieving state-of-the-art performance and providing interpretable attention weights.