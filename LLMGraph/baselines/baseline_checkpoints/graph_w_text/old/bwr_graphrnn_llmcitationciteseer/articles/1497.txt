Neural architecture search (NAS) has revolutionized the design of deep neural networks, but its computational cost remains a significant bottleneck. This paper presents a novel NAS approach, 'Hierarchical Graph Attention NAS' (HGANS), which leverages hierarchical graph attention mechanisms to efficiently search for optimal architectures. Our method reduces the search space by hierarchically grouping correlated operations and applying attention weights to identify the most relevant components. Experimental results on popular image classification benchmarks demonstrate that HGANS achieves state-of-the-art performance while reducing the search time by up to 75% compared to existing NAS methods.