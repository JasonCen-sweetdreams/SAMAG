 Few-shot learning has seen significant progress with the advent of graph neural networks (GNNs). However, existing approaches often struggle with scalability due to the quadratic complexity of attention mechanisms. This paper introduces Hierarchical Graph Attention Networks (HiGAT), a novel architecture that leverages hierarchical graph representations to reduce computational overhead. By adaptively aggregating attention weights across multiple scales, HiGAT achieves state-of-the-art performance on benchmark few-shot learning datasets while reducing inference time by up to 3.5x compared to existing GNN-based approaches.