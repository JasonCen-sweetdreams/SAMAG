Emotion recognition from multi-modal inputs (e.g., facial expressions, speech, and text) is a challenging task in AI research. While deep learning models have achieved high accuracy, their lack of interpretability hinders trust and understanding. This paper proposes a novel hybrid attention mechanism that combines self-attention and graph attention to selectively focus on informative modalities and features. We evaluate our approach on three benchmark datasets and demonstrate significant improvements in emotion recognition accuracy, while providing visual explanations of the model's decision-making process.