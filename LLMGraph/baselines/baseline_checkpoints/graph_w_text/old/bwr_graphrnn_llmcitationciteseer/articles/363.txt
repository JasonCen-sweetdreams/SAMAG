In multi-agent systems, decision-making often relies on complex interactions between agents. However, the lack of transparency in these interactions hinders trust and reliability. This paper proposes a novel hierarchical attention network (HAN) architecture that enables explainable decision-making in multi-agent systems. HAN learns to identify influential agents and their interactions, providing interpretable outcomes. We evaluate our approach on a real-world autonomous driving scenario, demonstrating improved decision accuracy and enhanced explainability compared to existing approaches.