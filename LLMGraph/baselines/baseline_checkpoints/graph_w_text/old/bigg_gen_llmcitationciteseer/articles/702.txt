Dialogue systems have seen significant advancements with the integration of multi-modal inputs, such as text, speech, and vision. However, these systems often lack transparency and interpretability, making it challenging to understand their decision-making processes. This paper presents a novel hierarchical attention network (HAN) architecture that explicitly models the interactions between different modalities, enabling explainable dialogue generation. Our approach leverages attention mechanisms to identify the most salient features across modalities and generates interpretable attention weights, which can be visualized to facilitate understanding of the system's decision-making process. Experimental results demonstrate the effectiveness of our approach in improving dialogue coherence and reducing ambiguity.