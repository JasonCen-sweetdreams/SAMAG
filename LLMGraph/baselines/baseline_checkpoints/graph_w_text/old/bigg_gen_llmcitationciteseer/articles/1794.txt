Visual question answering (VQA) models have achieved impressive performance, but their lack of transparency hinders trust and reliability. We propose a novel hierarchical attention-based framework, 'HAER', that generates explanations for its reasoning process in VQA tasks. Our approach leverages a multi-scale attention mechanism to selectively focus on relevant regions in the image and question, and a semantic graph-based reasoning module to provide interpretable explanations. Experimental results on the VQA-X dataset demonstrate that HAER achieves state-of-the-art performance while providing meaningful explanations for its answers, enabling more trustworthy AI systems.