Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a challenging task due to the complexity of human emotions and the need to fuse heterogeneous data. This paper proposes a novel Hierarchical Attention Network (HAN) that leverages the strengths of each modality to improve emotion recognition accuracy. Our HAN architecture consists of modality-specific attention modules and a hierarchical fusion layer, which adaptively weights and combines the outputs of each modality. Experimental results on two benchmark datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance with significant reductions in computational cost.