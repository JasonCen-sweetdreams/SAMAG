Emotion recognition from multi-modal data, such as speech, text, and facial expressions, is a challenging task due to the complexity and variability of human emotions. This paper proposes a novel hierarchical attention network (HAN) that leverages the strengths of different modalities to recognize emotions more accurately. Our approach uses a hierarchical fusion strategy, where modality-specific attention weights are learned to selectively focus on relevant features. We also introduce an explainability module that provides visualizations of the attention weights, allowing for better understanding of the decision-making process. Experimental results on a benchmark dataset show that our HAN model outperforms state-of-the-art methods in emotion recognition accuracy and provides valuable insights into the emotional cues used by the model.