Sentiment analysis from multi-modal data (e.g., text, images, and videos) is crucial for understanding user opinions. Existing methods often rely on early fusion of modalities or simple concatenation, which can lead to suboptimal performance. We propose a Hierarchical Graph Attention Network (HGAT) that models complex interactions between modalities using a graph-based framework. HGAT leverages attention mechanisms to selectively focus on relevant modalities and captures long-range dependencies. Experiments on three benchmark datasets demonstrate that HGAT outperforms state-of-the-art methods, achieving an average improvement of 4.2% in sentiment analysis accuracy.