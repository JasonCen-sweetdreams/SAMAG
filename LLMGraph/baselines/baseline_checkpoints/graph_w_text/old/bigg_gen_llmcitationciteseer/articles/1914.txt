Voice assistants have transformed the way we interact with technology, but their adoption by older adults is hindered by limitations in speech recognition, cognitive demands, and accessibility. This paper presents a novel multimodal framework, 'EasyVoice', which integrates computer vision, natural language processing, and machine learning to create a more inclusive and accessible voice assistant. EasyVoice incorporates facial expression recognition, gesture-based input, and personalized dialogue management to reduce cognitive load and improve user experience. Our user study with 30 older adults demonstrates significant improvements in task completion rates, user satisfaction, and perceived ease of use compared to commercial voice assistants.