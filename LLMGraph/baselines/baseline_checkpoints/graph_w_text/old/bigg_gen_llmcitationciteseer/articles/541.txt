Few-shot learning has gained significant attention in recent years, particularly in natural language processing (NLP) tasks. However, most existing approaches rely on meta-learning or finetuning, which can be computationally expensive and require large amounts of training data. This paper proposes a novel hierarchical attention network (HAN) architecture that leverages the concept of attention to learn task-agnostic representations from a few examples. Our approach consists of a hierarchical encoder that learns to attend to relevant input tokens and a task-specific decoder that adaptively generates output based on the attention weights. Experimental results on several benchmark datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance in few-shot learning settings.