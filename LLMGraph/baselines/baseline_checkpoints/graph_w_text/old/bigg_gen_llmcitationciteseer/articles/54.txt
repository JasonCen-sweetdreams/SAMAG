Deep learning models have become increasingly complex, making hyperparameter tuning a challenging and time-consuming task. This paper proposes a novel Bayesian optimization framework, 'HyperBO', which leverages a probabilistic surrogate model to efficiently search the hyperparameter space. We introduce a new acquisition function that balances exploration and exploitation, and demonstrate its effectiveness in tuning hyperparameters for various deep learning architectures on several benchmark datasets. Experimental results show that HyperBO outperforms state-of-the-art methods in terms of model performance and tuning time.