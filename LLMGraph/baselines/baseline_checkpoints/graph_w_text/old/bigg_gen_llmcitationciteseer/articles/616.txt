Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a challenging task, particularly when seeking to explain the underlying decision-making process. This paper introduces 'HMAN', a hierarchical attention network that integrates modality-specific and cross-modal attention mechanisms to recognize emotions from diverse input sources. We demonstrate HMAN's effectiveness on two benchmark datasets, outperforming state-of-the-art methods in both recognition accuracy and interpretability. Furthermore, we propose a novel explainability technique, 'EmoSal', which generates saliency maps highlighting the most informative regions in each modality that contribute to the emotion recognition outcome.