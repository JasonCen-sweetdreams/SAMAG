Emotion recognition from multi-modal data, such as speech, text, and vision, is crucial for various applications. However, existing methods suffer from high computational costs and limited scalability. We propose a novel Hierarchical Attention Network (HAN) that efficiently integrates modality-specific features and captures complex emotional patterns. HAN leverages attention mechanisms to selectively focus on informative modalities and temporal segments, reducing the need for exhaustive feature engineering. Experimental results on the CMU-MOSEI dataset demonstrate significant improvements in recognition accuracy and inference speed compared to state-of-the-art methods.