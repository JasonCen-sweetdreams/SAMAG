Multi-agent reinforcement learning (MARL) is a challenging problem that requires coordinating agents to achieve common goals. This paper proposes a novel hierarchical attention network (HAN) architecture that enables efficient MARL by selectively focusing on relevant agents and their interactions. Our HAN model consists of two levels of attention: agent-level attention that weights the importance of each agent's observations, and interaction-level attention that captures the relationships between agents. We evaluate our approach on several cooperative MARL benchmarks and demonstrate improved performance and reduced computational complexity compared to existing methods.