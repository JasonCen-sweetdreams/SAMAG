Autonomous vehicles (AVs) must make decisions in complex, dynamic environments with uncertain sensor readings. This paper presents a novel deep reinforcement learning (DRL) framework, 'UncerAV', that learns to navigate uncertain scenarios by incorporating Bayesian neural networks and probabilistic graph models. UncerAV outperforms state-of-the-art DRL methods on a simulated highway merge task, demonstrating improved decision-making under uncertainty and reduced collisions. We also provide a theoretical analysis of the framework's convergence properties and uncertainty quantification.