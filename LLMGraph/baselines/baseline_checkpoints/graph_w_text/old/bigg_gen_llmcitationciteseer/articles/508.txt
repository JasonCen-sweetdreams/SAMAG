Explainability is crucial in multi-agent reinforcement learning (MARL) as it enables humans to understand and trust the decision-making process of autonomous agents. This paper proposes a novel hierarchical attention mechanism (HAM) that selectively focuses on relevant agents and their interactions to generate interpretable explanations. Our HAM is integrated into a deep Q-network (DQN) framework and evaluated on a range of MARL benchmarks, demonstrating significant improvements in both task performance and explanation accuracy. We also provide a comprehensive analysis of the learned attention patterns, revealing insights into the coordination strategies employed by the agents.