Autonomous vehicles (AVs) rely on reinforcement learning (RL) to optimize control policies, but the lack of transparency in RL decision-making hinders trust and reliability. We propose an explainable RL framework, 'XRL', which integrates attention-based feature attribution and model-agnostic explanations to provide insights into the AV's decision-making process. XRL is evaluated on a realistic simulation environment, demonstrating improved transparency and interpretability without compromising control performance. We also investigate the impact of XRL on human-AV trust and show that explanatory capabilities lead to increased trust and acceptance.