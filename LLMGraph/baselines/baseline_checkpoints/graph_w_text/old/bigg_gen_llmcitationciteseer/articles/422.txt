Emotion recognition from multi-modal data, such as speech, text, and vision, remains a challenging task due to the varying importance of different modalities across different individuals and contexts. This paper proposes a Hierarchical Attention Network (HAN) that adaptively weighs modality-specific features at multiple levels of abstraction. Our HAN model consists of three stages: modality-specific attention, cross-modal attention, and a final hierarchical fusion layer. Experimental results on the Multimodal Emotion Recognition dataset demonstrate that our approach outperforms state-of-the-art methods, achieving a 12.5% improvement in overall emotion recognition accuracy.