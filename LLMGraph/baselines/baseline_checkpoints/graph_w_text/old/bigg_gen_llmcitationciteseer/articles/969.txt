Deep neural networks (DNNs) have been shown to be vulnerable to adversarial attacks, which can significantly degrade their performance. While various defense methods have been proposed, understanding the underlying mechanisms of these attacks remains an open challenge. This paper leverages gradient-based explanations to analyze the robustness of DNNs against adversarial attacks. We propose a novel framework, 'GradAttack', which utilizes gradient-based feature importance to identify the most vulnerable neurons and layers in a DNN. Experimental results on several benchmark datasets demonstrate that GradAttack can effectively detect and mitigate adversarial attacks, providing insights into the robustness of DNNs.