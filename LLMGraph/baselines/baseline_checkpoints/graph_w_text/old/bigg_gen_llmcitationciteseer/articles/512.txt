Emotion recognition from multi-modal inputs (e.g., speech, text, vision) remains a challenging task due to the complexity of human emotions and the lack of interpretability in existing models. We propose a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and features, enabling explainable emotion recognition. Our approach achieves state-of-the-art performance on the IEMOCAP dataset and provides insights into the emotional cues used by the model, facilitating trustworthiness and accountability in real-world applications.