Emotion recognition from multimodal data, such as speech, text, and vision, is a challenging task due to the complexities of human emotions and the heterogeneity of modalities. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) that effectively integrates and fuses multimodal features. HGAT employs a hierarchical architecture to model intra-modal and inter-modal relationships, and incorporates graph attention mechanisms to selectively focus on relevant features. Experimental results on the CMU-MultimodalSDK dataset demonstrate that HGAT outperforms state-of-the-art methods in emotion recognition, achieving an F1-score of 0.843. Our approach has potential applications in affective computing, human-computer interaction, and mental health monitoring.