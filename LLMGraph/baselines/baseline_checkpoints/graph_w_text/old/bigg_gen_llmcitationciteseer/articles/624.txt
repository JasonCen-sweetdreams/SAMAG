Emotion recognition is a crucial aspect of human-robot interaction, enabling robots to respond empathetically to users. This paper proposes a novel hierarchical attention network (HAN) that integrates multiple modalities, including facial expressions, speech, and physiological signals. The HAN learns to selectively focus on relevant modalities and features, achieving state-of-the-art performance on the EMotion Recognition in Human-Robot Interaction (EMRHI) dataset. We demonstrate the effectiveness of our approach in a real-world humanoid robot platform, enabling the robot to recognize and respond to user emotions with improved accuracy and naturalness.