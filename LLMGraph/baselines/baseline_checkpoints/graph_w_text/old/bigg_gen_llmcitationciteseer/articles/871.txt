Deep reinforcement learning (DRL) has achieved remarkable success in various applications, but the lack of explainability hinders its adoption in high-stakes domains. This paper proposes a novel approach to explaining DRL agents' decision-making processes by leveraging model-based state abstraction. We introduce a hierarchical abstraction framework that enables the agent to reason about its environment at multiple levels of granularity, providing insights into its decision-making process. Our experiments on a suite of Atari games demonstrate that our approach yields more interpretable and accurate explanations compared to existing methods, while maintaining competitive performance.