Few-shot image classification tasks require models to adapt quickly to new classes with limited training data. We propose a hierarchical meta-learning framework, 'MetaNest', which leverages a nested learning process to effectively learn task-specific representations. Our approach consists of a meta-learner that generates task-adaptive weights for a base learner, which in turn is trained on a few-shot task. We demonstrate that MetaNest outperforms state-of-the-art few-shot learning methods on various benchmark datasets, including mini-ImageNet and tiered-ImageNet.