Graph neural networks (GNNs) have achieved state-of-the-art performance in node classification tasks, but their computational complexity grows quadratically with the number of nodes. This paper proposes a novel hierarchical graph attention network (HGAT) that mitigates this issue by introducing a hierarchical representation of graphs. HGAT employs a self-attention mechanism to adaptively focus on relevant subgraphs, reducing the computational cost while maintaining performance. Experiments on benchmark datasets demonstrate that HGAT outperforms existing GNNs in terms of both accuracy and efficiency.