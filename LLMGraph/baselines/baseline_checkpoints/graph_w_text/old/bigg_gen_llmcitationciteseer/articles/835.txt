Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the inherent complexity of human emotions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages the strengths of each modality to recognize emotions accurately. Our approach employs attention mechanisms at multiple levels to selectively focus on relevant features, enabling explainable emotion recognition. Experimental results on the IEMOCAP dataset demonstrate that HAN outperforms state-of-the-art methods, achieving an average F1-score of 85.2% across six emotions. We also provide visualizations to illustrate the attention weights, facilitating interpretability and trust in AI-driven emotion recognition systems.