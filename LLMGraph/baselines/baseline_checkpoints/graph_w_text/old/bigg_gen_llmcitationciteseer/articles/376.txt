Interpretable AI models are crucial for building trust in natural language processing (NLP) systems. This paper introduces a novel hierarchical attention network (HAN) architecture for natural language inference (NLI) tasks, which provides transparent and explainable reasoning behind its predictions. Our HAN model incorporates a dynamic attention mechanism that progressively focuses on relevant context and premise snippets, generating visualizable attention maps that facilitate model interpretability. Experimental results on the SNLI and MultiNLI benchmark datasets demonstrate that our HAN model achieves state-of-the-art performance while providing insightful explanations for its predictions.