Emotion recognition in human-computer interaction (HCI) is crucial for developing empathetic and personalized systems. This paper introduces a novel hierarchical attention network (HAN) architecture that leverages multimodal data from facial expressions, speech, and physiological signals to identify emotions with high accuracy. Our approach exploits the hierarchical structure of emotional cues, attending to relevant modalities and features at multiple scales. Experimental results on a large-scale multimodal dataset demonstrate that HAN outperforms state-of-the-art methods, achieving a 15% improvement in emotion recognition F1-score.