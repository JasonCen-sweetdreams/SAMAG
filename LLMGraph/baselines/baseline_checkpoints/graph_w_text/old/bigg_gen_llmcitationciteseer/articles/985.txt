Virtual Reality (VR) has the potential to provide immersive experiences for individuals with disabilities. However, traditional interaction methods in VR, such as controllers or gestures, can be challenging or impossible for users with motor impairments. This paper presents EyeGazeLens, a novel gaze-based interaction technique that leverages eye-tracking technology to enable users to interact with virtual objects in a more intuitive and accessible way. Our approach uses machine learning algorithms to detect the user's gaze and intent, allowing for precise and efficient object selection and manipulation. We evaluate EyeGazeLens through a user study with participants with and without motor impairments, demonstrating significant improvements in interaction speed and accuracy.