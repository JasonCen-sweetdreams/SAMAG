Visual question answering (VQA) models often rely on complex neural architectures, making it challenging to understand their decision-making processes. This paper proposes a novel hierarchical attention network (HAN) for VQA that provides insights into the reasoning process. Our HAN model incorporates a cascade of attention mechanisms to focus on relevant regions of the image and question. We introduce a visualizationscheme to illustrate the attention weights, enabling the identification of salient image regions and question phrases. Experimental results on the VQA-CP v2 dataset demonstrate that our HAN model achieves state-of-the-art accuracy while providing interpretable explanations for its predictions.