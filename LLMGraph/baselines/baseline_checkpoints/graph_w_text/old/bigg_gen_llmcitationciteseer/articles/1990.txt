Multimodal sentiment analysis has garnered significant attention in recent years, but existing approaches often lack transparency in their decision-making processes. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates visual, textual, and audio features for multimodal sentiment analysis. Our model employs a hierarchical attention mechanism to selectively focus on relevant modalities and features, providing interpretable explanations for its predictions. Experimental results on the CMU-MOSI dataset demonstrate that our approach outperforms state-of-the-art methods while offering improved explainability and robustness to noisy inputs.