Deep reinforcement learning (DRL) has shown remarkable success in various applications, but its vulnerability to adversarial attacks remains a significant concern. This paper presents a novel approach to detect such attacks in DRL systems using graph neural networks (GNNs). We propose a GNN-based framework, 'RL-Guard', that learns to identify patterns in the agent's state-action graph indicative of adversarial perturbations. Experimental results on several Atari games demonstrate that RL-Guard achieves high detection accuracy and robustness against various attack strategies, thereby enhancing the reliability of DRL systems in safety-critical applications.