Multimodal dialogue systems have become increasingly prevalent, but their lack of transparency hinders trust and understanding. This paper presents a novel hierarchical attention network (HAN) architecture that enables explainable multimodal dialogue responses. Our approach integrates visual, acoustic, and linguistic features using a multi-level attention mechanism, allowing the model to selectively focus on relevant input modalities. We demonstrate the effectiveness of our HAN model on a benchmark multimodal dialogue dataset, achieving state-of-the-art performance while providing interpretable explanations for the generated responses.