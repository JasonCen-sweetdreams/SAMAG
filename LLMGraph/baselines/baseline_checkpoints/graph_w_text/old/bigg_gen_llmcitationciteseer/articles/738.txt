In multi-agent reinforcement learning, the curse of dimensionality arises when scaling to large numbers of agents and states. This paper presents HAT-MARL, a hierarchical attention network architecture that selectively focuses on relevant agents and states, reducing the computational complexity of policy updates. We demonstrate the efficacy of HAT-MARL in several benchmark environments, showcasing improved learning efficiency and scalability compared to existing methods. Furthermore, we analyze the attention patterns learned by HAT-MARL, providing insights into the coordination mechanisms emerging among agents.