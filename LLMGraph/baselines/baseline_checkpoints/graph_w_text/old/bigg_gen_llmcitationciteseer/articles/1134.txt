Multi-agent pathfinding is a challenging problem in AI, particularly in complex and dynamic environments. This paper presents a novel approach, 'MAP-RL', that leverages deep reinforcement learning to train multiple agents to navigate efficiently and avoid collisions. We propose a hierarchical framework that combines a high-level graph-based planning module with a low-level neural network controller, enabling the agents to adapt to changing scenarios. Experimental results on various benchmark maps demonstrate that MAP-RL outperforms state-of-the-art methods in terms of path length, computation time, and success rate.