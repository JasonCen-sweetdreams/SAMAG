Emotion recognition from multi-modal inputs, such as speech, text, and vision, is an essential task in human-computer interaction. While deep learning models have achieved state-of-the-art performance, they often lack interpretability. This paper proposes a hierarchical attention network (HAN) that selectively focuses on relevant modalities and time segments to recognize emotions. We introduce an attention regularization term that encourages the model to provide explanations for its predictions. Experimental results on the CMU-MOSI dataset demonstrate that our HAN model outperforms baselines in both accuracy and explainability, providing insights into the decision-making process.