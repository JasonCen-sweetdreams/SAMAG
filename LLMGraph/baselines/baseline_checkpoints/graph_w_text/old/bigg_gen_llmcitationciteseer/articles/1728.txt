Emotion recognition from multimodal data, such as speech, text, and vision, is a crucial task in human-computer interaction. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features to improve emotion recognition accuracy. Our approach exploits the structural dependencies between modalities using a hierarchical attention mechanism, enabling the model to capture complex contextual relationships. Experimental results on the IEMOCAP dataset demonstrate that HAN outperforms state-of-the-art methods by 12.4% in terms of weighted F1-score, achieving high accuracy and robustness in multimodal emotion recognition.