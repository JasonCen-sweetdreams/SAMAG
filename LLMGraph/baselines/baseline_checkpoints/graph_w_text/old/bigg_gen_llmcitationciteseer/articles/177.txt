Multimodal sentiment analysis has gained importance in recent years, but existing approaches often lack interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates visual, textual, and acoustic features to predict sentiment scores. Our HAN model employs attention mechanisms at multiple levels to capture intricate relationships between modalities and identify salient features contributing to sentiment. Experimental results on a benchmark dataset demonstrate improved performance and explainability compared to state-of-the-art multimodal fusion techniques.