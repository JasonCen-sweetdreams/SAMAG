Explainability is crucial in multi-agent systems, where autonomous agents make decisions that impact human lives. This paper presents a novel hierarchical attention network (HAN) architecture that learns to extract relevant features from agent interactions and generate interpretable explanations for their decisions. We introduce a novel attention mechanism that captures both local and global dependencies between agents, enabling the model to focus on the most influential agents and their interactions. Experimental results on a simulated autonomous vehicle scenario demonstrate that our HAN model outperforms existing explainability methods in terms of decision accuracy and explanation quality.