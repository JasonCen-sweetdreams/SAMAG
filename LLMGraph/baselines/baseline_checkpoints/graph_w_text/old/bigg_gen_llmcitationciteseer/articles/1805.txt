Multimodal sentiment analysis has become increasingly important in understanding human sentiment expressed through various modalities such as text, images, and audio. This paper proposes a novel Hierarchical Attention Network (HAN) that leverages the strengths of each modality to improve sentiment analysis. The proposed HAN consists of modality-specific attention modules that learn to weigh the importance of each modality, followed by a hierarchical fusion module that combines the modality-specific features. Experimental results on three benchmark datasets demonstrate that our approach outperforms existing state-of-the-art methods, achieving an average improvement of 4.2% in sentiment accuracy.