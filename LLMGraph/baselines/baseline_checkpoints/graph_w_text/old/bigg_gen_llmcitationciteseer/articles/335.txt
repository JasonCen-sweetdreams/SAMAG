Multimodal sentiment analysis has become increasingly important in various applications, but existing methods often lack transparency and interpretability. This paper proposes a novel hierarchical attention network (HAN) that integrates visual and textual features for sentiment analysis. Our model employs a hierarchical attention mechanism to selectively focus on relevant regions of the image and corresponding text segments, providing a more comprehensive understanding of the sentiment. We conduct experiments on two benchmark datasets and demonstrate that our approach outperforms state-of-the-art methods in terms of accuracy and provides insightful visualizations for explainability.