Explainability in multi-agent reinforcement learning (MARL) is crucial for real-world applications, such as autonomous driving and smart grids. We propose a novel hierarchical attention network (HAN) architecture that enables interpretable decision-making in MARL settings. Our HAN model consists of two interconnected attention mechanisms: one for inter-agent communication and another for intra-agent reasoning. We evaluate our approach on a suite of MARL benchmarks and demonstrate improved explainability and performance compared to state-of-the-art methods. Our results have significant implications for the development of transparent and trustworthy AI systems.