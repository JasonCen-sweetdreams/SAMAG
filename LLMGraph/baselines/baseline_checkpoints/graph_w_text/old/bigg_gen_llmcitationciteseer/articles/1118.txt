Multi-task learning (MTL) has become increasingly popular in deep learning, but it often requires significant computational resources and memory. We propose a novel hierarchical attention network (HAN) architecture that efficiently allocates attention across multiple tasks and incorporates task relationships. Our approach reduces the number of model parameters and computation required for MTL, while achieving improved performance on a range of benchmark datasets. We also provide a theoretical analysis of the attention mechanism, demonstrating its effectiveness in capturing task dependencies.