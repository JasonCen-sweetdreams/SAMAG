Zero-shot learning for multi-modal sentiment analysis remains a challenging problem, particularly when dealing with limited labeled data. This paper presents a novel approach, 'ContraSent', which leverages contrastive representation learning to effectively adapt to unseen modalities and sentiment classes. By constructing a unified embedding space across modalities, ContraSent enables efficient zero-shot learning via a simple nearest-neighbor search. Experimental results on a comprehensive benchmark dataset demonstrate the superiority of ContraSent over existing state-of-the-art methods, achieving an average improvement of 12.5% in sentiment classification accuracy.