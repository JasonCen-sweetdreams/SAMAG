Emotion recognition from multi-modal inputs (e.g., text, speech, vision) is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages both intra-modal and inter-modal attention mechanisms to capture nuanced emotional cues. We demonstrate the effectiveness of HAN on three benchmark datasets, achieving state-of-the-art performance in emotion recognition tasks. Furthermore, we provide visualizations of attention weights, enabling explainability and interpretability of the model's decision-making process.