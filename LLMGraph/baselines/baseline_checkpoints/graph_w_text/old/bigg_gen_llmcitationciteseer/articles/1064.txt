In multi-agent systems, efficient task allocation is crucial for achieving global objectives. This paper proposes a novel hierarchical reinforcement learning framework, 'HRL-MA', that coordinates agents to adapt to dynamic task environments. Our approach leverages a high-level planner to allocate tasks based on agent capabilities and a low-level learner to optimize task execution. We evaluate HRL-MA on a simulated warehouse management scenario and demonstrate improved task completion rates and reduced agent collisions compared to decentralized and centralized baselines.