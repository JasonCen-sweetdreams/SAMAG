Zero-shot learning (ZSL) enables models to recognize novel classes without training data, but existing approaches often suffer from high computational costs and inferior performance. This paper proposes a Hierarchical Attention Network (HAN) architecture that leverages attention mechanisms to selectively focus on relevant semantic features and instances. Our HAN model learns to adapt to new classes by hierarchically composing attention weights, resulting in efficient and accurate ZSL performance. Experiments on benchmark datasets demonstrate that HAN outperforms state-of-the-art methods while requiring fewer computational resources.