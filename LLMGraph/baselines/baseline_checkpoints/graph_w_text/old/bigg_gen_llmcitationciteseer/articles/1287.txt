Deep reinforcement learning (RL) has achieved remarkable success in autonomous driving, but its lack of transparency hinders widespread adoption. This paper proposes a novel explainable RL framework, 'CRL-AD', which incorporates contrastive regularization to disentangle and interpret the learned policy representations. By leveraging a contrastive loss function, we encourage the policy to focus on task-relevant features and provide visual explanations for its decision-making process. Experimental results on a simulated driving environment demonstrate that CRL-AD improves interpretability while maintaining competitive performance compared to state-of-the-art RL methods.