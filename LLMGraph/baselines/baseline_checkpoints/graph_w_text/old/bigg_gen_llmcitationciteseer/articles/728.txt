Emotion recognition from multi-modal inputs, such as facial expressions, speech, and physiological signals, remains a challenging task due to the complexity and variability of human emotions. This paper proposes a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features at multiple scales. Our HAN model consists of a feature-level attention mechanism that weights modality-specific features, and a modality-level attention mechanism that adaptively combines modalities. Experimental results on the RECOLA dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an F1-score of 0.843.