Distributed database systems face the challenge of optimizing complex queries across multiple nodes, leading to performance bottlenecks. This paper proposes a novel approach, 'ML-Optimizer', which leverages machine learning to predict optimal query plans. By training a graph neural network on a dataset of query plans and execution times, ML-Optimizer learns to identify efficient query patterns and adapt to changing workloads. Experimental results on a real-world distributed database system show that ML-Optimizer reduces average query latency by 35% compared to traditional rule-based optimizers, while maintaining high accuracy and robustness.