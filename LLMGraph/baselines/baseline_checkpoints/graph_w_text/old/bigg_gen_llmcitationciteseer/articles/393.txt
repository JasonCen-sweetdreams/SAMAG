Emotion recognition in human-computer interaction (HCI) is a crucial task, especially in applications like virtual assistants and affective computing. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates multi-modal features from speech, text, and vision to recognize emotions. Our approach leverages attention mechanisms to selectively focus on relevant modalities and features, leading to improved recognition accuracy. We evaluate our model on a large-scale emotion recognition dataset and demonstrate significant performance gains over state-of-the-art methods. The results have implications for developing more empathetic and human-centered HCI systems.