Deep neural networks have been shown to be vulnerable to adversarial attacks, which can significantly degrade their performance. This paper presents a novel approach to generating adversarial examples by analyzing the input-output Jacobian of the network. Our method, Jacobian-based Adversarial Attack (JAA), leverages the sensitivity of the network's output to input perturbations to craft targeted attacks. We demonstrate the effectiveness of JAA on several benchmark datasets, achieving state-of-the-art attack success rates while requiring fewer iterations than existing methods. Our findings highlight the importance of considering the input-output Jacobian in the design of robust neural networks.