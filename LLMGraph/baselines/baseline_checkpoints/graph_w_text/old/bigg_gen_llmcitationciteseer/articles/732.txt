Human-robot interaction (HRI) is a crucial aspect of robotics, but current methods struggle to provide explainability and adaptability in complex, multi-modal scenarios. This paper introduces a novel neural architecture, 'MMA-Net', which leverages multi-modal attention mechanisms to learn integrated representations of human-robot interactions. MMA-Net incorporates explicit attention modules for vision, language, and motion modalities, enabling the robot to selectively focus on relevant cues and generate interpretable explanations for its actions. We demonstrate the effectiveness of MMA-Net in a real-world HRI scenario, showcasing improved task performance and enhanced transparency.