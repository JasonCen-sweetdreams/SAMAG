Coordinating multi-agent systems is a challenging problem in artificial intelligence, particularly in scenarios where agents have partial observability and decentralized decision-making. This paper proposes a novel framework, 'Graph-RL', that leverages graph attention mechanisms to learn effective coordination strategies in decentralized multi-agent systems. We introduce a decentralized reinforcement learning algorithm that allows agents to learn from their local observations and communicate with neighboring agents through a graph-structured attention mechanism. Experimental results on several benchmark tasks demonstrate the effectiveness of Graph-RL in achieving better coordination and improved overall system performance compared to existing decentralized learning methods.