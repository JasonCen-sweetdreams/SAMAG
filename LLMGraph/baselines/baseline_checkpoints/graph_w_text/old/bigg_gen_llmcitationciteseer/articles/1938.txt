Explainable AI (XAI) techniques have been developed to provide insights into AI decision-making processes. However, XAI models can be vulnerable to adversarial attacks, which can compromise their explainability and reliability. This paper proposes a novel graph attention network (GAT)-based approach to detect adversarial attacks on XAI models. Our method leverages the graph structure of the input data to identify suspicious patterns indicative of attacks. Experimental results on several benchmark datasets demonstrate the effectiveness of our approach in detecting attacks with high accuracy, while also providing interpretable explanations for the detected anomalies.