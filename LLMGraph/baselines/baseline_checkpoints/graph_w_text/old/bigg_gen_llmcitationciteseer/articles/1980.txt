This paper introduces a novel multi-agent deep reinforcement learning framework for coordinating autonomous vehicles in complex urban scenarios. Our approach, called MAVRIC, utilizes a decentralized actor-critic architecture to learn policies for individual vehicles while considering the actions of neighboring agents. We demonstrate MAVRIC's effectiveness in a variety of simulated scenarios, including lane merging, intersection navigation, and pedestrian avoidance, achieving significant improvements in safety, efficiency, and scalability compared to traditional rule-based approaches.