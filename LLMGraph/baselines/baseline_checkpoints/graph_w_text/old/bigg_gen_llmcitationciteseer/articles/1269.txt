Emotion recognition from multi-modal inputs (e.g., facial expressions, speech, and text) is a challenging task due to the inherent variability and complexity of human emotions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and temporal segments to improve emotion recognition accuracy. We evaluate our approach on three benchmark datasets and demonstrate significant performance gains over state-of-the-art methods, achieving an average F1-score improvement of 12.3%. Our ablation studies further highlight the importance of hierarchical attention in capturing nuanced emotional cues.