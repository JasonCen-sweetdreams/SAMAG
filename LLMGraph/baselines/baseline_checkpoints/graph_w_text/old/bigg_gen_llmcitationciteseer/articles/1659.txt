Emotion recognition from multi-modal data, such as audio, video, and text, is a challenging task in affective computing. This paper introduces a novel hierarchical attention network (HAN) architecture that learns to selectively focus on relevant modalities and features for improved emotion recognition. Our HAN model consists of three stages: modality attention, feature attention, and emotion classification. Experimental results on the popular IEMOCAP dataset demonstrate that our approach achieves state-of-the-art performance with significant reductions in computational cost and memory usage compared to existing methods.