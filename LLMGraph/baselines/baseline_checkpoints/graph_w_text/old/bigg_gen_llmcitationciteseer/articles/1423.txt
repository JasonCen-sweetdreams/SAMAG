Anomaly detection in time-series data is crucial in various domains, but the lack of explainability hinders trust in deployed models. This paper proposes a hierarchical attention network (HAN) architecture that incorporates both local and global attention mechanisms to identify anomalies. Our approach leverages the strengths of self-attention and gated recurrent units to model complex temporal dependencies. We introduce a novel attention-based feature importance metric to provide interpretable explanations for detected anomalies. Experimental results on real-world datasets demonstrate the efficacy of HAN in terms of improved detection accuracy and explainability compared to state-of-the-art baselines.