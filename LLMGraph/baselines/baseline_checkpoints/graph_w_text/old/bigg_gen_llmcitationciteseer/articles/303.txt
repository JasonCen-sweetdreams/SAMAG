Neural ranking models have achieved state-of-the-art performance in information retrieval tasks, but their efficiency and scalability remain a concern. This paper proposes a novel document representation approach that leverages graph-based sentence embeddings to reduce the dimensionality of input documents. We introduce a sentence graph construction method that captures semantic relationships between sentences and a graph neural network-based embedding layer to learn compact sentence representations. Experimental results on the TREC Deep Learning Track dataset show that our approach achieves comparable ranking performance to state-of-the-art models while reducing the computational cost by up to 40%.