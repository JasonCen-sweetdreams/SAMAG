Cooperative task allocation is a fundamental problem in multi-agent systems, where agents need to dynamically adapt to changing environmental conditions. This paper presents a novel multi-agent reinforcement learning framework, 'CoopRL', that enables agents to learn cooperative strategies for task allocation in uncertain and dynamic environments. We introduce a decentralized actor-critic architecture that integrates graph neural networks to model agent interactions and attention mechanisms to focus on critical tasks. Experimental results on a simulated search-and-rescue scenario demonstrate that CoopRL outperforms state-of-the-art methods in terms of task completion rate and adaptability to environmental changes.