Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is crucial for human-computer interaction systems. However, existing approaches often lack interpretability, making it challenging to understand the decision-making process. This paper proposes a hierarchical attention network (HAN) that integrates modal-specific attention mechanisms with a hierarchical fusion strategy. Our approach achieves state-of-the-art performance on the CMU-MOSEI dataset and provides visualizations of attention weights, enabling the identification of salient features contributing to emotion recognition. We demonstrate the effectiveness of HAN in various real-world applications, including affective computing and mental health monitoring.