This paper proposes a novel hierarchical attention network (HAN) architecture for explainable time series forecasting. Our approach leverages self-attention mechanisms to model complex temporal dependencies and extract relevant features from multivariate time series data. We introduce a hierarchical attention scheme that adaptively selects relevant time steps and features, enabling the model to focus on the most informative regions of the input data. Experimental results on several benchmark datasets demonstrate the improved forecasting accuracy and interpretability of our HAN model compared to state-of-the-art baselines.