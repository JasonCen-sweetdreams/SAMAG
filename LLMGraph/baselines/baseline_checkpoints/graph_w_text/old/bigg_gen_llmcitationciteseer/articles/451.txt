Vision Transformers (ViTs) have achieved state-of-the-art performance in various computer vision tasks. However, they remain vulnerable to adversarial attacks, which can compromise their reliability in real-world applications. This paper proposes a novel self-supervised contrastive learning framework, 'SSC-ViT', to enhance the robustness of ViTs against adversarial attacks. By leveraging contrastive learning, we encourage the model to learn robust features that are invariant to perturbations. Our approach outperforms existing defense methods on several benchmarks, including ImageNet and CIFAR-10, and demonstrates improved transferability to unseen attacks.