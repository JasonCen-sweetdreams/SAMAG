Emotion recognition is a crucial aspect of human-computer interaction, enabling computers to respond empathetically to users. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates multiple modalities, including facial expressions, speech, and physiological signals. Our HAN model learns to selectively focus on salient features from each modality, achieving state-of-the-art performance on the EmoReact dataset. We also conduct ablation studies to demonstrate the effectiveness of our proposed attention mechanism in capturing complex emotional patterns.