Multimodal sentiment analysis has gained increasing attention in recent years, but existing methods often struggle to provide interpretable results. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that incorporates both visual and textual features to analyze multimodal data. Our approach utilizes a hierarchical attention mechanism to selectively focus on relevant regions of the input data, enabling more accurate sentiment prediction and providing explainable insights into the decision-making process. Experimental results on a benchmark dataset demonstrate the effectiveness of our approach, achieving state-of-the-art performance and outperforming existing multimodal sentiment analysis methods.