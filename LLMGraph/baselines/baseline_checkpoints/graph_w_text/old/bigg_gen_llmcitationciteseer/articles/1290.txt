Virtual reality (VR) has the potential to revolutionize accessibility, but existing interaction methods often rely on motor skills, excluding users with disabilities. This paper presents 'GazeVR', a novel gaze-based interaction framework for inclusive VR experiences. GazeVR leverages machine learning-based gaze estimation and integrates it with a probabilistic inference engine to enable intuitive, hands-free control. Our user study with participants with mobility impairments demonstrates significant improvements in task completion time and user satisfaction compared to traditional controller-based interactions.