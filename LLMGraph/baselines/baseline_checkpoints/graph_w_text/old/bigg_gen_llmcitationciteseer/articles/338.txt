Voice assistants have become ubiquitous in modern life, but their speech recognition capabilities often struggle with diverse accents and dialects. This paper presents a novel approach to accent-agnostic speech recognition, leveraging transfer learning and multi-task learning to improve the robustness of voice assistants to various accents. We design and evaluate a new dataset, 'AccentHub', comprising 10,000 hours of speech from 15 different accents. Our proposed model, 'AccentNet', outperforms state-of-the-art speech recognition systems on AccentHub, demonstrating its potential to enable more inclusive voice assistants.