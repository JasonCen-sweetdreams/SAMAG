Emotion recognition from multi-modal inputs (e.g., speech, text, vision) has seen significant progress with deep learning. However, most approaches are opaque, making it difficult to understand their decision-making processes. This paper proposes a Hierarchical Attention Network (HAN) that integrates modalities at multiple levels, enabling the identification of salient features contributing to emotion detection. Our HAN model achieves state-of-the-art performance on theMultimodal Emotion Recognition benchmark and provides visualizations of attention weights, facilitating interpretability and trust in AI-driven emotion recognition systems.