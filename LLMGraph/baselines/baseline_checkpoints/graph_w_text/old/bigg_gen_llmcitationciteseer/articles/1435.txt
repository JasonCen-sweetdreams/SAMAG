Multi-task learning (MTL) has become a crucial technique in computer vision, as it enables the sharing of knowledge across related tasks. However, existing MTL methods often suffer from task interference, leading to suboptimal performance. This paper proposes a novel hierarchical attention network (HAN) that addresses this issue by learning task-specific attention weights at multiple levels of abstraction. Our approach enables the model to selectively focus on relevant features and tasks, resulting in improved performance and efficiency. Experiments on several benchmark datasets demonstrate that HAN outperforms state-of-the-art MTL methods, achieving better accuracy and reduced computational cost.