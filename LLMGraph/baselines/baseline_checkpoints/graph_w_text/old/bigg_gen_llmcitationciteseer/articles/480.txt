Emotion recognition from multi-modal inputs, such as speech, text, and vision, is a crucial task in human-computer interaction. This paper presents a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features for emotion recognition. Our HAN model consists of two stages: a modality-level attention mechanism that weights the importance of each modality, and a feature-level attention mechanism that identifies salient features within each modality. Experimental results on the popular_CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art models in terms of accuracy and computational efficiency.