Explainable AI (XAI) has emerged as a crucial aspect of decision support systems (DSS) to ensure transparency and trustworthiness. This paper introduces a novel Hierarchical Attention Network (HAN) architecture that integrates attention mechanisms with hierarchical feature extraction to provide interpretable AI-driven decision-making. HAN learns to focus on relevant input features and contextual relationships, enabling the generation of meaningful explanations for DSS outputs. Experimental results on a real-world healthcare dataset demonstrate the effectiveness of HAN in improving decision accuracy and explanation quality compared to existing XAI methods.