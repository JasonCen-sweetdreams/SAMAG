Multi-modal dialogue systems have gained popularity in human-computer interaction, but their lack of transparency hinders trust and understanding. This paper presents a novel Hierarchical Attention Network (HAN) architecture for explainable dialogue systems. Our approach integrates visual and linguistic features using a multi-modal fusion mechanism, and employs hierarchical attention to identify relevant context and intent. We demonstrate the effectiveness of HAN on a large-scale dialogue dataset, achieving state-of-the-art performance in response generation and intent recognition tasks. Additionally, we provide visualizations and feature importance scores to facilitate model interpretability and understanding.