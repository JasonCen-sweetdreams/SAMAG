Emotion recognition systems often struggle to interpret and explain their decisions, particularly when dealing with multi-modal inputs (e.g., speech, text, and vision). We propose a hierarchical attention network (HAN) framework that incorporates both local and global attention mechanisms to selectively focus on salient features across modalities. Our approach enables the model to provide interpretable explanations for its emotion recognition predictions, improving trust and accountability in AI-driven decision-making. Experimental results on the CMU-MOSEI dataset demonstrate the effectiveness of HAN in achieving state-of-the-art performance while providing transparent and explainable results.