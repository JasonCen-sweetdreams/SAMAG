Distributed database systems (DDS) are increasingly used to manage large-scale data, but optimizing queries remains a challenging task. This paper proposes a novel approach to query optimization using reinforcement learning (RL). We design a deep RL framework that learns to optimize queries by interacting with the DDS environment, taking into account various performance metrics such as latency, throughput, and resource utilization. Our experimental results demonstrate that the proposed approach outperforms existing query optimization techniques, achieving an average reduction in query latency of 35% on a real-world dataset.