In this paper, we propose a decentralized task allocation framework for multi-agent systems using reinforcement learning. Our approach, called MATRL, enables agents to learn from their interactions with the environment and other agents, and adapt to changing task requirements and agent capabilities. We demonstrate the effectiveness of MATRL in a simulated logistics scenario, where it outperforms traditional centralized allocation methods in terms of task completion rate and agent utilization. We also show that MATRL scales to large numbers of agents and tasks, making it suitable for real-world applications.