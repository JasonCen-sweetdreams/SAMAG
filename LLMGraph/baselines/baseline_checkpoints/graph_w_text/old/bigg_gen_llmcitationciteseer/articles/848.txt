Multimodal emotion recognition (MER) systems often rely on fusion techniques that combine features from different modalities, such as speech, vision, and text. However, existing methods suffer from high computational costs and limited interpretability. This paper introduces a novel hierarchical attention network (HAN) for MER, which efficiently integrates modality-specific features using attention mechanisms. We propose a two-stage approach, where the first stage leverages intra-modal attention to extract relevant features, and the second stage employs inter-modal attention to fuse the representations. Experimental results on the CMU-MOSEI dataset demonstrate that our HAN outperforms state-of-the-art MER methods while reducing computational requirements by up to 40%.