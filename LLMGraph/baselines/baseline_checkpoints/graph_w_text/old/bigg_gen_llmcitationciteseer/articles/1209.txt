Distributed database systems are increasingly common in modern data centers, but optimizing queries across multiple nodes remains a challenging problem. This paper presents a novel approach to query optimization using reinforcement learning. Our method, 'DQRL', learns to select the optimal execution plan for a query by interacting with the database system and receiving rewards based on query latency. We demonstrate that DQRL outperforms traditional rule-based optimizers and is capable of adapting to changing workload patterns. Experimental results on a real-world distributed database system show significant improvements in query performance and scalability.