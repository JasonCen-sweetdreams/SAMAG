Sentiment analysis models often struggle to effectively integrate and weight information from multiple modalities, such as text, images, and audio. This paper proposes a novel hierarchical attention network architecture, HANA, which learns to selectively focus on relevant modalities and features at multiple scales. Our approach leverages self-attention mechanisms to model inter-modality relationships and adaptively fuse features, enabling more accurate sentiment predictions. Experimental results on benchmark datasets demonstrate that HANA outperforms state-of-the-art multi-modal fusion methods, while requiring fewer parameters and computations.