Virtual reality (VR) holds great promise for inclusive experiences, but current feedback mechanisms often rely on visual or auditory cues, excluding users with disabilities. This paper presents a novel adaptive multimodal feedback framework for VR, which integrates haptic, olfactory, and vestibular cues to accommodate diverse user needs. Our approach leverages machine learning-based user modeling and real-time sensor data to dynamically adjust feedback modalities. A user study with 30 participants demonstrates improved engagement, accessibility, and overall user experience in our adaptive VR system compared to traditional unimodal feedback.