Emotion recognition is a crucial aspect of human-robot interaction, enabling robots to respond empathetically to users. This paper presents a novel hierarchical attention network (HAN) architecture that integrates multi-modal data from speech, vision, and physiology to recognize emotions. Our HAN model consists of modality-specific attention modules that learn to selectively focus on relevant features, followed by a fusion layer that combines the modality-wise representations. Experimental results on the CMU-Multimodal dataset demonstrate that our approach outperforms state-of-the-art methods, achieving an average F1-score of 0.85 across six emotions. We also investigate the importance of individual modalities and their correlations, providing insights for robot design and emotion-aware interaction.