Autonomous vehicles rely on reinforcement learning (RL) to optimize control policies, but lack of transparency in decision-making hinders trust and safety. This paper proposes a novel hierarchical attention-based RL framework, 'HAREL', which incorporates explainability into policy optimization. HAREL learns to focus on relevant sensory inputs and abstract representations, providing insights into decision-making processes. We demonstrate improved performance and interpretability on a suite of autonomous driving benchmarks, showcasing the potential of HAREL for safety-critical applications.