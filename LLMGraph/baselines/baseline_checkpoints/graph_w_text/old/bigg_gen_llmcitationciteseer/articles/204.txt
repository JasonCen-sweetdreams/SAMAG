Question answering over knowledge graphs (KGs) has gained significant attention in recent years. However, existing methods suffer from either high computational complexity or inferior performance. This paper proposes a novel KG embedding model, 'HATKE', which leverages hierarchical attention mechanisms to selectively focus on relevant entities and relationships. We demonstrate that HATKE outperforms state-of-the-art methods on several benchmark datasets, achieving an average improvement of 12.5% in accuracy while reducing inference time by 30%. Our approach can be seamlessly integrated into various question answering architectures, making it a promising solution for real-world applications.