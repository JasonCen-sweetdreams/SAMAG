This paper presents a decentralized multi-agent reinforcement learning approach for adaptive traffic signal control. We model the traffic signal control problem as a partially observable Markov decision process and propose a novel algorithm, 'MARL-TSC', which enables multiple agents to learn cooperative policies for optimizing traffic flow. Our approach leverages graph neural networks to capture the spatial dependencies between intersections and incorporates a communication mechanism to facilitate information exchange between agents. Experimental results on a realistic traffic simulator demonstrate that MARL-TSC outperforms traditional fixed-time control and other state-of-the-art methods in reducing congestion and travel times.