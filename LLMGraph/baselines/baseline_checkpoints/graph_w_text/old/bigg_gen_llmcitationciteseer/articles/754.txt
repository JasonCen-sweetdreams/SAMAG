Deep neural networks have achieved state-of-the-art performance in various machine learning tasks, but their success heavily relies on proper hyperparameter tuning. This paper presents a Bayesian optimization framework, 'GP-Tune', which leverages Gaussian processes to efficiently search the hyperparameter space. We propose a novel acquisition function that balances exploration and exploitation, and demonstrate its effectiveness in tuning convolutional neural networks for image classification tasks. Experimental results show that GP-Tune outperforms popular tuning methods, including random search and grid search, while requiring significantly fewer function evaluations.