Voice assistants have become ubiquitous, but users with dysarthria, a speech disorder affecting articulation and intelligibility, often struggle to interact with these systems. This paper presents a novel approach to designing inclusive voice assistants that can better understand and respond to users with dysarthria. We propose a multi-modal fusion framework that combines acoustic, linguistic, and visual features to improve speech recognition accuracy. We also introduce a participatory design methodology that involves users with dysarthria in the design process. Our user study shows that the proposed system significantly improves the interaction experience for users with dysarthria, enabling them to successfully complete tasks with higher accuracy and confidence.