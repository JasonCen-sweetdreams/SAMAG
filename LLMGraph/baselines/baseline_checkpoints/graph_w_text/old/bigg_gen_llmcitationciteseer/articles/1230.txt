This paper presents a novel decentralized task allocation framework for autonomous agents, dubbed 'HRL-TA', which leverages hierarchical reinforcement learning to optimize task assignment and execution in dynamic environments. HRL-TA consists of a high-level task allocator that learns to allocate tasks to agents based on their capabilities and availability, and a low-level execution module that adapts to changing environmental conditions. Experimental results on a simulated multi-agent system demonstrate that HRL-TA outperforms traditional centralized and distributed approaches in terms of task completion rate, latency, and agent utilization.