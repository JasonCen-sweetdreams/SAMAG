Emotion recognition is a crucial aspect of human-computer interaction, but existing approaches often struggle with the complexity of multi-modal inputs. This paper introduces a novel hierarchical attention network (HAN) that leverages both spatial and temporal dependencies across modalities. Our HAN model consists of a hierarchical fusion mechanism that adaptively weights and integrates features from facial expressions, speech, and physiological signals. Experiments on a large-scale multi-modal dataset demonstrate that our approach achieves state-of-the-art performance in recognizing emotions and outperforms existing methods in handling noisy or missing data.