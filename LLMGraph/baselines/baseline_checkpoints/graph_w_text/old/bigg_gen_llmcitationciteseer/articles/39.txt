Knowledge graphs (KGs) are crucial for various AI applications, but often suffer from incomplete information. Entity disambiguation is a critical step in KG completion, as it resolves ambiguities in entity references. This paper proposes a novel graph attention network (GAT) architecture, 'KG-attend', which jointly models entity and relation semantics to disambiguate entities in KGs. KG-attend leverages graph attention to selectively focus on relevant entity-relation pairs and captures complex dependencies between them. Experimental results on several benchmark datasets demonstrate that KG-attend outperforms state-of-the-art methods in entity disambiguation and KG completion tasks.