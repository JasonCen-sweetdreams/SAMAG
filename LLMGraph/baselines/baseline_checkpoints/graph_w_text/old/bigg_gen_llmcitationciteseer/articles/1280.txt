Effective human-robot collaboration (HRC) requires seamless integration of multi-modal sensory inputs, including visual, auditory, and tactile data. This paper introduces a novel hierarchical attention network (HAN) architecture that enables efficient and interpretable fusion of these modalities. Our approach leverages separate attention mechanisms for each modality, which are then recursively combined to form a hierarchical representation. Experimental results on a real-world HRC dataset demonstrate that HAN outperforms state-of-the-art fusion methods in terms of task accuracy and computational efficiency, while providing insightful visualizations of attention weights.