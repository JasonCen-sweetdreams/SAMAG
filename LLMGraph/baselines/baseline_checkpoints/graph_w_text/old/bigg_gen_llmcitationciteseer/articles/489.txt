Explainability is a crucial aspect of natural language processing (NLP) models, as they are increasingly used in high-stakes applications. This paper proposes a novel hierarchical attention network (HAN) architecture that provides interpretable explanations for NLP tasks. Our HAN model employs a hierarchical structure to capture contextual relationships between words, sentences, and documents, and uses attention weights to highlight relevant input features. We demonstrate the effectiveness of our approach on several NLP benchmarks, including sentiment analysis, question answering, and text classification, and show that our model provides more accurate and informative explanations compared to existing attention-based methods.