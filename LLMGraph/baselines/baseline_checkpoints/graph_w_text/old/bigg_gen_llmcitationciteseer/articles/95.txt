Question answering over knowledge graphs (KGs) has gained significant attention in recent years. A crucial step in this process is learning effective KG embeddings that capture complex relationships between entities and their properties. This paper proposes a novel approach, HAT-KG, which incorporates hierarchical attention mechanisms to selectively focus on relevant KG subgraphs during embedding. Our experiments on several benchmarks demonstrate that HAT-KG outperforms state-of-the-art methods in terms of answer accuracy and inference speed, while requiring fewer parameters and computations.