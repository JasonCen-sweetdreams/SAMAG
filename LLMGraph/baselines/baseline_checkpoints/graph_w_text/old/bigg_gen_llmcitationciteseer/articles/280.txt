Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task, especially when requiring explainability. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that models complex relationships between modalities and affective features. Our approach leverages self-attention mechanisms to capture intra-modal and inter-modal dependencies, enabling the network to focus on salient features for emotion inference. We evaluate HAN on three benchmark datasets and demonstrate its superiority over state-of-the-art methods in terms of recognition accuracy and explainability, as measured by feature importance and attention visualization.