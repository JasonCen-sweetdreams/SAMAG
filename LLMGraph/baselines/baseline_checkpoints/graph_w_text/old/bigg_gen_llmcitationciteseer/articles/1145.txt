This paper presents a novel decentralized reinforcement learning framework for coordinating multi-agent systems in complex, dynamic environments. Our approach, called 'Dec-MARL', enables individual agents to learn optimal policies in a distributed manner, without relying on a centralized controller or explicit communication. We introduce a novel value function decomposition technique that allows agents to reason about their local objectives while accounting for the actions of neighboring agents. Empirical results on a series of benchmark problems demonstrate the scalability and adaptability of Dec-MARL in coordinating large teams of agents to achieve global objectives.