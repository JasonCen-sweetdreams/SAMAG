Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a challenging task, particularly when dealing with large datasets. This paper presents a novel Hierarchical Attention Network (HAN) architecture that leverages the strengths of both transformer-based encoders and graph neural networks. Our HAN model learns to selectively focus on relevant modalities and features, reducing computational overhead while improving recognition accuracy. Experimental results on the CMU-MOSI dataset demonstrate the effectiveness of our approach in recognizing emotions from noisy, real-world data.