Emotion recognition from multi-modal data, such as speech, text, and vision, remains a challenging task. This paper proposes a novel hierarchical attention network (HAN) architecture that learns to selectively focus on relevant modalities and segments. Our HAN model consists of a multi-modal fusion layer, followed by a hierarchical attention mechanism that adaptively weights the importance of each modality and segment. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal data, achieving an average F1-score improvement of 12.5%.