Accurate recognition of emotional states is crucial in human-computer interaction (HCI) to develop empathetic systems. This paper proposes a multimodal fusion approach that combines speech and facial expression features to recognize emotional states. We introduce a novel deep learning architecture that leverages attention mechanisms to adaptively weight the contributions of each modality. Experimental results on a large-scale emotional database demonstrate that our approach outperforms state-of-the-art unimodal and multimodal methods, achieving an average F1-score of 87.2% across six emotional categories.