Distributed database systems are increasingly used in modern data centers, but query optimization remains a significant challenge. In this paper, we propose a novel approach that leverages machine learning to predict optimal query execution plans. We develop a graph neural network (GNN) that learns to represent query graphs and database statistics, and demonstrate its effectiveness in reducing query latency and improving system throughput. Our experimental evaluation on a real-world dataset shows that our approach outperforms traditional rule-based optimizers and is robust to variations in workload and system configuration.