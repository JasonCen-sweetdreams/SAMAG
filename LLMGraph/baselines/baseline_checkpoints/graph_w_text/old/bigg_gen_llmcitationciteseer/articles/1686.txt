In complex multi-agent systems, efficient resource allocation is crucial for achieving collective goals. This paper proposes a hierarchical reinforcement learning framework, 'HierMA', which enables agents to coordinate their actions while balancing local and global objectives. We introduce a two-level hierarchy, where high-level agents learn to allocate resources and low-level agents adapt to changing environmental conditions. Experimental results on a simulated smart grid scenario demonstrate that HierMA outperforms traditional decentralized approaches in terms of resource utilization and agent satisfaction.