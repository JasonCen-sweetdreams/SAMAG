Visual question answering (VQA) tasks require models to accurately answer questions about images. However, existing models often lack interpretability, making it difficult to understand their decision-making processes. This paper proposes a novel hierarchical attention-based multi-modal fusion framework, 'HAFusion', which integrates visual and linguistic features to generate interpretable attention maps and answers. We evaluate HAFusion on the VQA 2.0 dataset and demonstrate its superiority over state-of-the-art methods in terms of both accuracy and explainability.