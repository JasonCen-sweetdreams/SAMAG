As multi-agent reinforcement learning (MARL) systems grow in complexity, understanding their decision-making processes becomes increasingly challenging. This paper introduces HAN-MARL, a novel hierarchical attention framework that enables explainable MARL by selectively emphasizing relevant agent interactions and observations. Our approach leverages a two-level attention mechanism, first aggregating local observations and then integrating agent-level representations to produce a global policy. Experimental results on several benchmark domains demonstrate that HAN-MARL outperforms state-of-the-art MARL methods while providing interpretable insights into agent decision-making processes.