Deep neural networks have achieved state-of-the-art performance in various applications, but their success heavily relies on careful hyperparameter tuning. This paper presents a novel Bayesian optimization approach, 'HyperBO', which leverages a probabilistic surrogate model to efficiently search for optimal hyperparameters. We propose a new acquisition function that balances exploration and exploitation, and demonstrate its effectiveness in reducing the number of evaluations required to achieve good performance. Experimental results on several benchmark datasets show that HyperBO outperforms popular hyperparameter tuning methods, including grid search and random search.