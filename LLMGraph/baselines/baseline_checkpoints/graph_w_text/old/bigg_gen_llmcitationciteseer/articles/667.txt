Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the varying importance of each modality. This paper proposes a Hierarchical Attention Network (HAN) that dynamically weights the contributions of each modality and captures both local and global contextual information. Our HAN model outperforms state-of-the-art methods on the CMU-MOSEI dataset, achieving an improvement of 12.3% in emotion recognition accuracy. We also demonstrate the effectiveness of our approach in real-world applications, such as affective computing and human-computer interaction.