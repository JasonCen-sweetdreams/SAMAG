Sentiment analysis has become a vital tool in various applications, including customer service and product development. However, most existing approaches rely on a single modality, such as text or images, and often lack interpretability. This paper proposes a hierarchical attention network (HAN) that integrates multiple modalities, including text, images, and audio, for sentiment analysis. Our HAN model learns to selectively focus on relevant regions across modalities, providing explainable insights into the sentiment prediction process. Experimental results on a large-scale multi-modal dataset demonstrate the effectiveness of our approach, achieving state-of-the-art performance while offering visual and textual explanations for the predicted sentiment.