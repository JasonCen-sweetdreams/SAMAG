Multimodal sentiment analysis (MSA) aims to analyze sentiment from multiple sources such as text, images, and audio. Existing methods often lack transparency and interpretability, making it challenging to understand the decision-making process. We propose a hierarchical attention network (HAN) for MSA, which leverages self-attention mechanisms to capture intra-modal and inter-modal relationships. Our approach generates attention weights that provide insights into the importance of each modality and feature, thus enabling explainability. Experimental results on two benchmark datasets demonstrate the effectiveness of our method, achieving state-of-the-art performance while providing interpretable results.