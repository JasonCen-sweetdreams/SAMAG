Emotion recognition from multi-modal inputs, such as speech, text, and vision, is a challenging task due to the complex relationships between modalities. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) to effectively capture and integrate modality interactions. HGAT employs a hierarchical graph structure to model both intra- and inter-modality relationships, and uses attention mechanisms to selectively focus on salient features. Experimental results on the IEMOCAP dataset demonstrate that HGAT outperforms state-of-the-art approaches in recognizing emotions from multi-modal inputs, achieving an average F1-score improvement of 7.2%.