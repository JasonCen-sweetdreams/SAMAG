This paper presents a decentralized multi-agent reinforcement learning framework for dynamic task allocation in complex systems. We introduce a novel communication protocol that enables agents to share task-related information and adapt to changing environmental conditions. Our approach leverages graph neural networks to model agent interactions and incorporates a hierarchical exploration strategy to balance global coordination and local optimization. Experimental results demonstrate improved task completion rates and reduced communication overhead compared to traditional centralized approaches.