Knowledge graph embeddings (KGEs) are crucial for various AI applications, but existing methods struggle to capture complex relationships between entities. We propose a novel hierarchical graph attention network (HGAT) architecture for multi-relational KGEs. HGAT leverages graph attention mechanisms to selectively focus on relevant relationships and entities at each hierarchical level, enabling more effective representation learning. Experimental results on benchmark datasets demonstrate that HGAT outperforms state-of-the-art KGE methods in link prediction, entity classification, and question answering tasks.