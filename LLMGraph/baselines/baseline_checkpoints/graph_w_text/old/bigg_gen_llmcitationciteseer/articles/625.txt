Few-shot learning has made significant progress in recent years, but existing methods struggle to scale to large graphs. This paper proposes Hierarchical Graph Attention Networks (HGAT), a novel architecture that leverages hierarchical graph representations to tackle few-shot learning on large-scale graph-structured data. HGAT employs a dual-attention mechanism that learns to selectively focus on relevant subgraphs and nodes, allowing it to adapt to novel classes with limited data. We conduct extensive experiments on several benchmarks, demonstrating HGAT's superior performance and scalability compared to state-of-the-art few-shot learning methods.