Multi-modal recommendation systems leverage heterogeneous data sources, such as text, images, and user interactions, to provide personalized suggestions. This paper proposes a novel graph neural network (GNN) framework, 'HMGNN', that effectively integrates and learns from these diverse modalities. By incorporating attention mechanisms and meta-learning, HMGNN adaptively weights and fuses the strengths of each modality to improve recommendation accuracy. Our experiments on three real-world datasets demonstrate significant performance gains over state-of-the-art methods, particularly in cold-start scenarios.