Sentiment analysis from multi-modal data (e.g., text, images, and videos) has become increasingly important for businesses and organizations. Prior works on multi-modal sentiment analysis have largely relied on early fusion methods, which fail to capture complex interactions between modalities. This paper proposes a novel Hierarchical Attention Neural Network (HANN) architecture that learns to selectively focus on relevant modalities and features at multiple levels. Experiments on a large-scale dataset demonstrate that HANN outperforms state-of-the-art methods by up to 12% in terms of accuracy and F1-score.