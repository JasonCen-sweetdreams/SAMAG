Neural architecture search (NAS) has revolutionized the design of deep neural networks, but existing methods suffer from high computational costs and limited scalability. This paper proposes a novel hierarchical graph-based encoding scheme, GraphNAS++, which significantly reduces the search space while preserving the expressive power of the architecture. We leverage graph neural networks to learn a hierarchical representation of the search space, allowing for efficient pruning and ranking of candidate architectures. Experimental results on several benchmark datasets demonstrate that GraphNAS++ achieves state-of-the-art performance with a 3x reduction in search time compared to existing methods.