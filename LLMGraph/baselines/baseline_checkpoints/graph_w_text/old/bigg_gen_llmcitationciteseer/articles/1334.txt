Zero-shot learning (ZSL) has gained significant attention in natural language processing (NLP) tasks, enabling models to generalize to unseen classes and datasets. However, existing ZSL methods often suffer from high computational costs and limited scalability. This paper proposes a novel approach, dubbed 'ZSL- Efficient', which leverages a hierarchical attention mechanism and a lightweight adapter module to achieve efficient ZSL for NLP tasks. Our experiments on several benchmark datasets demonstrate that ZSL-Efficient outperforms state-of-the-art ZSL methods while reducing computational costs by up to 75%.