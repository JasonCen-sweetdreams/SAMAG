The proliferation of voice assistants in smart homes and IoT devices has led to a growing need to understand user experience beyond traditional speech-based input. This paper proposes a novel multimodal fusion approach that combines speech and gesture recognition to analyze user interactions with voice assistants. Our approach leverages computer vision and machine learning techniques to identify patterns in user gestures that correlate with speech-based input. We conducted a user study with 30 participants, demonstrating that our approach can accurately predict user frustration and satisfaction levels, and provide actionable insights for improving voice assistant design. The results have implications for designing more intuitive and user-centered voice assistants.