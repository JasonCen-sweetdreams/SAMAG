Multimodal sentiment analysis (MSA) has gained popularity with the increasing availability of multimedia data. Existing approaches often rely on complex neural architectures, making it challenging to interpret the decision-making process. We propose a novel hierarchical attention network (HAN) for MSA, which incorporates attention mechanisms at both the feature and modal levels. Our approach enables the identification of salient features and modalities contributing to the sentiment prediction, thereby providing explainability. Experimental results on a benchmark dataset demonstrate the effectiveness of HAN in achieving state-of-the-art performance while offering insights into the decision-making process.