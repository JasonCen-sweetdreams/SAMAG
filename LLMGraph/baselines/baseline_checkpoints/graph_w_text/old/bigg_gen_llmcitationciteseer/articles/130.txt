Open-domain question answering (ODQA) has gained significant attention in recent years, but most state-of-the-art models rely on inefficient retrieval mechanisms that scale poorly with large knowledge bases. This paper presents DPR-QA, a novel dense passage retrieval framework that leverages pre-trained language models to encode passages and questions into a shared vector space. We demonstrate that DPR-QA achieves comparable or better performance than traditional sparse retrieval methods on several ODQA benchmarks while reducing the inference time by an order of magnitude. Furthermore, we analyze the effects of different encoding strategies and knowledge base sizes on DPR-QA's performance.