Conversational search systems aim to retrieve relevant information from natural language queries. However, the complexity of user queries often leads to ineffective retrieval. This paper introduces a novel deep learning approach, 'ConRef', which reformulates user queries to improve search effectiveness. ConRef leverages a transformer-based architecture to capture contextual dependencies between query terms and generates a reweighted query representation. Experimental results on the TREC Conversational Assistance Track dataset demonstrate that ConRef outperforms state-of-the-art query reformulation techniques, achieving a 15% increase in mean average precision.