Emotion recognition from multimodal data (e.g., audio, video, text) is an essential task in human-computer interaction. However, existing approaches often suffer from limited contextual understanding and modal imbalance. We propose a Hierarchical Attention Network (HAN) that leverages intra- and inter-modal relationships to improve emotion recognition performance. Our HAN model consists of a multimodal fusion module, which aggregates features from different modalities, and a hierarchical attention mechanism that selectively focuses on relevant regions and modes. Experimental results on the CMU-MOSI dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multimodal data, achieving an average F1-score improvement of 8.5%.