Emotion recognition in human-computer interaction (HCI) is crucial for creating empathetic interfaces. However, existing approaches struggle to handle multi-modal inputs (e.g., audio, video, and text) and often rely on hand-crafted features. This paper proposes a hierarchical attention network (HAN) that learns to fuse and selectively focus on relevant modalities for emotion recognition. Our HAN model consists of a novel attention mechanism that iteratively refines modality importance weights, allowing it to adapt to diverse emotional expressions and noisy inputs. Experimental results on the IEMOCAP and SEMAINE datasets demonstrate that our approach outperforms state-of-the-art methods in recognizing complex emotions and achieving robustness to missing or corrupted modalities.