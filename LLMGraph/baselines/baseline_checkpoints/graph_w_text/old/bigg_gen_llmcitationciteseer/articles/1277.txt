Virtual reality (VR) systems have limited understanding of users' attention and intentions, leading to suboptimal user experiences. We propose a novel gaze-informed multimodal interaction framework, 'GazeMix', which integrates eye-tracking, speech recognition, and hand-tracking sensors to infer users' focus and goals in VR environments. Our system employs a probabilistic graphical model to fuse multimodal cues, enabling more accurate and context-aware interaction. We demonstrate the effectiveness of GazeMix in a VR-based puzzle game, showing significant improvements in user performance and satisfaction compared to traditional input modalities.