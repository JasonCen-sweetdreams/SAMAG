In multi-agent systems, task-oriented coordination is crucial for achieving complex goals. This paper presents a hierarchical reinforcement learning framework, called 'HRL-Coor', that enables agents to learn coordinated policies for task decomposition and allocation. HRL-Coor employs a high-level task manager to decompose tasks into sub-tasks and allocate them to individual agents, which then learn to execute these sub-tasks using deep Q-networks. We evaluate HRL-Coor in a simulated disaster response scenario, demonstrating improved task completion rates and reduced communication overhead compared to decentralized reinforcement learning approaches.