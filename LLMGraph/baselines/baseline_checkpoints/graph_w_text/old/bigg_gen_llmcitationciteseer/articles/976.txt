Multimodal sentiment analysis has gained significance in understanding human emotions and opinions. However, existing approaches often lack transparency and interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates vision, language, and acoustic features to predict sentiment. We introduce a multimodal attention mechanism that selectively focuses on relevant modalities and time segments, enabling explainable sentiment predictions. Experimental results on the CMU-MOSEI dataset demonstrate that our HAN model outperforms state-of-the-art methods while providing insights into modality contribution and temporal attention patterns.