Accurate affective state recognition is crucial in human-computer interaction (HCI) to create empathetic and personalized interfaces. This paper presents EmoTract, a novel multimodal framework that integrates computer vision, natural language processing, and physiological signal processing to recognize users' emotional states. EmoTract leverages a hybrid deep learning architecture to fuse features from facial expressions, speech patterns, and galvanic skin response. Our evaluation on a diverse dataset of 150 participants shows that EmoTract outperforms existing unimodal and multimodal approaches, achieving an average F1-score of 0.92 in recognizing six basic emotions.