Emotion recognition from multi-modal data (e.g., facial expressions, speech, and text) is a challenging task due to the varying importance of different modalities. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features. Our HAN model consists of two stages: intra-modal attention to weigh feature importance within each modality, and inter-modal attention to combine representations from multiple modalities. Experimental results on the CMU-MOSEI dataset show that our approach outperforms state-of-the-art methods in recognizing emotions from speech, text, and facial expressions, while reducing computational complexity by 30%.