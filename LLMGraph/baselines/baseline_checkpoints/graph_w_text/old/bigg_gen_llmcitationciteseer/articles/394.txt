This paper presents a novel hierarchical attention network (HAN) for multi-modal emotion recognition. Our approach leverages the complementary strengths of speech, text, and vision modalities to improve emotion recognition accuracy. We propose a multi-level attention mechanism that adaptively weights the contributions of each modality and captures the hierarchical relationships between modalities. Experimental results on the IEMOCAP dataset demonstrate that our HAN outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving a weighted accuracy of 83.2% and an F1-score of 84.5%. Our approach has promising applications in human-computer interaction, affective computing, and mental health diagnosis.