Emotion recognition in human-robot interaction (HRI) is crucial for robots to respond empathetically. Existing approaches primarily rely on a single modality, such as speech or vision, and often struggle to capture the complexity of human emotions. We propose a novel hierarchical attention network (HAN) that integrates multimodal inputs from speech, vision, and tactile sensors to recognize emotions in HRI. Our HAN model learns to selectively focus on relevant modalities and temporal segments, improving emotion recognition accuracy and robustness. Experimental results on the HRI-E dataset demonstrate the superiority of our approach over state-of-the-art methods, with an average F1-score improvement of 12.5%.