Coordinating autonomous agent teams to achieve complex tasks requires efficient task decomposition and allocation. This paper presents a hierarchical task decomposition framework that leverages deep reinforcement learning to optimize task allocation and execution in multi-agent systems. Our approach, called HTD-Q, integrates a high-level task decomposition module with a low-level Q-learning module that adaptively adjusts to changing environmental conditions. Experimental results in a real-world robotic soccer domain demonstrate that HTD-Q outperforms state-of-the-art methods in terms of task completion time and team coordination.