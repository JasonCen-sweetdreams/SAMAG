Emotion recognition from multi-modal data, such as speech, text, and facial expressions, is a challenging task due to the complexity of human emotions and the varying quality of input data. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages the strengths of each modality to improve emotion recognition accuracy. Our HAN model employs a hierarchical attention mechanism to selectively focus on relevant features from each modality, and a gating mechanism to adaptively combine the outputs from each modality. Experimental results on the benchmark IEMOCAP dataset demonstrate that our approach achieves state-of-the-art performance, outperforming existing multi-modal fusion methods by a significant margin.