Emotion recognition from multi-modal inputs, such as speech, text, and vision, is a challenging task due to the complexity of human emotions and the heterogeneity of modalities. This paper introduces a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and time-steps to recognize emotions. Our HAN model consists of three levels of attention: modality-level, time-step-level, and utterance-level. Experimental results on the IEMOCAP and CMU-MOSEI datasets show that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an average F1-score improvement of 12.5%.