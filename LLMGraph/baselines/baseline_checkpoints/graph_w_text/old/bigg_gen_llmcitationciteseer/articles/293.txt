Emotion recognition is a crucial task in human-computer interaction, but existing approaches often struggle with the complexity of multi-modal data. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates facial, vocal, and linguistic cues for more accurate emotion recognition. Our HAN model employs a stacked attention mechanism to selectively focus on relevant features across modalities, and a hierarchical fusion strategy to combine modality-specific representations. Experimental results on the IEMOCAP dataset demonstrate significant improvements in recognition accuracy and F1-score compared to state-of-the-art methods.