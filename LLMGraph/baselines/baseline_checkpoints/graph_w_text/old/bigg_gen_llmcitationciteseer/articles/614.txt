Graph neural networks (GNNs) have achieved state-of-the-art performance in various node classification tasks. However, existing methods often suffer from over-smoothing and inability to capture hierarchical structures in graphs. This paper proposes a novel hierarchical attention-based GNN framework, 'HAGN', which leverages self-attention mechanisms to adaptively weight nodes at different hierarchical levels. We demonstrate the effectiveness of HAGN on several benchmark datasets, showing significant improvements in node classification accuracy and robustness to graph perturbations.