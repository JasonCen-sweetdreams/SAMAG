Multi-hop question answering (QA) requires models to reason over multiple sentences to arrive at the correct answer. While recent advances in transformer-based architectures have improved performance, they often lack transparency in their decision-making process. This paper proposes a novel hierarchical attention-based approach, 'HARE', that enables explainable reasoning in multi-hop QA. HARE utilizes a hierarchical attention mechanism to selectively focus on relevant sentences and entities, generating attention weights that provide insights into the model's reasoning process. Experimental results on the HotPotQA dataset demonstrate that HARE achieves state-of-the-art performance while providing interpretable explanations for its predictions.