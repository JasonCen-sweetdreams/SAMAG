Traditional document retrieval methods rely on bag-of-words representations, which fail to capture complex query semantics. This paper proposes a novel neural approach, 'NQGE', which learns graph embeddings of queries and documents in a shared vector space. By modeling query-document relationships as graph structures, NQGE can effectively capture entity relationships, synonyms, and context-dependent relevance. Experimental results on the TREC-8 dataset demonstrate that NQGE outperforms state-of-the-art methods in terms of precision, recall, and efficiency, especially for complex, multi-entity queries.