Emotion recognition in human-robot interaction (HRI) is crucial for creating empathetic and responsive robots. This paper proposes a novel hierarchical attention network (HAN) that integrates multi-modal inputs from speech, text, and vision to recognize emotions in HRI. Our HAN model consists of three hierarchical levels, each capturing attention weights at different granularities. We evaluate our approach on a large-scale HRI dataset and demonstrate significant improvements over state-of-the-art methods in recognizing complex emotions such as surprise and disgust. Furthermore, our model provides interpretable attention visualizations, enabling robots to better understand human emotions and respond accordingly.