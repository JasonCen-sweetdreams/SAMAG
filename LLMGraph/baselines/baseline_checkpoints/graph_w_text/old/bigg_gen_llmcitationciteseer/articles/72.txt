In multi-agent systems, decision-making often relies on complex interactions between agents, making it challenging to explain the underlying reasoning. This paper presents a novel hierarchical attention network (HAN) architecture that enables explainable decision-making in multi-agent settings. Our approach leverages attention mechanisms to selectively focus on relevant agents and their interactions, providing a transparent and interpretable decision-making process. We evaluate our HAN model on a real-world traffic management scenario, demonstrating improved performance and explainability compared to state-of-the-art methods. Our results have significant implications for the development of trustworthy AI systems in multi-agent environments.