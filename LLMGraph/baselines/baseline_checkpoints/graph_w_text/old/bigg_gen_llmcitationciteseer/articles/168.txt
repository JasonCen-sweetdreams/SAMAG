Effective human-robot interaction (HRI) relies on accurate emotion recognition. We propose a hierarchical attention network (HAN) that integrates facial expressions, speech, and physiological signals to recognize emotions in real-time. Our HAN framework consists of modality-specific attention modules that learn to focus on salient features in each input modality, followed by a hierarchical fusion layer that combines the modality-wise representations. Experimental results on a large-scale HRI dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing complex emotions, such as confusion and frustration, with improved robustness to noise and variations in lighting conditions.