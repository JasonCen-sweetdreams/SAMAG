Distributed database systems are increasingly popular for handling large-scale data analytics. However, optimizing query performance remains a significant challenge due to the complexity of query plans and data distribution. This paper proposes a novel approach that leverages machine learning to optimize query performance in distributed database systems. We develop a query performance predictor using graph neural networks, which takes into account query plans, data statistics, and system workload. Our approach is able to predict query execution times with high accuracy and outperforms traditional optimization techniques. Experimental results on a real-world distributed database system demonstrate significant improvements in query performance and resource utilization.