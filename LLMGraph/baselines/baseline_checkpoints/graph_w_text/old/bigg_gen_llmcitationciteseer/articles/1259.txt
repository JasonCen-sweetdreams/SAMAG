The increasing adoption of AI-driven sentiment analysis in high-stakes applications has raised concerns about model interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates explainability into the sentiment analysis pipeline. Our approach leverages hierarchical attention mechanisms to identify salient features and phrases contributing to the predicted sentiment, enabling users to understand model decisions. Experiments on several benchmark datasets demonstrate that HAN achieves state-of-the-art performance while providing transparent and interpretable results.