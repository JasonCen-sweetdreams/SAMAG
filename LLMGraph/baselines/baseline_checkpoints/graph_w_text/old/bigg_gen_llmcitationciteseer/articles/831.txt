Recent advances in natural language processing (NLP) have been driven by large-scale pre-training and fine-tuning. However, many real-world NLP applications require adapting to new tasks with limited labeled data. This paper proposes a novel meta-learning framework, 'MetaNLP', that enables few-shot learning for various NLP tasks. We introduce a task-agnostic meta-learner that learns to adapt to new tasks by exploiting cross-task similarities and differences. Experimental results on a range of few-shot NLP benchmarks demonstrate that MetaNLP achieves state-of-the-art performance, often surpassing task-specific fine-tuning approaches.