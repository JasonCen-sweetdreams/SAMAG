Autonomous vehicles require sophisticated control systems that can adapt to diverse driving scenarios. This paper presents a novel deep hierarchical reinforcement learning (HRL) framework for autonomous vehicle control, which leverages a hierarchical policy structure to balance exploration and exploitation. Our approach, 'DeepHiRL', features a high-level policy that selects among multiple low-level controllers, each specialized for specific driving tasks (e.g., lane following, intersection navigation). We demonstrate the effectiveness of DeepHiRL in simulated urban driving environments, achieving improved stability, safety, and fuel efficiency compared to state-of-the-art reinforcement learning methods.