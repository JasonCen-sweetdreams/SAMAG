In multi-agent systems, decision-making often relies on complex interactions between agents. While recent advances in deep learning have improved decision-making performance, the lack of interpretability hinders trust and accountability. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that enables explainable decision-making in multi-agent systems. By incorporating attention mechanisms at both the agent and system levels, HANs provide insights into the decision-making process, revealing the most influential agents and interactions. Experimental results on a decentralized resource allocation task demonstrate the effectiveness of HANs in improving decision-making performance while providing interpretable results.