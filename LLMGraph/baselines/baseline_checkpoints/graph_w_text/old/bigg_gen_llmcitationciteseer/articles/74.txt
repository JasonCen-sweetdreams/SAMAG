Virtual reality (VR) has immense potential for people with disabilities, but existing interfaces often rely on manual inputs, which can be challenging or impossible for individuals with motor impairments. This paper presents EyeGaze+, a novel gaze-based interface that enables users to interact with VR environments using only their eye movements. Our approach leverages machine learning-based gaze estimation and combines it with a probabilistic inference engine to accurately detect user intentions. We evaluate EyeGaze+ with 20 participants, including those with motor impairments, and demonstrate significant improvements in interaction speed and accuracy compared to traditional interfaces.