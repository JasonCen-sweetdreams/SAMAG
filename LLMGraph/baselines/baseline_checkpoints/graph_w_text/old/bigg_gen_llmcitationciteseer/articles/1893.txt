Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging AI problem. This paper introduces HAN-EMO, a hierarchical attention network that leverages intra- and inter-modal relationships to improve emotion recognition accuracy. Our model uses a novel self-attention mechanism to weight modalities based on their emotional relevance, enabling interpretable emotion predictions. Experimental results on the CMU-MOSEI dataset demonstrate HAN-EMO's superior performance over state-of-the-art approaches, achieving an average F1-score of 0.83 across six emotions.