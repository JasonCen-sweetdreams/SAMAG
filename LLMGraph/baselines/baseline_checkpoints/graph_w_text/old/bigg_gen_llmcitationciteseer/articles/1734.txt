Reinforcement learning (RL) has achieved remarkable success in various applications, but its vulnerability to adversarial attacks remains a significant concern. This paper focuses on partially observable Markov decision processes (POMDPs), which are commonly encountered in real-world scenarios. We propose a novel attack framework, 'AdverPO', that generates targeted perturbations to the observation space, effectively deceiving RL policies into taking suboptimal actions. Our experiments on a suite of POMDP benchmarks demonstrate the efficacy of AdverPO in compromising policy performance, emphasizing the need for robustness measures in RL-based systems.