Explainability is crucial in multi-agent reinforcement learning (MARL) for real-world applications. We propose a novel hierarchical attention network (HAN) architecture that enables interpretable policy learning in MARL. HAN consists of two stages: (1) intra-agent attention, which models the relationships between an agent's observations and actions, and (2) inter-agent attention, which captures the interactions between agents. Our approach outperforms existing MARL methods on a suite of cooperative and competitive tasks, while providing insights into the decision-making process. We demonstrate the effectiveness of HAN in a real-world robotics scenario, where agents learn to collaborate and communicate to achieve a common goal.