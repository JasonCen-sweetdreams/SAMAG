Multimodal sentiment analysis aims to predict sentiment from multimodal data, such as text, images, and audio. Existing approaches typically rely on early or late fusion, which can lead to information loss or misalignment. This paper proposes a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and regions within each modality. Our HAN model consists of three attention layers: intra-modality attention, inter-modality attention, and fusion attention. Experimental results on two benchmark datasets demonstrate that our approach outperforms state-of-the-art methods by 3.2% and 2.5% in terms of accuracy and F1-score, respectively.