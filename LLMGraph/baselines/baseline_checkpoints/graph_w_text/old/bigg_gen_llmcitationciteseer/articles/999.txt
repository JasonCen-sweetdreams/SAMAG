Few-shot image classification remains a challenging problem in computer vision, particularly when faced with novel classes and limited training data. This paper introduces a novel deep meta-learning approach, 'AdaTask', which leverages adaptive task embeddings to improve model adaptability and few-shot performance. Our method learns to generate task-specific embeddings that capture the underlying structure of each task, and uses these embeddings to modulate the model's behavior during few-shot learning. Experimental results on several benchmark datasets demonstrate that AdaTask outperforms state-of-the-art few-shot learning methods, achieving significant gains in accuracy and robustness.