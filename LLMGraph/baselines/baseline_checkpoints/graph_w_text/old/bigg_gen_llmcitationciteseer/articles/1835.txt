Dense passage retrieval (DPR) has emerged as a promising approach for open-domain question answering. However, existing methods often rely on simplistic query representations, which can limit their retrieval performance. This paper proposes a novel hierarchical attention network (HAN) to enhance query representation for DPR. Our HAN model learns to selectively focus on relevant query terms and contextualize them within the passage. Experimental results on the Natural Questions dataset demonstrate that our approach significantly outperforms state-of-the-art DPR methods, achieving a 7.5% absolute improvement in retrieval accuracy.