Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the complex relationships between modalities. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) to effectively capture these interactions. HGAT uses a hierarchical graph structure to model intra- and inter-modal relationships, and employs attention mechanisms to selectively focus on relevant modalities and features. Our experiments on the IEMOCAP dataset demonstrate that HGAT outperforms state-of-the-art methods in recognizing emotions from multi-modal input, achieving an F1-score of 0.853.