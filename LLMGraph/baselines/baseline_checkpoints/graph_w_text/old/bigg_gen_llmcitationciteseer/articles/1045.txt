Augmented reality (AR) systems have revolutionized human-computer interaction, but current interaction methods often rely on manual input or voice commands. This paper explores the potential of gaze-based interaction for AR, leveraging the high accuracy and naturalness of eye movements. We conduct a user study to analyze eye movement patterns in AR environments, identifying distinct saccadic and fixation behaviors. We then propose a machine learning-based approach to classify user intentions from gaze data, achieving an accuracy of 92.5% in distinguishing between selection and navigation tasks. Our findings inform the design of gaze-aware AR interfaces, enabling more intuitive and efficient user interaction.