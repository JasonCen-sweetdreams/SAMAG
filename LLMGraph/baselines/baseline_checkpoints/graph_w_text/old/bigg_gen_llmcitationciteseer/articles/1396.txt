Virtual reality (VR) has the potential to enhance accessibility for users with disabilities, but existing feedback mechanisms often rely on visual or auditory cues that may not be equally effective for all users. This paper presents a novel framework for adaptive multimodal feedback in VR, which leverages machine learning to predict individual user preferences and adapt feedback accordingly. Our approach integrates gaze-tracking, speech recognition, and haptic feedback to provide a more inclusive and immersive VR experience. A user study with 30 participants demonstrates significant improvements in task completion time and user satisfaction compared to traditional unimodal feedback methods.