Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task, particularly when dealing with ambiguity and variability in human emotional expressions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features for emotion recognition. Our approach leverages attention mechanisms at both the feature and modality levels, enabling the model to provide interpretable explanations for its predictions. Experimental results on the CMU-MOSEI dataset demonstrate the effectiveness of HAN in improving emotion recognition accuracy and providing meaningful interpretability.