Emotion recognition from multimodal inputs (e.g., speech, text, vision) remains a challenging task in affective computing. This paper presents a novel Hierarchical Attention Network (HAN) architecture that leverages both intra-modal and inter-modal relationships to improve emotion recognition accuracy. Our HAN model consists of modality-specific attention modules and a hierarchical fusion layer, allowing it to capture complex contextual dependencies across modalities. Experimental results on the IEMOCAP and CMU-MOSEI datasets demonstrate that our approach outperforms state-of-the-art multimodal emotion recognition methods, achieving an average F1-score improvement of 7.2%.