Emotion recognition from multimodal data (e.g., speech, text, vision) is a challenging problem in human-computer interaction. This paper proposes a novel hierarchical attention network (HAN) architecture that learns to selectively focus on relevant modality-specific features and interactions. Our approach achieves state-of-the-art performance on the CMU-MOSEI dataset and provides visualizations of attention weights, enabling explainability and interpretability of emotion recognition models. We demonstrate the effectiveness of HAN in various applications, including affective computing, sentiment analysis, and human-robot interaction.