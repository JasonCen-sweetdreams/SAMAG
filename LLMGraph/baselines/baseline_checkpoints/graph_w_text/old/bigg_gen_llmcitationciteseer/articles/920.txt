Multimodal sentiment analysis (MSA) involves inferring sentiment from heterogeneous data sources, such as text, images, and audio. We propose a novel hierarchical attention network (HAN) that captures both intra-modal and inter-modal relationships between input modalities. Our HAN architecture consists of modality-specific attention layers and a fusion layer that adaptively weights the modalities based on their relevance to the sentiment task. Experimental results on the MM-IMDB dataset demonstrate that our approach outperforms state-of-the-art methods in MSA, especially in scenarios with missing or noisy modalities.