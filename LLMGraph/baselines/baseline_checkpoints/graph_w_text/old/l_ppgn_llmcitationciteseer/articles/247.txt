Emotion recognition from multimodal inputs (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the heterogeneity of modalities. We propose a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and temporal segments. Our HAN model consists of multiple attention layers that recursively refine the attention weights, enabling the model to capture subtle emotional cues. Experiments on three benchmark datasets demonstrate that our approach outperforms state-of-the-art multimodal fusion methods and provides interpretable attention visualizations.