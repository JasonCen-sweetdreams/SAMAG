Explainability is a critical component in real-world reinforcement learning (RL) applications, as it enables humans to understand and trust the decision-making process of agents. This paper presents a novel hierarchical attention framework, HARE, that disentangles complex reward functions into interpretable components. HARE leverages a hierarchical neural architecture to identify and prioritize relevant state features, thereby enabling the agent to focus on the most informative aspects of the environment. We demonstrate the effectiveness of HARE on a variety of RL benchmarks, showcasing improved interpretability and robustness compared to state-of-the-art methods.