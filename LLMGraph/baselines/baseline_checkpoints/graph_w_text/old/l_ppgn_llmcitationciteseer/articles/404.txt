Emotion recognition is a crucial aspect of human-computer interaction, but existing approaches often rely on single modalities (e.g., facial expressions or speech). This paper proposes a novel hierarchical attention network (HAN) that integrates multimodal inputs (vision, audio, and text) to recognize emotions in a more comprehensive and accurate manner. Our HAN model learns to selectively focus on relevant modalities and features, leveraging their complementary strengths to improve emotion recognition performance. Experiments on a large-scale multimodal dataset demonstrate the effectiveness of our approach, outperforming state-of-the-art single-modality and early-fusion methods.