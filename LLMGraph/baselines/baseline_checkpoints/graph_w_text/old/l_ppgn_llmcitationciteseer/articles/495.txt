Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a challenging task, particularly when requiring model interpretability. We propose a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features for emotion classification. Our approach employs a hierarchical attention mechanism, where modality-level attention weights are learned to combine features from each modality, followed by a feature-level attention module to identify salient features within each modality. Experiments on the IEMOCAP dataset demonstrate that our HAN model outperforms state-of-the-art methods in emotion recognition accuracy while providing insightful attention visualizations for model explainability.