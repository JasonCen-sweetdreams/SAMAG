Emotion recognition is a crucial aspect of human-robot interaction, enabling robots to respond empathetically and build trust with users. This paper presents a novel hierarchical attention network (HAN) architecture that integrates multimodal inputs from speech, vision, and physiological sensors to recognize emotions in real-time. Our approach leverages attention mechanisms to selectively weigh the importance of different modalities and temporal segments, achieving state-of-the-art performance on the EMOTION dataset. We demonstrate the effectiveness of our HAN model in a human-robot interaction scenario, where it enables a robot to recognize and respond to user emotions with improved accuracy and empathy.