This paper presents a novel approach to gesture recognition systems that adapt to the unique abilities and limitations of individuals with motor impairments. We propose a machine learning framework that leverages transfer learning and multimodal sensor fusion to recognize gestures from a diverse range of input devices, including wearable sensors, computer vision, and eye-tracking systems. Our user-centered design methodology involves participatory design workshops and iterative testing with individuals with motor impairments, resulting in a system that achieves high recognition accuracy and user satisfaction. We demonstrate the effectiveness of our approach through a case study involving individuals with cerebral palsy.