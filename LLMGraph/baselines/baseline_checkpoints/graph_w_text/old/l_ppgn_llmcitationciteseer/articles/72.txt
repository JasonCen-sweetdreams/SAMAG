Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task, particularly when it comes to explaining the underlying decision-making process. This paper proposes a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features for emotion classification. We introduce a novel attention mechanism that models the interactions between modalities and incorporates domain knowledge from psychology. Experimental results on a benchmark dataset demonstrate that our approach outperforms state-of-the-art methods in terms of accuracy and yields interpretable attention weights, providing insights into the emotional cues used by the model.