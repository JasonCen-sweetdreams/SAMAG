Recent advancements in Bayesian neural networks (BNNs) have shown promising results in various machine learning applications. However, their deployment on resource-constrained IoT devices remains challenging due to high computational costs and memory requirements. This paper presents a novel approach, 'LiteBNN', which leverages Bayesian model pruning and knowledge distillation to reduce the complexity of BNNs. Experimental results on several benchmark datasets demonstrate that LiteBNN achieves comparable accuracy to state-of-the-art BNNs while requiring up to 75% fewer parameters and 50% less computation.