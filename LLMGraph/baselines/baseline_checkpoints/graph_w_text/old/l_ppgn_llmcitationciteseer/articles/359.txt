Emotion recognition from multimodal data, such as speech, text, and facial expressions, is a challenging task due to the complexity of human emotions and the variability of modalities. We propose a novel Hierarchical Attention Network (HAN) architecture that captures both intra-modal and inter-modal relationships. Our HAN model consists of a hierarchical fusion module that integrates attention weights from different modalities, enabling the network to focus on the most informative features. Experimental results on three benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multimodal data, achieving an average F1-score improvement of 7.2%.