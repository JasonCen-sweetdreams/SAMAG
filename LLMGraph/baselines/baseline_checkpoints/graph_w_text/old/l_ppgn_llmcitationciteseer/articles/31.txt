Emotion recognition systems have gained popularity, but existing interfaces often neglect users with disabilities. This paper presents a novel multimodal conversational interface, 'EmoChat', that enables users to express emotions through speech, text, or gestures. We investigate the impact of interface modality on emotion recognition accuracy and user experience. Our user study with 30 participants, including individuals with visual and hearing impairments, shows that EmoChat achieves higher recognition accuracy and user satisfaction compared to unimodal interfaces. We discuss design implications for accessible HCI systems that support diverse user populations.