Explainable AI is crucial for real-world adoption of multi-agent reinforcement learning (MARL) systems. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates attention mechanisms with hierarchical abstraction to provide interpretable policy explanations for MARL. Our approach enables the identification of key agents, actions, and states contributing to joint policy decisions. Experimental results on a range of MARL benchmarks demonstrate improved explainability and policy performance compared to state-of-the-art methods.