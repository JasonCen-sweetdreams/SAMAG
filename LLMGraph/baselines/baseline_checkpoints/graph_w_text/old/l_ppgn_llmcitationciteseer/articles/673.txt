The proliferation of IoT devices and sensors has led to an explosion of time-series data, posing significant challenges for query optimization in distributed databases. This paper presents a novel approach, 'ChronoOpt', which leverages a combination of statistical modeling and machine learning to optimize query plans for time-series data. By incorporating a probabilistic model of data freshness and a cost-based optimization framework, ChronoOpt achieves an average query latency reduction of 35% compared to state-of-the-art approaches on a real-world dataset. We also demonstrate the scalability of ChronoOpt on a distributed cluster, highlighting its potential for large-scale industrial deployments.