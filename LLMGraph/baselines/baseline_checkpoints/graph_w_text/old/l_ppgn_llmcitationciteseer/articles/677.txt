Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) has gained significant attention in human-computer interaction and affective computing. This paper proposes a novel hierarchical attention network (HAN) that leverages the strengths of each modality to improve emotion recognition accuracy. Our HAN architecture consists of modality-specific attention modules that learn to weigh the importance of each input feature, followed by a hierarchical fusion layer that combines the modality-wise representations. Experimental results on the Iemocap and Mosei datasets demonstrate that our approach outperforms state-of-the-art methods in emotion recognition, particularly in the presence of noisy or missing modalities.