Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and frames to improve emotion recognition accuracy. Our HAN model consists of three stacked attention layers, each capturing different levels of abstraction: intra-modality, inter-modality, and temporal attention. Experimental results on the IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art methods by 12.5% in terms of weighted F1-score, achieving an average accuracy of 85.2% across six basic emotions.