This paper presents a novel framework for designing embodied conversational agents (ECAs) that can recognize and respond to users' emotions in virtual reality (VR) environments. We propose a multimodal fusion approach that combines facial expression analysis, speech prosody, and body language cues to improve emotion recognition accuracy. Our ECA, 'EmoBot', is equipped with a VR-compatible avatar that can exhibit empathetic responses, such as nodding or comforting gestures, to provide a more engaging and supportive user experience. Experimental results show that users perceive EmoBot as more trustworthy and empathetic compared to a text-based chatbot, highlighting the potential of ECAs in VR for affective computing applications.