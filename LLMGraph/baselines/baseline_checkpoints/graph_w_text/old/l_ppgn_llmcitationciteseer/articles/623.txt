Multi-agent reinforcement learning (MARL) has seen significant progress in recent years, but scaling to large numbers of agents remains a major challenge. This paper introduces Hierarchical Attention for Multi-Agent Reinforcement Learning (HAMRL), a novel approach that leverages hierarchical attention mechanisms to selectively focus on relevant agents and improve learning efficiency. Our experiments on several MARL benchmarks demonstrate that HAMRL outperforms state-of-the-art methods in terms of learning speed and final policy performance, while reducing computational overhead by up to 40%