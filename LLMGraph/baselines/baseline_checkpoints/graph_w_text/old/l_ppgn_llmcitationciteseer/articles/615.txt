Distributed database systems have become increasingly common in modern data management, but optimizing query performance remains a challenging task. This paper proposes a novel approach that leverages reinforcement learning to optimize query execution plans. Our method, 'RL-Optimizer', uses a deep Q-network to learn the optimal plan for a given query workload, taking into account factors such as data distribution, node availability, and network latency. Experimental results on a real-world distributed database system demonstrate that RL-Optimizer outperforms traditional optimization techniques by up to 30% in terms of query latency and resource utilization.