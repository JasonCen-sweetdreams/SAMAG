Voice assistants have become ubiquitous, but users with dysarthria, a speech disorder affecting articulation and intelligibility, face significant barriers to interaction. This paper presents a multimodal approach to designing inclusive voice assistants, combining speech recognition with complementary input modalities. We develop and evaluate a prototype system that incorporates gesture recognition, eye-tracking, and facial expression analysis to enhance interaction and improve user experience. Our user study with 20 participants with dysarthria demonstrates significant improvements in task completion rates and user satisfaction, highlighting the potential of multimodal designs for promoting accessibility in HCI.