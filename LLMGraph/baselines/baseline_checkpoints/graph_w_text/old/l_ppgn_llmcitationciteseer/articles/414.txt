Emotional intelligence (EI) is crucial for conversational agents to empathize with users and provide personalized support. This paper proposes a novel multimodal fusion framework to evaluate EI in conversational agents. We integrate speech, text, and facial expression features to analyze an agent's emotional understanding, empathy, and social skills. Our approach is evaluated on a dataset of human-agent interactions, demonstrating a significant improvement in EI assessment over unimodal methods. We also provide insights into the importance of each modality in EI evaluation, shedding light on the design of more emotionally intelligent conversational agents.