Distributed relational databases have become increasingly popular for handling large-scale data analytics workloads. However, optimizing queries in these systems remains a challenging task due to the complexity of the underlying infrastructure and the dynamic nature of the data. This paper proposes a novel approach to query optimization using reinforcement learning, which learns to adapt to changing database statistics and query patterns. Our approach, called 'RL-Optimizer', uses a deep Q-network to select the optimal query plan based on a reward function that balances query latency and resource utilization. Experimental results on a real-world dataset demonstrate that RL-Optimizer outperforms traditional query optimizers by up to 30% in terms of query execution time.