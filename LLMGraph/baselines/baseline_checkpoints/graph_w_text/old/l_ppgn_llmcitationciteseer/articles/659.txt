Deep neural networks have achieved state-of-the-art performance in various machine learning tasks, but their performance is heavily dependent on the choice of hyperparameters. Bayesian optimization with Gaussian processes (BO-GP) is a popular approach for hyperparameter tuning, but its computational cost can be prohibitive for large models. This paper proposes an efficient BO-GP algorithm that leverages a hierarchical search space decomposition and a novel acquisition function based on the expected improvement over the best observed value. Experimental results on several benchmark datasets demonstrate that our approach achieves significant speedups over existing BO-GP methods while maintaining competitive performance.