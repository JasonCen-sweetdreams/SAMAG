Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the need for interpretable models. This paper proposes a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and features for emotion recognition. Our HAN model incorporates a hierarchical self-attention mechanism that captures both local and global patterns in multi-modal data. Experimental results on the CMU-MOSEI dataset demonstrate the effectiveness of our approach in improving emotion recognition accuracy and providing explainable insights into the decision-making process.