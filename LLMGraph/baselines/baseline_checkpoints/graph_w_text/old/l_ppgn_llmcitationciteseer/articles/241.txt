Emotion recognition from multi-modal data, such as speech, text, and vision, has become increasingly important in human-computer interaction. However, existing models often lack interpretability and transparency. This paper introduces a novel hierarchical attention network (HAN) that leverages attention mechanisms to selectively focus on salient features across modalities. We propose an explainable emotion recognition framework that generates visualizations of attention weights, enabling insights into the decision-making process. Experiments on a benchmark dataset demonstrate improved recognition accuracy and human evaluators' subjective ratings of explainability compared to state-of-the-art models.