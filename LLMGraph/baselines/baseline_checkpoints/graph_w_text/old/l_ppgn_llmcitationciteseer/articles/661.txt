Emotion recognition in human-computer interaction (HCI) is a crucial aspect of creating empathetic systems. This paper proposes a novel hierarchical attention network (HAN) architecture that leverages multi-modal inputs (facial expressions, speech, and physiological signals) to recognize emotions. The HAN model consists of modality-specific attention modules that learn to weigh the importance of each input modality, followed by a hierarchical fusion module that integrates the modality-specific features. Experimental results on the RECOLA dataset demonstrate that our approach achieves state-of-the-art performance in recognizing emotions, outperforming existing uni-modal and multi-modal fusion approaches.