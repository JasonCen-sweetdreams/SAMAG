Recognizing emotions from multi-modal cues, such as speech, text, and facial expressions, is crucial for developing empathetic human-computer interfaces. This paper proposes a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features to improve emotion recognition accuracy. Our HAN model consists of modality-specific encoders, a hierarchical attention mechanism, and a fusion layer that combines outputs from each modality. Experimental results on the IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an average F1-score of 0.842.