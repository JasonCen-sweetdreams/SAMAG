Deep neural networks have shown promise in information retrieval tasks, but their computational cost can be prohibitive for large-scale document ranking. This paper proposes a novel neural ranking model, 'QRank', which incorporates query-aware attention mechanisms to selectively focus on relevant document regions. We introduce a hierarchical attention architecture that jointly learns query-document relevance and attention weights, reducing the computational complexity by an order of magnitude. Experimental results on the TREC Web Track dataset demonstrate that QRank achieves state-of-the-art ranking performance while significantly improving query processing efficiency.