Emotion recognition from multimodal inputs (e.g., speech, text, vision) is a challenging task due to the complexity of human emotions and the diversity of expression modalities. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages the strengths of both intra-modal and inter-modal attention mechanisms. Our HAN model learns to selectively focus on relevant segments and features from each modality, enabling more accurate emotion recognition. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art multimodal fusion methods, achieving an average F1-score of 83.2% across six emotions.