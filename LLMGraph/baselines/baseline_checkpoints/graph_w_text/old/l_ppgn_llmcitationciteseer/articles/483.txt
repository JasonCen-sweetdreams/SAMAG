Distributed relational databases have become increasingly popular for handling large-scale data storage and processing. However, query optimization remains a significant challenge in these systems. This paper proposes a novel adaptive query optimization framework that leverages machine learning techniques to improve query performance. Our approach uses a combination of feature extraction, clustering, and reinforcement learning to dynamically adjust the query plan based on the workload characteristics and system resources. Experimental results on a real-world dataset demonstrate that our approach achieves an average query latency reduction of 30% compared to traditional query optimization methods.