Emotion recognition from multimodal data is a challenging task due to the inherent complexity of human emotions and the variability of modalities. This paper introduces a novel Hierarchical Attention Network (HAN) for multimodal emotion recognition, which leverages the strengths of both visual and audio cues. The proposed HAN architecture consists of a feature-level attention mechanism that adaptively weights the importance of different modalities, followed by a hierarchical attention module that captures long-range dependencies across modalities. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art multimodal fusion techniques, achieving a significant improvement in emotion recognition accuracy.