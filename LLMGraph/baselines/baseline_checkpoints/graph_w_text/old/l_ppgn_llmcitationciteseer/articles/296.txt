Skeleton-based action recognition has gained significant attention in recent years, but existing methods struggle to effectively model complex relationships between body joints. This paper presents a novel hierarchical graph attention network (HGAT) that leverages multi-scale graph representations to capture both local and global dependencies in skeleton data. Our approach incorporates a hierarchical fusion mechanism to aggregate features from different graph scales, enabling the network to recognize actions with improved accuracy and robustness. Experimental results on popular datasets demonstrate the superiority of HGAT over state-of-the-art methods, particularly in scenarios with noisy or incomplete data.