Emotion recognition from multi-modal data (e.g., speech, text, vision) is a challenging task due to the complexity of human emotions and the variability of modalities. This paper presents a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features for improved emotion recognition. Our proposed HAN model consists of a feature-level attention mechanism and a modality-level attention mechanism, which are jointly optimized to capture intricate relationships between modalities. Experimental results on the benchmark CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods by 12.5% in terms of weighted F1-score, while reducing computational costs by 30%.