Emotion recognition from multi-modal inputs (e.g., facial expressions, speech, and text) is a challenging task due to the complexity of human emotions and the heterogeneity of modalities. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) that learns to selectively focus on relevant modalities and capture complex emotional dependencies. HGAT consists of a graph attention module that iteratively updates node representations, and a hierarchical fusion module that integrates modality-specific features. Experiments on the CMU-MOSEI dataset demonstrate that HGAT outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an average F1-score improvement of 12.3%.