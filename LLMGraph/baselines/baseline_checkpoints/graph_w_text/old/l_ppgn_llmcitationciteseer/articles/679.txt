Distributed database systems have become increasingly popular due to their ability to handle large amounts of data and scale horizontally. However, query optimization remains a challenging task in these systems. This paper proposes a novel approach to query optimization using machine learning techniques. We develop a learning-based cost model that predicts the execution time of queries and uses this information to optimize the query plan. Our approach is able to adapt to changing workloads and data distributions, and experimental results show that it outperforms traditional query optimization techniques in terms of response time and resource utilization.