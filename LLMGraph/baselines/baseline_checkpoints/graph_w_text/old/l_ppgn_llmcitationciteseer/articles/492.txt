Multimodal emotion recognition in conversational AI remains a challenging task due to the complexity of human emotions and the need to integrate heterogeneous input modalities. This paper proposes a novel hierarchical attention network (HAN) architecture that leverages both intra-modal and inter-modal attention mechanisms to selectively focus on relevant features across modalities. Our HAN model is evaluated on a large-scale multimodal dataset and achieves state-of-the-art performance on emotion recognition tasks, outperforming existing approaches by 12.5% on average. Ablation studies demonstrate the effectiveness of our hierarchical attention mechanism in capturing nuanced emotional cues.