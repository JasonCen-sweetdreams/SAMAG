Autonomous vehicles require real-time control systems that can adapt to diverse scenarios. This paper presents a novel reinforcement learning (RL) framework, 'RL-Ctrl', which leverages a hybrid approach combining model-based and model-free RL to achieve efficient and robust control. By integrating a learned dynamics model with a model-free policy, RL-Ctrl reduces the exploration-exploitation trade-off and improves control performance in complex environments. Experimental results on a realistic simulation platform demonstrate that RL-Ctrl outperforms state-of-the-art RL methods in terms of control stability and adaptability, paving the way for real-world autonomous vehicle deployment.