Knowledge graph embedding (KGE) has emerged as a crucial technique for representation learning on graph-structured data. However, existing KGE methods often suffer from inadequate modeling of complex relationships and hierarchical structures. We propose a novel approach, Hierarchical Graph Attention Networks (HGAT), which leverages attention mechanisms to selectively focus on relevant nodes and edges at multiple scales. Our experiments on benchmark datasets demonstrate that HGAT outperforms state-of-the-art KGE methods in link prediction and node classification tasks, while reducing computational overhead.