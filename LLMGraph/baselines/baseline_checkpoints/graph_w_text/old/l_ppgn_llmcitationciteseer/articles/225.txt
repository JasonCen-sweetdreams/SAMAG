Optimizing query performance is crucial in distributed database systems. Traditional query optimization techniques rely on manual tuning and heuristics, which can be time-consuming and suboptimal. This paper proposes a novel approach that leverages machine learning to optimize query plans. We design a model that learns from query execution history and database statistics to predict the optimal plan for a given query. Our experiments on a real-world distributed database system demonstrate a significant reduction in query latency and improvement in resource utilization compared to traditional optimization techniques.