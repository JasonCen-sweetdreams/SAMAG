Emotion recognition from multi-modal data (e.g., speech, text, and vision) is a challenging task, especially when considering the inherent ambiguity and context-dependence of emotional expressions. This paper proposes a novel hierarchical attention network (HAN) that integrates features from disparate modalities to recognize emotions in a more transparent and interpretable manner. Our HAN model comprises modular attention mechanisms that selectively focus on relevant modalities, temporal segments, and feature subsets, thereby providing insights into the decision-making process. Experimental results on a large-scale multi-modal dataset demonstrate the superiority of our approach in terms of recognition accuracy and explainability.