Affective computing has become increasingly important in human-computer interaction (HCI) as it enables systems to recognize and respond to users' emotional states. This paper presents EmotionAware, a novel multimodal affective computing framework that integrates computer vision, speech processing, and machine learning techniques to detect and analyze users' emotions in real-time. EmotionAware utilizes a hybrid approach combining rule-based and machine learning-based models to improve emotion recognition accuracy and robustness. We evaluate EmotionAware in a user study involving 100 participants, demonstrating its effectiveness in detecting emotions and enhancing user experience in HCI applications.