Deep neural networks (DNNs) are vulnerable to adversarial attacks, which can mislead them to make incorrect predictions. While various defense mechanisms have been proposed, most of them focus on empirical robustness evaluation without providing formal guarantees. This paper presents a Bayesian uncertainty estimation framework to analyze the robustness of DNNs against adversarial attacks. By modeling the uncertainty of neural network outputs using Bayesian neural networks, we can quantify the confidence of the model's predictions and detect potential vulnerabilities. Experimental results on several benchmark datasets demonstrate the effectiveness of our approach in identifying and mitigating adversarial attacks.