Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a challenging task, especially when explaining the decision-making process is crucial. This paper presents a novel Hierarchical Attention Network (HAN) architecture that leverages attention mechanisms to selectively focus on relevant modalities and features. Our approach achieves state-of-the-art performance on two benchmark datasets, outperforming existing methods by 12.5% and 8.2% respectively. We also propose an interpretability technique, 'Attention Flow', to visualize the attention weights and provide insights into the emotional cues driving the recognition process.