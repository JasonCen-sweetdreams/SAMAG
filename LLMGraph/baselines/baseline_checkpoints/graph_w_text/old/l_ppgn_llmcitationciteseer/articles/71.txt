Multimodal emotion recognition involves fusing features from diverse sources, such as speech, text, and vision. However, existing methods often suffer from high computational costs and neglect complex contextual relationships between modalities. This paper presents a novel Hierarchical Attention Network (HAN) architecture that efficiently integrates multimodal features through a hierarchical attention mechanism. Our approach adaptively weighs the importance of different modalities and captures long-range dependencies between them. Experimental results on the IEMOCAP and MMI datasets demonstrate that HAN outperforms state-of-the-art methods in terms of accuracy and inference speed.