Emotion recognition from multi-modal inputs, such as speech, text, and vision, is a challenging task due to the complexity of emotional expressions. This paper presents a novel hierarchical graph attention network (HGAT) that effectively captures both intra- and inter-modal relationships. Our HGAT model consists of a hierarchical graph encoder that leverages attention mechanisms to selectively focus on relevant modalities and a graph attention-based fusion layer that integrates information from multiple modalities. Experimental results on the IEMOCAP dataset show that HGAT outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an average F1-score of 83.2%.