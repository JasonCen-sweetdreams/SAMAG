Gesture recognition systems are vital for individuals with disabilities, but existing approaches often struggle with adaptability to diverse user populations and varying environmental conditions. This paper presents a novel deep learning framework, 'Adapta Gesture', which leverages transfer learning and domain adaptation to improve gesture recognition accuracy. Our approach incorporates a multi-modal fusion strategy that combines computer vision and sensor data, enabling more robust and personalized gesture recognition. Experimental results on a large-scale dataset demonstrate significant improvements in recognition accuracy and latency compared to state-of-the-art methods.