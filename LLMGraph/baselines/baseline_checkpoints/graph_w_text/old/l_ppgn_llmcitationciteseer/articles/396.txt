Emotion recognition from multi-modal data (e.g., audio, video, text) is a crucial task in affective computing. However, existing approaches struggle to efficiently process and fuse information from diverse modalities. We propose a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and features at multiple scales. Our HAN model consists of a modality-aware attention mechanism and a hierarchical feature fusion strategy, enabling efficient processing of high-dimensional multi-modal data. Experiments on several benchmark datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance with significant reductions in computational cost.