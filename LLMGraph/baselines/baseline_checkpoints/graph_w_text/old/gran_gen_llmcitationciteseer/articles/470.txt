Visual question answering (VQA) models often rely on complex neural networks, making it challenging to interpret their decision-making processes. This paper presents a novel hierarchical attention network (HAN) that incorporates explainability by design. Our HAN model consists of two stages: the first stage uses a spatial attention mechanism to focus on relevant regions of the image, while the second stage employs a question-guided attention module to selectively weigh the importance of different image regions. We evaluate our approach on the VQA 2.0 dataset and demonstrate improved performance compared to state-of-the-art methods, as well as provide visualizations of the attention weights to facilitate model interpretability.