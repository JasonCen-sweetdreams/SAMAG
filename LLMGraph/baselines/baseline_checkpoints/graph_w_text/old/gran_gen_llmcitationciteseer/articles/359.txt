Emotion recognition from multi-modal data, such as facial expressions, speech, and text, is a challenging task. This paper proposes a novel hierarchical attention network (HAN) architecture that leverages the complementary information from different modalities to improve emotion recognition accuracy. Our HAN model learns to selectively focus on the most informative regions and features across modalities, enabling explainable emotion recognition. Experimental results on the CMU-MultiModal dataset demonstrate that our approach outperforms state-of-the-art methods, achieving an average F1-score of 0.92 across six emotions.