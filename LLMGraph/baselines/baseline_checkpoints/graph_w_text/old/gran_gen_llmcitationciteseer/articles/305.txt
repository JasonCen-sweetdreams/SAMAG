Emotion recognition from multi-modal data (e.g., text, speech, vision) is a challenging task due to the complexity of human emotions and the heterogeneity of modalities. This paper proposes a novel hierarchical attention network (HAN) that captures both intra-modal and inter-modal relationships to improve emotion recognition. Our HAN model consists of three stages: modality-specific attention, cross-modal attention, and hierarchical fusion. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods by 12.5% in terms of weighted F1-score, achieving a new benchmark for multi-modal emotion recognition.