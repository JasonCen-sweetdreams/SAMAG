Multi-task learning has shown great promise in various applications, but its success relies heavily on the availability of large amounts of labeled data. This paper proposes a novel hierarchical attention network (HAN) that tackles the challenge of limited labeled data by selectively focusing on relevant tasks and instances. Our HAN model consists of two stages: task attention and instance attention, which adaptively adjust the weights of tasks and instances during training. Experimental results on several benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in multi-task learning with limited labeled data, while reducing computational costs.