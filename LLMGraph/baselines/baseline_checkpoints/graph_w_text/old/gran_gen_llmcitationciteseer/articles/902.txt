Accurate emotional state recognition is crucial in human-computer interaction (HCI) applications. This paper presents a novel multimodal fusion approach that combines speech, facial expressions, and physiological signals to recognize emotions. We propose a deep learning framework that leverages convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to extract features from each modality. Our fusion strategy uses a weighted attention mechanism to adaptively combine the modalities based on their reliability and relevance to the emotional context. Experimental results on a publicly available dataset demonstrate that our approach outperforms state-of-the-art unimodal and multimodal approaches, achieving an average F1-score of 0.92 for emotion recognition.