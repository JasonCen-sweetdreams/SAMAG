Zero-shot learning (ZSL) has achieved remarkable success, but the lack of interpretability hinders its adoption in high-stakes applications. We propose a Hierarchical Attention Network (HAN) that learns to selectively focus on relevant semantic attributes and generate explicit explanations for its predictions. Our approach leverages a novel attention mechanism that propagates uncertainty from the attribute level to the class level, enabling the model to recognize unseen classes without additional training data. Experimental results on benchmark datasets demonstrate that HAN outperforms state-of-the-art ZSL methods while providing insightful explanations for its decisions.