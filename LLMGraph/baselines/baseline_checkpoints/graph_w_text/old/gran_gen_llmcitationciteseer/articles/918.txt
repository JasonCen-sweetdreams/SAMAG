Few-shot image classification remains a challenging problem in machine learning, particularly when dealing with limited labeled data. This paper presents a novel deep meta-learning approach that leverages self-supervised pre-training to improve the adaptability and robustness of few-shot learners. Our method, 'MetaSSPD', combines a self-supervised contrastive loss with a meta-learning objective, enabling the model to learn generalizable representations from a large pool of unlabeled data. We demonstrate the effectiveness of MetaSSPD on several benchmark datasets, achieving state-of-the-art performance in few-shot classification tasks.