In multi-agent reinforcement learning, individual agents must balance exploration and exploitation to achieve optimal collective performance. This paper introduces a novel graph attention mechanism, 'GAT-Explore', that enables coordinated exploration among agents. By modeling agent interactions as a graph structure, GAT-Explore adaptively assigns exploration weights to each agent based on its relative importance in the graph. We demonstrate the effectiveness of GAT-Explore in several multi-agent domains, including a traffic management scenario, where it outperforms existing decentralized exploration strategies.