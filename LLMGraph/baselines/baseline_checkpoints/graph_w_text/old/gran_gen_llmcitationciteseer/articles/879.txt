Emotion recognition in human-computer interaction (HCI) is crucial for developing empathetic and personalized systems. This paper proposes a novel hierarchical attention network (HAN) for multi-modal emotion recognition, which integrates facial expressions, speech, and physiological signals. Our HAN model employs a hierarchical structure to capture both local and global contextual information, enabling more accurate emotion classification. We evaluate our approach on a large-scale dataset and demonstrate significant improvements over state-of-the-art methods, achieving an average F1-score of 0.85 across six emotions.