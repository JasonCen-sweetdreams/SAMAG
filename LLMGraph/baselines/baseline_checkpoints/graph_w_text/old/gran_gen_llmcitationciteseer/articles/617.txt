Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the inherent heterogeneity and complexity of human emotions. This paper introduces a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features to improve emotion recognition accuracy. Our HAN model consists of a modality-level attention mechanism that weights the importance of each modality and a feature-level attention mechanism that identifies salient features within each modality. Experiments on three benchmark datasets demonstrate that our approach outperforms state-of-the-art methods by up to 12% in terms of emotion recognition accuracy, while reducing computational overhead by up to 30%.