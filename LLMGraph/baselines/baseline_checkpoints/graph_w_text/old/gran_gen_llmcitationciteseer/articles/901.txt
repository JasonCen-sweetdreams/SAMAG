Emotion recognition from multi-modal data (e.g., speech, text, vision) is a challenging task due to the diverse and intricate relationships between modalities. This paper proposes a novel hierarchical attention mechanism, 'HierAttention', which learns to selectively focus on relevant modalities and features at multiple levels of abstraction. Experiments on three benchmark datasets demonstrate that HierAttention outperforms state-of-the-art methods in terms of accuracy and computational efficiency. We also provide insights into the learned attention patterns, revealing interesting correlations between modalities and emotion categories.