The coordination of heterogeneous autonomous agents is crucial in various applications, such as smart cities, robotic teams, and supply chains. This paper presents a novel hierarchical reinforcement learning framework, 'HRL-CA', for coordinating agents with different capabilities and objectives. HRL-CA leverages a top-down approach, where high-level policies guide the decision-making of low-level agents. We introduce a novel exploration strategy that adaptively adjusts the exploration-exploitation trade-off for each agent based on its local environment and the global system state. Experimental results on a simulated robotic team demonstrate that HRL-CA outperforms state-of-the-art methods in terms of task completion time and overall system efficiency.