Explainability is crucial in AI decision-making, particularly in high-stakes applications. This paper proposes a novel multi-modal fusion framework, 'HierFUSE', which integrates visual, textual, and numerical features to generate interpretable explanations for AI-driven decisions. We introduce a hierarchical attention mechanism that selectively focuses on relevant modalities and features, enabling the model to justify its predictions. Experimental results on a real-world medical diagnosis dataset demonstrate that HierFUSE outperforms state-of-the-art explainability methods while maintaining high decision accuracy.