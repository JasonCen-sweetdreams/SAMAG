Robot navigation in dynamic environments poses significant challenges due to the complexity of real-world scenarios. This paper presents a hierarchical reinforcement learning (HRL) framework that tackles this problem by decomposing the navigation task into sub-goals and learning hierarchical policies. Our approach, called HierNav, leverages a high-level policy to select sub-goals and a low-level policy to execute actions. We evaluate HierNav on a simulated robot navigation task and show that it outperforms flat reinforcement learning methods in terms of navigation efficiency and adaptability to changing environments.