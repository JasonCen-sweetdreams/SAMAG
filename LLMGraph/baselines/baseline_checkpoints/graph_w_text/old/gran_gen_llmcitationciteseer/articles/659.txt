Deep reinforcement learning (DRL) has shown remarkable success in autonomous driving, but its lack of transparency hinders trust and reliability. This paper proposes a novel explainable DRL framework, 'Xplore', which integrates attention mechanisms and model-based reasoning to generate interpretable explanations for agent decisions. Our approach enables the identification of critical regions in the input space and provides insights into the decision-making process. Experimental results on the CARLA simulator demonstrate that Xplore achieves state-of-the-art performance while providing meaningful explanations for its actions.