Explainable decision-making is crucial in multi-agent systems, where transparency and accountability are essential. This paper proposes a novel hierarchical attention network (HAN) architecture for explainable multi-agent decision making. Our HAN model learns to selectively focus on relevant agents, their interactions, and contextual features to generate decisions and corresponding explanations. We evaluate our approach on a real-world traffic management scenario, demonstrating improved decision-making performance and interpretable explanations compared to existing methods. Our work has implications for designing trustworthy and transparent AI systems in cooperative multi-agent environments.