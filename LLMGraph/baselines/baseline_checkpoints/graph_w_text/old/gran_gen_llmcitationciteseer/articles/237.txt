Voice assistants have become ubiquitous in modern life, but their design often neglects the needs of users with disabilities. This paper presents a multimodal framework for inclusive voice assistants, integrating computer vision, natural language processing, and haptic feedback to facilitate more accessible interaction. Our approach incorporates personalized models for users with varying abilities, including those with visual, auditory, or motor impairments. We conduct a user study with 30 participants and demonstrate significant improvements in task completion rates and user satisfaction compared to traditional voice assistants.