Deep reinforcement learning (DRL) has achieved remarkable success in various domains, but the lack of transparency in learned policies hinders trust and reliability. This paper presents a novel explainability framework, 'ExploR', which enables efficient attribution of policy decisions to input features. ExploR leverages attention mechanisms and gradient-based saliency maps to identify influential state variables and action components. We evaluate ExploR on several Atari games and a real-world robotics task, demonstrating improved interpretability and faithfulness to the underlying policy. Our approach facilitates the development of more trustworthy and accountable DRL systems.