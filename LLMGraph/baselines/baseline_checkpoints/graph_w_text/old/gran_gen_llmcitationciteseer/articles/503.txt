Multimodal emotion recognition (MER) is a challenging task that requires integrating and processing disparate data sources, including speech, text, and vision. This paper presents a novel hierarchical attention network (HAN) architecture, 'MERCURY', which learns to selectively focus on relevant modalities and features to improve MER performance. Our approach leverages a fusion module that adaptively weights modality-specific representations, enabling the model to capture nuanced emotional cues. Experimental results on the IEMOCAP and MMER datasets demonstrate that MERCURY outperforms state-of-the-art MER methods while reducing computational complexity.