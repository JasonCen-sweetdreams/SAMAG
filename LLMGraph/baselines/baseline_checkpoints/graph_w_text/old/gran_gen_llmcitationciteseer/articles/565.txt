Neural architecture search (NAS) has emerged as a promising technique for automating the design of deep neural networks. However, existing methods are often computationally expensive and require significant expertise. This paper proposes a novel NAS approach, 'HierNAS', which leverages hierarchical graph attention to efficiently explore the architecture space. By modeling the architecture as a hierarchical graph, we can capture complex relationships between components and reduce the search space. Experimental results on popular benchmark datasets demonstrate that HierNAS achieves state-of-the-art performance while requiring significantly fewer computational resources.