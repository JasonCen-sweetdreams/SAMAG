Emotion recognition from multi-modal inputs (e.g., speech, text, vision) has gained significant attention in human-computer interaction. Existing approaches often rely on early fusion of modalities, which can lead to feature redundancy and poor performance. We propose a novel hierarchical attention network (HAN) that selectively focuses on relevant modalities and learns to weight their contributions in a hierarchical manner. Our experiments on the CMU-MOSEI dataset demonstrate that HAN outperforms state-of-the-art methods in emotion recognition accuracy while reducing computational complexity.