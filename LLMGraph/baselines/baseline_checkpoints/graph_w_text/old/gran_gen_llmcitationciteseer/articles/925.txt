Sentiment analysis in social media is a challenging task due to the presence of multimodal data (text, images, videos). This paper proposes a novel Hierarchical Attention Network (HAN) that integrates visual and textual features to capture complex sentiment expressions. Our HAN model consists of two attention layers: a modality-level attention layer that weights the importance of each modality, and a feature-level attention layer that focuses on salient features within each modality. Experimental results on a large-scale social media dataset demonstrate that our approach outperforms state-of-the-art multimodal sentiment analysis models, achieving a 5.6% improvement in F1-score.