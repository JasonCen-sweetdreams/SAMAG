Multi-modal emotion recognition has garnered significant attention in recent years, but existing approaches often suffer from high computational costs and limited scalability. This paper presents a novel hierarchical attention network (HAN) architecture that leverages both visual and acoustic features to recognize emotions in real-time. Our HAN model learns to selectively focus on relevant modalities and temporal segments, reducing computational overhead while improving recognition accuracy. Experimental results on the Multimodal Emotion Recognition Challenge (MERC) dataset demonstrate that our approach outperforms state-of-the-art methods by 12.5% in terms of F1-score, while reducing inference time by 30%.