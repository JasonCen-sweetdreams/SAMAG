Conversational agents require accurate emotion recognition to provide empathetic responses. While existing approaches focus on single modalities, we propose a hierarchical attention network (HAN) that integrates speech, text, and facial expression features. Our HAN model employs a novel multi-modal fusion mechanism, which adaptively weights modality-specific attention scores to capture complex emotional cues. Experimental results on the IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving a 12.5% relative improvement in F1-score.