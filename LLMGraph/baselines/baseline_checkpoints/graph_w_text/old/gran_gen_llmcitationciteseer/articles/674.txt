Deep neural networks have achieved state-of-the-art results in natural language inference (NLI) tasks, but their lack of transparency hinders trust and interpretation. This paper proposes a hierarchical attention network (HAN) that incorporates explainability mechanisms into the model architecture. Our approach involves attention-based feature extraction, hierarchical reasoning, and visualizable attention weights. We evaluate our model on the Stanford Natural Language Inference (SNLI) dataset and demonstrate improved performance compared to existing state-of-the-art models while providing insightful explanations for the predicted entailment relationships.