Emotion recognition from multimodal data, such as speech, text, and vision, has numerous applications in human-computer interaction and affective computing. However, existing multimodal fusion methods often lack transparency and interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates modality-specific attention mechanisms to selectively focus on salient features and weigh their contributions to emotion recognition. Our experiments on the IEMOCAP dataset demonstrate that HAN outperforms state-of-the-art multimodal fusion methods while providing insights into the decision-making process through attention visualization.