E-commerce search engines face the challenge of retrieving relevant products from a vast inventory, often relying on textual and visual features. This paper proposes a novel hierarchical neural ranking model, 'HierRank', that leverages both modalities to improve search accuracy. HierRank employs a multi-task learning framework, where a shared encoder is trained to learn joint representations of text and images, followed by modality-specific ranking modules. Experimental results on a large-scale e-commerce dataset demonstrate that HierRank outperforms state-of-the-art methods in terms of recall and normalized discounted cumulative gain.