Multimodal sentiment analysis involves learning from multiple sources of data, such as text, images, and audio. However, existing approaches often rely on simple concatenation or averaging of modality-specific features, ignoring the complex relationships between them. We propose a Hierarchical Attention Network (HAN) that leverages both intra-modal and inter-modal attention mechanisms to selectively focus on relevant features and modalities. Experimental results on the benchmark CMU-MOSI dataset demonstrate that our HAN model outperforms state-of-the-art methods, achieving a 5.2% improvement in sentiment classification accuracy.