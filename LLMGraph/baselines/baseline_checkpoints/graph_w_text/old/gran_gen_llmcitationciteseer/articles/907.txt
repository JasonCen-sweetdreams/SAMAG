Emotion recognition is a crucial aspect of human-computer interaction, enabling computers to respond empathetically to users. This paper proposes a novel Hierarchical Attention Network (HAN) architecture for multimodal emotion recognition, leveraging both audio and visual cues. Our HAN model employs a hierarchical structure to capture contextual relationships between utterances and facial expressions, while attention mechanisms selectively focus on salient features. Experimental results on the IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multimodal input, achieving an average F1-score of 82.1%. The proposed architecture has implications for developing more empathetic and human-centered AI systems.