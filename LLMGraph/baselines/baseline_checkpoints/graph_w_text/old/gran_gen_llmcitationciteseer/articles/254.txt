Emotion recognition from multi-modal inputs (e.g., audio, video, and text) remains a challenging task, particularly when explaining the underlying decision-making process. We propose a novel hybrid attention mechanism that integrates both spatial and temporal attention to effectively capture relevant features from each modality. Our approach leverages a hierarchical fusion strategy to combine modality-specific representations, resulting in improved emotion recognition performance. We also introduce a novel explainability technique, 'EmoSal', which generates saliency maps to visualize the contribution of each modality to the predicted emotion. Experiments on the IEMOCAP dataset demonstrate the effectiveness of our approach in terms of both recognition accuracy and explainability.