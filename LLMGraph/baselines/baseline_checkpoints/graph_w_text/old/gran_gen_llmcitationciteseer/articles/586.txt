Deep reinforcement learning (DRL) has achieved remarkable success in various AI applications, but its robustness against adversarial attacks remains an open challenge. This paper investigates the vulnerability of DRL policies to perturbations in the state and action spaces. We develop a framework to generate targeted attacks using a surrogate model and evaluate the robustness of several state-of-the-art DRL algorithms. Our experimental results show that even slight perturbations can significantly degrade the performance of DRL policies, and we provide insights into the design of more robust algorithms.