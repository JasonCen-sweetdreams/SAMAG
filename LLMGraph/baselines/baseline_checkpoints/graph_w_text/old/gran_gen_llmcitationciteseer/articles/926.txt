Multi-modal sentiment analysis has gained increasing attention in recent years, but the lack of interpretability in deep learning models hinders their trustworthiness. This paper proposes a hierarchical attention network (HAN) that incorporates both visual and textual features to predict sentiment. Our approach leverages self-attention mechanisms to selectively focus on relevant regions of the input data, enabling explainable predictions. Experimental results on a benchmark dataset demonstrate that HAN outperforms state-of-the-art methods while providing insightful visualizations of the attention weights, facilitating model interpretability and trust.