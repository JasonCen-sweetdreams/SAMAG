Explainability is crucial in multi-agent reinforcement learning (MARL) systems, where decision-making processes involve complex interactions between agents. This paper introduces Hierarchical Attention Networks (HANs) to improve interpretability in MARL. HANs leverage hierarchical representations of agent interactions, enabling the identification of influential agents and actions. We propose a novel attention mechanism, 'Agent-Action Aware Attention', which selectively focuses on relevant agent interactions to generate interpretable explanations. Experimental results on a variety of MARL benchmarks demonstrate the effectiveness of HANs in improving both performance and explainability.