Emotion recognition from multi-modal data (e.g., speech, text, and vision) is a challenging task due to the inherent complexity of human emotions and the varying importance of different modalities. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features for emotion recognition. Our HAN model consists of a feature-level attention mechanism that weights the importance of different modalities, followed by a modality-level attention mechanism that adaptively combines the outputs from each modality. Experimental results on the IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art methods in terms of emotion recognition accuracy and computational efficiency.