Robot control in uncertain environments is a challenging problem due to the need to balance exploration and exploitation. This paper proposes a hierarchical reinforcement learning (HRL) framework that integrates task-oriented and motion-oriented policies to achieve adaptive robot control. Our approach leverages a high-level task policy to guide the robot's overall behavior, while a low-level motion policy refines the control actions based on the current environment state. We demonstrate the effectiveness of our HRL framework in a series of simulated and real-world robotic manipulation tasks, showcasing improved adaptability and robustness in the face of uncertainty.