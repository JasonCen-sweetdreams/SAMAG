Emotion recognition is a crucial aspect of human-computer interaction, enabling systems to respond empathetically to user needs. This paper proposes a novel hierarchical attention network (HAN) for multi-modal emotion recognition, fusing facial expressions, speech, and physiological signals. Our HAN model employs a bottom-up attention mechanism to selectively focus on relevant modalities and a top-down attention mechanism to integrate them. We evaluate our approach on a large-scale emotion recognition dataset, achieving state-of-the-art performance and demonstrating its robustness to varying user demographics and environmental conditions.