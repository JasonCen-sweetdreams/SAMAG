Distributed database systems (DDBS) are increasingly prevalent in modern data processing, but optimizing query performance remains a significant challenge. This paper presents a novel approach that leverages machine learning (ML) to predict and mitigate query bottlenecks in DDBS. We propose a query performance prediction model based on graph neural networks (GNNs) that captures complex query patterns and resource utilization. Our experimental results on a real-world DDBS benchmark demonstrate significant improvements in query execution time and resource efficiency compared to traditional optimization techniques.