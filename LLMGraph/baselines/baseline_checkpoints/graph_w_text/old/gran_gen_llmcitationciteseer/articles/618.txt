Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotional expressions. Existing approaches often rely on fusion techniques or modality-specific models, which can be computationally expensive or neglect cross-modal interactions. This paper proposes a Hierarchical Attention Network (HAN) that leverages attention mechanisms to selectively focus on relevant modalities and regions. Our experiments on the CMU-MOSEI dataset demonstrate that HAN outperforms state-of-the-art methods in terms of both accuracy and computational efficiency, achieving a 12.5% relative improvement in F1-score while reducing inference time by 35%