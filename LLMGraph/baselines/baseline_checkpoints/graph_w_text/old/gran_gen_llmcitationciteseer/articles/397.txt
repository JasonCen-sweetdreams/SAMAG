As robots increasingly collaborate with humans in complex tasks, explaining their decision-making processes is crucial for trust and safety. This paper presents a novel multi-modal fusion framework, 'ExplainRobo', which leverages graph attention networks to integrate visual, linguistic, and sensorimotor data from human-robot interactions. By learning attention weights over the graph, ExplainRobo generates interpretable explanations for robot actions, improving human understanding and trust in collaborative tasks. Our experiments on a real-world robotic assembly dataset demonstrate the effectiveness of ExplainRobo in enhancing human-robot collaboration and reducing errors.