Virtual assistants have revolutionized human-computer interaction, but their effectiveness for people with disabilities remains limited. This paper presents a novel multimodal interaction framework that incorporates emotion-awareness to improve the accessibility of virtual assistants for individuals with disabilities. Our framework integrates computer vision, natural language processing, and affective computing to recognize and respond to users' emotional cues. We evaluate our framework through a user study with 30 participants with disabilities, demonstrating significant improvements in interaction satisfaction and assistance effectiveness.