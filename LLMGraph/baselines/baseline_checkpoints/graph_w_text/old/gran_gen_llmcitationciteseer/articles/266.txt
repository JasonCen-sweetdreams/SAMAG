Multimodal sentiment analysis has gained significant attention in recent years, but existing approaches often lack interpretability. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates visual, textual, and acoustic features to predict sentiment scores. Our key innovation is the introduction of explainable attention mechanisms that provide insights into the importance of each modality and feature at multiple levels of abstraction. Experimental results on the CMU-MOSEI dataset demonstrate that our HAN model outperforms state-of-the-art multimodal fusion techniques while offering meaningful explanations for its predictions.