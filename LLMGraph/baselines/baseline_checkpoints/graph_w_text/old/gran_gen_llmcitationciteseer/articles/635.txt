Cloud computing platforms face the challenge of efficiently allocating resources to diverse workloads. This paper presents a novel multi-agent reinforcement learning (MARL) framework, 'CloudAllocator', that adapts to dynamic workload patterns and optimizes resource utilization. Our approach leverages decentralized, cooperative learning among agents to allocate resources in real-time, while incorporating constraints on latency, throughput, and energy consumption. Experiments on a realistic cloud simulation platform demonstrate that CloudAllocator outperforms traditional, heuristic-based allocation methods and achieves significant energy savings.