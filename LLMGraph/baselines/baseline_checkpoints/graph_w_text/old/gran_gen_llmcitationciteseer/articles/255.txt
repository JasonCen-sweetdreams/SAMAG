This paper proposes a decentralized multi-agent planning framework that leverages hierarchical reinforcement learning to coordinate agents in complex, dynamic environments. Our approach, called Hierarchical Multi-Agent Actor-Critic (HMAAC), consists of a hierarchy of agents that learn to coordinate their actions through shared goals and sub-goals. We demonstrate the effectiveness of HMAAC in a simulated smart city scenario, where agents must adapt to changing traffic patterns and pedestrian behaviors to optimize traffic flow and pedestrian safety. Experimental results show that HMAAC outperforms traditional decentralized planning methods in terms of scalability, flexibility, and overall system performance.