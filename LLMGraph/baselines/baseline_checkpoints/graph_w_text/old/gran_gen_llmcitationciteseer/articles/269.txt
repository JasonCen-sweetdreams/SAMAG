Deep learning models have achieved state-of-the-art performance in multi-modal emotion recognition tasks, but their opacity hinders trust and understanding. We propose a novel hierarchical attention network (HAN) that incorporates explainability into the architecture. HAN uses attention weights to highlight salient regions in images and utterances, enabling interpretability of emotion predictions. Experimental results on the IEMOCAP and SEMAINE datasets demonstrate that HAN outperforms existing methods while providing insightful visualizations of the decision-making process.