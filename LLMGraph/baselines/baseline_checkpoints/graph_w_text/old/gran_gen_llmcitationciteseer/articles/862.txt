Emotion recognition is a crucial task in affective computing, with applications in human-computer interaction, healthcare, and education. This paper presents a novel multi-modal approach that leverages graph attention networks (GATs) and transfer learning to recognize emotions from facial expressions, speech, and text. Our proposed framework, 'EMO-GAT', incorporates a GAT-based fusion module to integrate features from different modalities and a pre-trained BERT model to capture contextual information from text data. Experimental results on the RECOLA dataset demonstrate that EMO-GAT outperforms state-of-the-art methods in recognizing emotions from individual and combined modalities.