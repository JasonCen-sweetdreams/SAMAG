This paper presents a novel hierarchical attention network (HAN) for multi-modal emotion recognition, which leverages linguistic, acoustic, and visual features to predict emotions in human-computer interactions. Unlike existing approaches, our HAN model provides explainability by identifying the most influential modalities and features contributing to the emotion recognition decision. We evaluate our approach on a benchmark dataset of human-computer interactions and demonstrate improved performance compared to state-of-the-art methods. Furthermore, we conduct a user study to validate the effectiveness of our model's explanations in improving human trust and understanding of AI-driven emotion recognition systems.