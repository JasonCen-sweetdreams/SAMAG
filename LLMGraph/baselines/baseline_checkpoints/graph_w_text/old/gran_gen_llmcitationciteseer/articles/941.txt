Explainability is crucial in multi-agent reinforcement learning (MARL) systems, where agents' decisions can have significant consequences. This paper proposes a novel hierarchical attention network (HAN) architecture for MARL, which learns to focus on relevant agents and their interactions to make more informed decisions. The HAN framework consists of two levels of attention: agent-level attention, which selects relevant agents, and interaction-level attention, which identifies crucial interactions between agents. We evaluate our approach on a range of MARL benchmarks and demonstrate improved performance and interpretability compared to state-of-the-art methods.