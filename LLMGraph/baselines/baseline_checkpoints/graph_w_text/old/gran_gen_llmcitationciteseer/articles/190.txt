Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that efficiently integrates and selectively focuses on the most informative modalities and features. Our approach leverages a hierarchical attention mechanism to recursively refine the attention weights at both modality and feature levels. Experimental results on two benchmark datasets demonstrate that our HAN model outperforms state-of-the-art methods in terms of recognition accuracy and computational efficiency.