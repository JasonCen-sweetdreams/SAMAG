Accurate emotional state recognition is crucial for developing empathetic human-computer interfaces. This paper presents a novel multimodal framework, 'EmoFusion', which integrates facial expression, speech, and physiological sensor data to recognize users' emotional states. We propose a hierarchical attention mechanism that adaptively weights the contributions of each modality based on their relevance to the emotional context. Experimental results on a large-scale dataset demonstrate that EmoFusion outperforms state-of-the-art approaches in recognizing nuanced emotions, such as anxiety and excitement.