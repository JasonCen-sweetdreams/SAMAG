Multimodal sentiment analysis has become a crucial task in various applications, including customer service and social media analysis. However, existing methods often lack transparency and interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages both visual and textual features to predict sentiment. Our model incorporates attention mechanisms at multiple levels, enabling the identification of salient features and their contributions to the final sentiment prediction. Experimental results on a large-scale multimodal dataset demonstrate the effectiveness of HAN in achieving state-of-the-art performance while providing insights into the decision-making process.