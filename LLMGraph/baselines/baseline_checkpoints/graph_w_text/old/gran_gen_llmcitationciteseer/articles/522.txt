Virtual reality (VR) technology has the potential to revolutionize various aspects of life, but users with visual impairments are often excluded from this experience. This paper presents a novel VR interface design that leverages audio-tactile feedback and machine learning-based gaze prediction to facilitate an inclusive and immersive experience for visually impaired users. We conducted a user study with 20 participants, demonstrating significant improvements in task completion time and user satisfaction compared to traditional VR interfaces. Our results provide insights into the design of accessible VR systems that can be adapted to diverse user needs.