Coordinating multiple agents in complex, dynamic environments is a challenging task in multi-agent systems. This paper introduces Hierarchical Attention Networks (HAN), a novel deep reinforcement learning architecture that leverages hierarchical attention mechanisms to selectively focus on relevant agents and environmental features. We demonstrate that HAN significantly improves coordination efficiency and robustness in simulated scenarios, outperforming state-of-the-art methods in both centralized and decentralized settings. Furthermore, we provide theoretical insights into the attention mechanisms' role in facilitating efficient communication and negotiation among agents.