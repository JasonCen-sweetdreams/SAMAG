Emotion recognition from multi-modal inputs, such as speech, text, and vision, has gained significant attention in recent years. However, most existing approaches lack interpretability, making it challenging to understand the decision-making process. This paper proposes a hierarchical attention network (HAN) that learns to selectively weigh and fuse features from different modalities. We introduce a novel attention mechanism that generates visual explanations for the predicted emotions, enabling the model to provide insights into the reasoning process. Experimental results on the CMU-MOSEI dataset demonstrate the effectiveness of our approach, achieving state-of-the-art performance while providing interpretable results.