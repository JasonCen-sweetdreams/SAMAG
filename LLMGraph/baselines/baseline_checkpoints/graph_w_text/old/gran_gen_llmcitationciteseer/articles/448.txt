Neural architecture search (NAS) has revolutionized the design of deep neural networks, but its computational cost remains a significant bottleneck. This paper proposes a novel hierarchical graph-based encoding (HGBE) method that dramatically reduces the search space while preserving the expressiveness of the architecture. By recursively applying a graph autoencoder to the architecture graph, we can efficiently explore the vast design space and identify high-performing models. Experimental results on several benchmark datasets demonstrate that our approach achieves state-of-the-art performance while requiring significantly fewer computations compared to existing NAS methods.