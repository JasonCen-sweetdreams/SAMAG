Emotion recognition from multi-modal data (e.g., speech, text, vision) is a challenging task due to the complex interactions between modalities. We propose a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and exploit their correlations. Our approach consists of two stages: modality-specific attention and hierarchical fusion. Experimental results on the CMU-MOSEI dataset demonstrate that HAN outperforms state-of-the-art methods in terms of accuracy and computational efficiency, achieving a relative improvement of 12.5% in F1-score and 35% reduction in inference time.