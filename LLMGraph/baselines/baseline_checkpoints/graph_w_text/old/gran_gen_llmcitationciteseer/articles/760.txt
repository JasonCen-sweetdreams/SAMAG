Emotion recognition from multi-modal inputs, such as speech, text, and vision, is a challenging AI problem. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features to improve emotion recognition accuracy. Our approach incorporates a hierarchical attention mechanism that leverages both intra-modality and inter-modality relationships to capture complex emotional cues. Experimental results on the benchmark IEMOCAP dataset demonstrate that our HAN model outperforms state-of-the-art methods in terms of recognition accuracy and provides interpretable emotion recognition results.