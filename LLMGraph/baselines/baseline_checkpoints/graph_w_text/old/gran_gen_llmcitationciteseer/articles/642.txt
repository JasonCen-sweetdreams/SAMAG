Explainability is crucial in multi-agent reinforcement learning (MARL) to understand agent decision-making and improve cooperation. We propose Hierarchical Attention Networks (HANs) to learn interpretable representations of agent interactions and policies. HANs consist of two layers: an attention-based graph neural network to model agent relationships and a hierarchical reinforcement learning module to learn task-specific policies. Our approach outperforms state-of-the-art MARL methods in two complex domains, while providing visualizable attention weights that reveal insightful patterns of agent communication and coordination.