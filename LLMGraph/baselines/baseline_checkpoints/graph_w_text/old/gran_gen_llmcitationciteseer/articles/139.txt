Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a challenging AI task. This paper proposes a hierarchical attention network (HAN) that integrates modality-specific and cross-modal attention mechanisms to improve emotional state recognition. Our HAN model learns to focus on relevant segments of each input modality and their interactions, resulting in improved recognition accuracy and explainability. We evaluate our approach on the IEMOCAP dataset and demonstrate significant performance gains over state-of-the-art methods, while providing visualizations that highlight the importance of each modality in emotion recognition.