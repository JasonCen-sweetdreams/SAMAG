Emotion recognition from multi-modal inputs (e.g., facial expressions, speech, and text) is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features for emotion recognition. Our approach leverages a hierarchical attention mechanism to capture both local and global patterns in the data, resulting in improved performance and reduced computational cost. Experimental results on the CMU-MOSI dataset demonstrate the effectiveness of our approach in recognizing emotions from multi-modal inputs.