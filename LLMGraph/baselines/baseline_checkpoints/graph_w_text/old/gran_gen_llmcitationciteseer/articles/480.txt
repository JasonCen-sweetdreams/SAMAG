Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task due to the inherent heterogeneity and complexity of human emotions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features for emotion prediction. Our HAN model consists of modality-specific attention modules and a hierarchical fusion layer, which enables effective fusion of multi-modal features. Experimental results on three benchmark datasets demonstrate the superiority of our approach over state-of-the-art methods, achieving an average improvement of 12.5% in emotion recognition accuracy.