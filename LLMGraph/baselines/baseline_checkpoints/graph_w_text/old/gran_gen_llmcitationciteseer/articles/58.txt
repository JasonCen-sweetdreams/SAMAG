Emotion recognition from multimodal data (e.g., speech, text, and vision) is crucial in human-computer interaction. However, existing models often lack transparency and interpretability, limiting their real-world applicability. We propose a novel Hierarchical Attention Network (HAN) that integrates modality-specific attention mechanisms to extract relevant features from each input modality. Our approach achieves state-of-the-art performance on the CMU-MOSEI dataset and provides visualizations of attention weights, enabling explainable emotion recognition. We demonstrate the effectiveness of HAN in various applications, including affective computing and sentiment analysis.