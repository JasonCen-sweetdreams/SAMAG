Deep neural networks are susceptible to adversarial attacks, which can compromise their reliability in safety-critical applications. We propose a novel approach to enhance the robustness of deep neural networks by incorporating Bayesian uncertainty estimation. Our method, 'BayesDef', leverages the epistemic uncertainty of the model to detect and defend against adversarial attacks. Experimental results on the CIFAR-10 and ImageNet datasets demonstrate that BayesDef outperforms state-of-the-art defense mechanisms in terms of robustness against a variety of attack types, while maintaining comparable accuracy on clean data.