Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task, particularly when it comes to understanding the underlying decision-making process. This paper proposes a novel hierarchical attention network (HAN) architecture that leverages the strengths of each modality to improve emotion recognition accuracy. We introduce a multi-level attention mechanism that selectively focuses on relevant features, allowing for more interpretable and explainable emotion recognition. Experimental results on three benchmark datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance while providing insights into the importance of each modality in the recognition process.