Emotion recognition from multi-modal data (e.g., speech, text, vision) is a crucial task in affective computing. However, existing approaches often lack interpretability, making it challenging to understand the decision-making process. We propose a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features, providing an explainable framework for emotion recognition. Our HAN model consists of modality-aware attention modules and a hierarchical fusion mechanism, enabling the model to capture complex interactions between modalities. Experimental results on the IEMOCAP dataset demonstrate the effectiveness of our approach, achieving state-of-the-art performance while providing insightful visualizations of attention weights.