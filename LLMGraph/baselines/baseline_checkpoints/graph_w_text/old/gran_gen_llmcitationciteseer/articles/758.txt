Robotic arm control tasks often require complex motion planning and adaptation to dynamic environments. This paper introduces a hierarchical reinforcement learning (HRL) framework that leverages a high-level policy to select tasks and a low-level policy to execute them. Our approach, 'HiRArm', utilizes a novel task embedding scheme to enable efficient transfer of knowledge between tasks and a curriculum learning strategy to adapt to changing environments. Experiments on a robotic arm simulator demonstrate that HiRArm outperforms traditional flat RL methods in terms of task completion time and adaptability to new tasks.