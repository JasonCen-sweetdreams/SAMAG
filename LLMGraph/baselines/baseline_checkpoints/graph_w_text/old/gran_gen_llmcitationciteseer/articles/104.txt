Emotion recognition from multi-modal inputs, such as facial expressions, speech, and text, is a challenging task in affective computing. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that jointly learns to extract and fuse features from different modalities. Our HAN model comprises two stages: an intra-modal attention mechanism that selectively focuses on relevant features within each modality, and an inter-modal attention mechanism that weights the importance of each modality for decision-making. Experimental results on the MMEmoDB dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, with improvements of 12.5% in F1-score and 15.2% in accuracy.