Emotion recognition from multi-modal data, such as facial expressions, speech, and text, is a challenging task due to the complexity of human emotions and the heterogeneity of modalities. This paper proposes a hierarchical attention network (HAN) that leverages the strengths of each modality to recognize emotions. Our HAN model consists of modality-specific attention modules and a hierarchical fusion module, which learns to weigh the importance of each modality and fusion level. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods in terms of emotion recognition accuracy and F1-score.