Augmented reality (AR) has the potential to revolutionize assistive technology, but existing interfaces often neglect the diverse needs of users with disabilities. This paper presents a novel multimodal interface design framework, 'ARIA', which integrates computer vision, natural language processing, and haptic feedback to create a more inclusive and accessible AR experience. We conducted a participatory design study with 20 users with varying abilities, resulting in a set of design guidelines and a prototype system that demonstrates improved usability and user satisfaction. Our approach has implications for the development of more inclusive AR-based assistive technologies.