Multimodal sentiment analysis has gained significant attention in recent years, but existing approaches often lack transparency in their decision-making processes. This paper proposes a novel hierarchical attention network (HAN) that integrates visual, textual, and acoustic features to analyze sentiment in multimodal data. Our HAN model employs a hierarchical attention mechanism to selectively focus on relevant input features and modalities, providing interpretable sentiment predictions. Experimental results on the CMU-MOSI dataset demonstrate the effectiveness of our approach, achieving state-of-the-art performance while offering insights into the model's decision-making process.