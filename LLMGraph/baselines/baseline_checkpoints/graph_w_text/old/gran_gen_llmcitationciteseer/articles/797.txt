Augmented reality (AR) interfaces have the potential to revolutionize human-computer interaction, but existing interaction methods are often cumbersome or limited. This paper presents a novel gaze-based interaction system for AR, leveraging deep learning to accurately predict user intentions. We propose a multi-modal fusion approach that combines eye-tracking data with computer vision and machine learning techniques to enable seamless, hands-free interaction with virtual objects. Our user study demonstrates significant improvements in interaction speed and accuracy compared to traditional AR input methods, with implications for a wide range of applications from gaming to healthcare.