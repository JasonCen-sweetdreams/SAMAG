Emotion recognition from multi-modal inputs, such as speech, text, and facial expressions, remains a challenging task due to the inherent uncertainty and variability in human emotional responses. This paper introduces a novel hierarchical attention network (HAN) that captures complex interactions between modalities and estimates uncertainty in emotion predictions. Our HAN model uses a Bayesian neural network to learn probabilistic attention weights, enabling the network to quantify uncertainty in its predictions. Experimental results on a large-scale multi-modal emotion recognition dataset demonstrate that our approach outperforms state-of-the-art methods in terms of accuracy and uncertainty calibration.