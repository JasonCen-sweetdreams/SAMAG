Real-world multi-agent systems often require humans to interpret and trust the decision-making processes of autonomous agents. This paper presents a novel hierarchical attention network (HAN) architecture that generates interpretable explanations for agent decisions. Our approach combines graph attention mechanisms with hierarchical clustering to identify influential agents and features that drive decision-making. Experiments on a simulated traffic management scenario demonstrate that HAN explanations improve human trust and understanding of agent behavior, leading to more effective human-agent collaboration. We also show that HAN outperforms baseline explanation methods in terms of accuracy and fidelity.