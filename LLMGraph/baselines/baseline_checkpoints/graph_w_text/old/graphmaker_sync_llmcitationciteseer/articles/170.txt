Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel hierarchical attention network (HAN) that integrates features from different modalities and selectively focuses on relevant regions to improve emotion recognition accuracy. We introduce a multi-task learning framework that jointly optimizes emotion recognition and modality importance estimation, enabling explainable emotion recognition. Experimental results on the IEMOCAP and SEMAINE datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance and providing insights into the importance of each modality for emotion recognition.