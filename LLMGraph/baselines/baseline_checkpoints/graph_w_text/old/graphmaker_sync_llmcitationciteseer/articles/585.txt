Emotion recognition is a crucial aspect of human-computer interaction, enabling systems to respond empathetically to user needs. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates multi-modal inputs from facial expressions, speech, and physiological signals. Our approach leverages attention mechanisms to selectively focus on salient features across modalities, improving emotion recognition accuracy and robustness. Experimental results on a large-scale dataset demonstrate the effectiveness of HAN in recognizing complex emotions, outperforming state-of-the-art fusion-based methods.