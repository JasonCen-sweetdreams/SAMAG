Distributed databases have become increasingly popular for handling large-scale data storage and processing. However, optimizing queries in such systems remains a challenging task due to the complex interactions between nodes. This paper proposes a novel approach to query optimization using reinforcement learning (RL). We design a RL framework that learns to adapt to changing workload patterns and node configurations, leading to improved query performance and reduced latency. Experimental results on a real-world distributed database system demonstrate that our approach outperforms traditional rule-based optimization techniques by up to 30%.