Multimodal sentiment analysis (MSA) aims to determine the sentiment expressed in multimedia data, such as videos, images, and text. While deep learning models have achieved state-of-the-art performance in MSA, they often struggle to effectively integrate and weigh the contributions of different modalities. We propose a novel hierarchical attention network (HAN) that explicitly models the relationships between modalities and learns to selectively focus on the most informative features. Our experiments on several benchmark datasets demonstrate that HAN outperforms existing methods, particularly in scenarios where modalities have disparate importance.