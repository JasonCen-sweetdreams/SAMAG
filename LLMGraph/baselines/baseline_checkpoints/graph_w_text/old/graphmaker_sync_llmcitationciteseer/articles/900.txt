Multimodal retrieval systems struggle to effectively incorporate contextual information, leading to suboptimal query representations. This paper proposes a novel context-aware query expansion approach, 'CAQE', which leverages deep neural networks to integrate visual, textual, and acoustic features. Our method utilizes a multimodal attention mechanism to adaptively weight modalities based on their relevance to the query, and a graph-based expansion strategy to incorporate context from neighboring documents. Experimental results on the Flickr30k dataset demonstrate significant improvements in retrieval accuracy and robustness over state-of-the-art baselines.