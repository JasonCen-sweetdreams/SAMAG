Emotion recognition from multi-modal data, such as facial expressions, speech, and text, has numerous applications in human-computer interaction. This paper presents a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features to improve emotion recognition accuracy. Our HAN model comprises multiple attention layers, each specializing in a specific modality, and a fusion layer that integrates the outputs. We evaluate our approach on the benchmark Multimodal Emotion Recognition dataset and demonstrate significant improvements over state-of-the-art methods. Our results show that the proposed HAN architecture effectively captures complex emotional patterns and outperforms existing models in recognizing emotions from multi-modal data.