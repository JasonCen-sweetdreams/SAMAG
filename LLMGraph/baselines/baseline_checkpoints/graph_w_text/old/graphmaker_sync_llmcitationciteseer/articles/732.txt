Emotion recognition from multi-modal inputs (e.g., text, speech, vision) is a challenging task due to the variability in human emotional expressions. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features for improved emotion recognition. Our HAN model consists of two-level attention mechanisms: (1) modality-level attention to weigh the importance of each input modality, and (2) feature-level attention to highlight salient features within each modality. Experimental results on the benchmark datasets (e.g., IEMOCAP, SEMAINE) demonstrate the effectiveness of our approach, achieving state-of-the-art performance with reduced computational resources.