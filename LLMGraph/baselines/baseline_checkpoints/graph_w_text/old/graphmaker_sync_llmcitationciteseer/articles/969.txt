Voice assistants have become ubiquitous, but their use is often hindered for individuals with disabilities. This paper presents a novel multimodal interaction framework, 'AccesVoice', which enables users with disabilities to interact with voice assistants more effectively. Our approach integrates computer vision, machine learning, and HCI principles to recognize and respond to users' gestures, facial expressions, and voice commands. We evaluate AccesVoice with 30 participants with varying disabilities, demonstrating significant improvements in task completion rates and user satisfaction. Our framework has implications for promoting digital inclusion and enhancing accessibility in smart homes and public spaces.