Emotion recognition from multi-modal inputs, such as facial expressions, speech, and text, is a challenging task due to the complexity of human emotions and the varying quality of input signals. This paper proposes a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and time segments to improve emotion recognition accuracy. Our HAN model consists of three attention layers: modality-level, segment-level, and feature-level, which are trained end-to-end using a multi-task learning framework. Experimental results on the IEMOCAP and SEMAINE datasets show that our approach outperforms state-of-the-art methods, achieving an average F1-score of 0.85 across different emotion categories.