Multimodal sentiment analysis involves analyzing sentiment from multiple sources such as text, images, and audio. However, existing approaches suffer from the limitation of modeling complex interactions between modalities. This paper proposes a novel Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and their interactions. Our HAN consists of a modality-level attention mechanism that weights the importance of each modality and a feature-level attention mechanism that captures fine-grained interactions between modalities. Experimental results on two benchmark datasets demonstrate the effectiveness of our approach in multimodal sentiment analysis, outperforming state-of-the-art methods by a significant margin.