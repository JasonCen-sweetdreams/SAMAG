As virtual reality (VR) applications become increasingly prevalent, understanding user feedback and preferences is crucial for optimizing user experience. This paper presents a novel approach to adaptive visualization in VR, leveraging multimodal user feedback from speech, gestures, and brain-computer interfaces. We propose a machine learning-based framework that integrates these modalities to generate personalized, real-time visualizations that enhance user engagement and comprehension. A user study with 30 participants demonstrates significant improvements in task performance and user satisfaction compared to traditional visualization methods.