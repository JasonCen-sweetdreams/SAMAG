Emotion recognition is a crucial aspect of human-computer interaction, enabling systems to respond empathetically to users. This paper presents a novel hierarchical attention network (HAN) for multi-modal emotion recognition, combining facial expression, speech, and text cues. Our HAN model consists of modality-specific attention layers that adaptively weigh feature importance, followed by a fusion layer that integrates modalities. We evaluate our approach on the EMOTIC dataset, achieving state-of-the-art performance with an accuracy of 92.1% and F1-score of 0.914. Our results demonstrate the effectiveness of hierarchical attention in capturing complex emotional patterns across multiple modalities.