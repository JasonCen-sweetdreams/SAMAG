Emotion recognition from multi-modal data (e.g., text, speech, vision) remains a challenging task due to the complexity of human emotions and the noise inherent in each modality. We propose a novel hierarchical graph attention network (HGAT) that captures both intra- and inter-modal relationships to improve emotion recognition. Our approach leverages graph attention mechanisms to model the interactions between modalities and within-modal features, enabling the network to selectively focus on informative regions. Experimental results on the CMU-MOSEI dataset demonstrate that HGAT outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs.