Explainability is a crucial aspect of multi-agent systems, where agents must make decisions based on complex, high-dimensional state spaces. This paper proposes a novel hierarchical attention network (HAN) architecture that learns to focus on relevant features and agents when making decisions. Our approach consists of two stages: first, a graph attention network (GAT) is used to model agent interactions, and then, a hierarchical attention mechanism is applied to select the most informative agents and features. We evaluate our approach on a variety of multi-agent environments, demonstrating improved decision-making performance and interpretability compared to state-of-the-art methods.