Deep neural networks have achieved remarkable performance in various applications, but their lack of transparency remains a major concern. This paper proposes a novel hierarchical attention mechanism, HierAtt, which improves the explainability of deep neural networks by identifying relevant feature interactions at multiple scales. We demonstrate the effectiveness of HierAtt on several benchmark datasets, including ImageNet and 20 Newsgroups, showcasing significant improvements in feature importance ranking and model interpretability. Furthermore, we provide theoretical insights into the benefits of hierarchical attention in reducing the dimensionality of explanation spaces.