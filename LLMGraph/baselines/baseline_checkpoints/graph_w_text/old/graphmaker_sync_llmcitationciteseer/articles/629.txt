Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task due to the complexity of human emotions and the heterogeneity of modalities. This paper proposes a novel hierarchical graph attention network (HGAT) that leverages the strengths of graph neural networks and attention mechanisms. HGAT learns to selectively focus on relevant modalities and interactions between them, enabling more accurate emotion recognition. Experimental results on several benchmark datasets demonstrate that HGAT outperforms state-of-the-art methods, achieving an average F1-score improvement of 12.3%.