Sentiment analysis from multimodal data (e.g., text, images, audio) is a challenging task due to the heterogeneous nature of the input. This paper proposes a Hierarchical Attention Network (HAN) that learns to selectively focus on relevant modalities and features. Our model employs a novel modality-aware attention mechanism that adapts to the input data, resulting in improved sentiment classification performance. Experiments on two benchmark datasets demonstrate that HAN outperforms state-of-the-art multimodal fusion methods, achieving an absolute increase of 3.5% in accuracy.