Multimodal information retrieval (MIR) systems face the challenge of effectively combining textual and visual features to retrieve relevant results. We propose a novel query expansion framework that leverages reinforcement learning to iteratively refine the query representation. Our approach, dubbed 'RL-QE', learns to select the most informative expansion terms from a multimodal knowledge graph, incorporating both visual and textual contexts. Experiments on a large-scale MIR dataset demonstrate significant improvements in retrieval performance and robustness to query ambiguity.