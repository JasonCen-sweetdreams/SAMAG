Deep reinforcement learning (DRL) has achieved remarkable success in various domains, but its vulnerability to adversarial attacks remains a major concern. This paper investigates the robustness of DRL policies under partial observability, where the agent receives incomplete or noisy state information. We propose a novel attack framework, 'PO-Attack', which leverages the uncertainty principle to craft targeted perturbations that maximize the policy's expected cumulative reward. Our experiments on several Atari games demonstrate that PO-Attack significantly degrades the performance of state-of-the-art DRL algorithms, highlighting the need for more robust and reliable policies in real-world applications.