Graph neural networks (GNNs) have achieved state-of-the-art results in node representation learning tasks. However, existing GNN architectures suffer from high computational complexity and memory requirements, limiting their applicability to large-scale graphs. This paper proposes a novel hierarchical graph attention network (HGAT) that leverages attention mechanisms to selectively aggregate features from neighboring nodes and layers. We theoretically analyze the time and space complexity of HGAT and demonstrate its efficiency on several benchmark datasets. Experimental results show that HGAT achieves competitive performance to state-of-the-art GNNs while reducing computational costs by up to 70%.