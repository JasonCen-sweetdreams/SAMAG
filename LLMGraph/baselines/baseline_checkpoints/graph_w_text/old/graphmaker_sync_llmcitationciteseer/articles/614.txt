Multi-modal sentiment analysis has gained popularity with the proliferation of social media platforms, but current approaches often struggle with large-scale datasets and heterogeneous data types. We propose a novel hierarchical attention network (HAN) that integrates visual, textual, and acoustic features to analyze sentiment in videos. Our HAN model employs a multi-level attention mechanism to selectively focus on relevant modalities and temporal segments, reducing computational complexity by 30% compared to existing state-of-the-art methods. Experiments on the CMU-MOSI and YouTube datasets demonstrate the effectiveness of our approach in capturing nuanced sentiment expressions.