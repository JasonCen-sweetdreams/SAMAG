Node classification is a fundamental task in graph-structured data analysis, but existing methods often struggle with scalability and interpretability. We propose a hierarchical graph attention network (HGAT) that leverages a self-attention mechanism to selectively focus on relevant neighboring nodes and their features. Our approach reduces the computational complexity of node classification by adaptively pruning the graph structure. Experiments on several benchmark datasets demonstrate that HGAT outperforms state-of-the-art methods in terms of accuracy and computational efficiency, while providing insightful visualizations of attention patterns.