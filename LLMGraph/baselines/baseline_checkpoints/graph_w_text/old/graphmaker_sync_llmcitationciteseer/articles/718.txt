Autonomous systems, such as drones and self-driving cars, rely on reinforcement learning (RL) to make decisions in complex, dynamic environments. However, the lack of transparency in RL models hinders their adoption in safety-critical applications. This paper proposes a novel hierarchical attention mechanism, 'HAT', which enables explainable RL policies. HAT selectively focuses on relevant state features and highlights critical decision-making steps, facilitating human understanding and trust. We evaluate HAT on a series of simulated robotics tasks, demonstrating improved policy transparency and interpretability without sacrificing performance.