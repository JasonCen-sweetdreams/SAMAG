Multimodal sentiment analysis has gained significance in recent years, but existing methods often lack interpretability. We propose a novel hierarchical attention network (HAN) that integrates visual, textual, and auditory features to predict sentiment scores. Our approach leverages attention mechanisms to highlight salient regions in each modality, enabling explainable sentiment analysis. We evaluate HAN on three benchmark datasets and demonstrate improved performance over state-of-the-art multimodal fusion techniques. Furthermore, our visualizations and attention heatmaps provide insights into the decision-making process, facilitating model interpretability and trustworthiness.