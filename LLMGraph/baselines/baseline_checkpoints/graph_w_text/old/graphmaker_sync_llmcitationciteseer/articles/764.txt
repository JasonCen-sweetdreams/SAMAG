Deep neural networks (DNNs) often require significant computational resources and memory, hindering their deployment on resource-constrained devices. This paper presents a novel Bayesian neural network (BNN) pruning method, 'HierVar', which leverages hierarchical variational inference to efficiently identify and remove redundant weights. By modeling weight distributions as hierarchical Gaussian processes, HierVar infers a sparse representation of the network, reducing the number of parameters while maintaining accuracy. Experiments on several benchmark datasets demonstrate that HierVar outperforms state-of-the-art pruning methods in terms of compression ratio and accuracy preservation.