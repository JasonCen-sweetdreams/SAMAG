Reinforcement learning (RL) has achieved remarkable success in various domains, but the lack of interpretability hinders its widespread adoption. This paper proposes a novel hierarchical attention framework, 'HARE', which embeds states and actions in a shared latent space, enabling explainable RL. HARE leverages attention mechanisms to selectively focus on relevant state-action pairs, facilitating the discovery of meaningful patterns and relationships. Our approach improves upon existing methods in terms of both performance and interpretability, as demonstrated on a range of Atari games and robotic control tasks.