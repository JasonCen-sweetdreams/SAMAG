Multimodal sentiment analysis involves analyzing sentiment from multiple sources such as text, images, and audio. However, existing approaches often lack interpretability, making it challenging to understand the decision-making process. This paper proposes a novel hierarchical attention network (HAN) that integrates multimodal attention mechanisms to identify relevant features and their interactions. We evaluate our approach on a large multimodal dataset and demonstrate improved performance compared to state-of-the-art baselines. Furthermore, we provide visualizations of the attention weights, enabling explainability and insights into the model's decision-making process.