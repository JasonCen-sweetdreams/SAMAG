Multi-agent systems often consist of agents with diverse capabilities and objectives. Coordinating these agents to achieve a common goal is a challenging problem. This paper presents a novel approach that leverages hierarchical reinforcement learning (HRL) to coordinate heterogeneous agents. We propose a two-level HRL framework, where a high-level policy coordinates the agents' actions, and low-level policies control each agent's behavior. We introduce a novel reward function that encourages agents to collaborate and adapt to changing environments. Experimental results on a simulated search-and-rescue scenario demonstrate that our approach outperforms state-of-the-art methods in terms of task completion time and overall system efficiency.