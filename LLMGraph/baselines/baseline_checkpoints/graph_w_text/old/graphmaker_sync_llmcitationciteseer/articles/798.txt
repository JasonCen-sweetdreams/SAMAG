Multi-modal documents, such as those containing text, images, and videos, are increasingly prevalent in modern information retrieval systems. This paper proposes a novel hierarchical graph neural network (HGNN) architecture for efficient retrieval of multi-modal documents. Our approach models the complex relationships between different modalities using a hierarchical graph structure, and leverages graph attention mechanisms to selectively focus on the most informative modalities. Experimental results on a large-scale multi-modal dataset demonstrate that our HGNN-based approach outperforms state-of-the-art methods in terms of retrieval accuracy and efficiency.