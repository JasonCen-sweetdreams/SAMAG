Multimodal information retrieval (MMIR) systems struggle to effectively incorporate user feedback due to the complexity of representing and combining heterogeneous data types. This paper introduces 'ARF-MMIR', a novel adaptive relevance feedback framework that dynamically adjusts the weighting of textual, visual, and audio features based on user interactions. Our approach leverages a graph-based neural network to capture cross-modal relationships and incorporates a self-attention mechanism to refine feature importance. Experimental results on a large-scale MMIR benchmark demonstrate significant improvements in retrieval performance and user satisfaction.