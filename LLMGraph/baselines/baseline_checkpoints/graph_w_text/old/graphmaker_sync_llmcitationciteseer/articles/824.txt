Emotion recognition from multi-modal inputs, such as speech, text, and vision, remains a challenging task in affective computing. This paper proposes a hierarchical attention network (HAN) that learns to fuse features from different modalities and selectively focus on the most informative regions. Our HAN model consists of two stages: modality-specific attention networks that extract emotional cues from each input modality, and a multi-modal fusion network that integrates the attention-weighted features. Experiments on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an average F1-score of 83.2%