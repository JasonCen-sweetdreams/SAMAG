In multi-agent systems, agents often have limited knowledge about their environment and other agents' actions. This paper proposes a decentralized reinforcement learning framework, 'MADEC', which enables agents to learn effective policies despite incomplete information. MADEC combines graph neural networks with decentralized actor-critic methods, allowing agents to infer missing information and adapt to changing environments. We evaluate MADEC on a variety of multi-agent scenarios, including traffic control and robotic navigation, and demonstrate significant improvements in overall system performance compared to existing decentralized RL approaches.