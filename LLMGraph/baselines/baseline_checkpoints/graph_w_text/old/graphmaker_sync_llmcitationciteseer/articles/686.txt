Coordinating multiple agents to achieve a common goal in complex, dynamic environments is a challenging problem in multi-agent systems. This paper proposes a novel deep hierarchical reinforcement learning framework, 'HierMA', which enables agents to learn coordinated policies through a hierarchical decomposition of the task. Our approach consists of a high-level coordinator that selects tasks and allocates agents, while each agent learns a low-level policy using deep Q-networks. We evaluate HierMA in a simulated robotic soccer domain and demonstrate significant improvements in coordination and task completion compared to existing decentralized and centralized methods.