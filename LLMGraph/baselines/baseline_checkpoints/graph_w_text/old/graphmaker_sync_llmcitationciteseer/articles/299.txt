Conversational AI systems often struggle to maintain context and coherence across multiple turns of dialogue. This paper introduces a novel hierarchical attention network (HAN) architecture that models both utterance-level and turn-level dependencies to improve dialogue management. Our HAN uses a stacked self-attention mechanism to capture long-range dependencies and a graph-based attention mechanism to selectively focus on relevant context. Experimental results on the DSTC-8 and Ubuntu dialogue datasets demonstrate significant improvements in response generation quality and coherence compared to state-of-the-art baselines.