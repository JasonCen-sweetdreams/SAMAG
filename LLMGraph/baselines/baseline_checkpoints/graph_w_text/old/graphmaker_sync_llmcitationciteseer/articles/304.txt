In complex systems, coordinating multiple heterogeneous agents to optimize resource allocation is a challenging problem. This paper proposes a novel approach using multi-objective reinforcement learning (MORL) to enable agents to adapt to changing environments and optimize conflicting objectives. We introduce a hierarchical framework that integrates MORL with a decentralized coordination mechanism, allowing agents to negotiate and adjust their policies in real-time. Experimental results in a simulated logistics domain demonstrate improved resource allocation efficiency and adaptability compared to traditional single-objective RL approaches.