Multimodal sentiment analysis has gained significant attention in recent years due to its applications in human-computer interaction and affective computing. However, existing approaches often struggle to effectively integrate and attend to heterogeneous data sources such as text, images, and audio. This paper proposes a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features at multiple abstraction levels. Our experiments on a large-scale multimodal dataset demonstrate that HAN outperforms state-of-the-art methods in sentiment classification and provides interpretable insights into the contribution of each modality to the overall sentiment.