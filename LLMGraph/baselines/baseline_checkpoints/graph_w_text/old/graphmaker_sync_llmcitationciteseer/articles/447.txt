Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) has seen significant progress with the advent of deep learning. However, most state-of-the-art models lack transparency in their decision-making processes, hindering trust and interpretability. This paper proposes a novel hierarchical attention network (HAN) architecture that learns to selectively focus on relevant modalities and features for emotion recognition. Our approach achieves competitive performance on the benchmark CMU-MOSEI dataset while providing visualizations of attention weights, enabling explainability and feature importance analysis. We demonstrate the effectiveness of HAN in various emotion recognition tasks, including sentiment analysis and emotional speech classification.