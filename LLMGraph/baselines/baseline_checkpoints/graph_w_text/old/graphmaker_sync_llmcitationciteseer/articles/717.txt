Emotion recognition from multi-modal data, such as speech, text, and facial expressions, remains a challenging task due to the complexity of human emotions and the disparate formats of the input data. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) that integrates graph attention mechanisms with hierarchical fusion to effectively capture the complex relationships between different modalities. Our experiments on the CMU-MOSEI dataset demonstrate that HGAT outperforms state-of-the-art methods in emotion recognition tasks, achieving an average F1-score of 0.82 and improving the overall recognition accuracy by 12.5%. The proposed approach has significant implications for affective computing and human-computer interaction applications.