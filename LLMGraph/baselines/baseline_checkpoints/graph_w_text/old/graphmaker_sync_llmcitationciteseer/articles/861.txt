As Virtual Reality (VR) technology advances, understanding user behavior and experience becomes crucial. This paper presents EyeTrackingLens, a novel multimodal fusion framework that combines eye tracking, facial expression analysis, and physiological signal processing to enhance user experience in VR. Our approach leverages machine learning algorithms to identify patterns in user behavior, enabling the development of more engaging and personalized VR applications. We evaluated EyeTrackingLens in a user study involving 30 participants, demonstrating significant improvements in user satisfaction and engagement compared to traditional VR systems.