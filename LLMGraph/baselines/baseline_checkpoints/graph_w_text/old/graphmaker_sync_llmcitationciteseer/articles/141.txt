Real-world visual recognition tasks often exhibit long-tail distributions, where a few classes dominate the dataset. This paper proposes a hierarchical contrastive learning framework, 'HiCLR', which tackles the long-tail problem by learning a nested representation hierarchy. We introduce a novel contrastive loss function that encourages the model to discriminate between samples at multiple levels of abstraction. Experiments on challenging long-tail benchmarks demonstrate that HiCLR achieves state-of-the-art performance while reducing the computational cost by 30% compared to existing methods.