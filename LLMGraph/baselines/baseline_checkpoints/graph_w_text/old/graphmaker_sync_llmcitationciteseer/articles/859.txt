Multimodal sentiment analysis has become increasingly important in understanding human behavior and opinions. However, existing deep learning models lack interpretability, making it difficult to trust their predictions. This paper proposes a novel Hierarchical Attention Network (HAN) for multimodal sentiment analysis, which incorporates attention mechanisms at both the feature and modal levels. Our approach enables the identification of salient features and modalities contributing to the sentiment prediction, thereby improving model explainability. Experimental results on a large-scale multimodal dataset demonstrate the effectiveness of HAN in achieving state-of-the-art performance while providing insightful explanations.