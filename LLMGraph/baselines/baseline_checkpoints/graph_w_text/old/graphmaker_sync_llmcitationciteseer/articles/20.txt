Emotion recognition from multi-modal data, such as speech, text, and vision, is a crucial task in human-computer interaction. However, existing approaches often suffer from high computational complexity and limited performance. This paper proposes a novel hierarchical attention network (HAN) that leverages the strengths of each modality to recognize emotions more accurately and efficiently. Our HAN model consists of multiple layers of attention mechanisms that selectively focus on relevant modalities and features, thereby reducing the dimensionality of the input data and improving recognition performance. Experimental results on the Multimodal Emotion Recognition (MER) dataset demonstrate that our approach outperforms state-of-the-art methods in terms of both accuracy and computational efficiency.