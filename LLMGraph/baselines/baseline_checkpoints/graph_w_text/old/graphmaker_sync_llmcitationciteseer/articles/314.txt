This paper presents a novel approach to designing adaptive interfaces for people with disabilities using multimodal fusion. We propose a framework that combines computer vision, speech recognition, and gesture recognition to create a more accessible and inclusive interaction experience. Our system, called 'AccessibleFuse', uses machine learning algorithms to dynamically adjust the interface based on the user's abilities and preferences. We evaluated AccessibleFuse with a group of users with varying disabilities and found significant improvements in usability and satisfaction. The results suggest that multimodal fusion can be an effective way to create more accessible interfaces, and we discuss the implications for future HCI research.