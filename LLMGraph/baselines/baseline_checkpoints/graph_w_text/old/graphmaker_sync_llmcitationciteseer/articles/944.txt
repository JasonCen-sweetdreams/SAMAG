Multimodal emotion recognition is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel hierarchical attention network (HAN) that integrates audio, video, and textual features to recognize emotions. Our HAN model employs a cascaded attention mechanism to selectively focus on relevant modalities and features, thereby improving emotion recognition accuracy. Experimental results on the IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art multimodal fusion methods, achieving an average F1-score of 84.2%.