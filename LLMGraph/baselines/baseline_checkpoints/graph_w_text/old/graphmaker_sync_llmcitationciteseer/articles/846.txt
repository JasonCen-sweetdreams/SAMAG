Multimodal sentiment analysis (MSA) aims to infer sentiment from text, images, and videos. While deep learning models have shown promising results, they often lack interpretability. This paper proposes a hierarchical attention network (HAN) for MSA, which leverages attention mechanisms to highlight relevant input features contributing to the sentiment prediction. Our framework integrates a novel multimodal fusion strategy, enabling the model to capture complex relationships between modalities. Experimental results on three benchmark datasets demonstrate that HAN outperforms state-of-the-art methods in both accuracy and explainability, providing insights into the decision-making process.