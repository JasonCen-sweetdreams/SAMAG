The growing adoption of AI models in high-stakes applications has sparked concerns about their interpretability and trustworthiness. This paper presents a novel multi-modal fusion approach, 'HAFusion', which leverages hierarchical attention mechanisms to combine visual, textual, and numerical features for explainable AI decision-making. Our framework generates attention-based importance scores for each modality, enabling users to understand the contribution of individual features to the model's predictions. Experiments on a medical diagnosis dataset demonstrate that HAFusion outperforms state-of-the-art fusion methods in terms of accuracy and interpretability, while providing insights into the decision-making process.