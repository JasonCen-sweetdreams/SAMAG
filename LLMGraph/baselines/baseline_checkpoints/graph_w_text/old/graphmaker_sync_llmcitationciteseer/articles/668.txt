Explainability is crucial in reinforcement learning (RL) for autonomous systems, where decision-making transparency is essential for safety and trust. This paper proposes a hierarchical attention-based RL framework, 'HARE', that incorporates interpretable attention mechanisms to highlight relevant state features and action contributions. We demonstrate the effectiveness of HARE in a series of experiments on a simulated autonomous driving domain, showcasing improved explainability and comparable performance to state-of-the-art RL methods. Our approach enables the identification of critical factors influencing policy decisions, facilitating the development of more reliable and trustworthy autonomous systems.