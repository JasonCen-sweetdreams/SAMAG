Explainable AI is crucial in multi-agent systems, where decision-making involves complex interactions. We propose a novel hierarchical attention network (HAN) architecture, 'ExplainAgent', which learns to selectively focus on relevant agents and features to generate interpretable decisions. Our approach integrates a graph attention mechanism to model agent relationships and a hierarchical attention framework to capture feature importance. Experimental results on a real-world traffic management dataset demonstrate that ExplainAgent outperforms state-of-the-art methods in decision accuracy and interpretability, enabling human-AI collaboration in complex scenarios.