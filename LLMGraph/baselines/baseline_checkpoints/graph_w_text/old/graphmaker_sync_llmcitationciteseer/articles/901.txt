Emotion recognition is crucial for human-robot interaction (HRI) to enable empathetic and personalized responses. This paper presents a novel multimodal fusion approach that combines facial expressions, speech, and physiological signals to recognize emotions in HRI scenarios. We propose a hierarchical attention mechanism that adaptively weights the importance of each modality based on the context and individual characteristics. Experimental results on a large-scale HRI dataset demonstrate significant improvements in emotion recognition accuracy and robustness compared to state-of-the-art methods. Our approach has potential applications in social robots, affective computing, and human-centered AI systems.