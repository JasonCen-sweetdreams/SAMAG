Avatars are increasingly used in virtual environments to facilitate human-computer interaction. However, their emotional expressiveness is often limited, leading to a lack of user engagement. This paper presents a multimodal approach to enhance avatar emotional expressiveness by integrating facial expressions, body language, and speech prosody. We proposed a novel machine learning model that learns to generate coherent and context-dependent emotional cues from user input. Our evaluation shows that users perceive avatars with enhanced emotional expressiveness as more relatable and engaging, leading to improved user experience in virtual environments.