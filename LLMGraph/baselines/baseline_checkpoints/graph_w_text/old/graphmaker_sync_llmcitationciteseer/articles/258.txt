Recognizing emotions from multi-modal inputs, such as speech, text, and facial expressions, is a challenging task in affective computing. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant modalities and features. Our HAN model consists of three stages: (1) modality-specific attention, (2) feature-level attention, and (3) cross-modal fusion. Experimental results on the benchmark MMEmoDB dataset demonstrate that our approach outperforms state-of-the-art methods, achieving an average F1-score of 0.87 across five emotions. We also provide ablation studies to highlight the importance of hierarchical attention in multi-modal emotion recognition.