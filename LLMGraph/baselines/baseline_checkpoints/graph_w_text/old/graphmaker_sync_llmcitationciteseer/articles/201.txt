Multi-modal dialogue systems require effective representation of knowledge graphs to facilitate context-aware conversational understanding. This paper proposes a novel knowledge graph embedding method, 'KGEMM', which leverages a combination of graph attention mechanisms and multimodal fusion techniques. We demonstrate that KGEMM outperforms state-of-the-art methods on benchmark datasets, achieving significant improvements in dialogue response generation and question answering tasks. Our approach enables more accurate and informative dialogue systems, with potential applications in virtual assistants and customer service platforms.