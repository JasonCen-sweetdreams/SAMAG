Emotion recognition is a crucial aspect of human-computer interaction, but existing approaches often rely on a single modality (e.g., facial expressions or speech). This paper proposes a hierarchical attention network (HAN) that integrates multi-modal inputs (vision, audio, and physiology) to recognize emotions in a more comprehensive and accurate manner. Our HAN model consists of modality-specific encoders, attention layers, and a fusion module that adaptively weights the importance of each modality. Experiments on the SEMAINE and RECOLA datasets demonstrate that our approach outperforms state-of-the-art methods and achieves improved robustness to noisy or missing data.