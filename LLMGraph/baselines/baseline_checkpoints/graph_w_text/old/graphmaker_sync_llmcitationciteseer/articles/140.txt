Visual question answering (VQA) tasks require the fusion of information from multiple modalities, including images and natural language. While attention mechanisms have shown promise in this domain, they often suffer from computational complexity and limited scalability. This paper presents a novel attention network architecture, 'MultiModal Transformer' (MMT), which leverages sparse attention patterns and hierarchical fusion to efficiently integrate multi-modal representations. Our experiments on the VQA-CP v2 dataset demonstrate that MMT achieves state-of-the-art performance with significant reductions in computational cost and memory requirements.