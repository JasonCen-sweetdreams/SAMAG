Emotion recognition from multi-modal inputs, such as speech, text, and facial expressions, is a challenging task in affective computing. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages the strengths of both local and global feature extractors. Our HAN model employs a hierarchical attention mechanism to selectively focus on salient features across modalities, allowing it to capture complex emotional patterns. Experimental results on a benchmark multi-modal emotion recognition dataset demonstrate that our approach outperforms state-of-the-art methods, achieving a 12% improvement in recognition accuracy.