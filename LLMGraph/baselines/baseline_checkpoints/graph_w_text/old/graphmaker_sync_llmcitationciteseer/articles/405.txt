Hyperparameter tuning is a crucial step in training deep neural networks, but it can be computationally expensive and time-consuming. This paper proposes a Bayesian optimization approach, 'BOHT', that leverages a probabilistic surrogate model to efficiently search the hyperparameter space. We introduce a novel acquisition function that balances exploration and exploitation, and demonstrate its effectiveness on several benchmark datasets. Experimental results show that BOHT achieves comparable or better performance than state-of-the-art methods while reducing the number of evaluations by up to 50%