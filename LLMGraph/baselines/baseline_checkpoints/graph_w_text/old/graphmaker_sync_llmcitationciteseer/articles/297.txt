Multi-modal time series forecasting is a challenging problem that involves predicting future values in multiple related time series. While deep learning models have shown promise in this domain, they often struggle with scalability and interpretability. This paper proposes a novel architecture, 'MTS-Transformer', which leverages the strengths of transformer models and graph neural networks to handle large-scale multi-modal time series data. We demonstrate the effectiveness of MTS-Transformer on several benchmark datasets, achieving state-of-the-art performance while providing insights into the underlying relationships between modalities.