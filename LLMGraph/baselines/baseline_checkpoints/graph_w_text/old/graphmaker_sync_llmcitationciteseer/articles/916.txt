The growing demand for AI-powered applications on resource-constrained devices has fueled the need for efficient neural architecture search (NAS) methods. This paper proposes a novel reinforcement learning (RL) framework, 'RL-NAS+', which leverages a hierarchical search space to optimize both the architecture and its corresponding hyperparameters. Our approach incorporates a novel reward function that balances model accuracy, latency, and memory usage. Experimental results on several benchmark datasets demonstrate that RL-NAS+ outperforms state-of-the-art NAS methods in terms of search time and model efficiency, making it an attractive solution for real-world deployments.