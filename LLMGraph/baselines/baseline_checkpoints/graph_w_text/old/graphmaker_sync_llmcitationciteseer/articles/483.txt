Emotion recognition from multi-modal inputs, such as speech, text, and vision, is a challenging task due to the complexity of human emotions and the lack of interpretability in existing models. We propose a Hierarchical Attention Network (HAN) that leverages the strengths of each modality to recognize emotions in a more interpretable manner. Our model employs a hierarchical attention mechanism to selectively focus on relevant modalities and features, allowing for more accurate and explainable emotion recognition. Experimental results on the IEMOCAP dataset demonstrate the effectiveness of our approach, achieving state-of-the-art performance while providing insights into the decision-making process.