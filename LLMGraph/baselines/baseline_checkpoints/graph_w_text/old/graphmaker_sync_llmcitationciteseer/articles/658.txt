Sentiment analysis in social media has become a crucial task for businesses and organizations to understand public opinion. However, the presence of diverse modalities such as text, images, and videos makes it challenging. This paper proposes a novel Hierarchical Attention Network (HAN) for multimodal sentiment analysis. Our approach leverages the strengths of different modalities by hierarchically fusing attention weights. Experimental results on a large-scale social media dataset demonstrate that our approach outperforms state-of-the-art multimodal fusion methods, achieving an average improvement of 7.2% in sentiment classification accuracy.