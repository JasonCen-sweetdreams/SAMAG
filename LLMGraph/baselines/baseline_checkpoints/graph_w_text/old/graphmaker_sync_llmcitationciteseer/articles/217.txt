Emotion recognition systems often rely on multimodal inputs, such as speech, text, and vision. However, most existing approaches lack transparency in their decision-making processes. This paper proposes a hierarchical attention network (HAN) framework that selectively focuses on relevant modalities and features to improve emotion recognition accuracy. Our HAN model comprises two stages: modality attention, which weighs the importance of each modality, and feature attention, which highlights salient features within each modality. Experimental results on the CMU-MOSEI dataset demonstrate the effectiveness of our approach in recognizing emotions and providing interpretable explanations.