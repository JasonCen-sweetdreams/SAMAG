Multi-modal sentiment analysis has gained increasing attention due to the proliferation of multimedia content on social media platforms. However, existing methods often lack transparency and interpretability, making it challenging to understand the decision-making process. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates visual, textual, and acoustic features for multi-modal sentiment analysis. Our attention mechanism enables the identification of salient features and their relationships, providing explainable insights into the sentiment classification process. Experimental results on the CMU-MOSI dataset demonstrate the effectiveness of our approach, achieving state-of-the-art performance while offering improved interpretability.