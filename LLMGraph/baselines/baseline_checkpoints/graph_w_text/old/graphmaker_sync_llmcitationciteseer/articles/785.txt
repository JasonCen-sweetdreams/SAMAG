Visual Question Answering (VQA) tasks require models to integrate information from multiple sources, including images, questions, and context. This paper presents a novel Hierarchical Attention Network (HAN) architecture that efficiently fuses multi-modal inputs for improved VQA performance. Our proposed HAN model consists of a question-guided image attention module and a context-aware question attention module, enabling it to selectively focus on relevant regions of the input data. Experimental results on the VQA 2.0 dataset demonstrate that our approach achieves state-of-the-art performance with reduced computational overhead.