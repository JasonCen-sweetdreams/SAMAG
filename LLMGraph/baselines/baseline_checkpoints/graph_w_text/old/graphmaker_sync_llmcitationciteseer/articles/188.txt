Emotion recognition from multi-modal inputs, such as facial expressions, speech, and text, has become increasingly important in human-computer interaction. However, existing methods often rely on complex fusion strategies, leading to high computational costs. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that efficiently integrates multi-modal features through a hierarchical attention mechanism. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, while reducing inference time by up to 30%.