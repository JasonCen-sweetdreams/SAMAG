Emotion recognition is a crucial aspect of human-computer interaction, enabling systems to respond empathetically to users. This paper introduces a novel hierarchical attention network (HAN) that integrates facial expressions, speech, and physiological signals to recognize emotions in real-time. Our HAN model comprises multiple attention layers that selectively focus on pertinent modalities and features, leading to improved recognition accuracy. Experimental results on a large-scale multimodal dataset demonstrate the efficacy of our approach, achieving an average F1-score of 0.92 across six emotions. We also conduct a user study, showing that HAN-based systems can enhance user experience and engagement in affective computing applications.