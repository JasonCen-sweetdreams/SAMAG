Emotion recognition from multi-modal data, such as speech, text, and facial expressions, is a crucial task in human-computer interaction. However, existing approaches often suffer from high computational complexity and ignore the hierarchical relationships between modalities. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages multi-scale attention mechanisms to efficiently fuse and select relevant features from each modality. Experimental results on the IEMOCAP and SEMAINE datasets demonstrate that HAN outperforms state-of-the-art methods in terms of emotion recognition accuracy and computational efficiency.