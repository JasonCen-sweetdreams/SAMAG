Deep reinforcement learning (DRL) has achieved remarkable success in complex environments, but its lack of transparency hinders its adoption in high-stakes applications. This paper proposes a novel explainability framework, 'RL-Explain', which integrates attention mechanisms and model-based reasoning to provide interpretable insights into DRL decision-making. We demonstrate the effectiveness of RL-Explain in Atari games and a real-world autonomous driving scenario, showcasing improved performance and trustworthiness. Our approach enables the identification of critical state features and action sequences, facilitating the development of more robust and transparent AI systems.