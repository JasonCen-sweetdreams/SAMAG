Knowledge graph embedding (KGE) has become a crucial task in various AI applications. However, existing methods suffer from limitations, such as ignoring hierarchical relationships and relying on shallow models. We propose a novel KGE framework, 'HierAttKG', which leverages hierarchical attention networks to learn more informative representations. Our approach captures multi-level dependencies between entities and relations, leading to improved performance on link prediction and entity classification tasks. Experimental results on benchmark datasets demonstrate the effectiveness of HierAttKG, achieving state-of-the-art results in several settings.