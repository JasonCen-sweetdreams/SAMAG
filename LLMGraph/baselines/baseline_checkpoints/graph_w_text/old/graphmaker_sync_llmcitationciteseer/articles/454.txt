Gesture recognition is a crucial aspect of Human-Computer Interaction (HCI), enabling intuitive and natural user interfaces. This paper presents a novel approach to gesture recognition by fusing multimodal inputs from computer vision, speech, and accelerometer sensors. We propose a deep learning architecture that leverages convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to learn robust feature representations and recognize gestures in real-time. Our experiments demonstrate significant improvements in recognition accuracy and robustness compared to unimodal approaches, particularly in noisy and dynamic environments.