Multi-agent decision-making systems often rely on complex algorithms that lack interpretability, making it challenging to understand the rationale behind their decisions. This paper introduces a novel Hierarchical Attention Network (HAN) architecture for explainable multi-agent decision-making. Our approach enables agents to selectively focus on relevant information from their local observations and communicate effectively with other agents. We evaluate the proposed HAN model on a variety of multi-agent tasks, including autonomous driving and robotic soccer, demonstrating improved performance and interpretability compared to state-of-the-art baselines.