Query expansion is a crucial step in ad-hoc retrieval, as it aims to reformulate the original query to better capture the user's information need. In this paper, we propose a novel deep reinforcement learning framework, 'RL-QE', which learns to generate effective query expansions by interacting with a simulated retrieval environment. Our approach leverages a policy gradient method to optimize the query expansion policy, and incorporates a reward function that balances the trade-off between precision and recall. Experimental results on the TREC Robust04 dataset demonstrate that RL-QE outperforms state-of-the-art query expansion methods, achieving a significant improvement in mean average precision.