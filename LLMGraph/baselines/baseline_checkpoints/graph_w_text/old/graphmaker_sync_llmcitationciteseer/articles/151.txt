Dialogue systems have become increasingly prevalent in human-computer interaction, but their lack of transparency hinders trust and understanding. We propose a novel hierarchical attention network (HAN) architecture that leverages multi-modal inputs (text, speech, and vision) to generate more informative and controllable responses. By learning to attend to relevant input features at multiple levels, our HAN-based dialogue system achieves state-of-the-art performance on a benchmark dataset while providing visual explanations for its responses. We demonstrate the applicability of our approach in a real-world conversational AI application, highlighting its potential for improving human-AI collaboration.