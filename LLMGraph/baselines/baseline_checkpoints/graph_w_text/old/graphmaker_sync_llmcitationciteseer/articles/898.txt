Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a challenging task, particularly when dealing with ambiguous or conflicting cues. This paper introduces a hierarchical attention network (HAN) that selectively focuses on relevant modalities and features to improve recognition accuracy and interpretability. Our HAN model consists of a multi-modal fusion layer and a hierarchical attention mechanism that recursively refines attention weights. Experimental results on the IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art multi-modal emotion recognition models while providing insight into the decision-making process.