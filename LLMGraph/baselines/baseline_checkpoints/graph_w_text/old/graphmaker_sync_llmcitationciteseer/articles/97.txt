As AI systems increasingly interact with humans, there is a growing need for transparent and explainable decision-making in multi-agent settings. This paper proposes a novel hierarchical attention network (HAN) architecture for multi-agent decision-making, which integrates both local and global contextual information to generate interpretable decisions. Our approach leverages graph attention mechanisms to model agent relationships and incorporates a novel explainability module to provide insight into the decision-making process. Experimental results on a real-world autonomous vehicle dataset demonstrate improved decision accuracy and explainability compared to state-of-the-art methods.