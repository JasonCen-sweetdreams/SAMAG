Recommender systems have become increasingly reliant on multi-modal data, such as text, images, and graphs. However, effectively integrating and exploiting these diverse modalities remains a significant challenge. We propose a novel Hierarchical Graph Attention Network (HGAT) that adaptively fuses multi-modal data to generate personalized recommendations. HGAT employs a hierarchical graph attention mechanism to selectively weigh and combine modality-specific features, enabling more effective and efficient recommendation generation. Experimental results on a large-scale dataset demonstrate that HGAT outperforms state-of-the-art methods in terms of recommendation accuracy and diversity.