As human-robot interaction (HRI) systems become increasingly prevalent, there is a growing need for explainable multimodal fusion methods that can effectively integrate visual, auditory, and linguistic cues. This paper proposes a novel hybrid attention mechanism that combines the strengths of both spatial and channel attention to enable more accurate and interpretable fusion of multimodal inputs. We evaluate our approach on a real-world HRI dataset and demonstrate significant improvements in both performance and explainability compared to state-of-the-art fusion methods.