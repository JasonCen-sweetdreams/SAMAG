Visual question answering (VQA) tasks require models to integrate information from different modalities, including images and natural language. This paper presents a novel hierarchical graph attention network (HGAT) framework that effectively fuses multi-modal features in VQA. HGAT uses graph attention mechanisms to model relationships between visual objects and question entities, while also incorporating hierarchical fusion of modalities at different levels of abstraction. We evaluate HGAT on the VQA 2.0 benchmark and demonstrate significant improvements in accuracy over state-of-the-art models, particularly for complex questions requiring multi-hop reasoning.