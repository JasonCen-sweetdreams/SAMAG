Multimodal fusion is a crucial component in Visual Question Answering (VQA) tasks, as it combines language and vision features to generate accurate answers. However, existing fusion methods often suffer from high computational complexity and neglect the hierarchical relationships between modalities. This paper proposes a novel Hierarchical Attention Network (HAN) for efficient multimodal fusion in VQA. HAN leverages a hierarchical attention mechanism to selectively focus on relevant regions in images and words in questions, reducing the dimensionality of the fused representation while preserving crucial information. Experimental results on the VQA 2.0 dataset demonstrate that HAN outperforms state-of-the-art fusion methods in terms of accuracy and inference speed.