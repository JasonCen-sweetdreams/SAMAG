Neural architecture search (NAS) has revolutionized the design of deep neural networks. However, existing NAS methods are often computationally expensive and require significant resources. This paper introduces 'ARAS', an efficient NAS framework that adaptively allocates resources to promising architectures during search. We propose a novel reinforcement learning-based controller that dynamically adjusts the number of iterations and batch sizes for each architecture, thereby reducing the search cost by up to 50%. Experimental results on CIFAR-10 and ImageNet datasets demonstrate that ARAS discovers highly accurate models with fewer computational resources.