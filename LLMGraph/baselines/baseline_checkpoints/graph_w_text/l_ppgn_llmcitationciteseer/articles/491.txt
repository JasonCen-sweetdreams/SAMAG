Traditional touch-based interfaces can be inaccessible or uncomfortable for individuals with motor impairments. This paper presents a novel approach to designing adaptive touchless gestures for inclusive interfaces. We propose a machine learning-based framework that leverages computer vision and user behavior modeling to recognize and adapt to individual users' gestures in real-time. Our approach enables users to interact with interfaces using personalized, intuitive gestures, and provides a more inclusive and accessible experience. We evaluate our approach through a user study with participants with and without motor impairments, demonstrating improved interaction accuracy and user satisfaction.