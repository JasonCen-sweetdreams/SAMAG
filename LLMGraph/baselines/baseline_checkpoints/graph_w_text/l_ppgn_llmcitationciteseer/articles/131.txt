Emotion recognition from multimodal inputs (e.g., facial expressions, speech, and text) remains a challenging task, especially when it comes to model interpretability. This paper presents a novel Hierarchical Attention Network (HAN) architecture that captures both intra- and inter-modality relationships to improve emotion recognition accuracy. Our approach incorporates a feature-wise attention mechanism that selectively focuses on relevant modalities and a hierarchical fusion strategy that aggregates features at multiple levels. Experimental results on three benchmark datasets demonstrate that HAN outperforms state-of-the-art multimodal emotion recognition methods while providing insightful attention visualizations for better model interpretability.