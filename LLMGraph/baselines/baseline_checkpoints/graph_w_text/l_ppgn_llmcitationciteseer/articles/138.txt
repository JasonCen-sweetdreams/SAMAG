This paper presents a novel approach to personalized gesture recognition using deep learning models on multimodal sensor data. Our system, ' GestureFusion', integrates data from computer vision, inertial measurement units, and electromyography sensors to recognize user-specific gestures. We propose a transfer learning framework that adapts to individual users' gesture patterns, achieving an average recognition accuracy of 95.2% on a dataset of 20 users. Experimental results show that GestureFusion outperforms state-of-the-art methods in terms of accuracy and robustness to sensor noise and user variability.