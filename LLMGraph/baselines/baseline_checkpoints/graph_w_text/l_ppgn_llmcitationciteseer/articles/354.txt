Social media platforms generate vast amounts of multimodal data, comprising text, images, and videos. Sentiment analysis on such data is crucial for businesses and organizations to understand public opinions. This paper proposes a novel hierarchical attention network (HAN) architecture that leverages the strengths of both visual and textual features for multimodal sentiment analysis. Our approach incorporates a modality-agnostic attention mechanism to selectively focus on relevant features from each modality, resulting in improved sentiment prediction accuracy. Experimental results on a large-scale social media dataset demonstrate the effectiveness of our approach, outperforming state-of-the-art multimodal fusion techniques.