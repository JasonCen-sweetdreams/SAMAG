In complex systems, multiple agents often compete for shared resources, leading to inefficient allocation and suboptimal performance. This paper proposes a novel framework for coordinating multi-agent systems using reinforcement learning. We introduce a decentralized, partially observable Markov decision process (Dec-POMDP) that models the agents' interactions and resource dependencies. Our approach, called 'MA-RLA', leverages deep Q-networks to learn policies for each agent, which are then coordinated using a hierarchical clustering algorithm to minimize resource conflicts. Experimental results on a simulated smart grid scenario demonstrate that MA-RLA outperforms traditional allocation methods in terms of resource utilization and system-wide efficiency.