Multimodal sentiment analysis has witnessed significant advancements with the advent of deep learning. However, existing methods often rely on opaque representations, hindering interpretability and trustworthiness. This paper proposes a novel Hierarchical Attention Network (HAN) architecture, which integrates visual, textual, and acoustic modalities for sentiment analysis. Our approach employs modality-specific attention mechanisms and a hierarchical fusion strategy to generate explainable representations. Experimental results on the CMU-MOSI and ICT-MMM datasets demonstrate the efficacy of HAN in achieving state-of-the-art performance while providing transparent insights into the decision-making process.