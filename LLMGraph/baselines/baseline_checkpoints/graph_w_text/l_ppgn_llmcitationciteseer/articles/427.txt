Distributed database systems have become increasingly popular due to their ability to handle large-scale data and provide high availability. However, query optimization remains a significant challenge in these systems. This paper proposes a novel approach to query optimization using machine learning. We develop a framework that leverages query execution history and system metrics to train a machine learning model that predicts optimal query plans. Our approach is designed to work in a distributed setting, where multiple nodes are involved in query execution. We evaluate our approach using a real-world dataset and show that it outperforms traditional query optimization techniques in terms of query execution time and resource utilization.