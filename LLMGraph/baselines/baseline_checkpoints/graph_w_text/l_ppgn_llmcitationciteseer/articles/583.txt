 Distributed database systems are increasingly used in modern data-intensive applications, but optimizing query performance remains a challenging task. This paper proposes a novel approach that leverages machine learning to predict query execution times and optimize query plans. We present a framework that learns from historical query data and adapts to changing workloads, achieving significant improvements in query response times and resource utilization. Experimental results on a real-world dataset demonstrate the effectiveness of our approach in reducing query latency and improving system throughput.