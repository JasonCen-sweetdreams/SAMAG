Deep neural networks are vulnerable to adversarial attacks, which compromise their reliability in safety-critical applications. We propose a novel regularization technique, Input Gradient Regularization (IGR), to enhance the robustness of deep learning models against adversarial perturbations. IGR leverages the input gradient information to penalize the model's sensitivity to perturbations, thereby promoting robust feature learning. We conduct extensive experiments on multiple benchmark datasets and demonstrate that IGR outperforms state-of-the-art adversarial training methods in terms of robustness and accuracy. Our approach provides a promising direction for developing robust deep learning models.