In multi-agent systems, efficient task allocation is crucial for achieving global objectives. This paper proposes a novel reinforcement learning framework, 'MASTAR', which enables agents to learn autonomous task allocation strategies. By modeling the task allocation problem as a decentralized partially observable Markov decision process, MASTAR leverages deep Q-networks to learn optimal policies. We evaluate MASTAR on a simulated disaster response scenario, demonstrating significant improvements in task completion rates and reduced communication overhead compared to traditional centralized allocation methods.