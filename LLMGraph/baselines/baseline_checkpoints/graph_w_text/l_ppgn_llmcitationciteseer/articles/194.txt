Deep neural networks have been shown to be vulnerable to adversarial attacks, which can be crafted by adding imperceptible perturbations to the input data. In this paper, we investigate the robustness of deep neural networks against adversarial attacks using gradient-based methods. We propose a novel approach to analyze the robustness of neural networks by computing the gradient of the loss function with respect to the input data. Our approach provides a more accurate estimate of the robustness of neural networks compared to existing methods, and we demonstrate its effectiveness on several benchmark datasets. Furthermore, we show that our approach can be used to improve the robustness of neural networks by incorporating it into the training process.