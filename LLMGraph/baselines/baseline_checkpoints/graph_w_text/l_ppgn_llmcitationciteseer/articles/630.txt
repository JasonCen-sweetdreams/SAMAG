Knowledge graph embedding (KGE) has emerged as a promising approach for link prediction tasks. However, existing methods often struggle to effectively model complex relationships and hierarchical structures present in real-world graphs. This paper proposes a novel KGE framework, 'HATKE', which leverages hierarchical attention mechanisms to selectively focus on relevant entities and relations. We demonstrate that HATKE outperforms state-of-the-art methods on benchmark datasets, achieving significant improvements in both accuracy and computational efficiency. We further explore the interpretability of HATKE through visualizations of attention weights, providing insights into the learned graph patterns.