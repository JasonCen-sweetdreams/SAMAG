Learning from demonstration (LfD) is a promising approach to teach robots new skills, but it often requires large amounts of annotated data and lacks transparency. This paper presents a novel hierarchical visual attention mechanism, 'HVA-LfD', which enables robots to learn from demonstrations while providing explainable decision-making. Our approach leverages a two-level attention framework, focusing on both the demonstration sequence and the robotic task at hand. We evaluate HVA-LfD on a robot grasping task and show that it outperforms state-of-the-art LfD methods in terms of task success rate and interpretability.