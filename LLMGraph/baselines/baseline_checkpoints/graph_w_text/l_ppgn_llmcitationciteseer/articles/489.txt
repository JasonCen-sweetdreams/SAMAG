Smart home automation systems require users to explicitly configure and customize settings, which can be time-consuming and error-prone. This paper presents a novel multimodal interaction framework that elicits user preferences through a combination of speech, gesture, and gaze inputs. We design and evaluate a prototype system that integrates a conversational agent, computer vision, and machine learning to infer user intentions and adapt the smart home environment accordingly. Our user study demonstrates significant improvements in user satisfaction, efficiency, and accuracy compared to traditional interfaces.