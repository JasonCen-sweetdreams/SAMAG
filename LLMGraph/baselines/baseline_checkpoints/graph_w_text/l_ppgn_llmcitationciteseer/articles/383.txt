Deep learning models have achieved state-of-the-art performance on graph-structured data, but their robustness against adversarial attacks remains a concern. This paper proposes a novel evaluation framework, 'GraphAdv', to assess the robustness of graph neural networks (GNNs) against targeted attacks. We design a set of graph perturbation methods that simulate real-world attacks, such as node manipulation and edge rewiring. Our experiments on benchmark datasets demonstrate that GraphAdv can effectively identify vulnerabilities in popular GNN architectures, providing insights for improving their robustness. We also propose a simple yet effective defense mechanism based on graph attention, which enhances model robustness without sacrificing performance.