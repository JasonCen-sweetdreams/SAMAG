The increasing adoption of AI systems in high-stakes applications has highlighted the need for transparency and interpretability. This paper presents a novel hierarchical attention network (HAN) architecture that facilitates explainable decision-making in multi-modal AI systems. Our HAN model integrates attention mechanisms across different modalities (vision, language, and sensors) to identify relevant features and relationships that contribute to the AI's decision. We evaluate our approach on a real-world autonomous driving dataset and demonstrate significant improvements in model interpretability while maintaining state-of-the-art performance.