Emotion recognition from multimodal data, such as speech, text, and vision, is a crucial task in human-computer interaction. This paper presents a novel hierarchical attention network (HAN) architecture that leverages the strengths of each modality to identify emotions more accurately. Our approach incorporates modality-specific attention mechanisms and a hierarchical fusion strategy to capture both local and global contextual information. We demonstrate the effectiveness of our approach on two benchmark datasets, achieving state-of-the-art performance while providing interpretable insights into the emotional cues used by the model.