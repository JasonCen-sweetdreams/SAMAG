This paper presents a decentralized multi-agent reinforcement learning framework for adaptive traffic signal control. We model the problem as a partially observable Markov decision process, where each agent corresponds to a traffic intersection. Our approach leverages graph convolutional networks to learn the local traffic dynamics and interaction between adjacent intersections. We demonstrate the effectiveness of our method in a simulated urban traffic network, achieving a 25% reduction in average travel time compared to traditional fixed-timing control strategies.