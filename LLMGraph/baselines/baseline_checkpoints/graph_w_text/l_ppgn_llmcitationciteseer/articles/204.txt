Sentiment analysis of multi-modal data (e.g., text, images, and videos) is a challenging task due to the inherent complexity of modeling relationships between different modalities. We propose a novel Hierarchical Graph Attention Network (HGAT) that captures both intra- and inter-modality interactions. HGAT employs a hierarchical graph structure to model the relationships between modalities, and attention mechanisms to selectively focus on relevant information. Experimental results on several benchmark datasets demonstrate that HGAT outperforms state-of-the-art methods in multi-modal sentiment analysis tasks, achieving an average improvement of 4.5% in F1-score.