As conversational AI systems become increasingly prevalent, it is essential to design interfaces that foster trust and understanding. This paper presents a novel approach to adaptive transparency, where the system dynamically adjusts the level of explanation and justification to the user's needs and preferences. We developed a probabilistic model that infers user uncertainty and tailored our transparency mechanisms to provide contextual insights. A user study with 120 participants demonstrated that our approach improves user trust and satisfaction, while reducing feelings of frustration and confusion.