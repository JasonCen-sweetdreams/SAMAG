In multi-agent reinforcement learning, independent exploration can lead to inefficient learning and suboptimal policies. We propose a novel framework, Graph-CE, which enables agents to share information and coordinate exploration through a graph-based communication protocol. By modeling the agents' interactions as a graph, we can identify and exploit structural dependencies between agents, leading to faster convergence and improved policy performance. Experimental results in a variety of multi-agent environments demonstrate the effectiveness of Graph-CE in reducing exploration time and improving overall team performance.