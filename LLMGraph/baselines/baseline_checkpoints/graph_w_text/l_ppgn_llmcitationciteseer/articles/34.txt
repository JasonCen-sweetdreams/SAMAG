Visual question answering (VQA) models have achieved impressive performance, but their lack of transparency hinders their adoption in real-world applications. We propose a novel multi-modal attention fusion framework, 'MMAF', which integrates visual, linguistic, and spatial attention mechanisms to generate interpretable explanations for VQA models. MMAF leverages a hierarchical graph-based architecture to capture complex relationships between visual objects, question keywords, and spatial contexts. Our experiments on the VQA-X dataset demonstrate that MMAF improves both VQA accuracy and explanation quality compared to state-of-the-art models, while providing insights into the decision-making process.