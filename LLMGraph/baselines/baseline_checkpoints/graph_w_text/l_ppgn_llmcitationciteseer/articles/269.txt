Visual question answering (VQA) has made significant progress with the advent of deep learning models. However, existing approaches often lack transparency and interpretability, making it challenging to understand the decision-making process. This paper proposes a novel hierarchical attention network (HAN) architecture that incorporates both spatial and semantic attention mechanisms to generate explanations for VQA tasks. Our HAN model attends to relevant regions of the image and identifies key objects, attributes, and relationships, providing a step-by-step explanation for the predicted answer. Experimental results on the VQA-CP v2 dataset demonstrate that our approach outperforms state-of-the-art models in terms of accuracy and explainability.