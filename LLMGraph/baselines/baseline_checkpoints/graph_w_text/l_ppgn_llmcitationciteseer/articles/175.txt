Emotion recognition is a crucial aspect of human-computer interaction, but existing approaches often rely on single-modal inputs (e.g., facial expressions or speech). This paper proposes a novel hierarchical attention network (HAN) that integrates multiple modalities (video, audio, and text) to recognize emotions in a more comprehensive and robust manner. Our HAN model learns to selectively focus on relevant modalities and temporal segments, achieving state-of-the-art performance on three benchmark datasets. We also provide insights into the learned attention patterns, demonstrating the model's ability to capture subtle emotional cues.