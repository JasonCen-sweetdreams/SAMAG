Virtual reality (VR) has the potential to revolutionize human-computer interaction, but current VR systems often neglect the importance of embodied cognition in user experience. This paper presents a study on gesture-based interaction design, exploring how the incorporation of embodied cognition principles can enhance user engagement and performance in VR environments. We designed and evaluated a novel gesture-based interface for a VR puzzle game, leveraging cognitive architectures and machine learning algorithms to recognize and respond to users' gestures. Results show significant improvements in user satisfaction, task completion time, and cognitive load compared to traditional controller-based interfaces.