Emotion recognition in human-computer interaction (HCI) is a crucial task for developing empathetic systems. However, existing approaches often rely on single-modal inputs (e.g., facial expressions or speech) and struggle to capture the complexity of human emotions. We propose a hierarchical attention network (HAN) that integrates multimodal inputs from facial expressions, speech, and physiological signals. Our HAN architecture employs a novel attention mechanism that adaptively weights the importance of each modality based on the input context. Experiments on a large-scale multimodal dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions and achieving more accurate affective state tracking.