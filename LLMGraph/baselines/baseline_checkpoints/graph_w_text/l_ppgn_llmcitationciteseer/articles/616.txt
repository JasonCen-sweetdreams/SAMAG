Neural architecture search (NAS) has emerged as a promising approach for automating the design of deep neural networks. However, the exponential growth of the search space with increasing network complexity hinders the efficiency of NAS methods. This paper introduces GraphATT, a novel NAS framework that leverages graph-based attention mechanisms to prune the search space and focus on promising architectures. We demonstrate the effectiveness of GraphATT on several benchmark datasets, achieving state-of-the-art results while reducing the search time by an order of magnitude compared to existing methods.