Multi-turn dialogue systems have become increasingly prevalent in AI applications, but their opaqueness hinders trust and understanding. This paper proposes a novel hierarchical attention network (HAN) architecture for explainable dialogue systems. Our HAN framework consists of two levels of attention: utterance-level and token-level. The utterance-level attention weights capture the contextual relevance of each utterance, while the token-level attention weights highlight the important words and phrases contributing to the response generation. We evaluate our approach on three benchmark datasets and demonstrate significant improvements in both response quality and explainability.