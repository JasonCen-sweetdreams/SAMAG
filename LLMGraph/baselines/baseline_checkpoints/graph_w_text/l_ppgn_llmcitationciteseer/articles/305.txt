Neural retrieval models have shown promising results in information retrieval tasks, but they often suffer from the query mismatch problem. Query expansion techniques can alleviate this issue, but existing methods can be computationally expensive or require extensive domain knowledge. This paper proposes a novel query expansion approach, 'SparseQE', which leverages sparse attention mechanisms to selectively expand relevant terms. We show that SparseQE improves the retrieval performance of state-of-the-art neural models on several benchmark datasets while reducing the computational overhead. Furthermore, we provide an in-depth analysis of the attention patterns, revealing insights into the underlying retrieval mechanisms.