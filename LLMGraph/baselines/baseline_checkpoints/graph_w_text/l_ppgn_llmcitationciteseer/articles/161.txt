Traditional document representation methods in information retrieval (IR) often struggle to capture complex semantic relationships between entities and contexts. This paper presents a novel hierarchical attention framework, 'HAT-IR', which learns to represent documents as a hierarchy of attention weights. Our approach leverages a multi-layer transformer encoder to model intra-sentence and inter-sentence relationships, enabling more accurate relevance scoring. Experimental results on several benchmark IR datasets demonstrate that HAT-IR outperforms state-of-the-art models in terms of retrieval accuracy and efficiency.