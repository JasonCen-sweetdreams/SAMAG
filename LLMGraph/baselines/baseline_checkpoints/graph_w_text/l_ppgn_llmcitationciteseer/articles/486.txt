Emotion recognition in social media has become increasingly important for sentiment analysis and mental health monitoring. However, existing approaches struggle to effectively fuse and weigh multi-modal inputs (e.g., text, images, audio) from diverse platforms. This paper proposes a novel hierarchical attention network (HAN) architecture that adaptively learns to focus on relevant modalities and features for efficient emotion detection. Experimental results on a large-scale, multi-modal social media dataset demonstrate that HAN outperforms state-of-the-art methods, achieving improved accuracy and robustness in the presence of noisy or missing data.