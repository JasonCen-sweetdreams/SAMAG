Multimodal dialogue systems have gained popularity in various applications, but their lack of transparency hinders trust and adoption. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates visual, acoustic, and linguistic features to generate interpretable responses. Our approach leverages attention weights to highlight relevant input features, enabling the identification of key factors influencing the system's decisions. Experimental results on a benchmark multimodal dialogue dataset demonstrate improved response accuracy and human-perceived explainability compared to state-of-the-art models.