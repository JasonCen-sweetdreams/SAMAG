Deep reinforcement learning (DRL) models have achieved impressive results in complex decision-making tasks, but their opacity hinders trust and interpretability. We propose a novel hybrid explainability approach, 'SymExplain', which combines the strengths of symbolic model extraction and attention-based attribution methods. SymExplain extracts a symbolic representation of the DRL policy, enabling the identification of causal relationships between state features and actions. Our experiments on Atari games and a real-world robotics task demonstrate that SymExplain outperforms existing explainability methods in terms of accuracy and fidelity, providing actionable insights for DRL model improvement.