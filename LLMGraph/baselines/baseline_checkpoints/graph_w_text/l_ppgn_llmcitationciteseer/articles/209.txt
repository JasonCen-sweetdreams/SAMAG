Emotion recognition from multi-modal inputs, such as speech, text, and vision, is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a hierarchical attention neural network (HANN) that learns to selectively focus on relevant modalities and features at multiple levels of abstraction. Our approach leverages a novel hierarchical attention mechanism that adaptively weights modalities and features based on their importance for emotion recognition. Experimental results on the CMU-Multimodal SDK dataset demonstrate that HANN outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an accuracy of 85.2%.