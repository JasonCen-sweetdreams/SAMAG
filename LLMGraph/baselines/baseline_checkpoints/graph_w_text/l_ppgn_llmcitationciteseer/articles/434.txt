This paper proposes a novel hierarchical attention network (HAN) for multimodal sentiment analysis, which integrates visual and textual features to better capture nuanced sentiment expressions. Our HAN model consists of two stages: a feature-level attention mechanism that selectively weighs modalities, and a sentence-level attention mechanism that focuses on salient regions. Experimental results on three benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in both sentiment classification and aspect-based sentiment analysis tasks.