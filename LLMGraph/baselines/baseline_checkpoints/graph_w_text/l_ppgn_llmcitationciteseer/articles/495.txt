Touchscreen devices often pose significant accessibility challenges for visually impaired individuals. This paper presents a novel approach to enhancing touchscreen interaction accessibility through the integration of tactile feedback and machine learning-based gesture recognition. We designed and evaluated a wearable tactile feedback system that provides haptic cues to guide users' fingers during gesture-based interactions. Our machine learning model, trained on a dataset of gesture samples from visually impaired users, achieves an accuracy of 92.5% in recognizing gestures. User studies demonstrate significant improvements in interaction accuracy and user satisfaction compared to traditional touchscreen interfaces.