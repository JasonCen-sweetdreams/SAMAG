Emotional intelligence (EI) is a crucial aspect of human-AI interaction, but current models struggle to capture the complexity of emotional cues across different modalities (e.g., speech, text, vision). This paper presents a novel Hierarchical Attention Network (HAN) architecture that integrates multi-modal inputs to infer emotional states. Our approach utilizes a hierarchical attention mechanism to selectively focus on relevant modalities and temporal segments, enabling the model to capture subtle emotional patterns. Experimental results on a large-scale, multi-modal EI dataset demonstrate the effectiveness of HAN in improving emotional recognition accuracy and robustness.