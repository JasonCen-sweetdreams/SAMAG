Multi-label classification is a fundamental problem in machine learning, where a single instance can be associated with multiple labels. However, annotating large datasets with multiple labels can be time-consuming and expensive. This paper proposes a novel hierarchical attention network (HAN) that selectively focuses on relevant labels and instances, reducing the annotation burden. Our approach leverages label correlations and instance relationships to learn a hierarchical representation, enabling efficient learning from limited annotations. Experimental results on benchmark datasets demonstrate that HAN outperforms state-of-the-art methods in terms of both accuracy and annotation efficiency.