Multi-task learning (MTL) has become a cornerstone of computer vision, enabling models to jointly learn multiple tasks and improve overall performance. However, existing MTL approaches suffer from scalability issues, particularly when dealing with large-scale datasets and numerous tasks. This paper proposes a novel hierarchical attention network (HAN) architecture that efficiently learns task relationships and selectively attends to relevant features for each task. Our approach achieves state-of-the-art performance on several benchmark datasets, including Cityscapes and NYUv2, while requiring significantly fewer computational resources. We also provide theoretical insights into the learnability of HANs and their advantages over flat attention mechanisms.