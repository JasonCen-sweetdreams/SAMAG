Emotion recognition from multi-modal data (e.g., speech, text, vision) is a challenging task due to the heterogeneity of modalities and the scarcity of labeled data. We propose a novel hierarchical contrastive learning framework, which leverages the shared and modality-specific representations to improve emotion recognition. Our approach consists of two stages: (1) modality-specific contrastive learning to capture intra-modality patterns, and (2) hierarchical contrastive learning to align and fuse representations across modalities. Experiments on three benchmark datasets demonstrate that our approach outperforms state-of-the-art methods, achieving an average improvement of 12.5% in emotion recognition accuracy.