Virtual reality (VR) has the potential to revolutionize various aspects of life, but current VR systems often exclude individuals with disabilities. This paper focuses on developing a gesture recognition system that enables users with motor impairments to interact with VR environments more effectively. We propose a novel deep learning architecture that leverages transfer learning and multi-modal fusion to recognize gestures from electromyography (EMG) and computer vision inputs. Our user study involving 20 participants with motor impairments demonstrates that our system achieves an average recognition accuracy of 92.5%, outperforming existing state-of-the-art approaches. Our work has significant implications for promoting inclusivity in VR and enhancing the overall user experience.