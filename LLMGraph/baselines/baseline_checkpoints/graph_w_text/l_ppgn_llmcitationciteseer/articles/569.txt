Deep neural networks (DNNs) have been shown to be vulnerable to adversarial attacks, which can significantly degrade their performance. Evaluating the robustness of DNNs against such attacks is critical to ensuring their reliability in safety-critical applications. This paper proposes a novel Bayesian approximation approach to efficiently evaluate the robustness of DNNs against adversarial attacks. We derive a closed-form expression for the Bayesian neural network's predictive uncertainty, which allows us to quantify the robustness of the model. Experimental results on multiple benchmark datasets demonstrate that our approach outperforms existing methods in terms of robustness evaluation accuracy while reducing computational overhead.