Multimodal sentiment analysis is a challenging task that requires processing and integrating information from diverse sources such as text, images, and audio. This paper proposes a novel zero-shot learning approach that leverages graph attention networks to capture complex relationships between modalities. Our method, called GraphZero, learns to attend to relevant modalities and generate sentiment scores without requiring labeled data for new classes. Experimental results on three benchmark datasets demonstrate the superiority of GraphZero over existing state-of-the-art methods, achieving an average improvement of 12.5% in sentiment accuracy.