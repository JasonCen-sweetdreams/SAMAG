Deep neural networks require careful hyperparameter tuning to achieve optimal performance. However, this process is often manual, time-consuming, and dataset-dependent. This paper introduces a novel meta-learning approach, 'HyperMeta', which learns to adapt hyperparameters across diverse datasets and models. By leveraging a meta-learner to generate hyperparameter suggestions, we demonstrate improved performance and reduced tuning time on various benchmark datasets. Our approach is particularly effective for transfer learning and few-shot learning scenarios, where hyperparameter tuning is crucial but challenging.