Explainability in AI decision-making is crucial for building trust in autonomous systems. Knowledge graph embedding (KGE) has shown promise in modeling complex relationships between entities. However, existing KGE methods suffer from scalability issues and lack interpretability. This paper proposes 'ExplainKG', a novel KGE framework that leverages graph attention mechanisms and hierarchical clustering to encode knowledge graphs efficiently. We evaluate ExplainKG on a benchmark dataset and demonstrate its ability to provide interpretable explanations for AI-driven decision-making while maintaining predictive performance.