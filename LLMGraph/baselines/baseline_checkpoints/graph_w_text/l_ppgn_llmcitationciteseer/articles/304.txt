Wearable sensors have shown significant potential in recognizing emotional states from physiological signals. However, existing approaches often rely on hand-crafted features and shallow learning models, limiting their accuracy and generalizability. This paper proposes a multimodal deep learning framework, 'EmoSense', which integrates data from electrodermal activity, heart rate, and acceleration sensors to recognize emotional states. Our approach leverages a novel attention-based fusion mechanism to selectively weight sensor modalities and capture complex patterns. Experimental results on a large-scale dataset demonstrate EmoSense outperforms state-of-the-art methods by 12.5% in F1-score, achieving an accuracy of 92.1% in recognizing four emotional states (happiness, sadness, fear, and anger).