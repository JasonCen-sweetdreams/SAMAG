Ad-hoc retrieval remains a challenging task in information retrieval, particularly when dealing with short queries and scarce relevance feedback. This paper proposes a novel query expansion approach that leverages supervised neural document embeddings to capture semantic relationships between query terms and relevant documents. Our model, dubbed 'QNDE', is trained on a large dataset of annotated query-document pairs and learns to project queries and documents into a shared latent space. Experimental results on several TREC benchmarks demonstrate significant improvements in retrieval performance compared to traditional pseudo-relevance feedback and word embedding-based methods.