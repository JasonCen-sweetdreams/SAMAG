Augmented reality (AR) systems often rely on gesture recognition to enable intuitive user interactions. However, existing approaches can be limited by their reliance on predefined gesture sets and lack of adaptability to individual users' needs. This paper presents 'GESTAR', a novel framework that leverages machine learning and computer vision to recognize and adapt to users' unique gestures in real-time. We evaluate GESTAR using a comprehensive dataset of gestures from diverse users, demonstrating significant improvements in recognition accuracy and user satisfaction compared to traditional approaches.