Autonomous vehicles (AVs) are poised to revolutionize urban transportation, but their safe and efficient operation requires sophisticated coordination mechanisms. This paper proposes a decentralized multi-agent reinforcement learning framework, 'MAV-Coord', for AV coordination in complex urban scenarios. MAV-Coord leverages graph neural networks to model inter-agent interactions and employs a novel, asynchronous Q-learning algorithm that adapts to dynamic environment changes. Experimental results on a large-scale, real-world traffic dataset demonstrate that MAV-Coord outperforms state-of-the-art, centralized approaches in terms of traffic flow efficiency and safety.