Multimodal sentiment analysis (MSA) aims to predict sentiment from heterogeneous data sources, including text, images, and audio. Existing approaches often suffer from inadequate modeling of complex relationships between modalities. We propose a Hierarchical Graph Attention Network (HGAT) that effectively captures both intra- and inter-modality interactions. HGAT employs a novel attention mechanism that adaptively weights modality-specific features and iteratively refines sentiment representations at multiple scales. Experiments on benchmark MSA datasets demonstrate significant performance improvements over state-of-the-art methods, highlighting the effectiveness of HGAT in modeling multimodal sentiment.