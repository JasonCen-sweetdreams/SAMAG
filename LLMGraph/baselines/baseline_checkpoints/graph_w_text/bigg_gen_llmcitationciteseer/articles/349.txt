Deep reinforcement learning (DRL) has achieved remarkable success in complex decision-making tasks, but its lack of interpretability hinders trust and adoption. This paper presents a novel, model-agnostic explainability framework for DRL, dubbed 'DeepShapley'. By integrating Shapley values with feature importance metrics, DeepShapley quantifies the contribution of individual state features to the agent's policy decisions. We demonstrate the effectiveness of DeepShapley on several Atari games and a real-world robotics task, showcasing its ability to provide meaningful explanations without sacrificing performance.