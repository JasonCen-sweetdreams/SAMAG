Recognizing emotions from multi-modal inputs, such as speech, text, and facial expressions, is a challenging task. We propose a novel Hierarchical Attention Network (HAN) that leverages the strengths of each modality to improve emotion recognition accuracy. HAN consists of three levels of attention: intra-modal, inter-modal, and fusion-level attention. Experimental results on the IEMOCAP dataset demonstrate that HAN outperforms state-of-the-art methods, achieving an F1-score of 0.83 for emotion classification. We also analyze the attention weights to gain insights into the importance of each modality for emotion recognition.