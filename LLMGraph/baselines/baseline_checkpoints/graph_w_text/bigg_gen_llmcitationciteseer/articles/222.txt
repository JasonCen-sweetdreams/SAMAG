Deep neural networks (DNNs) are increasingly vulnerable to sophisticated attacks that exploit unseen vulnerabilities. This paper proposes an adversarial training framework, 'AdvShield', which leverages a novel ensemble-based approach to improve robustness against diverse attack vectors. Our method combines a generator network with a discriminator network to craft perturbations that simulate real-world attacks, thereby enhancing model resilience. Experimental results on benchmark datasets demonstrate that AdvShield outperforms state-of-the-art defense methods in detecting and mitigating unseen attacks, ensuring improved model reliability in real-world applications.