This paper presents a novel approach to designing gesture-based interfaces for older adults with cognitive impairments. Our system, 'EasyGest', uses machine learning to adapt to individual users' abilities and preferences, providing real-time feedback and guidance to facilitate accurate gesture recognition. A user study with 20 older adults demonstrates significant improvements in gesture recognition accuracy and user satisfaction compared to traditional gesture-based interfaces. We also explore the impact of cognitive load on gesture performance and discuss implications for future design. The results have important implications for the development of accessible and inclusive interfaces for older adults.