Emotion recognition from multimodal data, such as speech, text, and vision, is a crucial AI capability. However, existing approaches often lack transparency and interpretability. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that enables explainable multimodal emotion recognition. HAN uses a modular design to learn hierarchical representations of emotions, allowing for the identification of salient features and modalities contributing to the recognition process. Experimental results on the IEMOCAP dataset demonstrate improved recognition accuracy and robustness, as well as meaningful attention patterns that provide insights into the emotional cues.