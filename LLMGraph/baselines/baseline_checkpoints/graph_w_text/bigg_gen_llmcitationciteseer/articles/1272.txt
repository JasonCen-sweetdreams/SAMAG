Few-shot learning has gained significant attention in recent years, but existing methods often struggle to generalize to new classes with limited training data. We propose a novel hierarchical attention network (HAN) that leverages both local and global feature representations to improve few-shot learning performance. Our approach adaptively selects the most relevant features and weights them according to their importance, resulting in improved classification accuracy and reduced computational overhead. Experiments on several benchmark datasets demonstrate the effectiveness of HAN in both inductive and transductive few-shot learning settings.