This paper presents a novel approach to coordinating multi-agent systems for adaptive resource allocation in complex, dynamic environments. We propose a hierarchical reinforcement learning framework that leverages a decentralized, actor-critic architecture to learn optimal resource allocation policies. Our approach enables agents to adapt to changing environmental conditions and learn from experience, while a higher-level coordinator optimizes overall system performance. Experimental results in a simulated smart grid scenario demonstrate improved resource allocation efficiency and resilience to disturbances compared to traditional, centralized optimization methods.