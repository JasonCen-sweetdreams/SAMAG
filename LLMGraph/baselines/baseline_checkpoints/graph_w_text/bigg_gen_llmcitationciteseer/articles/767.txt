Deep reinforcement learning (DRL) agents have achieved state-of-the-art performance in various tasks, but their vulnerability to adversarial attacks raises concerns about their reliability. This paper investigates the adversarial robustness of DRL policies in multi-agent scenarios, where agents interact with each other and the environment. We propose a novel framework, 'RoboGame', which utilizes a min-max formulation to train agents that are resilient to adversarial attacks. Our approach leverages the competitive nature of multi-agent games to encourage agents to develop robust policies. Experimental results on several Atari games demonstrate that RoboGame improves the adversarial robustness of DRL policies, while maintaining their performance in nominal environments.