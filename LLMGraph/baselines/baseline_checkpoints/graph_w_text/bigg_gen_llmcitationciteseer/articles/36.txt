In multi-agent reinforcement learning, coordinated exploration is crucial to achieve efficient learning and optimal policy convergence. However, in partially observable environments, agents must balance individual exploration with coordination to overcome the curse of dimensionality. This paper proposes a novel algorithm, CEPO (Coordinated Exploration with Partial Observability), which leverages graph-based communication structures to facilitate information sharing among agents. CEPO adaptively adjusts exploration rates based on the observed environment dynamics, enabling agents to focus on informative regions while mitigating the effects of partial observability. Experimental results on a range of benchmark problems demonstrate CEPO's improved learning efficiency and policy quality compared to state-of-the-art decentralized exploration methods.