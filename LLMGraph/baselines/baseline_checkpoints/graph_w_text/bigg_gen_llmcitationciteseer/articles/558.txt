Virtual reality (VR) has become increasingly prevalent in various domains, including education, entertainment, and therapy. However, current gesture recognition systems often struggle to accurately recognize user intentions, leading to frustration and decreased user experience. This paper explores the role of embodied cognition in VR gesture recognition, examining how users' physical movements influence their mental representations of actions. We present a novel VR-based experiment that leverages machine learning and computer vision techniques to analyze user gestures and corresponding cognitive processes. Our results show that incorporating embodied cognition principles into gesture recognition models leads to improved accuracy and reduced latency, with implications for enhancing overall VR usability.