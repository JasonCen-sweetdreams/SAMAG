Graph neural networks (GNNs) have achieved state-of-the-art performance in various graph-structured data tasks. However, their robustness to adversarial attacks remains largely unexplored. This paper presents a comprehensive evaluation of the vulnerability of GNNs to different types of attacks, including node-level, edge-level, and graph-level perturbations. Our experiments demonstrate that GNNs are susceptible to these attacks, leading to significant performance degradation. We also investigate the effectiveness of various defense strategies, including input preprocessing, graph denoising, and adversarial training. Our results highlight the need for developing more robust GNN models and provide insights for designing effective defense mechanisms.