Multi-hop question answering (MHQA) has gained significant attention due to its ability to reason over multiple sentences. However, existing models often lack interpretability, making it challenging to understand their decision-making process. This paper proposes a novel Hierarchical Attention Network (HAN) that incorporates explainability into MHQA. Our model employs a hierarchical attention mechanism that selectively focuses on relevant sentences, entities, and relationships, providing a transparent understanding of the reasoning process. Experimental results on the HotPotQA dataset demonstrate that our approach achieves state-of-the-art performance while offering improved interpretability.