Voice assistants have become ubiquitous, but their design often neglects the needs of users with disabilities. This paper presents a multimodal interaction framework that enables users with disabilities to interact with voice assistants more effectively. We propose a novel fusion of speech, gesture, and gaze-based input modalities, which are processed using machine learning algorithms to improve recognition accuracy. Our user study with 30 participants with disabilities shows that our approach significantly reduces error rates and improves user satisfaction compared to traditional voice-only interfaces.