Traditional ranking models in information retrieval (IR) rely on simplistic term-frequency representations, neglecting the rich semantic structures present in documents. This paper proposes a novel hierarchical attention network (HAN) architecture that learns to selectively focus on relevant document segments and terms. Our approach incorporates a document-aware attention mechanism that captures long-range dependencies and contextual relationships between tokens. Experimental results on the TREC-8 benchmark demonstrate significant improvements in mean average precision (MAP) and normalized discounted cumulative gain (NDCG) over state-of-the-art neural ranking models.