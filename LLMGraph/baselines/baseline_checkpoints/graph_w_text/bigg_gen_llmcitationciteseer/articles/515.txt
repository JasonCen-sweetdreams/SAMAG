Multimodal sentiment analysis in social media has become increasingly important for businesses and organizations to understand public opinions. This paper proposes a novel hierarchical attention network (HAN) that integrates visual and textual features from social media posts to improve sentiment analysis accuracy. Our approach employs a multimodal fusion module to combine convolutional neural network (CNN) and recurrent neural network (RNN) features, followed by a hierarchical attention mechanism that selectively focuses on relevant input features. Experimental results on a large-scale dataset of social media posts demonstrate that our HAN model outperforms state-of-the-art multimodal sentiment analysis methods and provides interpretable insights into the importance of different input features.