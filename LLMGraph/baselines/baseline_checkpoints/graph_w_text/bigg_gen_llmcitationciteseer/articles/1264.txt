Decentralized control of autonomous agents in real-world scenarios often involves partial observability and stochasticity. This paper presents a novel approach to coordinating agents using decentralized reinforcement learning (DRL) in partially observable Markov decision processes (POMDPs). We propose a communication-efficient algorithm, 'DRL-COM', that leverages local observations and intermittent communication to learn effective policies for multi-agent tasks. Experimental results in a simulated search-and-rescue scenario demonstrate the superior performance of DRL-COM compared to centralized and independent-learning baselines, showcasing its potential for real-world applications.