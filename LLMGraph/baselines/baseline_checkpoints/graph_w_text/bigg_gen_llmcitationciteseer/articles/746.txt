Deep neural networks often require significant computational resources, limiting their deployment on resource-constrained devices. This paper presents an adaptive gradient sparsification technique, 'AdaSparse', which dynamically adjusts the sparsity of gradients during backpropagation to reduce computation and memory requirements. Our approach leverages a novel, differentiable sparsity metric that adaptively prunes gradients based on their importance. Experimental results on several benchmark datasets demonstrate that AdaSparse achieves comparable model accuracy to dense gradient updates while reducing compute time by up to 45% on mobile devices.