Deep reinforcement learning (DRL) has achieved impressive results in various domains, but its vulnerability to adversarial attacks raises concerns about its reliability. This paper presents a novel approach to improve the robustness of DRL agents against adversarial perturbations. We leverage meta-learning to learn a robust policy that adapts to different attack scenarios. Our method, called 'Meta-Adv', uses a meta-controller to adjust the exploration-exploitation trade-off during training, leading to improved robustness against unseen attacks. Experimental results on popular benchmarks demonstrate the effectiveness of Meta-Adv in enhancing the robustness of DRL agents.