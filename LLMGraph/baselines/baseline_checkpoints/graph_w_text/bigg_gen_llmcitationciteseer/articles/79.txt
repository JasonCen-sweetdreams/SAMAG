Deep neural ranking models have achieved state-of-the-art performance in ad-hoc retrieval tasks, but they often struggle with limited query context. This paper proposes a novel query expansion approach, 'NeuralQE', that leverages the semantic representation capabilities of pre-trained language models. NeuralQE adaptively generates context-aware query expansions by bridging the gap between the original query and relevant documents in the collection. Our experiments on the TREC-8 and Robust04 datasets demonstrate significant improvements in retrieval effectiveness, particularly for queries with limited context, and outperform existing query expansion methods.