Deep reinforcement learning (DRL) has achieved impressive results in various domains, but its brittleness to adversarial attacks hinders its deployment in real-world applications. This paper proposes a novel adversarial training framework, 'AdvRL', which leverages a zero-sum game formulation to learn robust policies. We introduce a dual-optimizer approach that jointly updates the policy and adversary networks, promoting exploration-exploitation trade-offs. Empirical evaluation on popular DRL benchmarks demonstrates that AdvRL significantly enhances the robustness of DRL agents against both white-box and black-box attacks, while maintaining competitive performance.