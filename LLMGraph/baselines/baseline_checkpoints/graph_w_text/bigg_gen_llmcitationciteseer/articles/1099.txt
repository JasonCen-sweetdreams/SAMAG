Multi-agent reinforcement learning has seen significant progress in recent years, but scalability remains a major challenge. This paper presents a novel hierarchical attention network (HAN) architecture that enables efficient learning in large-scale multi-agent environments. By selectively focusing on relevant agents and their interactions, HAN reduces the complexity of the learning problem and achieves state-of-the-art performance in several benchmark domains. We demonstrate the effectiveness of HAN in scenarios with varying degrees of agent heterogeneity and dynamic behavior.