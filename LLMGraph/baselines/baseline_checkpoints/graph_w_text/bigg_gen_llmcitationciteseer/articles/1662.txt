Sentiment analysis from multimodal data, such as text, images, and videos, is a challenging task due to the complexity of capturing cross-modal interactions. This paper presents a novel Hierarchical Attention Network (HAN) that leverages self-attention mechanisms to model intra-modal and inter-modal relationships. Our approach first learns modality-specific representations using separate attention modules, and then combines them using a hierarchical fusion strategy. Experimental results on three benchmark datasets demonstrate that our HAN model outperforms state-of-the-art multimodal sentiment analysis methods, achieving an average improvement of 4.2% in terms of F1-score.