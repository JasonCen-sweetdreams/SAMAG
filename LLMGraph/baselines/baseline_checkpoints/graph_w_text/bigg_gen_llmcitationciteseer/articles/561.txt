Emotion recognition from multi-modal data, such as speech, text, and facial expressions, is a challenging task due to the heterogeneity and complexity of modalities. This paper proposes a novel hybrid attentional graph neural network (HAGNN) that integrates graph attention and multi-head self-attention mechanisms to effectively capture inter-modal relationships and intra-modal patterns. Experimental results on the CMU-MOSEI dataset demonstrate that HAGNN outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs, achieving an average F1-score of 0.83 across six emotions.