Autonomous navigation in dynamic environments remains a challenging problem in AI research. This paper presents a novel hierarchical reinforcement learning framework, 'HRL-NAV', which learns to navigate complex scenarios by decomposing the task into hierarchical sub-goals. We introduce a new curiosity-driven exploration strategy that adaptively adjusts the exploration-exploitation trade-off based on the environment's dynamics. Our experiments demonstrate that HRL-NAV outperforms state-of-the-art methods in various scenarios, including pedestrian-rich urban environments and warehouses with moving obstacles.