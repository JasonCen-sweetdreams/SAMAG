Autonomous vehicles rely on machine learning models to make critical decisions, but these models are vulnerable to adversarial attacks. This paper presents a novel approach to detect such attacks using explainable reinforcement learning. Our method, 'Explain2Detect', integrates a reinforcement learning framework with model interpretability techniques to identify suspicious input patterns. We evaluate Explain2Detect on a simulated autonomous driving environment and demonstrate its effectiveness in detecting targeted attacks while maintaining a low false positive rate. Our approach provides a promising direction for enhancing the security and reliability of autonomous vehicles.