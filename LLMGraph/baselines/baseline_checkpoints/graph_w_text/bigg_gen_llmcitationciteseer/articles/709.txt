As robots increasingly interact with humans in various settings, understanding human emotions is crucial for effective communication. This paper proposes a novel multimodal fusion approach for emotional sentiment analysis in human-robot interaction. We leverage facial expression, speech, and physiological signals to recognize emotional states and improve the robot's empathetic response. Our experimental results show that the proposed approach outperforms unimodal methods, achieving an accuracy of 93.5% in recognizing six basic emotions. We also demonstrate the effectiveness of our approach in a real-world scenario, where a robot uses emotional intelligence to provide comfort to users in a healthcare setting.