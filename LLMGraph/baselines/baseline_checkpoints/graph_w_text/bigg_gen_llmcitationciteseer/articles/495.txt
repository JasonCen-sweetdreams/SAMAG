Neural architecture search (NAS) has revolutionized the design of deep neural networks, but its computational cost remains a major bottleneck. This paper presents a novel hierarchical Bayesian optimization (HBO) framework for efficient NAS. By modeling the search space as a hierarchical graph, our approach reduces the number of evaluations required to find high-performing architectures. We demonstrate the effectiveness of HBO-NAS on several benchmark datasets, achieving state-of-the-art results while reducing the search time by up to 75% compared to existing methods.