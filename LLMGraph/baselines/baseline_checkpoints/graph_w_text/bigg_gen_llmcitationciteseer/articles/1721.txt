Virtual assistants (VAs) have become ubiquitous in modern computing, but their lack of emotional intelligence can lead to user frustration. This paper presents a novel framework for recognizing and responding to user frustration in VAs, using a multimodal approach that combines speech, facial expression, and physiological signal analysis. Our system, 'EmpathyVA', employs machine learning models to detect early signs of frustration and adapt the VA's response to mitigate user annoyance. A user study involving 30 participants demonstrates that EmpathyVA significantly reduces user frustration and improves overall interaction experience.