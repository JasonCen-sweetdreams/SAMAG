Deep reinforcement learning (DRL) has achieved remarkable success in various applications, but its vulnerability to adversarial attacks has raised significant concerns. This paper presents a comprehensive robustness analysis of DRL algorithms under different types of adversarial attacks, including observation and action perturbations. We propose a novel attack detection mechanism based on uncertainty estimation and demonstrate its effectiveness in identifying attacks. Our experiments on several benchmarks show that the proposed approach significantly improves the robustness of DRL algorithms, paving the way for their deployment in safety-critical applications.