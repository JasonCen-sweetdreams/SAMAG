Deep neural networks are increasingly used for online learning tasks, where the model must adapt to streaming data. However, this setting poses significant challenges, including concept drift, limited labeled data, and computational constraints. We propose an adaptive regularization framework, 'AdaReg', that dynamically adjusts the regularization strength for each layer based on the observed data distribution. Our approach leverages a novel online estimation of the Hessian matrix, enabling efficient computation of the regularization term. Experimental results on several benchmark datasets demonstrate that AdaReg outperforms state-of-the-art online learning methods in terms of accuracy and adaptability to concept drift.