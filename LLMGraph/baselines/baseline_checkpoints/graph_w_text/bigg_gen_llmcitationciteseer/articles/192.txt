Few-shot learning has seen significant advances in recent years, but most methods rely on single-modal data and are limited by their inability to effectively leverage multimodal information. We propose a novel Hierarchical Graph Attention Network (HiGAT) framework that learns to integrate information from multiple modalities and adapt to new tasks with limited labeled data. HiGAT consists of a hierarchical graph attention mechanism that selectively focuses on relevant modalities and a meta-learning strategy that enables fast adaptation to new tasks. Experimental results on several multimodal benchmarks demonstrate the effectiveness of HiGAT in few-shot learning scenarios, outperforming state-of-the-art methods by a significant margin.