This paper presents a novel approach to designing accessible gestural interfaces for individuals with motor disabilities. We propose a machine learning-based framework that learns to recognize and adapt to the unique gesture patterns of users with mobility impairments. Our approach leverages a combination of computer vision and human-computer interaction techniques to enable users to interact with digital systems using intuitive and personalized gestures. We evaluate our approach through a user study with participants with motor disabilities, demonstrating improved accuracy and usability compared to traditional gestural interfaces.