Virtual assistants have become ubiquitous in modern computing, but their effectiveness relies on accurately recognizing user intentions. This paper introduces 'GazeIntend', a novel HCI framework that leverages gaze tracking to infer user goals in virtual environments. Our approach combines convolutional neural networks with probabilistic graphical models to recognize gaze patterns and contextualize them within a task-based ontology. We evaluate GazeIntend on a dataset of 30 participants interacting with a virtual smart home interface, demonstrating a 25% reduction in misrecognized intentions compared to state-of-the-art speech-based approaches.