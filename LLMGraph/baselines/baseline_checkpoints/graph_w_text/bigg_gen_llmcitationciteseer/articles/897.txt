Traditional gesture recognition systems rely on machine learning algorithms that focus on hand or body pose features. However, embodied cognition theory suggests that gestures are deeply rooted in cognitive processes, such as attention and intention. This paper presents a novel gesture recognition framework that incorporates cognitive models to better understand user intentions. Our approach leverages multimodal sensor data, including EEG, eye-tracking, and kinematic data, to recognize gestures in a more human-centered way. Experimental results show that our embodied cognition-inspired system achieves higher accuracy and robustness in recognizing gestures compared to state-of-the-art machine learning-based approaches.