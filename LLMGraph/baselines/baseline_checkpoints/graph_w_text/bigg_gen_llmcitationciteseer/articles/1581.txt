Multimodal dialogue systems have shown great promise in human-computer interaction, but their lack of transparency and explainability hinders their adoption in real-world applications. This paper proposes a novel hierarchical attention network (HAN) architecture that integrates visual, acoustic, and linguistic features to generate interpretable responses. Our HAN model comprises two stages: 1) a multimodal feature fusion layer that learns to weigh the importance of each modality, and 2) a hierarchical attention mechanism that focuses on relevant input segments and generates attention visualizations. Experimental results on a large-scale multimodal dialogue dataset demonstrate that our approach outperforms state-of-the-art models in terms of response accuracy and explanation quality.