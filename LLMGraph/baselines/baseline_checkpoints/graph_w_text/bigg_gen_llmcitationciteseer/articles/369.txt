Multimodal sentiment analysis (MSA) has garnered significant attention in recent years, but existing approaches often lack transparency in their decision-making processes. This paper proposes a novel Hierarchical Attention Network (HAN) for MSA, which incorporates both intra-modal and inter-modal attention mechanisms to selectively focus on salient features from textual, visual, and acoustic inputs. We demonstrate the effectiveness of our approach on three benchmark datasets, achieving state-of-the-art results while providing interpretable insights into the sentiment analysis process.