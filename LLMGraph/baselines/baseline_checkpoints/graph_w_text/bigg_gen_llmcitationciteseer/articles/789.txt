In multi-agent systems, efficient task allocation is crucial for achieving optimal performance. This paper proposes a decentralized task allocation framework that leverages deep reinforcement learning to learn agent-specific policies. Our approach, dubbed 'TaskDRL', uses a decentralized actor-critic architecture to enable agents to learn from their local observations and communicate with each other through a sparse, delayed communication network. We demonstrate the effectiveness of TaskDRL in a variety of scenarios, including dynamic task arrival and departure rates, and show that it outperforms traditional, centralized allocation methods in terms of task completion rate and system efficiency.