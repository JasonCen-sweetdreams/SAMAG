Reinforcement learning (RL) has shown promise in robotics, but real-world applications require efficient learning in complex, high-dimensional state-action spaces. We propose a novel hierarchical RL framework, 'HierRL', which leverages a two-level abstraction hierarchy to decompose complex tasks into simpler sub-tasks. By learning a high-level policy over sub-task abstractions, HierRL significantly reduces the exploration-exploitation tradeoff, leading to faster learning and improved performance. We demonstrate HierRL's effectiveness on a real-world robotic arm manipulation task, achieving a 3x speedup over state-of-the-art RL methods.