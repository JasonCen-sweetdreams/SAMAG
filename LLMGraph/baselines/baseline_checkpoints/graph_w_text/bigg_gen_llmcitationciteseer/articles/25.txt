Multimodal sentiment analysis is a challenging task that requires effectively fusing and interpreting information from diverse modalities. This paper proposes a hierarchical attention network (HAN) that leverages the strengths of both visual and textual features to improve sentiment prediction. The HAN model employs a novel multimodal attention mechanism to selectively focus on relevant regions of interest in images and corresponding textual segments. Our experiments on the CMU-MOSI dataset demonstrate that the proposed approach outperforms state-of-the-art methods in terms of sentiment accuracy and provides interpretable insights into the decision-making process.