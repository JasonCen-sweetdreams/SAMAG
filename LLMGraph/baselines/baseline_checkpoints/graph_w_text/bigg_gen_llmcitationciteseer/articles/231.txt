Knowledge graph embedding (KGE) has become a crucial technique for various AI applications, including question answering and recommender systems. However, existing KGE methods often suffer from inefficient negative sampling strategies, leading to suboptimal model performance. This paper presents an adaptive negative sampling approach, called AdaNS, which dynamically adjusts the sampling distribution based on the entity similarity and graph structure. We demonstrate that AdaNS significantly improves the performance of popular KGE models on several benchmark datasets, including WN18RR and FB15k-237, achieving state-of-the-art results in some cases.