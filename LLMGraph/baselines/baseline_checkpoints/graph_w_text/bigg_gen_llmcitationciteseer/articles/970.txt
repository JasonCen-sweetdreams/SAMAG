Deep reinforcement learning (DRL) has achieved remarkable successes in various applications, but its vulnerability to adversarial attacks has raised concerns. Existing defenses often focus on known attack types, leaving DRL systems susceptible to unknown threats. This paper proposes a novel adversarial training framework, 'RobustRL', which leverages a dual-agent approach to generate diverse, unknown perturbations. By incorporating these perturbations into the training process, we significantly improve the robustness of DRL policies against a wide range of attacks. Experimental results on multiple Atari games demonstrate that RobustRL outperforms state-of-the-art defenses in terms of resistance to unknown attacks, while maintaining competitive performance on clean data.