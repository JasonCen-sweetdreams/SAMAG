Multimodal sentiment analysis in online reviews is a challenging task due to the complexity of human emotions and the diversity of modalities. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that effectively integrates textual, visual, and acoustic features to predict sentiment. Our HAN model employs a hierarchical attention mechanism to selectively focus on relevant modalities and aspects of the input data, leading to improved performance and interpretability. Experiments on a large-scale multimodal review dataset demonstrate that our approach outperforms state-of-the-art methods and provides valuable insights into the role of each modality in sentiment analysis.