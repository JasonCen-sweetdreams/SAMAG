Virtual reality (VR) systems rely heavily on accurate user input, but existing interaction methods often fall short in terms of naturalism and user experience. This paper explores the potential of gaze-based interaction in VR, leveraging machine learning algorithms to decode eye movement patterns. We collected a dataset of eye tracking data from 30 participants and trained a convolutional neural network to classify gaze targets in 3D space. Our results show that our approach achieves an average accuracy of 92.1% in identifying gaze targets, outperforming traditional ray-casting methods. We discuss implications for VR interface design and potential applications in fields such as education and healthcare.