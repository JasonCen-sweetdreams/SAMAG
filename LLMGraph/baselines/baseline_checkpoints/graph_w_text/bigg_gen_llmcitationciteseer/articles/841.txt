Sparse document retrieval is a challenging task in information retrieval, where the goal is to retrieve relevant documents from a large corpus given a query. Traditional sparse retrieval methods rely on bag-of-words representations, which ignore the semantic relationships between query terms. This paper proposes a novel neural ranking model, 'SparseNRM', that leverages pre-trained language models to capture the contextualized semantic meaning of query terms. We introduce a hierarchical attention mechanism that adaptively weights the importance of each query term based on its relevance to the document. Experimental results on several benchmark datasets show that SparseNRM outperforms state-of-the-art sparse retrieval methods by a significant margin, achieving higher mean average precision and recall.