 Few-shot learning has garnered significant attention in recent years, but most existing methods rely on complex architectures or large datasets. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) that leverages graph-structured data to tackle few-shot learning in a scalable and efficient manner. HGAT utilizes a hierarchical attention mechanism to adaptively weight the importance of different graph nodes and edges, thereby capturing complex relationships between classes. Our experiments on several benchmark datasets demonstrate that HGAT achieves state-of-the-art performance in few-shot settings while requiring significantly fewer parameters and computations compared to existing methods.