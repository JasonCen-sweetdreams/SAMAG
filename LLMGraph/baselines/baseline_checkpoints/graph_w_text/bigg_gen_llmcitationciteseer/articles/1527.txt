Accurate disease diagnosis relies on the effective integration of multi-modal data, including medical images, clinical text, and genomic data. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that learns to selectively focus on relevant features across different modalities. Our approach leverages a hierarchical attention mechanism to model complex relationships between modalities and identify discriminative features for disease diagnosis. Experimental results on a large-scale multi-modal dataset demonstrate that HAN outperforms state-of-the-art methods in terms of diagnosis accuracy and robustness to noisy or missing data.