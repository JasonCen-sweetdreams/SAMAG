Knowledge graph embedding (KGE) has emerged as a crucial technique for AI applications. However, existing KGE methods struggle to capture complex relationships and hierarchical structures in large-scale graphs. We propose a novel approach, Hierarchical Attention-based Knowledge Embedding (HAKE), which leverages attention mechanisms to learn representations that preserve both local and global structural information. Our experiments on benchmark datasets demonstrate that HAKE achieves state-of-the-art performance in link prediction and entity classification tasks, outperforming popular KGE methods such as TransE and DistMult.