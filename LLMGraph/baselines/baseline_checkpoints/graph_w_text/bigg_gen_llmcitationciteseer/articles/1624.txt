Sentiment analysis has become a crucial task in natural language processing, but existing methods often struggle to effectively combine multiple modalities, such as text and images. This paper proposes Hierarchical Attention Networks (HAN), a novel deep learning architecture that leverages hierarchical representations to jointly model cross-modal interactions and sentiment cues. Experimental results on a large-scale dataset demonstrate that HAN outperforms state-of-the-art methods in terms of accuracy and computational efficiency, making it suitable for real-world applications.