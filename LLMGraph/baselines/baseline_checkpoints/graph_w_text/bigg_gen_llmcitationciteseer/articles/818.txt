Cloud computing platforms face the daunting task of allocating resources efficiently to meet dynamic workload demands. This paper proposes a novel deep hierarchical reinforcement learning framework, 'CloudRL', which learns to optimize resource allocation in a hierarchical manner. CloudRL comprises a high-level planner that allocates resources to virtual machines and a low-level scheduler that allocates resources within each virtual machine. We employ a hierarchical actor-critic architecture with a novel attention mechanism to improve the learning efficiency and adapt to changing workload patterns. Our experiments on a real-world cloud computing dataset demonstrate that CloudRL outperforms state-of-the-art baselines in terms of resource utilization and workload satisfaction.