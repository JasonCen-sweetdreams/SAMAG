Knowledge graph completion is a fundamental task in AI, aiming to infer missing relationships between entities. Existing methods often suffer from high computational complexity and limited scalability. This paper proposes a novel hierarchical graph attention network (HGAT) framework, which leverages attention mechanisms at multiple scales to selectively focus on relevant graph structures. By recursively applying graph attention and graph convolutional layers, HGAT achieves state-of-the-art performance on several benchmark datasets while reducing computational costs by up to 30%. We also provide theoretical guarantees on the approximation error and convergence of HGAT.