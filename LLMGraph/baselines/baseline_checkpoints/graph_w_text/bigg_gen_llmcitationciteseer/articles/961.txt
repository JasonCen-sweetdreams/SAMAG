Multi-task learning (MTL) has become increasingly popular in deep learning, as it enables models to leverage shared knowledge across related tasks. However, existing MTL approaches often suffer from task interference, where the performance of one task deteriorates due to the presence of other tasks. This paper proposes a novel hierarchical attention network (HAN) architecture that mitigates task interference by adaptively allocating attention weights to different tasks. Our experiments on several benchmark datasets demonstrate that HAN outperforms state-of-the-art MTL methods in terms of overall performance and task-specific accuracy.