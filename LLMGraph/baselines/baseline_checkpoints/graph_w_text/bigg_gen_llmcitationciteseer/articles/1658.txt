Effective human-robot interaction (HRI) relies on seamless fusion of multimodal inputs, including vision, speech, and gesture. However, existing approaches often lack interpretability, raising concerns about trust and safety. We propose a hierarchical attention network (HAN) that integrates these modalities while providing explicit attention weights for explainability. Our HAN framework consists of a feature-level attention module for early fusion and a decision-level attention module for late fusion. Experimental results on the HRI-Dataset demonstrate improved performance and interpretability compared to state-of-the-art multimodal fusion methods.