Voice-based interfaces have become ubiquitous in modern computing, but user frustration remains a significant barrier to adoption. This paper presents a multimodal analysis of user frustration in voice-based interfaces, combining acoustic, linguistic, and physiological features to identify early indicators of frustration. We collected a novel dataset of 100 participants interacting with a voice-based virtual assistant and developed a machine learning model that achieves an F1-score of 0.85 in detecting frustration. Our results show that acoustic features, such as speech rate and pitch, are strong predictors of user frustration, and that incorporating physiological features, such as heart rate and skin conductance, improves model performance. We discuss implications for design and highlight opportunities for real-time frustration detection and mitigation.