Medical image analysis often involves fusion of multi-modal data from various sources. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) framework that effectively integrates features from different modalities. Our approach leverages graph attention mechanisms to model complex relationships between modalities and uses a hierarchical structure to capture both local and global features. Experimental results on a large-scale brain tumor segmentation dataset demonstrate that HGAT outperforms state-of-the-art fusion methods, achieving improved accuracy and robustness in the presence of noisy or missing data.