Deep reinforcement learning (DRL) has achieved impressive results in various domains, but its vulnerability to adversarial attacks raises concerns about its reliability. This paper introduces a novel approach to enhancing the robustness of DRL agents against adversarial perturbations. We formulate the problem as a multi-arm bandit (MAB) problem, where the agent must balance exploration of the environment with exploitation of the learned policy. Our proposed algorithm, 'Robust-MAB', adaptively adjusts the exploration rate based on the observed reward distribution, thereby improving the agent's robustness to adversarial attacks. Experimental results on the Atari benchmark demonstrate significant improvements in robustness compared to state-of-the-art DRL methods.