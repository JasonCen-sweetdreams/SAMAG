As reinforcement learning (RL) agents become increasingly prevalent in real-world applications, understanding their decision-making processes is crucial for trust and reliability. This paper presents a novel hierarchical attention framework, 'HierAtt', for interpreting RL policies. HierAtt incorporates both spatial and temporal attention mechanisms to identify critical state features and actions that contribute to the agent's behavior. We demonstrate the effectiveness of HierAtt on several Atari games and a real-world robotics task, showcasing improved interpretability and transparency in RL policy decisions.