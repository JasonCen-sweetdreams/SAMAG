Human activity recognition (HAR) is a fundamental task in various applications, including healthcare, surveillance, and human-computer interaction. This paper presents a novel multi-modal fusion framework that combines wearable sensor data with computer vision features to achieve real-time HAR. We propose a hierarchical attention mechanism that adaptively weights the contributions of each modality based on the activity context. Our approach outperforms state-of-the-art HAR methods on a benchmark dataset, achieving an average F1-score of 94.2% across six activities. We also demonstrate the feasibility of our approach on a wearable device, achieving real-time inference with a latency of less than 100ms.