Autonomous robot navigation in dynamic environments remains a challenging problem due to the complexity of dealing with changing obstacles and goals. This paper presents a hierarchical reinforcement learning (HRL) framework that enables robots to adapt to dynamic environments. Our approach consists of a high-level policy that selects sub-goals and a low-level policy that executes actions to achieve the selected sub-goals. We introduce a novel sub-goal discovery mechanism that leverages graph-based clustering to identify meaningful regions in the environment. Experimental results in simulated and real-world scenarios demonstrate the effectiveness of our approach in improving navigation efficiency and adapting to changing environments.