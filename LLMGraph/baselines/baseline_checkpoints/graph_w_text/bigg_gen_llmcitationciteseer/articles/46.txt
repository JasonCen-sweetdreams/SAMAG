Traditional relevance feedback methods in information retrieval (IR) systems rely on user interactions, which can be sparse and unreliable. This paper proposes a novel reinforced learning framework, 'CoRIF', that leverages contextual information to adaptively refine the retrieval model. We formulate the feedback process as a Markov decision process and employ a deep Q-network to optimize the retrieval policy. Experiments on a large-scale news dataset demonstrate that CoRIF significantly improves retrieval accuracy and reduces the number of required feedback iterations compared to state-of-the-art methods.