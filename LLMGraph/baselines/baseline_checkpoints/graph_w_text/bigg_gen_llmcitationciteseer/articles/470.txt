Augmented reality (AR) has vast potential for enhancing human-computer interaction, but existing interfaces often neglect users with disabilities. This paper presents 'GazeAR', a novel framework that leverages gaze-based interaction to enable individuals with mobility impairments to seamlessly interact with AR environments. We develop a machine learning model that accurately detects and interprets gaze patterns, facilitating intuitive object selection and manipulation in 3D space. Our user study with 20 participants demonstrates significant improvements in task completion time, accuracy, and overall user experience, highlighting the promise of gaze-based AR interfaces for inclusive design.