The widespread adoption of deep neural networks (DNNs) in high-stakes applications has sparked a growing need for explainability. This paper presents a novel approach to explaining DNN decisions using hierarchical attention mechanisms. Our method, HierAtt, recursively applies attention weights to capture complex patterns in feature interactions. We demonstrate that HierAtt outperforms existing explainability methods in terms of faithfulness and interpretability on a range of benchmark datasets. Furthermore, we provide theoretical insights into the conditions under which HierAtt is guaranteed to produce faithful explanations.