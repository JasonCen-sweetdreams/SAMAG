Deep neural networks have become ubiquitous in various AI applications, but their training remains computationally expensive. This paper presents a refined gradient descent with momentum (RGDM) algorithm that adaptively adjusts the learning rate and momentum parameters based on the gradient's norm and curvature. We theoretically prove that RGDM converges faster than standard stochastic gradient descent (SGD) and momentum SGD, especially for deep networks with complex architectures. Experimental results on CIFAR-10 and ImageNet datasets demonstrate that RGDM achieves higher accuracy and faster convergence than state-of-the-art optimizers, making it a promising choice for large-scale deep learning tasks.