Neural architecture search (NAS) has emerged as a promising technique for automating the design of deep neural networks. However, the computational cost of NAS can be prohibitively high due to the need to perform exhaustive hyperparameter tuning. This paper proposes a meta-learning approach that learns to adapt hyperparameters across different neural architectures and datasets. Our method, 'Meta-Tune', leverages a learned prior over hyperparameters to efficiently explore the search space and identify optimal configurations. Experimental results on popular NAS benchmarks demonstrate that Meta-Tune achieves state-of-the-art performance while reducing the computation time by up to 5x.