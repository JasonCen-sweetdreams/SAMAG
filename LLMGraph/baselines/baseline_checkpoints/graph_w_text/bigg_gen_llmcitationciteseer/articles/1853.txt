Explainability is a crucial aspect of trustworthy AI systems. We propose a novel hierarchical knowledge graph embedding framework, 'HiKE', which captures complex relationships between entities and provides interpretable reasoning paths. HiKE leverages a multi-level attention mechanism to focus on relevant graph regions and adaptively aggregate entity representations. We demonstrate the effectiveness of HiKE in a variety of AI reasoning tasks, including question answering, semantic role labeling, and event forecasting, outperforming state-of-the-art models in terms of both accuracy and explainability.