Multimedia search engines face the challenge of retrieving relevant documents from large collections. This paper proposes a novel deep neural ranking model, 'MultiRank', which leverages multimodal fusion and self-attention mechanisms to capture semantic relationships between query and document features. We demonstrate the effectiveness of MultiRank on a large-scale dataset, achieving significant improvements in precision and recall over state-of-the-art methods. Furthermore, we show that our approach can be efficiently deployed on modern GPU architectures, making it suitable for real-time search applications.