Multimodal fusion in deep learning has become increasingly prevalent in various applications, including computer vision, natural language processing, and speech recognition. However, the lack of interpretability in these models hinders their adoption in high-stakes domains. This paper proposes a novel hierarchical attention mechanism, 'HAtt', which enables explainable multimodal fusion by selectively weighting and aggregating modality-specific representations. We demonstrate the effectiveness of HAtt on several benchmark datasets, achieving state-of-the-art performance while providing insightful visualizations of modality attention patterns.