Autonomous vehicles rely on efficient control systems to navigate complex scenarios in real-time. This paper presents a novel hierarchical reinforcement learning (HRL) framework, 'Hierarchical Autonomous Control' (HAC), which leverages a multi-level decision-making process to balance exploration and exploitation. HAC integrates a high-level policy network with a low-level control module, enabling the vehicle to adapt to changing environments while maintaining stability and safety. We demonstrate the effectiveness of HAC in a simulated urban driving scenario, achieving improved control performance and reduced computational overhead compared to existing reinforcement learning approaches.