This paper presents a novel approach to multi-modal retrieval, which integrates visual and textual features using deep neural networks. Our proposed method, called Adaptive Re-ranking (AR), leverages the strengths of both modalities to improve retrieval accuracy. AR adaptively adjusts the weights of visual and textual features based on the query and document characteristics, achieving better performance compared to fixed-weight fusion methods. Experimental results on a large-scale multi-modal dataset demonstrate the effectiveness of AR in improving retrieval precision and recall.