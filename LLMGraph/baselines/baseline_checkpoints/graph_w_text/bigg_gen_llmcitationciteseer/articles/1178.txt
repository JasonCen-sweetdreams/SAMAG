Emotion recognition from facial expressions, speech, and physiological signals is crucial for human-computer interaction. This paper proposes a novel multimodal fusion approach, 'EmoFuse', which leverages transfer learning from pre-trained convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to recognize emotions in real-time. We introduce a hierarchical attention mechanism that adaptively weights the modalities based on their relevance to the emotion being recognized. Experimental results on the benchmark RECOLA dataset demonstrate that EmoFuse outperforms state-of-the-art methods in terms of accuracy and latency, making it suitable for real-world affective computing applications.