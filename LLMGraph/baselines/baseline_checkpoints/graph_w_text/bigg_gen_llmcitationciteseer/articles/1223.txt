Information retrieval (IR) systems rely on efficient document filtering to reduce the search space and improve retrieval accuracy. This paper proposes a novel neural ranking model that leverages latent semantic analysis (LSA) to learn dense representations of documents and queries. Our approach, dubbed 'NeuralLSA', incorporates a hierarchical attention mechanism to capture complex contextual relationships between documents and queries. Experimental results on the TREC-8 dataset demonstrate that NeuralLSA outperforms state-of-the-art filtering methods, achieving a 23.5% improvement in mean average precision (MAP) while reducing computational overhead by 40%