Augmented reality (AR) displays are increasingly used in various domains, but traditional interaction methods often detract from the user experience. This paper proposes a novel gaze-based interaction approach for AR displays using deep learning. We develop a convolutional neural network (CNN) that accurately detects gaze direction from eye-tracking data, and integrate it with a sensor fusion framework to enable seamless user interaction. Our experimental results show that the proposed approach outperforms existing state-of-the-art methods in terms of accuracy and user satisfaction.