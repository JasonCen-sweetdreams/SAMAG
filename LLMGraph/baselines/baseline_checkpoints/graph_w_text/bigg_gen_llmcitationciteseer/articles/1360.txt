In multi-agent reinforcement learning, the exploration problem is exacerbated by the presence of multiple agents, leading to inefficient and uncoordinated exploration. This paper proposes a novel approach, 'GraphShare', which leverages graph-based policy sharing to facilitate coordinated exploration among agents. By modeling agent interactions as a graph, we enable agents to share their policy updates and adapt to their neighbors' exploration strategies. We demonstrate the efficacy of GraphShare in several multi-agent environments, showcasing improved exploration efficiency and faster convergence to optimal policies.