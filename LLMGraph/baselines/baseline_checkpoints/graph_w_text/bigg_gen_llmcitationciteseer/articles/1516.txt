Virtual reality (VR) interfaces have the potential to revolutionize human-computer interaction, but users with disabilities often face barriers to access. This paper presents an emotive gesture recognition system, 'EmoGest', which enables users to control VR environments using subtle hand and finger movements. Our approach leverages machine learning and computer vision to identify emotional states from gesture patterns, facilitating more intuitive and inclusive interaction. We evaluate EmoGest with a diverse group of users, demonstrating improved accessibility and user satisfaction compared to traditional controller-based interfaces.