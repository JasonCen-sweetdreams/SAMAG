Graph neural networks (GNNs) have achieved state-of-the-art performance in node classification tasks, but their computational complexity hinders scalability to large graphs. This paper proposes a novel hierarchical graph attention network (HGAN) architecture that reduces computational overhead while maintaining accuracy. HGAN employs a hierarchical clustering scheme to group nodes into clusters, followed by attention-based feature aggregation at each level. We demonstrate the efficacy of HGAN on several benchmark datasets, achieving significant speedups over existing GNN models while maintaining competitive performance.