Augmented reality (AR) interfaces rely on accurate gesture recognition to provide seamless user experiences. However, existing approaches often struggle with variability in user behavior and environmental conditions. This paper presents a novel gaze-informed gesture recognition framework that leverages eye-tracking data to disambiguate gesture meanings. We propose a multimodal fusion architecture that combines computer vision, machine learning, and gaze analysis to improve recognition accuracy and robustness. Experimental results demonstrate that our approach outperforms state-of-the-art gesture recognition methods in AR scenarios, particularly in cases of occlusion or ambiguous hand poses.