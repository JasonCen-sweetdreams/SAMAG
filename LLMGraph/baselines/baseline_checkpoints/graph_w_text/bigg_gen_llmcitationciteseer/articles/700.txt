The increasing adoption of AI-powered cybersecurity systems necessitates transparency and interpretability to build trust and identify vulnerabilities. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) architecture that leverages graph-based representations to model complex system interactions. By incorporating explainable attention mechanisms, HGAT provides insights into the decision-making process, enabling the identification of critical nodes and edges in the system. We evaluate HGAT on a real-world cyber-attack dataset, demonstrating improved performance and explainability compared to state-of-the-art AI-powered cybersecurity systems.