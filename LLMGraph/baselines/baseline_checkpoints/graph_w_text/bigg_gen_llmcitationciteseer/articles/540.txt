Multimodal sentiment analysis has gained significant attention in recent years, but existing approaches often rely on a single modality or fail to capture complex relationships between modalities. This paper proposes a novel heterogeneous ensemble learning framework, 'ModaFuse', which integrates multiple modalities (text, image, and audio) using a stacked generalization approach. We introduce a modality-aware attention mechanism that adaptively weights the contributions of each modality based on the input data. Experimental results on three benchmark datasets demonstrate that ModaFuse achieves state-of-the-art performance in multimodal sentiment analysis tasks, outperforming single-modality and homogeneous ensemble methods.