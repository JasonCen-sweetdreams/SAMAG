Visual Question Answering (VQA) tasks require the integration of visual and textual information to generate accurate answers. While recent approaches have utilized multi-modal fusion techniques, they often suffer from inefficiencies in processing and representing the diverse input modalities. This paper presents a novel attention-based fusion mechanism, 'MMFusion', which adaptively weights and combines the visual and textual features based on their relevance to the question. We demonstrate the effectiveness of MMFusion on several VQA benchmarks, achieving state-of-the-art performance while reducing computational costs by up to 30%.