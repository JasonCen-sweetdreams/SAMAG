Cooperative task allocation in multi-agent systems often relies on centralized decision-making, which can be vulnerable to single-point failures and limited scalability. This paper presents a decentralized multi-agent reinforcement learning (MARL) framework, 'CoopAllocator', which enables agents to learn cooperative strategies for task allocation in a distributed manner. We introduce a novel communication protocol that facilitates exchange of partial observations and intentions among agents, improving overall system efficiency and adaptability. Experimental results on a simulated package delivery domain demonstrate the effectiveness of CoopAllocator in achieving high task completion rates and robustness to agent failures.