Event extraction from unstructured data is a crucial task in various applications, including information retrieval and natural language processing. This paper proposes a novel hierarchical representation learning framework for multi-modal event extraction, leveraging both textual and visual features. We introduce a graph-based attention mechanism to capture contextual relationships between entities and events, and a hierarchical encoder to learn representations at different levels of abstraction. Experimental results on a benchmark dataset show that our approach outperforms state-of-the-art methods in terms of event detection and argument role identification accuracy.