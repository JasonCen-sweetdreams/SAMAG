Emotion recognition is a challenging task that requires integrating information from multiple modalities, such as speech, text, and vision. This paper proposes a novel Hierarchical Graph Attention Network (HGAN) architecture that leverages graph-based representations to model complex relationships between modalities. Our approach incorporates attention mechanisms to selectively focus on salient features and adaptively weight modality-specific contributions. Experimental results on the IEMOCAP dataset demonstrate significant improvements in multi-modal emotion recognition accuracy compared to state-of-the-art methods.