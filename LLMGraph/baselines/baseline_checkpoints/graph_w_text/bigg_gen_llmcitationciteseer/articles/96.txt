Transformer-based models have achieved state-of-the-art results in various natural language processing (NLP) tasks, but their opacity hinders model interpretability and trustworthiness. This paper proposes a novel technique, 'TransExplainer', which leverages attention weights and gradient-based saliency maps to provide insights into the decision-making process of transformer models. We evaluate TransExplainer on several NLP benchmarks and demonstrate its effectiveness in identifying salient input features, visualizing attention patterns, and explaining model predictions. Our approach enables developers to better understand and refine complex NLP models, ultimately leading to more accurate and reliable AI systems.