Emotion recognition in human-computer interaction (HCI) is a challenging task due to the complexity of human emotions and the variability of modalities. This paper presents a novel hybrid attention mechanism that combines visual, acoustic, and linguistic features to recognize emotions in multi-modal data. Our approach leverages self-attention to model intra-modal relationships and cross-modal attention to capture inter-modal interactions. Experimental results on the SEMAINE database demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from facial expressions, speech, and text, achieving an average F1-score of 0.85.