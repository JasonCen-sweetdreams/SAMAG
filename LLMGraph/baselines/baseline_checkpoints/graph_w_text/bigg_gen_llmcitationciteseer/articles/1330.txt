Multimodal sentiment analysis involves predicting sentiment from multiple sources of data, such as text, images, and audio. Existing approaches often suffer from feature misalignment and modality imbalance. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) that leverages graph-based multimodal fusion to address these limitations. HGAT employs a hierarchical attention mechanism to selectively focus on relevant modalities and features, enabling more accurate sentiment predictions. Our experiments on the CMU-MOSI and POM datasets demonstrate the superiority of HGAT over state-of-the-art methods, with significant improvements in sentiment classification accuracy and F1-score.