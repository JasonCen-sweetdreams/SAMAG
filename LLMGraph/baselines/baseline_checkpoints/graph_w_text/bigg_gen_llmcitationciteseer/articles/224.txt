This paper presents a novel dialogue management framework for conversational AI, HADM, which leverages hierarchical attention mechanisms to model multi-turn dialogues. HADM uses a graph-based representation to capture the conversation context and incorporates a hierarchical attention module to selectively focus on relevant dialogue history. We evaluate HADM on several benchmark datasets and demonstrate significant improvements in dialogue coherence, fluency, and overall user satisfaction. Our results show that HADM outperforms state-of-the-art models in generating contextually relevant and engaging responses.