The surge in deep learning model complexity poses significant challenges for deployment on resource-constrained devices. This paper proposes a novel neural architecture search (NAS) method, 'RL-NAS', which leverages reinforcement learning to efficiently explore the vast search space of potential architectures. By incorporating a reward function that balances model accuracy and resource efficiency, RL-NAS discovers compact and accurate models tailored to specific device constraints. Experiments on several benchmark datasets demonstrate that RL-NAS outperforms existing NAS methods in terms of search efficiency and model performance on resource-limited devices.