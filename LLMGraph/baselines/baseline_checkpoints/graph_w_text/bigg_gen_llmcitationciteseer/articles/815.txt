Multi-modal retrieval systems aim to retrieve relevant documents from heterogeneous sources, such as text, images, and videos. However, existing ranking models often struggle to capture the complex relationships between modalities. We propose a novel deep neural network architecture, 'MMRank', which learns to rank documents by jointly modeling the semantic relationships between text, visual, and acoustic features. Our approach leverages a multi-task learning framework to learn shared representations across modalities, and incorporates a attention-based ranking mechanism to weigh the importance of each modality. Experimental results on a large-scale multi-modal dataset demonstrate the effectiveness of MMRank in improving retrieval performance and robustness.