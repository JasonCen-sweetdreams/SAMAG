Multimodal sentiment analysis (MSA) has gained increasing attention in recent years, but existing methods struggle to effectively fuse and align multimodal features. This paper proposes a novel hierarchical attention network (HAN) for MSA, which leverages self-attention mechanisms to learn modality-specific and cross-modal representations. Our HAN consists of a feature-level attention module, a modality-level attention module, and a sentiment fusion module. Extensive experiments on three benchmark datasets demonstrate that our approach outperforms state-of-the-art methods, achieving significant improvements in sentiment accuracy and robustness.