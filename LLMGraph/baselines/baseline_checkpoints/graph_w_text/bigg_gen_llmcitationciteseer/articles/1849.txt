This paper addresses the problem of coordinated exploration in multi-agent systems, where agents need to balance individual exploration with collective goal-oriented behavior. We propose a novel deep reinforcement learning framework, 'MA-CERL', which combines centralized critics with decentralized actor networks. Our approach enables agents to learn coordinated exploration strategies that adapt to changing environmental conditions and improve overall system performance. Experimental results in a simulated robotics domain demonstrate the effectiveness of MA-CERL in achieving faster convergence and higher rewards compared to existing methods.