Explainability techniques for deep neural networks (DNNs) are crucial for building trust in AI systems. However, recent studies have shown that these techniques can be vulnerable to adversarial attacks. This paper proposes a novel adversarial training framework, 'AdvEx', which aims to improve the robustness of explainability methods against such attacks. We introduce a novel loss function that jointly optimizes the model's performance and explainability, while incorporating adversarial perturbations to simulate real-world attacks. Experimental results on popular image classification datasets demonstrate that AdvEx significantly improves the robustness of feature attribution methods, such as saliency maps and feature importance, against state-of-the-art attacks.