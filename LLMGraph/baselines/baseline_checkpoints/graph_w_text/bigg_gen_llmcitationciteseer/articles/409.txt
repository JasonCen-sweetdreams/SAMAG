Deep neural networks have achieved state-of-the-art performance in various applications, but their success heavily relies on careful hyperparameter tuning. Unfortunately, this process is often time-consuming and computationally expensive. This paper proposes a novel Bayesian optimization framework, 'HyperBO', which leverages a probabilistic search strategy to efficiently identify optimal hyperparameters. We introduce a new acquisition function that adaptively balances exploration and exploitation, leading to faster convergence and improved performance. Experimental results on several benchmark datasets demonstrate that HyperBO outperforms existing tuning methods, including grid search and random search, while requiring significantly less computational resources.