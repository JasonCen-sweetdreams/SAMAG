Sentiment analysis has become a crucial task in natural language processing, but most existing approaches focus on single-modal inputs (e.g., text or images). This paper proposes a novel hierarchical attention network (HAN) architecture that fuses multi-modal inputs (text, images, and videos) to analyze sentiment in a more comprehensive manner. Our HAN model employs attention mechanisms at both the feature and modality levels to selectively weigh the importance of different input modalities and features. Experimental results on a large-scale, multi-modal sentiment analysis dataset demonstrate the effectiveness of our approach, achieving state-of-the-art performance with significant improvements over single-modal and existing multi-modal baselines.