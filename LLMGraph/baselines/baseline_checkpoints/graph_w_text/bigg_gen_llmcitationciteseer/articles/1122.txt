Emotion recognition is a crucial aspect of human-computer interaction (HCI), enabling computers to adapt to users' affective states. This paper presents a novel gaze-informed multimodal fusion approach that combines eye-tracking, facial expressions, and speech signals to recognize emotions. Our method leverages a graph-based attention mechanism to selectively weigh the contributions of each modality, based on the user's gaze patterns. Experimental results on a multimodal emotion dataset demonstrate improved recognition accuracy and robustness compared to state-of-the-art methods, with potential applications in affective computing, virtual reality, and mental health monitoring.