Autonomous drone navigation in dynamic environments poses significant challenges due to the need to adapt to changing obstacles and goals. This paper proposes a hierarchical reinforcement learning framework, 'HDRL-Drone', which combines high-level planning with low-level control. Our approach utilizes a graph-based abstraction of the environment to plan feasible paths, while a deep reinforcement learning module refines the control policy to accommodate real-time sensor inputs. Experimental results in a simulated urban setting demonstrate the effectiveness of HDRL-Drone in navigating complex scenarios with improved efficiency and safety.