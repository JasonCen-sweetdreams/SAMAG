Graph neural networks (GNNs) have achieved state-of-the-art performance on various graph-based tasks, but their black-box nature hinders their adoption in high-stakes applications. This paper presents a novel graph attention mechanism, 'ExplainGAT', which incorporates attention weights as a proxy for feature importance. We introduce a scalable sampling-based approach to compute attention weights, enabling ExplainGAT to handle large-scale graphs. Experimental results on multiple node classification benchmarks demonstrate that ExplainGAT not only achieves comparable performance to state-of-the-art GNNs but also provides meaningful explanations for its predictions.