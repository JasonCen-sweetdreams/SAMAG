Dialogue state tracking (DST) is a crucial component in task-oriented dialogue systems, and explainability is essential for building trustworthy AI systems. This paper proposes a novel hierarchical attention network (HAT) for DST, which incorporates both local and global attention mechanisms to capture complex contextual relationships in dialogues. We introduce a novel explainability module that generates visualizations of attention weights, enabling users to understand the model's decision-making process. Experimental results on two benchmark datasets demonstrate that HAT outperforms state-of-the-art DST models while providing interpretable explanations for its predictions.