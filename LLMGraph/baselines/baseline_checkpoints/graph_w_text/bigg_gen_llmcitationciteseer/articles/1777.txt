Emotion recognition from multimodal data, such as speech, text, and vision, is a challenging task due to the complexity of human emotions and the varying importance of different modalities. This paper proposes a Hierarchical Graph Attention Network (HGAT) to jointly model intra- and inter-modality relationships. Our approach leverages graph attention mechanisms to adaptively weigh the importance of different modalities and their interactions, leading to improved emotion recognition performance. Experimental results on the CMU-MOSEI dataset demonstrate the effectiveness of HGAT in recognizing emotions from multimodal data, outperforming state-of-the-art methods.