Knowledge graph embeddings (KGEs) are crucial for various AI applications, but their performance often hinges on the quality of relation representations. This paper proposes 'ARA-KGE', a novel KGE method that incorporates adaptive relation attention to selectively emphasize relevant relations during training. Our approach leverages a graph neural network to learn attention weights that adapt to the entity-relation graph structure and task requirements. Experimental results on benchmark datasets show that ARA-KGE outperforms state-of-the-art KGE methods in link prediction and question answering tasks, while reducing training time by up to 30%