Explainability is a crucial aspect of Artificial Intelligence (AI) systems, particularly in Natural Language Processing (NLP) tasks like sentiment analysis. This paper presents a novel Hierarchical Attention Network (HAN) architecture that incorporates explainable AI techniques to identify influential words and phrases in sentiment-bearing texts. Our approach leverages a multi-level attention mechanism to capture both local and global contextual information, enabling the model to provide transparent and interpretable sentiment predictions. Experimental results on benchmark datasets demonstrate the effectiveness of our approach in achieving state-of-the-art performance while providing insights into the decision-making process.