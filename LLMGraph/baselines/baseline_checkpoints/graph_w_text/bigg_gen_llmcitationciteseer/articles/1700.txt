Autonomous navigation in dynamic environments poses significant challenges due to the need to adapt to changing circumstances. This paper presents a hierarchical reinforcement learning (HRL) framework that enables robots to learn both high-level navigation policies and low-level control skills. Our approach leverages a novel hierarchical abstraction of the environment, allowing the agent to focus on relevant aspects of the state space. We demonstrate the efficacy of our approach in a series of experiments involving a robotic vehicle navigating through crowded streets and construction zones, and show that it outperforms state-of-the-art methods in terms of task completion rate and safety.