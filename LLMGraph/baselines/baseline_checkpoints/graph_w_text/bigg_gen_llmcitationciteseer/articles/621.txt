Existing time-series anomaly detection methods often rely on labeled data, which can be scarce and expensive to obtain. This paper proposes a self-supervised representation learning framework, 'TSR', that leverages the inherent temporal structure of time-series data to learn effective representations for anomaly detection. TSR employs a contrastive loss function that encourages the model to distinguish between normal and anomalous patterns, without requiring explicit labels. We demonstrate the effectiveness of TSR on several benchmark datasets, achieving state-of-the-art results in unsupervised anomaly detection.