Explainability in reinforcement learning (RL) is crucial for real-world applications, particularly in autonomous systems. This paper introduces 'HERO', a hierarchical framework that integrates model-based RL with interpretable decision-making. HERO employs a nested structure of models, where a high-level policy selects among low-level controllers, each explaining its decisions through attention-based visualizations. We demonstrate HERO's effectiveness in a simulated autonomous driving environment, showcasing improved performance, explainability, and safety compared to state-of-the-art RL baselines.