Recent advances in deep reinforcement learning (DRL) have led to the development of complex policies capable of solving challenging tasks. However, these policies remain vulnerable to adversarial attacks that manipulate the state observations to induce suboptimal behavior. This paper presents a novel attack framework, 'HierAttack', which targets DRL policies with hierarchical state abstractions. We demonstrate that our attack can significantly degrade the policy's performance by injecting carefully crafted perturbations into the abstracted state representations. Furthermore, we evaluate the robustness of various DRL algorithms and provide insights into the design of more resilient policies.