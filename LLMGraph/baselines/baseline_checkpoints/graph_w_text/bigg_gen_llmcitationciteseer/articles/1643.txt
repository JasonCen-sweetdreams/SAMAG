Deep reinforcement learning (DRL) has achieved remarkable success in complex domains, but its black-box nature hinders trust and understanding. This paper proposes a novel explainability framework, 'ExplainDRL', which integrates attribution methods with model-based RL to provide interpretable insights into agent decision-making. We introduce a hierarchical attention mechanism that highlights relevant state features and abstract representations, enabling users to understand policy decisions in high-dimensional environments. Experimental results on Atari games and robotic control tasks demonstrate the effectiveness of ExplainDRL in improving transparency and trust in DRL systems.