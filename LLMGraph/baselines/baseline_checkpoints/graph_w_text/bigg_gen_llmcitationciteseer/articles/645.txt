Multimodal sentiment analysis has gained significant attention in recent years, but existing approaches often struggle to provide interpretable results. This paper proposes a novel hierarchical attention-based neural network architecture, 'HANS', which effectively integrates textual, visual, and acoustic features to predict sentiment scores. By introducing a hierarchical attention mechanism that selectively focuses on relevant modalities and features, HANS achieves state-of-the-art performance on several benchmark datasets. Furthermore, we provide a comprehensive analysis of the attention weights, demonstrating the model's ability to provide explainable insights into the sentiment prediction process.