Deep neural networks often struggle to generalize to out-of-distribution (OOD) inputs, which can lead to errors in critical applications. This paper proposes a novel approach to OOD detection based on adaptive confidence thresholding. Our method dynamically adjusts the confidence threshold for each input based on the model's uncertainty estimates, improving detection performance on unseen datasets. We evaluate our approach on several benchmark datasets and demonstrate significant improvements over existing methods, including the popular Mahalanobis distance-based approach.