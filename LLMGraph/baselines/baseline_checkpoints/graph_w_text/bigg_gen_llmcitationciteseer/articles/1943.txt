Deep neural networks have been shown to be vulnerable to adversarial attacks, which can significantly degrade their performance. In this paper, we propose a novel approach to analyze the robustness of deep neural networks against adversarial attacks using Bayesian uncertainty estimation. Our method, called 'BayesRobust', leverages Bayesian neural networks to model the uncertainty of the network's predictions and detect potential adversarial examples. We demonstrate the effectiveness of BayesRobust on several benchmark datasets, including ImageNet and CIFAR-10, and show that it outperforms state-of-the-art robustness analysis methods. Our results have important implications for the development of robust and trustworthy AI systems.