Deep learning-based sentiment analysis models have achieved state-of-the-art performance on various modalities, including text, images, and videos. However, their lack of transparency and interpretability hinders their adoption in high-stakes applications. This paper proposes a novel hierarchical attention network (HAN) that integrates multi-modal inputs and provides explainable sentiment predictions. Our HAN framework consists of modality-specific attention modules and a fusion layer that adaptively weights the importance of each modality. We evaluate our approach on a benchmark dataset and demonstrate improved performance and interpretability compared to existing multi-modal sentiment analysis models.