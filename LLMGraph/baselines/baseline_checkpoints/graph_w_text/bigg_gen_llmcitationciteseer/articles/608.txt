In multi-agent systems, cooperative task planning is a challenging problem due to the complexity of interactions between agents. This paper proposes a novel approach that leverages deep reinforcement learning to enable agents to learn cooperative policies. We introduce a decentralized framework, 'CoopPlan', which combines decentralized partially observable Markov decision processes (Dec-POMDPs) with deep Q-networks. CoopPlan allows agents to learn to coordinate their actions and achieve common goals in complex, dynamic environments. Experimental results on a set of benchmark scenarios demonstrate the effectiveness of CoopPlan in achieving higher task success rates and improved agent coordination compared to traditional planning methods.