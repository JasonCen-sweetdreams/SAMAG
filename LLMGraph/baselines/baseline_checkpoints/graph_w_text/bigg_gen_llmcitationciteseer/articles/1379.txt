Emotion recognition in conversational systems has become increasingly important for human-computer interaction. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages multi-modal input from speech, text, and facial expressions. Our HAN model consists of two attention layers: a lower-level attention that selectively focuses on relevant modalities and an upper-level attention that integrates the outputs from each modality. Experimental results on the IEMOCAP dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from conversations, achieving an F1-score of 0.83.