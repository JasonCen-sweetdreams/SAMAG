Cooperative task allocation is a fundamental problem in multi-agent systems. This paper presents a novel hierarchical multi-agent reinforcement learning framework, 'CoopHRL', which enables agents to learn cooperative strategies for task allocation. CoopHRL consists of a high-level coordinator that allocates tasks to agents and a low-level learner that updates the coordinator's policy. We employ a decentralized actor-critic method to update the agents' policies, allowing them to adapt to changing task requirements. Experimental results in a simulated warehouse environment demonstrate that CoopHRL outperforms existing methods in terms of task completion time and resource utilization.