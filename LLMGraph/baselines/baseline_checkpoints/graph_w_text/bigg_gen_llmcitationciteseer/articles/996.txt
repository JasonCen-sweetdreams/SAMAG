Virtual reality (VR) has become increasingly popular, but users often experience fatigue and discomfort due to inadequate interface designs. This paper presents a novel gaze-based adaptive interface framework, 'GAZE+', which leverages eye-tracking and machine learning to dynamically adjust VR interface elements based on user attention and behavior. We evaluated GAZE+ in a user study and found significant improvements in user satisfaction, task completion time, and reduced eye movement. Our approach has implications for enhancing user experience in various VR applications, including gaming, education, and healthcare.