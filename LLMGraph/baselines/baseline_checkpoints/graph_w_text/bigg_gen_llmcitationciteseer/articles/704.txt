Accurate emotional state recognition is crucial for developing empathetic human-computer interfaces. This paper presents a novel multimodal fusion framework, 'EmoFuse', which combines computer vision, speech recognition, and physiological signal processing to recognize emotional states in real-time. We introduce a attention-based architecture that adaptively weighs the contributions of each modality based on their contextual relevance. Our experiments on a large, diverse dataset show that EmoFuse outperforms state-of-the-art unimodal and multimodal approaches, achieving a recognition accuracy of 92.1% on a 7-class emotion classification task.