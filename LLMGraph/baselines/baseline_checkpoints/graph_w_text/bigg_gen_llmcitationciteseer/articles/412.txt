Virtual reality (VR) systems often rely on cumbersome controllers or limited gesture recognition, hindering immersive experiences. We present 'GazeVR', a novel multimodal interaction framework that leverages gaze tracking, voice commands, and hand gestures to enable more natural and expressive interactions in VR. Our approach utilizes a probabilistic fusion model to integrate input from various modalities, allowing for robust and flexible interaction. A user study demonstrates that GazeVR significantly improves task completion times and user satisfaction compared to traditional controller-based interfaces.