Emotion recognition from multi-modal data (e.g., speech, text, and vision) is a crucial task in human-computer interaction. However, existing approaches often rely on complex models that struggle to scale with large datasets. This paper proposes a novel hybrid attention mechanism that combines self-attention and graph-based attention to effectively capture intra-modal and inter-modal relationships. Our approach, dubbed 'HybridEmo', achieves state-of-the-art performance on three benchmark datasets while reducing computational requirements by up to 30%. We also provide an ablation study to demonstrate the effectiveness of each attention component.