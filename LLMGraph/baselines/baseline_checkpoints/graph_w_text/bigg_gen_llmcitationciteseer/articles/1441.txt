Deep neural networks have been shown to be vulnerable to adversarial attacks, which can lead to misclassification and security breaches. Existing detection methods rely on input preprocessing, anomaly detection, or statistical analysis. This paper proposes a novel approach, GradientExplanation (GE), which leverages explainable AI techniques to identify adversarial attacks. GE analyzes the gradient patterns of the target model during inference to detect abnormal behavior indicative of an attack. Our experiments on ImageNet and CIFAR-10 datasets demonstrate that GE outperforms state-of-the-art methods in detecting adversarial attacks while maintaining high accuracy on clean data.