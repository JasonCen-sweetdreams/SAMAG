Visual question answering (VQA) models have achieved impressive performance, but their decision-making processes remain opaque. This paper proposes a novel hierarchical attention-based approach, 'HARE', which integrates explainable reasoning into VQA models. HARE uses a graph-based attention mechanism to highlight relevant regions in the image and identify the most informative question tokens. Our experiments on the VQA 2.0 dataset demonstrate that HARE not only improves answer accuracy but also provides interpretable visual and linguistic explanations for its predictions.