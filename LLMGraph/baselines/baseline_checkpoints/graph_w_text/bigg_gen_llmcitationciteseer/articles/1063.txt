Despite their impressive performance, deep learning models remain vulnerable to adversarial attacks. This paper explores a novel approach to improve the robustness of deep neural networks by leveraging geometric perturbations. We propose a new family of adversarial attacks based on differential geometry, which can be used to craft perturbations that are imperceptible to humans yet effective in misleading the model. We also develop a defense mechanism that incorporates these perturbations into the training process, yielding models with state-of-the-art robustness to various types of attacks. Experimental results on several benchmark datasets demonstrate the efficacy of our approach.