Emotion recognition from multimodal inputs, such as speech, text, and facial expressions, remains a challenging task. Existing fusion methods often rely on complex feature engineering or concatenation of modalities, leading to increased computational costs and decreased interpretability. We propose a novel graph attention network (GAT)-based approach, 'MMEF', that efficiently fuses multimodal features by modeling their relationships as a graph. Our experiments on the IEMOCAP dataset demonstrate that MMEF achieves state-of-the-art emotion recognition performance with reduced computational overhead compared to conventional fusion methods.