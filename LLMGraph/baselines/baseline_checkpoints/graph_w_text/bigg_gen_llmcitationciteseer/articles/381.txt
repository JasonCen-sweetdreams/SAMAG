Visual question answering (VQA) models often struggle to provide interpretable explanations for their predictions. We propose a novel attention-based multimodal fusion framework, 'ExplainVQA', which integrates visual and linguistic features to generate transparent and accurate answers. Our approach leverages attention mechanisms to weigh the importance of different visual regions and linguistic tokens, enabling the model to focus on relevant input features. Experiments on the VQA-X dataset demonstrate that ExplainVQA outperforms state-of-the-art VQA models while providing insightful visual and linguistic explanations for its predictions.