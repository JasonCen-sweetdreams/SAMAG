While Transformers have achieved state-of-the-art results in natural language processing, their application to computer vision tasks remains limited. This paper proposes a novel Hierarchical Attention-based Transformer (HAT) architecture for image classification, which leverages self-attention mechanisms to capture both local and global contextual information. Our approach integrates a pyramid pooling module to reduce spatial dimensions and increase channel capacity, enabling the model to learn robust features from high-resolution images. Experimental results on the ImageNet dataset demonstrate that HAT outperforms convolutional neural networks and vision Transformers in terms of accuracy and robustness to adversarial attacks.