Graph Neural Networks (GNNs) have demonstrated exceptional performance in various graph-based applications. However, their vulnerability to adversarial attacks has raised concerns about their reliability. This paper presents a comprehensive study of defense mechanisms against adversarial attacks on GNNs. We evaluate the effectiveness of six state-of-the-art defense techniques, including adversarial training, graph purification, and attention mechanisms, on three popular GNN architectures. Our results show that while individual defense mechanisms provide some robustness, a combination of techniques can significantly improve the resilience of GNNs to targeted attacks. We also propose a novel ensemble-based defense approach that achieves the best overall performance.