Autonomous vehicles require efficient motion planning to navigate complex environments. This paper presents a novel deep hierarchical reinforcement learning (DHRL) framework that integrates high-level planning with low-level control. Our approach leverages a hierarchical policy structure, where a high-level planner generates waypoints and a low-level controller executes them. We introduce a new exploration strategy that encourages the agent to visit diverse scenarios, improving the overall robustness of the motion planning policy. Experimental results on a realistic simulation platform demonstrate that our DHRL approach outperforms state-of-the-art methods in terms of safety, efficiency, and adaptability.