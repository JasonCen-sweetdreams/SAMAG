Deep learning models have achieved state-of-the-art performance in various applications, but their hyperparameter tuning remains a labor-intensive and computationally expensive task. This paper presents a novel Bayesian optimization framework, 'BOHB', that efficiently explores the hyperparameter space to find optimal configurations. BOHB leverages a probabilistic surrogate model to guide the search and incorporates a dynamic resource allocation strategy to adapt to changing computing resources. Experimental results on several benchmark datasets demonstrate that BOHB outperforms existing tuning methods, reducing the tuning time by up to 75% while maintaining comparable model performance.