Explainability is a crucial aspect of AI-driven decision-making systems, particularly in high-stakes applications. This paper presents a novel Hierarchical Attention Network (HAN) architecture that generates interpretable explanations for AI-driven decisions. Our approach leverages multi-level attention mechanisms to identify relevant input features and their relationships, providing a hierarchical understanding of the decision-making process. Experimental results on a healthcare dataset demonstrate that HAN outperforms state-of-the-art explainability methods in terms of accuracy and interpretability, while maintaining decision-making performance.