Graph neural networks (GNNs) have achieved state-of-the-art performance in various graph-based applications. However, their performance is highly dependent on the choice of hyperparameters, which is often a time-consuming and labor-intensive process. This paper proposes a Bayesian optimization framework, 'GNN-BO', for efficient hyperparameter tuning of GNNs. We leverage a probabilistic surrogate model to search for the optimal hyperparameters, and demonstrate its effectiveness on several graph-based benchmarks. Experimental results show that GNN-BO outperforms grid search and random search in terms of both optimization speed and model performance.