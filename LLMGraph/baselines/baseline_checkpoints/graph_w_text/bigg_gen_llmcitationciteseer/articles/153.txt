The proliferation of Internet of Things (IoT) devices has created a pressing need for efficient resource allocation strategies. This paper proposes a novel multi-agent reinforcement learning framework, 'CoopRA', which enables cooperative resource allocation in IoT networks. We model the problem as a decentralized partially observable Markov decision process, where each agent represents an IoT device. Our approach leverages a distributed actor-critic architecture to learn a coordination mechanism that maximizes the overall network utility. Experimental results on a simulated IoT network demonstrate that CoopRA outperforms traditional centralized and decentralized approaches, achieving significant improvements in resource utilization and network performance.