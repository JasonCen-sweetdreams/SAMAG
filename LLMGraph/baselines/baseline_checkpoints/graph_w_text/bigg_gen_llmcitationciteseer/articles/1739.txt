Coordinating the actions of multiple agents in complex environments is a challenging problem in artificial intelligence. This paper proposes a novel hierarchical reinforcement learning framework, 'HierMA', which enables agents to learn both individual and coordinated behaviors. HierMA uses a hierarchical structure to decompose the global reward function into local rewards, allowing agents to focus on their individual objectives while still contributing to the global goal. We evaluate HierMA in a simulated search-and-rescue scenario, demonstrating improved coordination and task completion rates compared to existing decentralized reinforcement learning approaches.