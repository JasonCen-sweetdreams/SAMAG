Multimodal emotion recognition is a challenging task that involves fusing features from multiple sources such as speech, text, and vision. While deep learning models have achieved state-of-the-art performance, they often lack interpretability. This paper proposes a hierarchical attention framework that selectively focuses on relevant features across modalities and time-steps, providing insights into the decision-making process. Our approach leverages a novel multimodal fusion strategy and a hierarchical attention mechanism that captures both local and global contextual information. Experimental results on the CMU-MOSEI dataset demonstrate that our model achieves significant improvements in emotion recognition accuracy while providing visual explanations for its predictions.