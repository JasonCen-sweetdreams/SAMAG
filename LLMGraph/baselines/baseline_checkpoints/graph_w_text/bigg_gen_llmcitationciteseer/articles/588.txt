Multi-task learning (MTL) has become increasingly popular in natural language processing (NLP) due to its ability to leverage shared knowledge across related tasks. However, existing MTL approaches often suffer from task interference and lack of interpretability. We propose a novel hierarchical attention network (HAN) architecture that addresses these limitations. Our HAN model uses a hierarchical attention mechanism to selectively focus on relevant tasks and taskspecific features, improving overall performance and providing insights into task relationships. Experimental results on several benchmark NLP datasets demonstrate the effectiveness of our approach, outperforming state-of-the-art MTL methods.