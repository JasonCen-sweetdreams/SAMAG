Deep neural networks (DNNs) are vulnerable to adversarial attacks, which can significantly degrade their performance. This paper proposes a novel defense mechanism, Hierarchical Feature Alignment (HFA), to enhance the robustness of DNNs against such attacks. HFA aligns features at multiple scales to detect and correct misaligned representations, thereby improving the model's resilience to adversarial perturbations. Experimental results on ImageNet and CIFAR-10 datasets demonstrate that HFA outperforms state-of-the-art defense methods in terms of both accuracy and robustness, while maintaining inference efficiency.