Query expansion (QE) is a crucial component in modern search engines, aiming to improve retrieval accuracy by reformulating the original query. While traditional QE methods rely on hand-crafted rules or supervised learning, we propose a novel approach that leverages reinforcement learning (RL) to optimize the QE process. Our framework, RLQE, learns to select the most effective expansion terms by interacting with a simulated environment that mimics user behavior. Experimental results on the TREC Web Track dataset demonstrate that RLQE outperforms state-of-the-art QE methods, achieving a 12% increase in mean average precision.