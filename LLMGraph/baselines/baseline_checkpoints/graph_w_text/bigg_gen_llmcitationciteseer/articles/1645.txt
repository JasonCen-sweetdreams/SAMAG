Few-shot learning has emerged as a crucial challenge in machine learning, where models must adapt to novel classes with limited training data. This paper proposes a novel deep meta-learning framework, 'AdaTask', which leverages adaptive task embeddings to effectively generalize to new tasks. Our approach integrates a meta-learner that generates task-specific embeddings, which are used to condition the base learner's weights. We demonstrate the efficacy of AdaTask on benchmark few-shot image classification datasets, achieving state-of-the-art performance while reducing the need for extensive retraining.