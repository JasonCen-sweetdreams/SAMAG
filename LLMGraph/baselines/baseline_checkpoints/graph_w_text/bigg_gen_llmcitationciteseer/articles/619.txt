Multi-task learning has shown promising results in various applications, but scaling it to a large number of tasks remains a significant challenge. This paper proposes a novel approach, Meta-TOME, which leverages meta-learning to learn task embeddings that capture task relationships and similarities. By doing so, Meta-TOME enables efficient task clustering and adaptive task weighting, leading to improved performance and scalability in multi-task learning scenarios. Experimental results on several benchmark datasets demonstrate the effectiveness of Meta-TOME in improving model performance and reducing computational costs.