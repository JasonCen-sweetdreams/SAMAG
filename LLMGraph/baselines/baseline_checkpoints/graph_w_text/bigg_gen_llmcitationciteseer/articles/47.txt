Graph neural networks (GNNs) have shown promising results in node classification tasks, but their black-box nature hinders their adoption in high-stakes applications. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) architecture, which incorporates attention mechanisms at multiple scales to provide interpretable node representations. Our approach learns to recursively aggregate node features while highlighting the most informative neighbors, resulting in improved classification performance and explainability. We evaluate HGAT on several benchmark datasets, demonstrating its effectiveness in classifying nodes with complex dependencies.