Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task, especially when no labeled data is available for a target domain. This paper introduces 'HGAN-ZSL', a novel zero-shot learning framework that leverages hierarchical graph attention networks to capture complex relationships between modalities. Our approach learns to generate domain-invariant representations by jointly optimizing a graph-based attention mechanism and a modality-agnostic emotion classification objective. Experimental results on several benchmark datasets demonstrate the superior performance of HGAN-ZSL compared to state-of-the-art zero-shot learning methods, achieving an average improvement of 12.4% in recognition accuracy.