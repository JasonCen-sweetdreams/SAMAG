Embodied conversational agents (ECAs) are increasingly used in human-computer interaction, but their emotional intelligence and feedback mechanisms remain limited. This paper proposes an affective resonance model (ARM) that enables ECAs to recognize, adapt to, and respond to users' emotional states in real-time. ARM integrates multimodal sensing (facial expressions, speech, and physiological signals) with machine learning-based affective computing. We evaluate ARM in a user study, demonstrating improved user engagement, emotional understanding, and overall interaction quality. Our findings have implications for the design of empathetic and socially aware ECAs in various applications, such as virtual coaching, mental health support, and customer service.