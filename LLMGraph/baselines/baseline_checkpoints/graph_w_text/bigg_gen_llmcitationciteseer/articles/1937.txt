Graph neural networks (GNNs) have shown remarkable success in modeling complex graph-structured data. However, their computational complexity grows exponentially with the graph size, making them inefficient for large-scale graphs. This paper introduces a novel attention mechanism, 'HierAtt', designed specifically for hierarchical GNNs. HierAtt adaptively allocates attention weights based on graph topology and node features, reducing computational overhead by up to 70%. We demonstrate the efficacy of HierAtt on several benchmark datasets, achieving state-of-the-art results on graph classification tasks while significantly improving inference speed.