Autonomous drones operating in uncertain environments require efficient and adaptive navigation strategies. We propose a novel reinforcement learning framework that leverages a hierarchical policy architecture to balance exploration and exploitation. Our approach, 'Uncertainty-Aware Drone Navigation' (UADN), incorporates a probabilistic uncertainty model to estimate the confidence of sensor readings, enabling the drone to adapt to changing environmental conditions. Experimental results demonstrate that UADN outperforms existing methods in terms of navigation efficiency and reliability in simulations and real-world experiments.