Explainable AI is crucial for building trust in human-AI interactions. This paper presents a novel Hierarchical Attention Network (HAN) architecture for multi-modal dialogue systems that generates interpretable explanations for its responses. Our approach leverages attention mechanisms to identify salient input features and dialogue history, and incorporates a hierarchical fusion layer to combine modalities. We evaluate our model on a benchmark dataset and demonstrate improved explanation quality and dialogue coherence compared to state-of-the-art methods. Our approach has implications for developing more transparent and user-friendly conversational AI systems.