Multimodal emotion recognition is a crucial aspect of human-computer interaction, as it enables computers to understand and respond to human emotions more effectively. This paper proposes a hierarchical attention network (HAN) that integrates facial expression, speech, and physiological signals to recognize emotions. Our HAN model consists of three stages: modality-specific attention, multimodal fusion, and hierarchical attention. We evaluate our approach on the SEMAINE dataset and show that it outperforms state-of-the-art methods in recognizing emotions from multimodal data. Furthermore, we demonstrate the robustness of our model to variations in lighting conditions and individual differences.