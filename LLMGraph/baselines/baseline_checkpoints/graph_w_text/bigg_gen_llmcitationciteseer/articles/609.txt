Multimodal human-computer interaction (HCI) systems rely on accurate gesture recognition to provide seamless user experiences. This paper presents a novel hierarchical Bayesian network (HBN) framework for gesture recognition, which models the complex relationships between various sensor modalities. Our approach leverages the strengths of both model-based and data-driven techniques, achieving improved recognition accuracy and robustness in diverse real-world scenarios. We evaluate our HBN framework using a comprehensive dataset of gestures and demonstrate its effectiveness in reducing errors and improving system responsiveness.