Affective state recognition from human-computer interaction (HCI) is crucial for developing empathetic and personalized systems. This paper presents EmoReact, a novel multimodal framework that recognizes users' emotional states from their interactions with computers. EmoReact combines facial expression analysis, speech pattern recognition, and physiological signal processing to detect emotions. We conducted a user study with 50 participants, showing that EmoReact achieves an overall accuracy of 87.2% in recognizing six basic emotions. The proposed framework has implications for affective computing, human-centered AI, and HCI applications.