Virtual reality (VR) environments have the potential to revolutionize human-computer interaction. However, current VR systems rely on cumbersome controllers or invasive eye-tracking devices. This paper proposes a novel gaze-based interaction system, 'GazeVR', which leverages deep learning to infer user intentions from gaze patterns. We introduce a multi-task CNN architecture that jointly predicts gaze direction, fixation points, and user actions. Our experiments demonstrate that GazeVR achieves high accuracy and low latency, enabling seamless interaction in VR environments. We also explore the applicability of GazeVR in various scenarios, including gaming, education, and healthcare.