Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the complexity and variability of human emotions. This paper proposes a novel hierarchical attention network (HAN) architecture that learns to selectively focus on relevant modalities and features for improved emotion recognition. Our approach employs a multi-level attention mechanism, combining modality-aware and feature-aware attention to capture both inter-modal and intra-modal relationships. Experimental results on the IEMOCAP and CMU-MOSEI datasets demonstrate the effectiveness of our HAN model, achieving state-of-the-art performance and outperforming existing methods by up to 10% in terms of accuracy and F1-score.