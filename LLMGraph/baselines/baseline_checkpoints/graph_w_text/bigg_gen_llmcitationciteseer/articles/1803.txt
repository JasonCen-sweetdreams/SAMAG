Graph neural networks (GNNs) have achieved state-of-the-art performance in node classification tasks, but often fail to capture hierarchical relationships between nodes. We propose a novel hierarchical attention-based GNN framework, 'HAGNN', which leverages multi-scale attention mechanisms to model complex dependencies between nodes at different granularities. Our experiments on several benchmark datasets demonstrate that HAGNN outperforms existing GNN architectures, especially in scenarios with sparse node labels or complex graph structures.