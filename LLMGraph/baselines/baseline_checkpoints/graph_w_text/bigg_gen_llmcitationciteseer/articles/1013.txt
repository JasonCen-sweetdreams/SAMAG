Online learning algorithms are often plagued by concept drift, noisy data, and class imbalance, leading to degraded performance over time. This paper proposes a novel online learning framework, 'AdaReg', which adaptively regularizes the model to mitigate these issues. We introduce a dynamic regularization scheme that incorporates uncertainty estimates and class balancing techniques to improve robustness and fairness. Experimental results on several real-world datasets demonstrate that AdaReg achieves state-of-the-art performance on imbalanced classification tasks, while being computationally efficient and scalable to large datasets.