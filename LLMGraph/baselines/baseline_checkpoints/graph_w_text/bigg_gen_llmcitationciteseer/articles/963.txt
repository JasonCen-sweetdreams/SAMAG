Emotion recognition from multimodal data has numerous applications in human-computer interaction, healthcare, and affective computing. This paper proposes a novel hierarchical attention network (HAN) for real-time multimodal emotion recognition. Our approach leverages modality-specific attention mechanisms to selectively focus on relevant input features, and a hierarchical fusion strategy to integrate information across modalities. Experimental results on a benchmark dataset demonstrate that HAN outperforms state-of-the-art methods in recognizing emotions from audio, video, and physiological signals, achieving an accuracy of 92.5% in real-time settings.