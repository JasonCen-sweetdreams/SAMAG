Virtual reality (VR) has the potential to revolutionize accessibility for individuals with disabilities, but existing interaction methods can be limiting. This paper presents a novel gaze-based multimodal interaction framework, 'GazeVR', which leverages eye-tracking, voice commands, and hand gestures to enable more intuitive and inclusive user experiences in VR. We evaluate GazeVR using a user study with participants with and without disabilities, demonstrating significant improvements in task completion time, accuracy, and user satisfaction. Our approach has implications for enhancing accessibility in various VR applications, including education, healthcare, and entertainment.