Multimodal reasoning in visual question answering (VQA) requires models to integrate information from both visual and language modalities. While recent advances in transformer-based architectures have improved performance, they often lack interpretability. This paper proposes a Hierarchical Attention Network (HAN) that incorporates multimodal attention mechanisms to selectively focus on relevant regions in images and words in questions. We demonstrate that HAN improves VQA performance on benchmark datasets while providing explainable attention maps that reveal the model's decision-making process. Our results show that HAN outperforms state-of-the-art models on complex VQA tasks, such as counting and spatial reasoning.