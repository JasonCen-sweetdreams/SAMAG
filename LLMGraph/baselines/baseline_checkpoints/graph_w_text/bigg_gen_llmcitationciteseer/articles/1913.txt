Deep neural networks are often vulnerable to exploding gradients during training, leading to poor convergence or divergence. This paper introduces a novel adaptive gradient clipping (AGC) technique that dynamically adjusts the clipping threshold based on the gradient norm distribution. Our approach significantly improves training stability and robustness, while also reducing the computational overhead associated with traditional gradient clipping methods. Experimental results on various benchmark datasets demonstrate the effectiveness of AGC in achieving faster convergence and better generalization performance.