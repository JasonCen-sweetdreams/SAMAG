Multimodal sentiment analysis (MSA) is a challenging task that involves analyzing sentiment from heterogeneous data sources such as text, images, and audio. This paper proposes a novel hierarchical attention network (HAN) that integrates uncertainty estimation to improve the robustness of MSA models. Our HAN model consists of modality-specific attention layers that learn to weight the importance of different modalities, followed by a hierarchical fusion layer that aggregates the modality-specific features. We further incorporate Bayesian neural networks to estimate the uncertainty of the sentiment predictions, enabling the model to provide confidence scores for its predictions. Experimental results on the CMU-MOSI dataset demonstrate that our approach outperforms state-of-the-art MSA models in terms of accuracy and robustness.