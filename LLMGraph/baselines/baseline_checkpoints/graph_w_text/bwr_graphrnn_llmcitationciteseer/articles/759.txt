Deep neural networks have achieved state-of-the-art performance in various machine learning tasks, but their hyperparameter tuning remains a challenging and time-consuming process. This paper proposes a novel Bayesian optimization method, 'BO-Hyper', which leverages a probabilistic surrogate model to efficiently explore the hyperparameter space. We introduce a new acquisition function that balances exploration and exploitation, and demonstrate its effectiveness in tuning hyperparameters for various deep learning architectures. Experimental results on several benchmark datasets show that BO-Hyper outperforms existing methods, achieving better model performance with significantly reduced computational cost.