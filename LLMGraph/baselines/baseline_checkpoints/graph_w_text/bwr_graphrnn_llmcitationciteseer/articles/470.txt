Emotion recognition from multi-modal data, such as speech, text, and vision, is a challenging task due to the complexity of human emotions and the need to integrate heterogeneous data sources. This paper proposes a hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features to improve emotion recognition accuracy. Our HAN model incorporates a novel attention mechanism that adaptively weights modalities based on their emotional relevance, leading to improved performance and interpretability. Experimental results on three benchmark datasets demonstrate the effectiveness of our approach in recognizing emotions from multi-modal data.