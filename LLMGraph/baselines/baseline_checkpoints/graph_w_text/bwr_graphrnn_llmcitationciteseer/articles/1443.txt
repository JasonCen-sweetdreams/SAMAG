Sentiment analysis is a crucial task in natural language processing, but existing methods often struggle to effectively integrate multimodal data. This paper proposes a novel hierarchical attention network (HAN) architecture that captures complex relationships between text, image, and audio features. Our approach employs a multi-level attention mechanism that selectively focuses on relevant modalities and learns to weigh their importance adaptively. Experiments on a large-scale dataset demonstrate that our HAN model outperforms state-of-the-art baselines in terms of accuracy and inference speed, making it suitable for real-world applications.