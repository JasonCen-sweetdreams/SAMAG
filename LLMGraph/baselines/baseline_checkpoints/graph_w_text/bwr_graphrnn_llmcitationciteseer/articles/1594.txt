Deep reinforcement learning (DRL) has achieved remarkable success in complex environments, but the lack of transparency in decision-making processes hinders trust and interpretability. This paper proposes a novel hierarchical explainability framework for DRL, enabling the dissection of complex policies into comprehensible, modular components. We introduce a hierarchical attention mechanism that highlights critical state features and action influences, facilitating the identification of causal relationships. Experimental results on Atari games and robotic manipulation tasks demonstrate the effectiveness of our approach in improving policy understanding and facilitating error analysis.