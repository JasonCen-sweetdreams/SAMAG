Neural information retrieval (NeuIR) models have shown promising results in various retrieval tasks. However, learning effective document representations remains a challenging task. This paper proposes a novel contrastive learning approach, 'CLR-DR', for learning document representations. CLR-DR leverages large-scale unlabeled text data to learn robust document embeddings that capture semantic relationships between documents. Experimental results on several benchmark datasets demonstrate that CLR-DR outperforms existing state-of-the-art NeuIR models in terms of retrieval accuracy and efficiency.