In multi-agent systems, coordinating agents to allocate resources efficiently is a challenging problem. This paper proposes a hierarchical reinforcement learning (HRL) framework that enables agents to learn cooperative policies for real-time resource allocation. Our approach consists of two layers: a high-level layer that uses a decentralized actor-critic method to learn resource allocation policies, and a low-level layer that employs a model-free deep Q-network to adapt to changing environmental conditions. We evaluate our approach on a realistic grid management scenario and demonstrate significant improvements in resource utilization and allocation efficiency compared to traditional decentralized approaches.