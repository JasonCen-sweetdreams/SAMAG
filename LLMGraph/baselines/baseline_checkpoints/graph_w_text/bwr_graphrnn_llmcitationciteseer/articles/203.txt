This paper presents a decentralized task allocation framework for multi-agent systems, where agents learn to allocate tasks autonomously using deep reinforcement learning. Our approach, called 'Dec-TARL', utilizes a decentralized actor-critic architecture, where each agent maintains its own policy and value functions, and communicates with its neighbors to achieve collective task allocation. We evaluate Dec-TARL in a simulated disaster response scenario, demonstrating improved task allocation efficiency and adaptability compared to traditional centralized approaches. Our results show that Dec-TARL can effectively handle dynamic task arrivals, agent failures, and communication constraints, making it a promising solution for real-world multi-agent systems.