Open-domain question answering requires efficiently retrieving relevant passages from a large corpus. This paper introduces Hierarchical Document Embeddings (HiDE), a novel neural retrieval model that learns to represent documents in a hierarchical fashion, capturing both local and global semantic structures. HiDE outperforms state-of-the-art dense retrieval models on several benchmark datasets, achieving a 15% improvement in retrieval accuracy while reducing computational cost by 30%. We also demonstrate the effectiveness of HiDE in an end-to-end question answering pipeline, showcasing its potential for real-world applications.