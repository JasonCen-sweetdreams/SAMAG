Eye movement simulation is crucial for realistic virtual reality (VR) experiences. Existing methods rely on simplistic heuristics or labor-intensive data collection. We propose EyeTrackerGAN, a generative adversarial network (GAN) that learns to simulate eye movements in VR from a small set of user interactions. Our GAN consists of a generator network that produces eye movement patterns and a discriminator network that evaluates their realism. Experimental results on a public VR dataset show that EyeTrackerGAN outperforms state-of-the-art methods in terms of fixation accuracy and visual attention metrics, enabling more immersive and interactive VR experiences.