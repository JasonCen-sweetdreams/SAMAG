Learning effective embeddings from multi-modal data (e.g., images, text, audio) is crucial for various AI applications. However, existing methods often rely on computationally expensive fusion strategies or suffer from modality imbalance. We propose HAtten, a hierarchical attention network that adaptively integrates modality-specific features using a novel, self-supervised attention mechanism. HAtten learns to focus on relevant modalities and capture complex relationships between them, resulting in improved performance on various downstream tasks. Experimental results on several benchmarks demonstrate the efficiency and effectiveness of HAtten compared to state-of-the-art multi-modal embedding methods.