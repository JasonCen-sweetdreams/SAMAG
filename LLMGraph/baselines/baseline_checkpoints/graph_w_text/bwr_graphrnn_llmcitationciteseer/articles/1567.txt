Multi-task learning on knowledge graphs has gained popularity with the emergence of large-scale datasets. However, existing methods suffer from high computational costs and poor scalability. In this paper, we propose a novel framework, 'HATKE', which leverages hierarchical attention mechanisms to selectively focus on relevant graph regions for each task. Our approach reduces the number of model parameters and accelerates training by learning shared, task-agnostic graph embeddings. Experimental results on multiple benchmark datasets demonstrate that HATKE outperforms state-of-the-art methods in terms of both efficiency and accuracy.