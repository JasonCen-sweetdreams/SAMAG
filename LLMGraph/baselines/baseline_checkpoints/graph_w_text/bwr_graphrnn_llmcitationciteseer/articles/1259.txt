Voice assistants have become ubiquitous in daily life, but their interactions can inadvertently perpetuate cognitive biases, exacerbating social inequalities. This paper presents a novel, inclusive design framework for voice assistants that incorporates multimodal interfaces and adaptive feedback mechanisms to detect and mitigate biases. Our approach leverages machine learning-driven sentiment analysis and user modeling to recognize bias-prone interactions, and responds with contextually appropriate, bias-reducing interventions. A user study with 120 participants demonstrates that our framework significantly increases user awareness of biases and promotes more equitable interactions.