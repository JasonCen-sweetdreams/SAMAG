The growing demand for AI applications on resource-constrained devices necessitates the development of efficient deep learning models. This paper proposes a novel sparse attention mechanism, 'SparseAtt', which reduces computational overhead by selectively allocating attention weights to input features. We employ a learnable thresholding function to dynamically prune attention weights, achieving significant memory and computational savings. Experimental results on benchmark datasets demonstrate that SparseAtt outperforms state-of-the-art efficient attention mechanisms while maintaining comparable accuracy to standard attention-based models.