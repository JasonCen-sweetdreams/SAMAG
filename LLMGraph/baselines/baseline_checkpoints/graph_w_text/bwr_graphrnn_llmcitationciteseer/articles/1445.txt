Emotion recognition from multi-modal inputs (e.g., speech, text, vision) is a crucial task in human-computer interaction. While deep neural networks have achieved state-of-the-art performance, they often lack interpretability. This paper proposes a Hierarchical Attention Network (HAN) that integrates multi-modal features and provides explainable emotion recognition. Our HAN model uses attention mechanisms to selectively focus on relevant features and modalities, enabling the identification of important contributors to emotion detection. Experimental results on the IEMOCAP dataset demonstrate that our approach outperforms existing methods in terms of recognition accuracy and provides meaningful insights into the decision-making process.