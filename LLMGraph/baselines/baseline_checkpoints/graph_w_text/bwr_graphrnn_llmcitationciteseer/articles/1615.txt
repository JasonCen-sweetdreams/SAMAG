Few-shot learning has gained significant attention in recent years, but existing approaches often struggle to scale to large, complex datasets. This paper presents a novel hierarchical graph attention network (HGAT) architecture that leverages graph neural networks and attention mechanisms to learn robust, hierarchically-organized representations for few-shot classification tasks. We demonstrate the effectiveness of HGAT on several benchmark datasets, achieving state-of-the-art performance in various few-shot learning scenarios, including cross-domain and cross-task settings.