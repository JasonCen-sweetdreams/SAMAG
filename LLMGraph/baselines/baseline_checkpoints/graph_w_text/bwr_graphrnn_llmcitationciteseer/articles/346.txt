Virtual assistants have become an integral part of modern life, but their usability is often limited for individuals with disabilities. This paper presents a novel multimodal framework for designing inclusive virtual assistants that can adapt to the diverse needs of users with disabilities. We propose a hybrid approach combining speech, gesture, and eye-tracking inputs to enhance accessibility. Our user study with 30 participants with disabilities demonstrates significant improvements in task completion rates and user satisfaction compared to traditional text-based interfaces. We also discuss the implications of our findings for future HCI research and the development of more inclusive AI systems.