Visual question answering (VQA) models typically rely on complex neural network architectures to integrate visual and linguistic information. This paper introduces a novel hierarchical attention network (HAN) that efficiently fuses multi-modal inputs to improve VQA performance. Our approach leverages a stack of attention modules to selectively focus on relevant image regions and question words, enabling more accurate and interpretable question answering. We evaluate our HAN model on the VQA 2.0 dataset and demonstrate significant improvements in accuracy and inference speed compared to state-of-the-art baselines.