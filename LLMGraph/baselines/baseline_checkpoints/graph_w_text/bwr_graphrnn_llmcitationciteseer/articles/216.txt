Reinforcement learning (RL) agents often struggle to provide insights into their decision-making processes, hindering their adoption in high-stakes applications. This paper proposes a novel approach to explainable RL, 'GraphX', which leverages graph-based temporal abstraction to identify and visualize key state transitions and their relationships. By learning a hierarchical graph representation of the environment, GraphX enables agents to justify their actions by tracing them back to underlying causal factors. We demonstrate the effectiveness of GraphX in a range of Atari games and a real-world robotics task, showcasing its potential to increase trust and understanding in RL systems.