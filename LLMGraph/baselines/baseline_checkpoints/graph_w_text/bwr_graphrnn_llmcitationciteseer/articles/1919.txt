Voice assistants have become ubiquitous in modern life, but their speech recognition systems often perpetuate cultural biases, leading to inaccurate or unfair interactions with users from diverse linguistic and cultural backgrounds. This paper presents an empirical study investigating the cultural bias in three popular voice assistants, identifying significant disparities in recognition rates for accents and dialects from underrepresented groups. We propose a framework for designing inclusive voice assistants, incorporating culturally sensitive training data and adaptive language models that can better serve diverse user populations.