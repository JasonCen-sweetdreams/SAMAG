In multi-agent systems, coordinated exploration is crucial for efficient learning and optimal decision-making. This paper proposes a novel deep reinforcement learning framework, 'CoEx', that enables agents to learn coordinated exploration strategies in complex, partially observable environments. Our approach integrates deep Q-networks with a novel attention mechanism that facilitates communication and coordination among agents. Experimental results on a suite of multi-agent benchmarks demonstrate that CoEx significantly outperforms existing methods in terms of exploration efficiency and task completion rates.