This paper presents a novel Hierarchical Attention Network (HAN) architecture for multi-agent reinforcement learning, which tackles the problem of scalable and efficient decision-making in complex environments. Our HAN framework consists of two levels of attention: intra-agent attention for individual policy learning and inter-agent attention for cooperative behavior modeling. Experimental results on a variety of multi-agent benchmarks demonstrate that our approach outperforms state-of-the-art methods in terms of learning speed, policy adaptation, and overall task performance.