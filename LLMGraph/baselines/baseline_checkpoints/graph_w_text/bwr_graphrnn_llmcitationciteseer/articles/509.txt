Neural architecture search (NAS) has become a crucial component in designing efficient deep learning models. In this paper, we propose a novel graph-based reinforcement learning approach, dubbed 'GraphNAS', which leverages graph neural networks (GNNs) to efficiently explore the vast architecture space. By representing neural architectures as graphs, we can effectively capture their structural relationships and identify promising candidates using a reinforcement learning policy. Experimental results on several benchmark datasets demonstrate that GraphNAS achieves state-of-the-art performance while reducing the search cost by an order of magnitude compared to existing NAS methods.