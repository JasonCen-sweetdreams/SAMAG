Deep neural networks (DNNs) have shown remarkable success in various machine learning tasks, but their vulnerability to adversarial attacks raises concerns about their reliability. This paper investigates the robustness of DNNs against gradient-based attacks, including Projected Gradient Descent (PGD) and Fast Gradient Sign Method (FGSM). We propose a novel framework for evaluating the robustness of DNNs by analyzing the gradient distribution of the input data. Our experiments on the CIFAR-10 dataset demonstrate that our method can effectively identify the most vulnerable DNN architectures and provide insights into the design of more robust models.