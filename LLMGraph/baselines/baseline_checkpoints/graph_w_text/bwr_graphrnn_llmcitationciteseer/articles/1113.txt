Emotion recognition is a crucial aspect of human-computer interaction (HCI), enabling systems to respond empathetically to users. This paper presents EmoReact, a novel multimodal framework that integrates facial expression, speech, and physiological signals to recognize emotions in real-time. We propose a hierarchical fusion approach that leverages the strengths of each modality, achieving an average F1-score of 0.92 on a benchmark dataset. EmoReact is designed to be adaptable to various HCI applications, including affective computing, social robots, and virtual assistants.