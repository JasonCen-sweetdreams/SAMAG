This paper investigates the problem of task decomposition in multi-agent reinforcement learning (MARL) settings with stochastic environments. We propose a hierarchical task decomposition framework, 'HTD-MARL', which enables agents to learn complex tasks by recursively breaking them down into simpler sub-tasks. Our approach leverages a novel hierarchical graph neural network to model the task dependencies and agent interactions. Experimental results on a suite of challenging MARL benchmarks demonstrate that HTD-MARL achieves significant improvements in task completion rates and overall system efficiency compared to existing MARL methods.