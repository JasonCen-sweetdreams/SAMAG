Neural information retrieval (IR) models have shown promising results in document ranking tasks. However, they often rely on dense document embeddings, which can lead to scalability issues and slow query response times. This paper presents a novel approach to generate query-adaptive document embeddings, termed 'QADE', which dynamically adjusts the embedding space based on the query context. Our method leverages a graph attention mechanism to selectively highlight relevant document regions, enabling faster and more accurate retrieval. Experimental results on the TREC CAR dataset demonstrate that QADE outperforms state-of-the-art neural IR models while reducing the average query latency by 30%