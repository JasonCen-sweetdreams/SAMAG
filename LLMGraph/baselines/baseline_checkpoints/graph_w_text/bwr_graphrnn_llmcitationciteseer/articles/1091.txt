Virtual reality (VR) technology has the potential to revolutionize various aspects of life, but existing navigation methods can be inaccessible to people with disabilities. This paper presents a novel gaze-based interface, 'GazeNav', which enables users to navigate virtual environments using only their eye movements. We propose a machine learning model that predicts the user's intended navigation direction based on their gaze patterns and pupil dilation. Our user study with 20 participants shows that GazeNav outperforms traditional joystick-based navigation in terms of accuracy and user satisfaction, and is particularly beneficial for individuals with mobility impairments.