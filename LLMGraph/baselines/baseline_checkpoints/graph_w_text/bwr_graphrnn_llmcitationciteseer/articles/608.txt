In multi-agent systems, efficient task allocation is crucial for optimal performance. This paper presents a novel approach to coordinated exploration, where agents adaptively allocate tasks based on their individual capabilities and environmental uncertainty. We introduce a decentralized algorithm that leverages reinforcement learning to learn a probabilistic task allocation policy, which is updated through online learning and coordination with neighboring agents. Experimental results in a simulated robotic search and rescue scenario demonstrate improved task completion rates and reduced communication overhead compared to state-of-the-art decentralized allocation methods.