Urban traffic management is a complex problem that requires coordinating multiple autonomous agents to optimize traffic flow and reduce congestion. This paper proposes a hierarchical reinforcement learning framework that enables agents to learn both high-level traffic management strategies and low-level control policies. Our approach leverages a decentralized actor-critic architecture, where each agent learns to optimize its local traffic signal control while coordinating with neighboring agents to achieve global objectives. Experimental results on a realistic traffic simulation platform demonstrate that our approach improves traffic flow by up to 25% compared to traditional traffic signal control methods.