Multi-agent dialogue systems require effective reasoning mechanisms to generate coherent and context-dependent responses. This paper proposes a novel hierarchical attention-based framework, 'HARE', which integrates symbolic reasoning with neural networks to provide interpretable explanations for dialogue decisions. We introduce a multi-level attention mechanism that selectively focuses on relevant dialogue history, agent roles, and context features to infer implicit intentions and goals. Experimental results on a challenging dialogue benchmark demonstrate that HARE outperforms state-of-the-art baselines in terms of response accuracy and explanation quality, while enabling transparent and trustworthy decision-making in human-agent interactions.