 Embodied cognition posits that the mind is shaped by sensorimotor experiences. Virtual reality (VR) offers a unique platform to explore this concept, but current systems lack a comprehensive understanding of multimodal fusion. We propose a novel framework, 'EmboVR', which integrates visual, auditory, and haptic cues to create a more immersive and embodied experience. Our system leverages machine learning to predict user intentions and adapt the simulation accordingly. A user study demonstrates improved cognitive performance and presence in VR compared to traditional unimodal approaches.