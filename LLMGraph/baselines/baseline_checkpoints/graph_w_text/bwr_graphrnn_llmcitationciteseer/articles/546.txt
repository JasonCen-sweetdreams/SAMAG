Multimodal sentiment analysis (MSA) aims to jointly analyze text, image, and audio cues to determine the sentiment of a message. Existing MSA approaches often rely on early fusion or simple concatenation of modalities, neglecting the complex interactions between them. This paper proposes a novel hierarchical attention framework, 'MHAtt', which captures intra- and inter-modal relationships through recursive attention mechanisms. We evaluate MHAtt on three MSA benchmarks, achieving state-of-the-art performance and showcasing its robustness to modality imbalance and noisy data.