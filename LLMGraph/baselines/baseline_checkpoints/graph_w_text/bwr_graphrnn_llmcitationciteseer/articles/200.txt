Augmented reality (AR) interfaces often rely on manual input, which can be cumbersome and limiting. This paper presents EyeGazeLens, a novel gaze-based interface for AR that leverages deep learning to accurately infer user intent. Our approach uses a convolutional neural network (CNN) to analyze eye movement patterns and identify fixation points, enabling users to interact with virtual objects in 3D space. We evaluate EyeGazeLens using a custom-built AR platform and demonstrate improved interaction time and accuracy compared to traditional input methods. The implications of this work are significant, enabling more natural and intuitive human-computer interaction in AR environments.