Emotion recognition from multi-modal inputs has become increasingly important in human-computer interaction. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that integrates facial expressions, speech, and physiological signals to recognize emotions. We introduce a hierarchical attention mechanism that selectively focuses on relevant modalities and time segments, enabling the network to capture complex emotional patterns. Experimental results on the RECOLA dataset demonstrate that our approach outperforms state-of-the-art methods in terms of recognition accuracy and robustness to noisy inputs.