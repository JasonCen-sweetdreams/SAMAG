Bayesian neural networks (BNNs) offer a promising approach to uncertainty estimation in deep learning, but their high computational requirements hinder deployment in resource-constrained environments. We propose a novel pruning method, 'BayesDistil', which leverages knowledge distillation to reduce the complexity of BNNs while preserving their uncertainty estimation capabilities. Our approach involves training a compact, deterministic student network to mimic the predictive distribution of the original BNN, resulting in significant reductions in model size and inference time. Experiments on various image classification benchmarks demonstrate the efficacy of BayesDistil in maintaining accuracy and uncertainty calibration while achieving substantial speedups.