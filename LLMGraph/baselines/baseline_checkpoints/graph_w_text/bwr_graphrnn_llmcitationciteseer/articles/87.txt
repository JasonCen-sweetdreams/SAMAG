Time-series forecasting is a crucial task in various domains, but the lack of transparency in modern deep learning models hinders their adoption. This paper proposes a novel Hierarchical Attention Network (HAN) architecture that leverages both local and global temporal dependencies to improve forecasting accuracy. The key innovation is the incorporation of attention weights as a means to provide interpretable insights into the model's decision-making process. We evaluate HAN on several real-world datasets, demonstrating significant improvements over state-of-the-art methods while facilitating model explainability through visualized attention patterns.