Dialogue state tracking (DST) is a crucial component in task-oriented dialogue systems. While neural networks have achieved state-of-the-art performance, their lack of interpretability hinders trust and reliability. This paper proposes a hierarchical attention network (HAN) for DST, which incorporates both utterance-level and turn-level attention mechanisms. We introduce a novel hierarchical encoding scheme that captures the compositional structure of dialogue utterances, enabling the model to focus on relevant context when updating the dialogue state. Experimental results on the MultiWOZ dataset demonstrate that our HAN model outperforms existing DST systems while providing transparent and explainable state tracking.