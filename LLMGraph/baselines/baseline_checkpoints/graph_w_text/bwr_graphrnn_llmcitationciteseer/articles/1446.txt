Sentiment analysis is a crucial task in natural language processing, but it remains challenging when dealing with multi-modal data (e.g., text, images, and videos). This paper proposes a novel Hierarchical Attention Network (HAN) architecture to efficiently integrate information from multiple modalities. Our approach leverages a weighted attention mechanism to adaptively focus on relevant modalities and features, reducing the computational overhead. Experimental results on a large-scale multi-modal dataset demonstrate that our HAN model outperforms state-of-the-art methods in terms of accuracy and efficiency.