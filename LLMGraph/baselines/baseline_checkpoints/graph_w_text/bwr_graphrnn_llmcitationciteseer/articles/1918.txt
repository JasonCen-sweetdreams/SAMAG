Deep reinforcement learning (DRL) has revolutionized autonomous vehicle control, but the lack of transparency in decision-making processes hinders trust and reliability. This paper presents a novel explainability framework, 'RL-Explain', which leverages model-agnostic techniques to provide interpretable insights into DRL policies. We introduce a hierarchical attention mechanism that selectively focuses on relevant state features, enabling the generation of human-understandable explanations for vehicle control decisions. Experimental results on a simulated autonomous driving environment demonstrate the effectiveness of RL-Explain in improving safety, transparency, and trust in DRL-based autonomous vehicles.