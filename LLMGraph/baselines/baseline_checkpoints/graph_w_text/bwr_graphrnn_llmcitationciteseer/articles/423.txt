Deep reinforcement learning (DRL) has achieved remarkable success in various applications, but its vulnerability to adversarial attacks raises concerns about its reliability. In this paper, we propose a novel approach, 'AdvRLShield', which leverages adversarial training to robustify DRL policies against targeted attacks. We introduce a multi-objective optimization framework that balances policy performance and attack resilience. Experimental results on popular DRL benchmarks demonstrate that AdvRLShield significantly improves the robustness of DRL policies against various types of adversarial attacks, including observation and action perturbations.