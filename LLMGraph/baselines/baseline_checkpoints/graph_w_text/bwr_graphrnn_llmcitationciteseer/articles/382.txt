This paper introduces a hierarchical reinforcement learning framework for coordinating multiple autonomous vehicles in complex urban scenarios. We leverage a two-level hierarchy, where a high-level policy learns to allocate tasks and resources to individual vehicles, while low-level policies control the vehicles' trajectories. Our approach incorporates domain knowledge and safety constraints, ensuring efficient and safe coordination. We demonstrate the effectiveness of our method in a simulated environment, achieving improved throughput and reduced congestion compared to existing decentralized approaches.