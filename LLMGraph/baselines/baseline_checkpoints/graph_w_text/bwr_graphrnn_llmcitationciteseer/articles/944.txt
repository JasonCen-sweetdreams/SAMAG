Multimodal emotion recognition (MER) systems leverage audio, video, and text cues to analyze human emotions. However, existing approaches often rely on complex fusion strategies or ignore the hierarchical structure of emotions. We propose HAN-MER, a hierarchical attention network that selectively focuses on relevant modalities and emotional categories. Our model incorporates a novel attention mechanism that adaptively weights modality-specific features based on their contribution to emotion recognition. Experimental results on the IEMOCAP and CMU-MOSEI datasets demonstrate that HAN-MER outperforms state-of-the-art MER systems while reducing computational costs.