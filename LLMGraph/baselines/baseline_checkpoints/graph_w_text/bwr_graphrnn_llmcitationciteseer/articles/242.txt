Emotion recognition in human-computer interaction (HCI) is crucial for developing empathetic AI systems. Existing approaches often rely on single-modal inputs, neglecting the rich emotional cues present in multi-modal data. This paper proposes a Hierarchical Attention Network (HAN) that integrates facial expressions, speech, and physiological signals to recognize emotions in HCI. Our HAN model employs a novel attention mechanism that adaptively weights modalities based on their emotional relevance. Experimental results on a large-scale multimodal dataset demonstrate that our approach outperforms state-of-the-art single-modal and early-fusion methods, achieving a 12.5% improvement in emotion recognition accuracy.