Deep neural networks have achieved state-of-the-art performance in various computer vision tasks, but their lack of interpretability hinders their adoption in high-stakes applications. This paper proposes a novel multimodal attention mechanism that leverages visual and textual features to provide explainable predictions. Our approach, dubbed 'MMA-X', incorporates a hierarchical attention framework that selectively focuses on relevant input regions and attributes importance scores to input features. We evaluate MMA-X on several benchmark datasets and demonstrate significant improvements in model interpretability, while maintaining competitive performance.