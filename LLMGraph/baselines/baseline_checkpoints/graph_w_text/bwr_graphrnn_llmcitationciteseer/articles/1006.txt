Virtual reality (VR) systems often struggle to provide an optimal user experience due to the limitations of traditional input methods. This paper proposes an innovative eye movement-based adaptive interface (EMAI) that leverages gaze tracking and machine learning to dynamically adjust the VR environment according to the user's visual attention. Our approach integrates a convolutional neural network to classify eye movement patterns and predict user intentions, enabling the system to proactively offer personalized assistance and optimize interaction. Experimental results demonstrate significant improvements in user satisfaction, task completion time, and overall VR experience quality.