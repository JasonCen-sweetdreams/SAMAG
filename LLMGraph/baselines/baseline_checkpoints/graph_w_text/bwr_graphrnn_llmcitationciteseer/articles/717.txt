In-air hand tracking has numerous applications in Human-Computer Interaction, including virtual and augmented reality, gaming, and sign language recognition. However, existing gesture recognition systems suffer from high latency, limited robustness, and inadequate generalizability. This paper proposes a novel deep convolutional network (DCN) architecture, termed 'AirNet', which leverages a combination of spatial and temporal convolutional layers to recognize hand gestures from skeletal data. Our approach achieves state-of-the-art performance on the popular Hand Gesture dataset, with an average recognition accuracy of 95.2% and a latency of 30 ms. We also demonstrate the effectiveness of AirNet in a real-world virtual reality application.