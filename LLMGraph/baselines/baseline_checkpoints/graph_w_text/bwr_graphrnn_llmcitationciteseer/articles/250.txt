Ad-hoc search systems rely on effective query expansion to retrieve relevant documents. Recent neural retrieval models have shown promising results, but their query expansion mechanisms are often limited by the quality of the input queries. This paper proposes a novel optimization framework, 'QE-Opt', which leverages reinforcement learning to dynamically adapt query expansion strategies for neural retrieval models. Our approach incorporates a reward function that balances precision and recall, and we demonstrate significant improvements over state-of-the-art methods on several benchmark datasets. Experimental results show that QE-Opt can improve the mean average precision by up to 12% on the TREC-CAR dataset.