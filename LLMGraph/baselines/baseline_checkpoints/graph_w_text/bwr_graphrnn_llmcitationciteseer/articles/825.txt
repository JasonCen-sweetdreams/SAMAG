Neural Architecture Search (NAS) has emerged as a promising approach for automating the design of deep neural networks. However, existing methods often rely on expensive and time-consuming searches over large architecture spaces. This paper introduces a novel Bayesian hypernetwork-based NAS framework, 'BayesNAS', which leverages probabilistic modeling to efficiently explore the architecture space. By learning a distribution over hypernetwork weights, BayesNAS can generate high-performing architectures with significantly reduced computational costs. Experimental results on several benchmark datasets demonstrate the effectiveness of BayesNAS in discovering efficient and accurate neural architectures.