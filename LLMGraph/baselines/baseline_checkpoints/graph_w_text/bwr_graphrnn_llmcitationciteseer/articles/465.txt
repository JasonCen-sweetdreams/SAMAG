Individuals with motor impairments often rely on assistive devices that respond to explicit commands. However, these systems can be slow and laborious to use. We propose a novel approach to proactive assistive systems, which leverages gaze-based intention recognition to anticipate users' goals. Our method, 'GazeIntention', combines convolutional neural networks with graph-based models to analyze eye movement patterns and infer the user's intended action. Experimental results with 20 participants demonstrate that GazeIntention achieves an average recognition accuracy of 92.5%, outperforming state-of-the-art methods by 15.2%. We discuss the implications of our work for enhancing the quality of life for individuals with motor impairments.