Multimodal retrieval has gained significant attention in recent years, but existing methods often rely on traditional text-based query expansion techniques that neglect the semantic relationships between different modalities. This paper proposes a novel graph-based embedding approach that jointly models the textual and visual features of a query to generate a more comprehensive and effective expansion. Our method leverages a heterogeneous graph neural network to capture the intricate relationships between words, images, and their corresponding entities, resulting in a more accurate and robust query expansion. Experimental results on a large-scale multimodal dataset demonstrate the superior performance of our approach over state-of-the-art methods.