Emotion recognition in conversational agents is a crucial aspect of human-computer interaction. This paper proposes a novel hierarchical attention network (HAN) for multi-modal emotion recognition, which integrates linguistic, acoustic, and visual features from user interactions. Our HAN model consists of a feature-level attention mechanism to selectively weight modality-specific features and a conversation-level attention mechanism to capture contextual dependencies. Experimental results on a large-scale multimodal emotion dataset demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from conversations, achieving a 12.5% improvement in F1-score compared to the best baseline model.