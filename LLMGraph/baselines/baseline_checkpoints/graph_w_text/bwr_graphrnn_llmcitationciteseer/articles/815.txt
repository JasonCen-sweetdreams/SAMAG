Explainability is crucial for deploying multi-agent reinforcement learning (MARL) in real-world applications. We propose a novel Hierarchical Attention Network (HAN) framework that learns to identify relevant agents and their interactions, providing interpretable insights into MARL decision-making. HAN consists of two stages: a local attention module that highlights critical agents, and a global attention module that aggregates information across agents. We evaluate HAN on a variety of cooperative and competitive MARL environments, demonstrating improved explainability and performance compared to state-of-the-art MARL algorithms.