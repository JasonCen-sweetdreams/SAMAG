Deep reinforcement learning (DRL) has achieved remarkable success in various applications, but the lack of transparency in learned policies hinders trust and understanding. This paper presents a novel explainability framework, 'RL-Explain', which generates model-agnostic, interpretable explanations for DRL policies. We propose a combination of feature importance and attention-based techniques to identify key states, actions, and temporal dependencies driving policy decisions. Experimental results on several Atari games and a real-world robotics task demonstrate the effectiveness of RL-Explain in providing insightful explanations, improving policy understanding, and facilitating debugging.