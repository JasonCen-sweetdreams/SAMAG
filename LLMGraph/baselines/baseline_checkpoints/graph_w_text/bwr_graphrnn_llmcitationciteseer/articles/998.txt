Neural architecture search (NAS) has become a crucial step in deep learning model development. However, the search process can be computationally expensive and requires a significant amount of labeled data. This paper introduces a Bayesian meta-learning approach, dubbed 'BaNAS', which leverages knowledge from previous search tasks to efficiently explore the architecture space. By learning a probabilistic model of the search space, BaNAS can adapt to new tasks with limited data and computational resources. Our experiments on several benchmark datasets demonstrate that BaNAS outperforms state-of-the-art NAS methods in terms of search efficiency and final model performance.