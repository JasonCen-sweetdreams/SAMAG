Object detection remains a challenging task in computer vision, particularly in scenarios with multiple objects and varying scales. This paper proposes a novel transformer-based architecture, 'Hierarchical Attention Detector' (HAD), which leverages hierarchical attention mechanisms to selectively focus on relevant regions and scales. Our approach achieves state-of-the-art performance on the COCO dataset, outperforming existing detectors by 2.5% in terms of mean average precision. We also demonstrate the efficiency of HAD, which reduces computational cost by 30% compared to existing transformer-based detectors.