Emotion recognition is a crucial aspect of human-computer interaction, but existing approaches often rely on single modalities, such as facial expressions or speech. This paper presents a novel hierarchical attention network (HAN) that integrates multimodal inputs from facial, vocal, and textual cues to recognize emotions in real-time. Our HAN model employs a bottom-up attention mechanism to selectively focus on relevant modalities and features, achieving state-of-the-art performance on the Multimodal Emotion Recognition Challenge (MERC) dataset. We demonstrate the effectiveness of our approach in improving user experience and affective computing applications.