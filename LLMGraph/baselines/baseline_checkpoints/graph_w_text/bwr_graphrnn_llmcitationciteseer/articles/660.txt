Multimodal sentiment analysis in social media is a challenging task due to the complexity of integrating heterogeneous data sources. This paper proposes a novel hierarchical attention network (HAN) that learns to selectively focus on relevant modalities, such as text, images, and videos, to infer sentiment. Our approach leverages a stacked attention mechanism to model intra-modality and inter-modality relationships, enabling the capture of nuanced sentiment cues. Experimental results on a large-scale multimodal dataset demonstrate the effectiveness of our HAN architecture in improving sentiment analysis accuracy and robustness.