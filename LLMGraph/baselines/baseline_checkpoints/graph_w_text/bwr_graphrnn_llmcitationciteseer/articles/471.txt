Knowledge graph embeddings have revolutionized various AI applications, but existing methods struggle to capture hierarchical relations between entities. This paper introduces 'HREmbed', a novel framework that leverages graph attention mechanisms to learn entity and relation representations. HREmbed incorporates a hierarchical relation encoder, which models the semantic dependencies between relations at different levels of abstraction. Experimental results on benchmark datasets demonstrate that HREmbed outperforms state-of-the-art methods in link prediction, entity classification, and question answering tasks.