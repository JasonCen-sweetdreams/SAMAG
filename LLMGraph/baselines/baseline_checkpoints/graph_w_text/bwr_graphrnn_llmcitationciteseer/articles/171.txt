In dynamic environments, task allocation among agents is a challenging problem. This paper presents a novel approach that combines deep reinforcement learning with graph-based task allocation methods. Our algorithm, called 'ADAPT', uses a decentralized, asynchronous framework to enable agents to learn from their experiences and adapt to changing task requirements. Experimental results on a simulated disaster response scenario demonstrate that ADAPT outperforms traditional methods in terms of task completion efficiency and robustness to environmental uncertainty.