As AI models become increasingly ubiquitous, it is essential to develop robust defenses against adversarial attacks. In this paper, we propose a novel approach to detect adversarial attacks using graph-based attention mechanisms. Our method, dubbed 'GraphGuard', leverages the structural relationships between input features to identify suspicious patterns indicative of adversarial tampering. We demonstrate the efficacy of GraphGuard on a range of benchmark datasets, achieving state-of-the-art detection performance with improved explainability. Furthermore, we provide theoretical insights into the robustness of GraphGuard against adaptive attacks, laying the foundation for_future research in this area.