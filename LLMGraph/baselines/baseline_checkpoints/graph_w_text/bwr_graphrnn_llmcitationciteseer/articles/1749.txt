Deep reinforcement learning (DRL) has achieved remarkable successes in various applications, including game playing and robotics. However, the robustness of DRL policies against adversarial attacks remains a significant concern. This paper proposes a novel framework for enhancing the adversarial robustness of DRL policies via multi-agent games. We introduce a new type of adversary, called the 'robustness tester', which is trained to perturb the environment in a way that maximizes the policy's uncertainty. We show that our approach improves the robustness of DRL policies in various environments, including Atari games and robotic manipulation tasks.