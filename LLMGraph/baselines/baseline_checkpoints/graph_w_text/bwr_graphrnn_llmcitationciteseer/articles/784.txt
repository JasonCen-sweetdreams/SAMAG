Image retrieval systems often suffer from the semantic gap, where low-level visual features fail to capture high-level semantic concepts. This paper proposes a novel framework, 'Semantics-in-Embeddings' (SiE), which leverages deep learning-based embeddings to bridge this gap. SiE learns to represent images as dense vectors that encode both visual and semantic information, enabling more accurate and semantically meaningful image retrieval. Our experiments on several benchmark datasets demonstrate significant improvements in retrieval performance, with SiE outperforming state-of-the-art methods by up to 15% in terms of mean average precision.