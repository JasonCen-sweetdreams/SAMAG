Deep neural networks (DNNs) are vulnerable to adversarial attacks, which pose significant security threats in safety-critical applications. This paper proposes a novel approach to analyze the robustness of DNNs against adversarial attacks using orthogonal gradient alignment. We develop a theoretical framework to quantify the alignment between the gradient of the loss function and the gradient of the adversarial perturbation, and demonstrate its effectiveness in detecting and mitigating adversarial attacks. Experimental results on benchmark datasets show that our approach outperforms state-of-the-art defense methods in terms of robustness and efficiency.