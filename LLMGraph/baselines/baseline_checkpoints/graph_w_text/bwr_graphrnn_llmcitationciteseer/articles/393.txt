Traditional information retrieval (IR) systems rely on hand-crafted features and linear models, which struggle to capture complex semantic relationships between documents. This paper proposes a novel deep neural network architecture, 'SparseDoc', that learns sparse embeddings for efficient document retrieval. By leveraging a combination of convolutional and recurrent layers, SparseDoc captures both local and global patterns in document representations. Experimental results on several benchmark datasets demonstrate that SparseDoc outperforms state-of-the-art IR models in terms of retrieval accuracy and query latency, while reducing the dimensionality of document embeddings by up to 90%.