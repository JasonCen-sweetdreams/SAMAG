Neural architecture search (NAS) has emerged as a promising technique for automating the design of deep neural networks. However, most existing NAS methods suffer from high computational costs and limited scalability. This paper proposes a novel hierarchical reinforcement learning (HRL) framework for NAS, which leverages a two-level hierarchy of controllers to efficiently explore the architecture space. Our HRL-NAS approach achieves state-of-the-art performance on several benchmark datasets while reducing the search time by an order of magnitude compared to existing methods. We also provide a thorough analysis of the learned architectures and their transferability across different tasks.