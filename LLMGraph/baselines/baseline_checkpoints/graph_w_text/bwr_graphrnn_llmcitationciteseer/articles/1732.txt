Distributed information retrieval systems face the challenge of efficiently handling complex queries across a large number of nodes. This paper proposes a novel hierarchical caching approach that significantly reduces query latency and improves system scalability. Our method involves caching query results at multiple levels, including node-level, cluster-level, and global-level caches. We develop a probabilistic caching policy that adaptively adjusts cache sizes based on query patterns and node availability. Experimental results on a large-scale dataset demonstrate that our approach reduces average query latency by 37% compared to existing caching strategies.