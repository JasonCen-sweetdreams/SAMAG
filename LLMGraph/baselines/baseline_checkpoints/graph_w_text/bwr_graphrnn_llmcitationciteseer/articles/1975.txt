Dialogue state tracking is a crucial component of conversational AI systems, requiring the integration of multiple modalities such as speech, text, and vision. This paper introduces Hierarchical Graph Attention Networks (HGAT), a novel architecture that leverages graph attention mechanisms to model complex relationships between dialogue elements. HGAT learns to dynamically weight and combine modality-specific features, outperforming state-of-the-art models on the DSTC2 benchmark. We also propose a new evaluation metric, Modality-Aware F1, to better capture the nuances of multi-modal dialogue state tracking.