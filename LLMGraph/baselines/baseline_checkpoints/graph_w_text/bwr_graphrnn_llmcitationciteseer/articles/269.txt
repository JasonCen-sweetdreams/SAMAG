Multimodal sentiment analysis has gained significant attention in recent years, but existing approaches often rely on computationally expensive fusion methods or neglect modalities with limited data. This paper presents a novel hierarchical attention network (HAN) architecture that adaptively weighs and integrates features from text, image, and audio modalities. Our HAN model employs a two-stage attention mechanism, where modality-specific attention is first applied to extract relevant features, followed by a higher-level attention that selectively combines modalities based on their predictive power. Experimental results on three benchmark datasets demonstrate that our approach outperforms state-of-the-art methods while requiring significantly fewer computational resources.