Neural architecture search (NAS) has revolutionized the field of deep learning, but existing methods often suffer from high computational costs and limited scalability. This paper introduces Hierarchical Graph Attention NAS (HGA-NAS), a novel framework that leverages graph attention mechanisms to efficiently search for optimal architectures. By hierarchically modeling the architecture search space as a graph, HGA-NAS can capture complex relationships between different sub-structures and reduce the search cost by up to 3x compared to state-of-the-art methods. We demonstrate the effectiveness of HGA-NAS on several benchmark datasets, achieving competitive performance with significantly reduced computational resources.