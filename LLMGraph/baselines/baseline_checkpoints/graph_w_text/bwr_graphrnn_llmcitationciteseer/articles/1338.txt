Virtual reality (VR) systems often rely on manual input methods, which can be cumbersome and detract from the immersive experience. This paper presents EyeTracker, a novel gaze-based interface that leverages eye movement data to infer user intent and facilitate seamless interaction with virtual objects. Our approach employs a machine learning model to classify gaze patterns and detect fixation points, enabling users to select and manipulate objects with their eyes. We evaluate EyeTracker through a user study, demonstrating significant improvements in task completion time and user satisfaction compared to traditional input methods.