Virtual reality (VR) interfaces often require manual input, which can be cumbersome and detract from the immersive experience. This paper presents a novel gaze-based interaction system for VR, leveraging deep learning-based eye tracking to infer user intentions. We propose a convolutional neural network (CNN) architecture that accurately predicts gaze points from eye movement data, and integrate it with a VR platform to enable hands-free interaction. Our user study demonstrates significant improvements in interaction speed and accuracy compared to traditional manual input methods.