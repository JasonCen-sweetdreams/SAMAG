Deep reinforcement learning (DRL) has achieved remarkable success in various domains, but its vulnerability to adversarial attacks raises concerns about its deployment in safety-critical applications. This paper proposes a novel approach to enhance the robustness of DRL policies against adversarial perturbations. We introduce a constrained policy optimization framework that incorporates a robustness metric as a regularization term, encouraging the policy to be more resilient to perturbations. Experimental results on several benchmark environments demonstrate that our approach significantly improves the robustness of DRL policies while maintaining their performance.