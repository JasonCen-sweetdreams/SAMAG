In multi-agent systems, effective task allocation is crucial for achieving collective goals. However, the presence of untrustworthy agents can compromise system performance. This paper proposes a novel distributed task allocation framework that incorporates adaptive trust modeling to mitigate the impact of unreliable agents. Our approach leverages probabilistic graph theory to model agent trust and updates these models in real-time based on observed behavior. Experimental results on a simulated disaster response scenario demonstrate that our framework achieves significant improvements in task completion rates and reduces the influence of malicious agents compared to existing methods.