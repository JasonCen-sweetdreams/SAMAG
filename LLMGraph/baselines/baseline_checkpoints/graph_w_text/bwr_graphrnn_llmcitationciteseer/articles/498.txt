Conversational agents have emerged as a promising tool in healthcare, but their lack of emotional intelligence can lead to patient dissatisfaction and decreased trust. This paper presents a novel framework for designing emotionally intelligent conversational agents that can recognize and respond to users' emotions in a healthcare context. We propose a multimodal approach that combines natural language processing, computer vision, and affective computing to detect emotional cues. A user study with 100 participants demonstrates that our approach significantly improves user experience and perceived empathy in healthcare conversations.