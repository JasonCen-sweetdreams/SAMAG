Virtual reality (VR) applications often struggle to provide an immersive user experience due to the complexity of integrating multiple sensory inputs. This paper presents a novel approach to multimodal fusion that leverages gaze tracking to adaptively weight and combine visual, auditory, and haptic cues in real-time. Our system, called 'GazeFusion', uses a machine learning model to analyze the user's gaze patterns and dynamically adjust the modality fusion weights to optimize the user experience. We evaluate GazeFusion in a VR gaming scenario, demonstrating significant improvements in user engagement and task performance compared to traditional multimodal fusion approaches.