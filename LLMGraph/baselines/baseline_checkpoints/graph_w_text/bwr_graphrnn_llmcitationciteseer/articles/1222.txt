As Virtual Reality (VR) technology becomes increasingly prevalent, the need for intuitive and efficient interfaces grows. This paper presents a novel gesture-based interface framework, 'AdaptiGest', which leverages machine learning to adapt to individual users' gesture patterns and preferences in real-time. Our approach incorporates a hybrid sensor fusion model that combines data from hand tracking sensors, eye-tracking, and EEG to enable more accurate gesture recognition. We evaluate AdaptiGest through a user study, demonstrating significant improvements in user experience, reduced cognitive load, and enhanced overall system usability.