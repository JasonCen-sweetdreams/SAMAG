Virtual reality (VR) systems often rely on manual controllers for user input, limiting immersion and causing fatigue. We propose a novel gaze-based interaction method using deep recurrent neural networks (RNNs) to infer user intentions from eye movements. Our approach, 'Gaze2Act', leverages a combination of convolutional and recurrent layers to learn a mapping between gaze patterns and corresponding actions in VR. We evaluate Gaze2Act on a publicly available eye-tracking dataset and demonstrate significant improvements in interaction accuracy and user experience compared to state-of-the-art methods.