Transformer-based models have achieved state-of-the-art results in various natural language processing (NLP) tasks. However, their vulnerability to adversarial attacks raises concerns about their reliability in real-world applications. This paper investigates the robustness of transformer-based models to adversarial attacks, focusing on sentence classification and machine translation tasks. We propose a novel attack method, 'TFool', which generates perturbed input sequences that deceive the model while preserving semantic meaning. Experimental results show that TFool outperforms existing attacks in terms of success rate and transferability across different models and tasks. We also explore strategies for improving adversarial robustness, including data augmentation and adversarial training.