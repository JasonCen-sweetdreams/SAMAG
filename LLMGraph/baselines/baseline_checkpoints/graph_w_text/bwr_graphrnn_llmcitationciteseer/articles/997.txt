This paper addresses the problem of distributed task allocation in uncertain environments, where agents must adapt to changing task priorities and resource availability. We propose a novel multi-agent reinforcement learning framework, 'AdaptMAL', which leverages decentralized exploration and communication to optimize task allocation. Our approach integrates a probabilistic task model with a deep reinforcement learning algorithm, enabling agents to learn effective allocation strategies in real-time. Experimental results on a simulated disaster response scenario demonstrate that AdaptMAL outperforms traditional optimization methods, achieving higher task completion rates and reduced latency in the face of uncertainty.