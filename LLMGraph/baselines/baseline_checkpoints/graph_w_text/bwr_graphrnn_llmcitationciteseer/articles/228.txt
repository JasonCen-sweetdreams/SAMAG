Graph neural networks (GNNs) have achieved state-of-the-art performance in various graph-based tasks, but their robustness against adversarial attacks remains largely unexplored. This paper presents a comprehensive evaluation of existing robustness measures for GNNs, including graph pruning, adversarial training, and input preprocessing. We propose a novel attack method, 'Graph-FGSM', which leverages the fast gradient sign method to perturb graph structures and node features. Our experiments on several benchmark datasets demonstrate that Graph-FGSM outperforms existing attacks in terms of attack success rate and transferability across different GNN architectures.