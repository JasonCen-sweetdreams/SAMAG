Multi-agent systems have become increasingly prevalent in AI applications, but understanding the decision-making processes of these systems remains a significant challenge. This paper introduces a novel Hierarchical Attention Network (HAN) architecture that enables explainable multi-agent cooperation. Our approach leverages hierarchical attention mechanisms to identify relevant interactions between agents and their environment, providing insights into the decision-making process. Experimental results on a series of complex multi-agent tasks demonstrate the efficacy of HAN in improving cooperation and interpretability. We further provide a theoretical analysis of the proposed architecture, establishing connections to game theory and social network analysis.