Traditional ranking models for document retrieval rely on static features and scoring functions, which may not adapt well to changing user preferences and query distributions. This paper proposes an adaptive re-ranking framework that leverages reinforcement learning (RL) to optimize the ranking policy based on user interactions. Our approach, called 'AdaRank', uses a deep Q-network to learn a ranking policy that balances exploration and exploitation of relevant documents. Experimental results on the TREC-8 dataset demonstrate that AdaRank outperforms state-of-the-art ranking models in terms of mean average precision and normalized discounted cumulative gain, while reducing computational overhead.