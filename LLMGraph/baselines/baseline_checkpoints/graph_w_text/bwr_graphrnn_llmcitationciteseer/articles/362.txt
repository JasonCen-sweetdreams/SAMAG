Emotion recognition from multi-modal inputs, such as speech, text, and vision, is a crucial task in affective computing. We propose a novel attention mechanism, 'ModalGaze', which selectively focuses on the most informative modalities and time-steps to improve emotion recognition accuracy. Our approach leverages a hierarchical graph neural network to model cross-modal relationships and incorporates a uncertainty-based attention module to weigh the contributions of each modality. Experimental results on two benchmark datasets demonstrate that ModalGaze outperforms state-of-the-art methods in terms of recognition accuracy and computational efficiency.