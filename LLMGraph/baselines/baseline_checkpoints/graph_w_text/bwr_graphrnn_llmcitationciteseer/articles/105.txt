Task-oriented dialogue systems struggle to balance efficiency and accuracy, often relying on handcrafted rules or shallow reinforcement learning. This paper proposes a hierarchical reinforcement learning framework, 'HRL-Dialog', which leverages a novel two-level task decomposition mechanism. The high-level policy focuses on task progression, while the low-level policy handles turn-level dialogue management. We introduce a curriculum-based training approach, where the low-level policy is initially trained on a simulated user dataset and gradually adapted to real user interactions. Experimental results on a banking domain dataset demonstrate that HRL-Dialog achieves improved task completion rates, reduced dialogue turns, and enhanced user satisfaction compared to state-of-the-art methods.