Virtual reality (VR) systems often rely on explicit user input, such as button presses or gesture recognition, which can be cumbersome and limiting. This paper presents a novel gaze-based interaction technique for VR using deep reinforcement learning. Our approach, 'GazeRL', leverages a neural network to predict user intentions from eye-tracking data and generates corresponding actions in the virtual environment. We evaluate GazeRL in a series of user studies, demonstrating improved interaction efficiency and reduced cognitive load compared to traditional input methods. Our results have implications for enhancing VR usability and accessibility in various domains, including gaming, education, and healthcare.