Emotion recognition from multi-modal inputs (e.g., speech, text, and vision) is a challenging task due to the inherent complexity of human emotions and the need for interpretability. We propose a hierarchical attention network (HAN) that learns to selectively focus on relevant modalities and features, providing explainable emotion recognition. Our HAN consists of three stages: modality attention, feature attention, and emotion prediction. Experimental results on the CMU-MOSEI dataset demonstrate that our approach outperforms state-of-the-art methods in terms of both accuracy and explainability, providing insights into the emotional cues that drive the recognition process.