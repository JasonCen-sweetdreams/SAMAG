Traditional knowledge graph embedding methods struggle to capture complex relationships and contextual dependencies in multi-modal dialog systems. This paper proposes a novel hierarchical attention-based approach, 'HAT-KGE', which learns to selectively focus on relevant entities and relations within the graph. We introduce a multi-level attention mechanism that aggregates entity and relation embeddings across different modalities, enabling more effective and efficient dialog management. Experiments on a large-scale dialog dataset demonstrate significant improvements in response generation quality and dialog coherence compared to state-of-the-art baselines.