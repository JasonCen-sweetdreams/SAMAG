Traditional information retrieval (IR) systems rely on handcrafted features and linear ranking models, which struggle to capture complex document relationships. This paper proposes a novel hierarchical neural ranking model, 'HierRank', which integrates document embeddings, contextualized language models, and self-attention mechanisms. We demonstrate that HierRank outperforms state-of-the-art IR models on several benchmark datasets, achieving significant improvements in retrieval accuracy and efficiency. Our extensive experiments also reveal the importance of incorporating hierarchical structures and contextualized embeddings in neural IR models.