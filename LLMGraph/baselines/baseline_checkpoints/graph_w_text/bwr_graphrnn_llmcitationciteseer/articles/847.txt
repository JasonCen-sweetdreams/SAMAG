Cooperative task allocation in dynamic environments is a challenging problem in multi-agent systems. This paper introduces a novel approach to this problem using multi-agent reinforcement learning (MARL). We propose a decentralized MARL framework, where each agent learns to allocate tasks based on its local observations and communication with neighboring agents. Our approach uses a hierarchical graph neural network to model the agents' communication and a deep Q-network to learn the task allocation policy. Experimental results in a simulated search-and-rescue scenario demonstrate that our approach outperforms traditional methods in terms of task completion time and agent utility.