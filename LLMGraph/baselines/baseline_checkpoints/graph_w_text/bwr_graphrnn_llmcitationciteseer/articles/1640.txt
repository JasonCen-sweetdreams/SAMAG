Recent advances in transformer-based architectures have led to impressive performance gains in multi-task learning settings. However, these models remain vulnerable to adversarial attacks, which can significantly degrade their performance. This paper proposes a novel adversarial training framework, 'TransAdv', which leverages task-specific adversarial examples to improve the robustness of transformer-based multi-task models. We demonstrate that TransAdv achieves state-of-the-art results on a suite of benchmark datasets, while providing robustness guarantees against a range of adversarial attacks. Our approach has significant implications for real-world applications, where robustness is critical.