Virtual reality (VR) systems often struggle to provide an optimal user experience due to the complexity of visual stimuli and individual differences in visual attention. This paper presents a novel EyeTracking-based Adaptive Gaze Guidance (ETAGG) system that uses machine learning to predict and guide the user's gaze in real-time. ETAGG leverages a deep learning model to analyze the user's eye movements and adaptively adjusts the visual cues, reducing visual discomfort and improving task performance. Our user study with 30 participants demonstrates a significant reduction in visual fatigue and a 25% increase in task completion speed compared to state-of-the-art gaze-based systems.