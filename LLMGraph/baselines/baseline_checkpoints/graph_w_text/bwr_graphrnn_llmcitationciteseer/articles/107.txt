Ad-hoc retrieval often suffers from the vocabulary mismatch problem, where the query terms fail to match the relevant documents. Query expansion is a popular technique to address this issue, but existing methods rely on manual feature engineering and may not generalize well to diverse query types. This paper proposes a deep neural ranking model, 'NeuralQE', which learns to expand queries by ranking candidate terms based on their semantic relevance. We experiment with various neural architectures and find that a combination of BERT-based encoding and graph attention yields significant improvements in retrieval effectiveness. Our approach outperforms state-of-the-art query expansion methods on several benchmark datasets.