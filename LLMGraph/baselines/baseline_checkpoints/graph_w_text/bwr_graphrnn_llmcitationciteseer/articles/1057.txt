Ad-hoc search systems rely on query expansion to improve retrieval performance. However, existing methods often rely on hand-crafted features and struggle to capture complex semantic relationships between queries and documents. This paper proposes a neural query expansion framework, 'NeuroXPand', which leverages pre-trained language models to learn dense document embeddings. We introduce a novel attention mechanism that adapts to the query context, allowing the model to focus on relevant document aspects. Experimental results on the TREC-8 dataset demonstrate that NeuroXPand outperforms state-of-the-art baselines, achieving a 15% increase in mean average precision.