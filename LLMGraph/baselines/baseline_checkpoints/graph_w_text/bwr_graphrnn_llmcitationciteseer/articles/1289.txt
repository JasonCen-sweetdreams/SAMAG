This paper proposes a hybrid task-oriented dialogue management framework that leverages hierarchical reinforcement learning to improve dialogue efficiency and coherence. Our approach combines a high-level task manager with a low-level language generator, allowing the agent to adapt to changing user goals and preferences. We evaluate our approach using a large-scale dialogue dataset and demonstrate significant improvements in task success rate, dialogue length, and user satisfaction compared to state-of-the-art methods.