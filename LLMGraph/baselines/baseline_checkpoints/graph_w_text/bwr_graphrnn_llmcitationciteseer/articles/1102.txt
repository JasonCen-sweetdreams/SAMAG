Voice assistants have become ubiquitous, but users with speech disorders often face difficulties communicating with these systems. This paper presents a novel approach to personalized pronunciation adaptation, enabling voice assistants to better understand and respond to users with speech impairments. We conducted a user study with 30 participants and developed a machine learning-based model that adapts to individual pronunciation patterns. Results show significant improvements in recognition accuracy and user satisfaction. We discuss the implications of our findings for designing more inclusive and accessible voice assistants.