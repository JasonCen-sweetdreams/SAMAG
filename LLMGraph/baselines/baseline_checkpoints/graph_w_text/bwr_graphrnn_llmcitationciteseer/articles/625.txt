Emotion recognition from multi-modal data, such as text, audio, and video, is a challenging task due to the complexity of human emotions and the need to integrate information from diverse sources. This paper proposes a novel hierarchical attention network (HAN) for explainable emotion recognition. Our approach utilizes a stacked architecture with modality-specific attention mechanisms to selectively focus on relevant features from each modality. We also introduce an interpretable emotion representation scheme, enabling the model to provide insightful explanations for its predictions. Experimental results on three benchmark datasets demonstrate the effectiveness of our approach in achieving state-of-the-art performance and providing meaningful explanations for emotion recognition.