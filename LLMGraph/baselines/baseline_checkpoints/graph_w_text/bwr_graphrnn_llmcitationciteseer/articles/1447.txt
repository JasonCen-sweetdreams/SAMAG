Multi-hop question answering (QA) tasks require models to reason over multiple sentences or documents to answer a question. While recent advances in transformer-based models have improved performance, they often lack interpretability. We propose a novel hierarchical attention network (HAN) that incorporates attention mechanisms at multiple levels to selectively focus on relevant information. Our approach leverages a novel graph-based attention mechanism to model relationships between sentences and entities. Experimental results on the HotPotQA dataset demonstrate that our HAN model outperforms state-of-the-art models while providing interpretable explanations for its answers.