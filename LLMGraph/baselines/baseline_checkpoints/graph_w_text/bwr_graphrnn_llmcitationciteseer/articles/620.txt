Embodied conversational agents (ECAs) in virtual reality (VR) have the potential to revolutionize human-computer interaction. However, current ECAs often lack realistic nonverbal cues, leading to decreased user engagement and trust. This paper presents a novel multimodal interaction framework that incorporates gesture recognition, facial expression analysis, and natural language processing to create more empathetic and responsive ECAs. We evaluate our approach through a user study, demonstrating improved user experience and increased sense of presence in VR environments.