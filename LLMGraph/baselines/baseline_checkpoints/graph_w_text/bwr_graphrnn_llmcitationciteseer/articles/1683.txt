Deep learning models have achieved state-of-the-art performance in medical image classification, but their lack of transparency hinders trust in critical healthcare applications. This paper proposes a novel approach, 'MedXA', which leverages adversarial attacks to improve the explainability of medical image classification models. We develop a generative framework that creates perturbed images to simulate real-world variations, and use these to train an explainer model that provides robust feature importance scores. Experimental results on a large dataset of chest X-rays demonstrate that MedXA outperforms existing explainability methods in terms of fidelity and interpretability, while maintaining high classification accuracy.