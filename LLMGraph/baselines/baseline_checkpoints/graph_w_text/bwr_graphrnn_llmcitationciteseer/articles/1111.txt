Explainability is a crucial aspect of autonomous systems, where Reinforcement Learning (RL) agents need to make transparent decisions. We propose a Hierarchical Attention Network (HAN) framework that learns to focus on relevant state features and abstract representations, thereby providing insights into the decision-making process. Our method, called Explainable-HAN (E-HAN), consists of a hierarchical attention mechanism and a novel explainability module that generates human-readable explanations for the RL agent's actions. Experimental results on a real-world autonomous driving dataset demonstrate that E-HAN outperforms state-of-the-art RL methods in terms of both performance and explainability.