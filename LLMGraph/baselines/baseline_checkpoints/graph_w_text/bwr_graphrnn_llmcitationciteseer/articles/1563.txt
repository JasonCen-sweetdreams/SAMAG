Emotion recognition is a crucial aspect of human-robot interaction, but existing approaches often rely on a single modality (e.g., facial expressions or speech). This paper presents a novel hierarchical attention network (HAN) that integrates multi-modal inputs (vision, audio, and physiological signals) to recognize emotions in real-time. Our HAN architecture consists of modality-specific attention modules and a hierarchical fusion layer, enabling the model to selectively focus on relevant modalities and capture complex emotional patterns. Experimental results on a large-scale human-robot interaction dataset demonstrate improved emotion recognition accuracy and robustness compared to state-of-the-art uni-modal and early-fusion approaches.