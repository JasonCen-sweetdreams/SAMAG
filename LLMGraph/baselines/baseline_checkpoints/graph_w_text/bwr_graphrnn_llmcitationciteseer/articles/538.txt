Dialogue systems have seen significant advances with the advent of deep learning, but their opacity hinders widespread adoption. This paper proposes a hierarchical attention network (HAN) architecture for multi-turn dialogue systems, which provides inherent explainability through attention weights. Our HAN framework consists of two stages: (1) utterance-level attention to identify relevant context, and (2) dialogue-level attention to model conversation flow. We evaluate our approach on four benchmark datasets, demonstrating improved response accuracy and fluency while providing insights into the decision-making process through visualized attention patterns.