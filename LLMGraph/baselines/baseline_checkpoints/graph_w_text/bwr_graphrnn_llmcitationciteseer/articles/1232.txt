Coordinated exploration is a critical challenge in multi-agent systems, where agents need to balance individual learning with collective progress. This paper presents a novel graph-based reinforcement learning approach, 'GraphCoExp', that enables agents to share knowledge and adapt to changing environments. By modeling agent interactions as a graph, we develop a decentralized exploration strategy that leverages graph-based clustering and community detection to identify regions of high uncertainty. Experiments on a range of multi-agent environments demonstrate that GraphCoExp outperforms state-of-the-art methods in terms of exploration efficiency and collective reward.