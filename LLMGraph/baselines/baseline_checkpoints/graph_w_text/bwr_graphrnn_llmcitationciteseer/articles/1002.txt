Graph neural networks (GNNs) have achieved state-of-the-art performance in various graph-based applications, but their lack of transparency hinders trust and adoption in high-stakes domains. This paper presents a novel approach to explainability in GNNs, dubbed 'AttentiveImportance', which leverages attention mechanisms to identify salient features contributing to node representations. We demonstrate that our method outperforms existing feature importance techniques in terms of Faithfulness, Monotonicity, and Sparsity, while being computationally efficient. Experimental results on molecular graphs and social networks demonstrate the effectiveness of AttentiveImportance in providing actionable insights into GNN decision-making processes.