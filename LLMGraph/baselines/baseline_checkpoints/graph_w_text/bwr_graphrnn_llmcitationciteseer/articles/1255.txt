Conversational agents increasingly rely on sentiment analysis to provide personalized responses. However, existing approaches struggle to effectively process multimodal input (e.g., text, speech, and vision) and capture nuanced sentiment shifts. We propose a novel Hierarchical Attention Network (HAN) architecture that integrates modality-specific attention mechanisms to learn joint representations of multimodal input. Our approach achieves state-of-the-art performance on the CMU-MOSI benchmark, outperforming previous multimodal fusion techniques by 12.5% in F1-score. We also demonstrate the effectiveness of HAN in real-world conversational agent applications, such as customer service chatbots.