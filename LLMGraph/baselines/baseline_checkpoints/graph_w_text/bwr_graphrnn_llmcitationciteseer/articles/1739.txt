Virtual reality (VR) technology has the potential to revolutionize various fields, but users with motor disabilities often face significant barriers to access. This paper presents EyeGazePath, a novel gaze-based interaction technique that leverages machine learning to predict user intentions and enable seamless navigation in VR environments. Our approach addresses the limitations of existing gaze-based systems by incorporating a hierarchical attention mechanism that adaptively weights multiple gaze features. A user study with 20 participants demonstrates that EyeGazePath outperforms state-of-the-art gaze-based methods in terms of accuracy, speed, and user satisfaction.