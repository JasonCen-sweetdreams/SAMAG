Coordinating multi-agent systems to perform complex tasks is a challenging problem, particularly in dynamic environments. This paper proposes a novel approach that leverages deep reinforcement learning to adaptively allocate tasks among agents. Our method, dubbed 'MA-RL', uses a decentralized actor-critic architecture to learn coordinated policies that maximize task efficiency and minimize conflicts. We evaluate MA-RL on a simulated disaster response scenario and demonstrate significant improvements in task completion time and agent utility compared to traditional planning-based approaches.