Egocentric videos offer a unique opportunity for gaze-based authentication, which can provide an additional layer of security for wearable devices. This paper presents a deep learning framework that leverages spatiotemporal features from egocentric videos to authenticate users based on their gaze patterns. Our approach uses a convolutional neural network to extract features from the videos and a recurrent neural network to model the temporal dependencies of the gaze data. Experimental results on a dataset of 50 users show that our approach achieves an accuracy of 95.6% and outperforms existing methods by at least 10%. We also demonstrate the robustness of our approach against various types of attacks.