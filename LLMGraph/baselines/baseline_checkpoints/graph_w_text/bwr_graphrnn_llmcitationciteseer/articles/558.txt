Virtual reality (VR) systems often struggle to provide effective user feedback, leading to user frustration and disorientation. We present 'GazeGuide', a gaze-based adaptive feedback framework that leverages eye-tracking data to dynamically adjust feedback strategies in real-time. Our approach incorporates machine learning models to predict user attention and intent, and generates personalized feedback cues that adapt to individual user behavior. A comprehensive user study demonstrates that GazeGuide significantly improves user performance, reduces cognitive load, and enhances overall user experience in VR environments.