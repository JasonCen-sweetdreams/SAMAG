Coordinating multiple agents in complex, dynamic environments is a challenging problem. We propose a decentralized approach using Partially Observable Markov Decision Processes (POMDPs) to model agent decision-making. Our method, 'Dec-POMDP-Co', enables agents to learn coordination policies in a decentralized manner, without requiring a centralized controller or complete knowledge of the environment. We evaluate Dec-POMDP-Co in a simulated disaster response scenario and demonstrate improved coordination and task completion rates compared to traditional, centralized approaches.