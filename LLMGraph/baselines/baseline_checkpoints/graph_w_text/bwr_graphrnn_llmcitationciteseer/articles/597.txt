Open-domain question answering (ODQA) involves retrieving relevant passages from a large corpus to answer a given question. This paper proposes a novel hierarchical neural ranking model, 'HierRank', which leverages both coarse-grained document-level and fine-grained passage-level ranking to improve retrieval efficiency. We introduce a dual-encoder architecture that jointly optimizes document and passage representations, enabling the model to capture both global and local context. Experimental results on the Natural Questions dataset demonstrate that HierRank outperforms state-of-the-art retrieval models while reducing computational overhead by up to 30%.