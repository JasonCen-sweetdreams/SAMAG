Multimodal sentiment analysis has garnered significant attention in recent years, but existing approaches often struggle to effectively fuse and represent heterogeneous data from different modalities. This paper proposes a novel deep hierarchical representation learning framework, termed 'MHRL', which leverages a multimodal graph neural network to learn hierarchical representations from text, image, and speech inputs. We demonstrate that MHRL outperforms state-of-the-art methods on three benchmark datasets, achieving significant improvements in sentiment classification accuracy and correlation with human annotations. Our approach has promising implications for real-world applications, such as sentiment analysis in social media and customer service platforms.