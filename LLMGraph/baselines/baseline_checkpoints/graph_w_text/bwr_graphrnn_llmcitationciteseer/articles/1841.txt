Emotion recognition in conversational dialogue is a challenging task due to the complexity of human emotions and the variability of multi-modal cues. This paper presents a novel hierarchical attention network (HAN) that leverages linguistic, acoustic, and visual features to recognize emotions in conversational dialogue. Our HAN model consists of two stages: a feature attention stage that weights the importance of each modality, and a conversation attention stage that models the temporal dynamics of emotions within a conversation. Experimental results on the IEMOCAP and MELD datasets demonstrate that our approach outperforms state-of-the-art methods in recognizing emotions from multi-modal inputs.