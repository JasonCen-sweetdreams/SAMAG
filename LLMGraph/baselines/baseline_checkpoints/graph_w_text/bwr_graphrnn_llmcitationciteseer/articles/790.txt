Explainable AI (XAI) has become crucial for trustworthy decision-making in high-stakes applications. This paper presents a novel multimodal fusion framework, 'HAFEX', which leverages hierarchical attention mechanisms to integrate and weigh diverse input modalities (vision, language, and audio). Our approach generates interpretable importance scores for each modality, facilitating insights into the AI decision-making process. We evaluate HAFEX on a multimodal medical diagnosis dataset, demonstrating improved performance and explainability compared to state-of-the-art fusion methods.