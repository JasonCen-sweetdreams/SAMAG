Virtual reality (VR) systems often rely on cumbersome controllers or voice commands, limiting immersion and accessibility. We present a novel approach to gaze-based interaction in VR using deep learning. Our method, 'GazeNet', employs a convolutional neural network (CNN) to detect and classify gaze patterns in real-time, enabling users to interact with virtual objects through gaze alone. We evaluate GazeNet in a VR environment and demonstrate its accuracy and robustness in various scenarios, including object selection, navigation, and manipulation. The results show promising implications for enhancing user experience and accessibility in VR applications.