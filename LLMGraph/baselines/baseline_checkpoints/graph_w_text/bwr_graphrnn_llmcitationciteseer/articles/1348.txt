Graph neural networks (GNNs) have shown remarkable performance in node classification tasks, but their scalability remains a major concern. This paper proposes Hierarchical Graph Attention Networks (HGAT), a novel architecture that leverages hierarchical representations to reduce the computational complexity of GNNs. HGAT combines multi-scale graph attention with a hierarchical pooling mechanism, allowing it to effectively capture long-range dependencies and node relationships. We demonstrate the efficacy of HGAT on several large-scale graph datasets, achieving state-of-the-art performance while reducing computational costs by up to 50%.