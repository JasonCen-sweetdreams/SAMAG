Explainability in natural language processing (NLP) remains a significant challenge. This paper proposes a novel hierarchical attention network (HAN) that provides interpretable explanations for NLP tasks. Our architecture combines local and global attention mechanisms to identify relevant input features and context-dependent relationships. We evaluate HAN on sentiment analysis and question-answering tasks, demonstrating improved performance and transparency compared to existing state-of-the-art models. Our approach has significant implications for real-world applications, such as customer service chatbots and language translation systems.