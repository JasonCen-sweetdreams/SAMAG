Traditional information retrieval systems rely on hand-crafted features and weighting schemes, which often fail to capture complex document relationships. This paper proposes a novel deep neural network architecture, 'DocRank', which leverages self-attention mechanisms and contextualized embeddings to learn effective document representations. We demonstrate that DocRank outperforms state-of-the-art retrieval models on several benchmark datasets, including TREC and ClueWeb. Our approach also enables efficient querying and indexing, making it suitable for large-scale web search applications.