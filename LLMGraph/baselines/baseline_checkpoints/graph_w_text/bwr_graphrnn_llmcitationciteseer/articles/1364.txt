Embodied agents, such as robots and virtual assistants, require effective multi-modal interaction to understand and respond to human commands. This paper proposes a novel graph-based attention mechanism, 'GraphAttention', which integrates visual, auditory, and linguistic cues to improve agent understanding. Our approach models each modality as a node in a graph, allowing the agent to selectively focus on relevant inputs and capture complex contextual relationships. We evaluate GraphAttention on a benchmark dataset of human-robot interactions, demonstrating significant improvements in task completion rates and user satisfaction.