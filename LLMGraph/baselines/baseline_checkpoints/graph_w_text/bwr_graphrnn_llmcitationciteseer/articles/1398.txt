Dialogue management is a crucial component of conversational AI systems, requiring the integration of multiple tasks such as intent detection, slot filling, and response generation. This paper presents a novel hierarchical graph attention network (HGAT) architecture that leverages graph-based representations to model complex dialogue structures. Our approach hierarchically aggregates node features and applies attention mechanisms to selectively focus on relevant dialogue context, enabling efficient multi-task learning. Experimental results on the DSTC2 and MultiWOZ datasets demonstrate that our HGAT model outperforms state-of-the-art dialogue management systems, achieving higher task accuracy and improved response quality.