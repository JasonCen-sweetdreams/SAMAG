Current information retrieval systems struggle to effectively retrieve relevant results when dealing with multi-modal queries, such as those combining text and images. This paper proposes a novel deep learning-based approach to query expansion, dubbed 'QE-Multi', which leverages a transformer-based architecture to jointly model textual and visual features. By generating context-aware expansions, QE-Multi improves retrieval performance on a benchmark dataset by 15.2% compared to state-of-the-art methods. Furthermore, we demonstrate the effectiveness of QE-Multi in a real-world application, retrieving relevant product images from an e-commerce platform.