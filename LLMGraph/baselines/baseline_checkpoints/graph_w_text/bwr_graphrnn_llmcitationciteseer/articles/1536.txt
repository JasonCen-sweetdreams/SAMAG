Coordinating the behavior of multiple agents in complex, partially observable environments remains a challenging problem in artificial intelligence. This paper introduces a novel approach that leverages deep reinforcement learning to enable effective exploration and coordination among agents. Our method, dubbed 'MA-DEEP', employs a decentralized architecture where each agent learns to balance individual exploration and coordination with its peers. We demonstrate the efficacy of MA-DEEP in several benchmark domains, including robotic soccer and autonomous driving, and show that it outperforms existing state-of-the-art methods in terms of cumulative reward and coordination efficiency.