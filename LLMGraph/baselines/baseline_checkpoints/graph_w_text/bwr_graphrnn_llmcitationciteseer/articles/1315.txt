Motion planning for autonomous vehicles requires efficient and adaptive decision-making in complex environments. This paper presents a novel hierarchical reinforcement learning (HRL) framework that integrates high-level decision-making with low-level motion control. Our approach leverages a hierarchical state representation to decompose the motion planning problem into manageable sub-tasks, enabling the agent to learn a rich set of skills and adapt to diverse scenarios. Experimental results demonstrate improved navigation efficiency and robustness in realistic simulations, outperforming state-of-the-art motion planning methods.