Multimodal information retrieval (MMIR) has gained significant attention in recent years, but the challenge of effectively representing and querying heterogeneous data remains. This paper proposes a novel approach to query expansion using deep neural networks, which leverages the strengths of both visual and textual features. We introduce a multimodal autoencoder that learns to project query and document representations into a shared latent space, enabling efficient and effective query expansion. Experimental results on a large-scale MMIR dataset demonstrate significant improvements in retrieval performance and efficiency compared to traditional methods.