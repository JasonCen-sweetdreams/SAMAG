In virtual reality (VR) environments, understanding user intent is crucial for designing intuitive and efficient interfaces. This paper presents EyeGazeFlow, a novel multimodal fusion framework that combines eye gaze, head movements, and controller inputs to predict user intentions in VR. Our approach leverages a graph-based neural network to model the relationships between these modalities and outputs a probabilistic distribution over possible user intents. We evaluate EyeGazeFlow on a dataset of 30 users performing various tasks in a VR shopping scenario, achieving an average accuracy of 85.2% in intent prediction and outperforming state-of-the-art baselines.