Question answering (QA) over knowledge graphs (KGs) has garnered significant attention in recent years. However, existing methods often rely on shallow representations of KGs, which may not capture complex relationships between entities. We propose a novel framework, 'GATE', that leverages graph attention networks to learn dense entity and relation embeddings. Our experiments on the popular Wikidata5M dataset demonstrate that GATE outperforms state-of-the-art QA models, achieving a 12% improvement in F1-score. Furthermore, we show that GATE can be easily adapted to different QA tasks with minimal fine-tuning.