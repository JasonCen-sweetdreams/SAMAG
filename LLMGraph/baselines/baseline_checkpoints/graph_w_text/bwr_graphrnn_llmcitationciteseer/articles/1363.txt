This paper proposes a novel multimodal emotion recognition approach, integrating computer vision, speech processing, and physiological signals to detect users' emotional states in human-computer interaction. Our system, called EmoSense, leverages a deep learning framework to fuse features from facial expressions, speech tone, and electrodermal activity. We conducted a user study with 50 participants, demonstrating that EmoSense outperforms existing unimodal approaches in recognizing emotions, with an average accuracy of 85.2%. Our findings have implications for designing more empathetic and personalized human-computer interfaces.