Emotion recognition in conversational dialogue is a challenging task due to the complexity of human emotions and the scarcity of annotated multimodal datasets. This paper presents a novel hierarchical attention network (HAN) that jointly models acoustic, lexical, and visual features to recognize emotions in dialogue. Our HAN architecture consists of three stacked attention layers, each capturing different levels of abstraction in the input features. We evaluate our approach on the IEMOCAP dataset and demonstrate significant improvements over state-of-the-art multimodal fusion models. Our results show that the proposed HAN model can effectively capture subtle emotional cues and outperform human annotators in certain scenarios.