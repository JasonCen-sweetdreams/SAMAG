Error detection in multimodal interfaces is crucial for improving user experience. This paper proposes a novel approach, 'GazeErrorNet', which leverages convolutional neural networks (CNNs) to detect errors based on users' gaze patterns. We collect a dataset of gaze and user input data from a multimodal interface and train a CNN to classify errors. Our results show that GazeErrorNet achieves an accuracy of 92.5% in detecting errors, outperforming traditional rule-based approaches. We also explore the impact of gaze-based error detection on user satisfaction and interface usability.