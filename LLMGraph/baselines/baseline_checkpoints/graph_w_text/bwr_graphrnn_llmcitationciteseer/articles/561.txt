Question answering over knowledge graphs (KGs) has seen significant progress with the advent of graph neural networks. However, existing methods often struggle to model complex relationships and scale to large KGs. This paper introduces HyperQA, a novel framework that leverages hyperbolic attention mechanisms to efficiently embed KGs in a learnable, hierarchical space. We demonstrate that HyperQA outperforms state-of-the-art methods on several benchmark datasets, achieving improved answer accuracy and reduced computational overhead. Furthermore, we provide theoretical analyses and ablation studies to justify the effectiveness of our approach.