In multi-agent systems, efficient task allocation is critical for achieving collective goals. We propose a hierarchical reinforcement learning framework, 'HRL-TA', which combines a high-level planning module with low-level execution agents. The planning module learns to allocate tasks based on agent capabilities and task dependencies, while the execution agents learn to optimize their actions using deep Q-networks. Experimental results on a simulated logistics scenario demonstrate that HRL-TA outperforms traditional centralized planning methods in terms of task completion time and resource utilization.