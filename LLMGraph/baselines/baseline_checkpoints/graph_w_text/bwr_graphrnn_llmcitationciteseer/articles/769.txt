Emotion recognition is a crucial aspect of human-computer interaction, but existing approaches struggle to effectively integrate and process multi-modal data. This paper introduces a novel Hierarchical Attention Graph Neural Network (HAGNN) architecture that leverages graph-based representations to model complex relationships between facial expressions, speech features, and textual inputs. Our approach hierarchically aggregates attention weights across modalities, enabling the model to selectively focus on salient features for improved emotion recognition. Experimental results on the recently released Multi-Modal Emotion Dataset (MMED) demonstrate the superiority of HAGNN over state-of-the-art methods, achieving an average F1-score improvement of 12.5% across six emotions.