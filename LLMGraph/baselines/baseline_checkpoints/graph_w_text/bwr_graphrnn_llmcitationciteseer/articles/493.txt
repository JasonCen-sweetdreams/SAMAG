Visual question answering (VQA) models have made significant progress, but their lack of interpretability hinders their adoption in real-world applications. This paper proposes a novel Hierarchical Graph Attention Network (HGAT) that leverages multi-scale graph attention to selectively focus on relevant regions of the image and question. Our approach enables the generation of interpretable attention maps, providing insights into the decision-making process. We demonstrate the effectiveness of HGAT on the VQA 2.0 dataset, achieving state-of-the-art performance while offering unprecedented interpretability.