Deep reinforcement learning (DRL) has achieved remarkable success in various domains, but the lack of interpretability hinders its widespread adoption. This paper addresses the explainability challenge in DRL by introducing a novel contrastive regularization technique, dubbed 'ExploCE'. Our approach encourages the policy network to learn a compact and informative representation of the state-action space, which enables the generation of meaningful explanations for the agent's decisions. We demonstrate the effectiveness of ExploCE on a range of Atari games, showcasing improved transparency and performance compared to existing explanation methods.