Distributed relational databases have become increasingly prevalent in modern data centers, but optimizing query execution in these systems remains a significant challenge. This paper presents PARAGON, a novel parallel query optimization framework that leverages both statistical and machine learning-based techniques to identify optimal query plans. PARAGON integrates a cost-based optimizer with a deep reinforcement learning module that learns to predict query execution times and adapt to changing workload characteristics. Experimental results on a 16-node cluster demonstrate that PARAGON outperforms state-of-the-art optimizers by up to 3.5x in terms of query execution time and improves system throughput by 2.2x.