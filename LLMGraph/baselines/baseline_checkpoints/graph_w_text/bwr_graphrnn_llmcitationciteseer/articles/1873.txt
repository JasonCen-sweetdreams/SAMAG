Virtual reality (VR) technology has the potential to revolutionize various aspects of our lives, but individuals with disabilities often face barriers to adopting these systems. This paper presents a novel gaze-based interface that enables users with mobility impairments to interact with VR environments more easily. Our approach leverages convolutional neural networks to accurately detect and classify gaze patterns, and integrates them with a VR system to provide intuitive control. We evaluate our method through a user study with participants with and without disabilities, demonstrating improved accessibility and usability in VR applications.