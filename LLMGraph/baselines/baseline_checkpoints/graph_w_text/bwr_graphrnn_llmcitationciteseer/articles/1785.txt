Voice assistants have become ubiquitous, but their accessibility limitations persist, excluding individuals with disabilities. This paper presents a multimodal framework, 'IncluVA', which integrates computer vision, natural language processing, and human-computer interaction principles to create a more inclusive voice assistant. We propose a novel gesture recognition system that enables users to interact with the assistant using sign language, and a personalized error correction mechanism that adapts to individual users' speech patterns. Our user study with participants with disabilities demonstrates significant improvements in accessibility, usability, and user satisfaction.