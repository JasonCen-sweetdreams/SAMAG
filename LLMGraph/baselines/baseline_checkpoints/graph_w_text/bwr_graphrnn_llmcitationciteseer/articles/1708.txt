Deep neural networks are vulnerable to adversarial attacks, which can compromise their reliability in critical applications. Detecting these attacks is essential, but existing methods are often limited by their specificity to particular models or datasets. This paper proposes a meta-learning approach, 'MetaGuard', which learns to detect adversarial attacks across different models and datasets. By leveraging a few-shot learning paradigm, MetaGuard can adapt to new attack patterns and generalize to unseen models. Our experiments demonstrate the effectiveness of MetaGuard in detecting various types of attacks, including those crafted using state-of-the-art methods.