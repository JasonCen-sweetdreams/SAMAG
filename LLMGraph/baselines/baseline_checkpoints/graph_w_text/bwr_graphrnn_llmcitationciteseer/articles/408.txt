Emotion recognition from multimodal inputs, such as speech, text, and vision, is a challenging task due to the complexity of human emotions and the variability of modalities. This paper proposes a novel Hierarchical Attention Network (HAN) that integrates audio, textual, and visual features to recognize emotions. Our HAN model uses a hierarchical attention mechanism to selectively focus on relevant modalities and features, and a fusion layer to combine the modality-specific representations. Experiments on the IEMOCAP dataset show that our approach outperforms state-of-the-art methods, achieving an accuracy of 84.2% and an F1-score of 83.5% for emotion recognition.